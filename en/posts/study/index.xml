<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Study on Gary&#39;s House</title>
    <link>https://garyforreal.me/en/posts/study/</link>
    <description>Recent content in Study on Gary&#39;s House</description>
    <generator>Hugo -- 0.141.0</generator>
    <language>en</language>
    <lastBuildDate>Fri, 05 Dec 2025 15:29:10 +0000</lastBuildDate>
    <atom:link href="https://garyforreal.me/en/posts/study/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Weekly Paper Notes - 2025-12-05</title>
      <link>https://garyforreal.me/en/posts/study/paper-2025-12-05-weekly/</link>
      <pubDate>Fri, 05 Dec 2025 15:29:10 +0000</pubDate>
      <guid>https://garyforreal.me/en/posts/study/paper-2025-12-05-weekly/</guid>
      <description>&lt;h1 id=&#34;weekly-paper-notes&#34;&gt;Weekly Paper Notes&lt;/h1&gt;
&lt;h2 id=&#34;-multilingual&#34;&gt;üîç multilingual&lt;/h2&gt;
&lt;h3 id=&#34;llms-know-more-than-words-a-genre-study-with-syntax-metaphor--phoneticshttpsarxivorgabs251204957v1&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2512.04957v1&#34;&gt;LLMs Know More Than Words: A Genre Study with Syntax, Metaphor &amp;amp; Phonetics&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weiye Shi, Zhaowei Zhang, Shaoheng Yan, Yaodong Yang
&lt;strong&gt;Venue:&lt;/strong&gt; arXiv (2025)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Weekly Paper Notes - 2025-12-04</title>
      <link>https://garyforreal.me/en/posts/study/paper-2025-12-04-weekly/</link>
      <pubDate>Thu, 04 Dec 2025 08:53:15 +0000</pubDate>
      <guid>https://garyforreal.me/en/posts/study/paper-2025-12-04-weekly/</guid>
      <description>&lt;h1 id=&#34;weekly-paper-notes&#34;&gt;Weekly Paper Notes&lt;/h1&gt;
&lt;h2 id=&#34;-cross-lingual&#34;&gt;üîç cross-lingual&lt;/h2&gt;
&lt;h3 id=&#34;kda-knowledge-distillation-adapter-for-cross-lingual-transferhttpsaclanthologyorg2025inlg-main8&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2025.inlg-main.8/&#34;&gt;KDA: Knowledge Distillation Adapter for Cross-Lingual Transfer&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt;
&lt;strong&gt;Venue:&lt;/strong&gt;  (Tue,)&lt;/p&gt;
&lt;p&gt;Ta-Bao Nguyen, Nguyen-Phuong Phan, Tung Le and Huy Tien Nguyen in Proceedings of the 18th International Natural Language Generation Conference&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-code-switching&#34;&gt;üîç code-switching&lt;/h2&gt;
&lt;h3 id=&#34;can-code-switched-texts-activate-a-knowledge-switch-in-llms-a-case-study-on-english-korean-code-switchinghttpsaclanthologyorg2025findings-emnlp1215&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2025.findings-emnlp.1215/&#34;&gt;Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case Study on English-Korean Code-Switching&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt;
&lt;strong&gt;Venue:&lt;/strong&gt;  (Fri,)&lt;/p&gt;
&lt;p&gt;Seoyeon Kim, Huiseo Kim, Chanjun Park, Jinyoung Yeo and Dongha Lee in Findings of the Association for Computational Linguistics: EMNLP 2025&lt;/p&gt;</description>
    </item>
    <item>
      <title>DP_Question</title>
      <link>https://garyforreal.me/en/posts/study/dp_question/</link>
      <pubDate>Sun, 29 Sep 2024 18:20:59 +0900</pubDate>
      <guid>https://garyforreal.me/en/posts/study/dp_question/</guid>
      <description>&lt;h1 id=&#34;preface&#34;&gt;Preface&lt;/h1&gt;
&lt;p&gt;Because I always hit a wall with Dynamic Programming problems,  I would like to summarize the solution to Dynamic Programming problems(DP).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Basic solution to DP problemsÔºö&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Initial State&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;State Transition Equation&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;1--linear-dp&#34;&gt;1.  Linear DP&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;1. 1 ExampleÔºöLeetcode 300.  Longest Increasing Subsequence&lt;/strong&gt; &lt;br&gt;
Given an integer array nums,  return the length of the longest strictly increasing subsequence.&lt;/p&gt;
&lt;p&gt;Example 1:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Input:&lt;/strong&gt; nums = [10, 9, 2, 5, 3, 7, 101, 18]&lt;br&gt;
&lt;strong&gt;Output:&lt;/strong&gt; 4&lt;br&gt;
&lt;strong&gt;Explanation:&lt;/strong&gt; The longest increasing subsequence is [2, 3, 7, 101],  therefore the length is 4.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Markdown Study Notes</title>
      <link>https://garyforreal.me/en/posts/study/markdown_study_notes/</link>
      <pubDate>Thu, 26 Sep 2024 17:31:12 +0900</pubDate>
      <guid>https://garyforreal.me/en/posts/study/markdown_study_notes/</guid>
      <description>&lt;h1 id=&#34;preface&#34;&gt;Preface&lt;/h1&gt;
&lt;p&gt;This article mainly summarizes the common syntax of Markdown, so that you can write Markdown files better by yourself.&lt;/p&gt;
&lt;h1 id=&#34;1-basic-grammer&#34;&gt;1. Basic Grammer&lt;/h1&gt;
&lt;h2 id=&#34;11-title&#34;&gt;1.1 Title&lt;/h2&gt;
&lt;p&gt;Use&lt;code&gt;#&lt;/code&gt; to create a title. The number of &lt;code&gt;#&lt;/code&gt; you use indicates the number of heading levels.&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;Markdown&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;Display&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;# title1&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;h1&gt;title1&lt;/h1&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;# title2&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;h2&gt;title2&lt;/h2&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;# title3&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;h3&gt;title3&lt;/h3&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;12-line-break&#34;&gt;1.2 Line break&lt;/h2&gt;
&lt;p&gt;Add &lt;code&gt;2 or more spaces&lt;/code&gt; after the text, or &lt;code&gt;add a blank line&lt;/code&gt; (press enter twice).&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
