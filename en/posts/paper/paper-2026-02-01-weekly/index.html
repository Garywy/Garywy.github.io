<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Weekly Paper Notes - 2026-02-01 | Gary's House</title>
<meta name=keywords content><meta name=description content="Weekly Paper Notes
üîç multilingual
JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion
Authors: Anthony Chen, Naomi Ken Korem, Tavi Halperin, Matan Ben Yosef, Urska Jelercic, Ofir Bibi, Or Patashnik, Daniel Cohen-Or
Venue: arXiv (2026)
Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines."><meta name=author content="Gary"><link rel=canonical href=https://garyforreal.me/en/posts/paper/paper-2026-02-01-weekly/><meta name=google-site-verification content="XYZabc"><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><link crossorigin=anonymous href=/assets/css/stylesheet.7e5251d8716d933fafcf3df70d7ecd02729661d6d232fc5dd0b8a85ef75e3409.css integrity="sha256-flJR2HFtkz+vzz33DX7NAnKWYdbSMvxd0LioXvdeNAk=" rel="preload stylesheet" as=style><link rel=icon href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=16x16 href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=32x32 href=https://garyforreal.me/img/Q.jpg><link rel=apple-touch-icon href=https://garyforreal.me/Q.jpg><link rel=mask-icon href=https://garyforreal.me/Q.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://garyforreal.me/zh/posts/paper/paper-2026-02-01-weekly/><link rel=alternate hreflang=en href=https://garyforreal.me/en/posts/paper/paper-2026-02-01-weekly/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@200..900&display=swap" rel=stylesheet><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous referrerpolicy=no-referrer></script><meta property="og:title" content="Weekly Paper Notes - 2026-02-01"><meta property="og:description" content="Weekly Paper Notes
üîç multilingual
JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion
Authors: Anthony Chen, Naomi Ken Korem, Tavi Halperin, Matan Ben Yosef, Urska Jelercic, Ofir Bibi, Or Patashnik, Daniel Cohen-Or
Venue: arXiv (2026)
Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines."><meta property="og:type" content="article"><meta property="og:url" content="https://garyforreal.me/en/posts/paper/paper-2026-02-01-weekly/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-01T15:30:48+00:00"><meta property="article:modified_time" content="2026-02-01T15:30:48+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Weekly Paper Notes - 2026-02-01"><meta name=twitter:description content="Weekly Paper Notes
üîç multilingual
JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion
Authors: Anthony Chen, Naomi Ken Korem, Tavi Halperin, Matan Ben Yosef, Urska Jelercic, Ofir Bibi, Or Patashnik, Daniel Cohen-Or
Venue: arXiv (2026)
Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://garyforreal.me/en/posts/"},{"@type":"ListItem","position":2,"name":"Paper","item":"https://garyforreal.me/en/posts/paper/"},{"@type":"ListItem","position":3,"name":"Weekly Paper Notes - 2026-02-01","item":"https://garyforreal.me/en/posts/paper/paper-2026-02-01-weekly/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Weekly Paper Notes - 2026-02-01","name":"Weekly Paper Notes - 2026-02-01","description":"Weekly Paper Notes üîç multilingual JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion Authors: Anthony Chen, Naomi Ken Korem, Tavi Halperin, Matan Ben Yosef, Urska Jelercic, Ofir Bibi, Or Patashnik, Daniel Cohen-Or Venue: arXiv (2026)\nAudio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.\n","keywords":[],"articleBody":"Weekly Paper Notes üîç multilingual JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion Authors: Anthony Chen, Naomi Ken Korem, Tavi Halperin, Matan Ben Yosef, Urska Jelercic, Ofir Bibi, Or Patashnik, Daniel Cohen-Or Venue: arXiv (2026)\nAudio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.\nüìÑ Download PDF\nSWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents Authors: Yifeng Ding, Lingming Zhang Venue: arXiv (2026)\nTest-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.\nüìÑ Download PDF\nMaking Foundation Models Probabilistic via Singular Value Ensembles Authors: Mehmet Ozgur Turkoglu, Dominik J. M√ºhlematter, Alexander Becker, Konrad Schindler, Helge Aasen Venue: arXiv (2026)\nFoundation models have become a dominant paradigm in machine learning, achieving remarkable performance across diverse tasks through large-scale pretraining. However, these models often yield overconfident, uncalibrated predictions. The standard approach to quantifying epistemic uncertainty, training an ensemble of independent models, incurs prohibitive computational costs that scale linearly with ensemble size, making it impractical for large foundation models. We propose Singular Value Ensemble (SVE), a parameter-efficient implicit ensemble method that builds on a simple, but powerful core assumption: namely, that the singular vectors of the weight matrices constitute meaningful subspaces of the model‚Äôs knowledge. Pretrained foundation models encode rich, transferable information in their weight matrices. If the singular vectors are indeed meaningful (orthogonal) ‚Äúknowledge directions‚Äù. To obtain a model ensemble, we modulate only how strongly each direction contributes to the output. Rather than learning entirely new parameters, we freeze the singular vectors and only train per-member singular values that rescale the contribution of each direction in that shared knowledge basis. Ensemble diversity emerges naturally as stochastic initialization and random sampling of mini-batches during joint training cause different members to converge to different combinations of the same underlying knowledge. SVE achieves uncertainty quantification comparable to explicit deep ensembles while increasing the parameter count of the base model by less than 1%, making principled uncertainty estimation accessible in resource-constrained settings. We validate SVE on NLP and vision tasks with various different backbones and show that it improves calibration while maintaining predictive accuracy.\nüìÑ Download PDF\nMasalBench: A Benchmark for Contextual and Cross-Cultural Understanding of Persian Proverbs in LLMs Authors: Ghazal Kalhor, Behnam Bahrak Venue: arXiv (2026)\nIn recent years, multilingual Large Language Models (LLMs) have become an inseparable part of daily life, making it crucial for them to master the rules of conversational language in order to communicate effectively with users. While previous work has evaluated LLMs‚Äô understanding of figurative language in high-resource languages, their performance in low-resource languages remains underexplored. In this paper, we introduce MasalBench, a comprehensive benchmark for assessing LLMs‚Äô contextual and cross-cultural understanding of Persian proverbs, which are a key component of conversation in this low-resource language. We evaluate eight state-of-the-art LLMs on MasalBench and find that they perform well in identifying Persian proverbs in context, achieving accuracies above 0.90. However, their performance drops considerably when tasked with identifying equivalent English proverbs, with the best model achieving 0.79 accuracy. Our findings highlight the limitations of current LLMs in cultural knowledge and analogical reasoning, and they provide a framework for assessing cross-cultural understanding in other low-resource languages. MasalBench is available at https://github.com/kalhorghazal/MasalBench.\nüìÑ Download PDF\nTidyVoice 2026 Challenge Evaluation Plan Authors: Aref Farhadipour, Jan Marquenie, Srikanth Madikeri, Teodora Vukovic, Volker Dellwo, Kathy Reid, Francis M. Tyers, Ingo Siegert, Eleanor Chodroff Venue: arXiv (2026)\nThe performance of speaker verification systems degrades significantly under language mismatch, a critical challenge exacerbated by the field‚Äôs reliance on English-centric data. To address this, we propose the TidyVoice Challenge for cross-lingual speaker verification. The challenge leverages the TidyVoiceX dataset from the novel TidyVoice benchmark, a large-scale, multilingual corpus derived from Mozilla Common Voice, and specifically curated to isolate the effect of language switching across approximately 40 languages. Participants will be tasked with building systems robust to this mismatch, with performance primarily evaluated using the Equal Error Rate on cross-language trials. By providing standardized data, open-source baselines, and a rigorous evaluation protocol, this challenge aims to drive research towards fairer, more inclusive, and language-independent speaker recognition technologies, directly aligning with the Interspeech 2026 theme, ‚ÄúSpeaking Together.‚Äù\nüìÑ Download PDF\nKID: Knowledge-Injected Dual-Head Learning for Knowledge-Grounded Harmful Meme Detection Authors: Yaocong Li, Leihan Zhang, Le Zhang, Qiang Yan Venue: arXiv (2026)\nInternet memes have become pervasive carriers of digital culture on social platforms. However, their heavy reliance on metaphors and sociocultural context also makes them subtle vehicles for harmful content, posing significant challenges for automated content moderation. Existing approaches primarily focus on intra-modal and inter-modal signal analysis, while the understanding of implicit toxicity often depends on background knowledge that is not explicitly present in the meme itself. To address this challenge, we propose KID, a Knowledge-Injected Dual-Head Learning framework for knowledge-grounded harmful meme detection. KID adopts a label-constrained distillation paradigm to decompose complex meme understanding into structured reasoning chains that explicitly link visual evidence, background knowledge, and classification labels. These chains guide the learning process by grounding external knowledge in meme-specific contexts. In addition, KID employs a dual-head architecture that jointly optimizes semantic generation and classification objectives, enabling aligned linguistic reasoning while maintaining stable decision boundaries. Extensive experiments on five multilingual datasets spanning English, Chinese, and low-resource Bengali demonstrate that KID achieves SOTA performance on both binary and multi-label harmful meme detection tasks, improving over previous best methods by 2.1%‚Äì19.7% across primary evaluation metrics. Ablation studies further confirm the effectiveness of knowledge injection and dual-head joint learning, highlighting their complementary contributions to robust and generalizable meme understanding. The code and data are available at https://github.com/PotatoDog1669/KID.\nüìÑ Download PDF\nDimStance: Multilingual Datasets for Dimensional Stance Analysis Authors: Jonas Becker, Liang-Chih Yu, Shamsuddeen Hassan Muhammad, Jan Philip Wahle, Terry Ruas, Idris Abdulmumin, Lung-Hao Lee, Wen-Ni Liu, Tzu-Mi Lin, Zhe-Yu Xu, Ying-Lung Lin, Jin Wang, Maryam Ibrahim Mukhtar, Bela Gipp, Saif M. Mohammed Venue: arXiv (2026)\nStance detection is an established task that classifies an author‚Äôs attitude toward a specific target into categories such as Favor, Neutral, and Against. Beyond categorical stance labels, we leverage a long-established affective science framework to model stance along real-valued dimensions of valence (negative-positive) and arousal (calm-active). This dimensional approach captures nuanced affective states underlying stance expressions, enabling fine-grained stance analysis. To this end, we introduce DimStance, the first dimensional stance resource with valence-arousal (VA) annotations. This resource comprises 11,746 target aspects in 7,365 texts across five languages (English, German, Chinese, Nigerian Pidgin, and Swahili) and two domains (politics and environmental protection). To facilitate the evaluation of stance VA prediction, we formulate the dimensional stance regression task, analyze cross-lingual VA patterns, and benchmark pretrained and large language models under regression and prompting settings. Results show competitive performance of fine-tuned LLM regressors, persistent challenges in low-resource languages, and limitations of token-based generation. DimStance provides a foundation for multilingual, emotion-aware, stance analysis and benchmarking.\nüìÑ Download PDF\nMiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without Forgetting Authors: Jing Xu, Minglin Wu, Xueyuan Chen, Xixin Wu, Helen Meng Venue: arXiv (2026)\nSelf-supervised learning (SSL) has greatly advanced speech representation learning, but multilingual SSL models remain constrained to languages encountered during pretraining. Retraining from scratch to incorporate new languages is computationally expensive, while sequential training without migitation strategies often leads to catastrophic forgetting. To address this, we propose MiLorE-SSL, a lightweight framework that combines LoRA modules with a soft mixture-of-experts (MoE) mechanism for efficient continual multilingual training. LoRA provides efficient low-rank adaptation, while soft MoE promotes flexible expert sharing across languages, reducing cross-lingual interference. To further mitigate forgetting, we introduce limited replay data from existing languages, avoiding reliance on large historical corpora. Experiments on ML-SUPERB demonstrate that MiLorE-SSL achieves strong performance in new languages and improves the ability in existing ones with only 2.14% trainable parameters.\nüìÑ Download PDF\nFFE-Hallu:Hallucinations in Fixed Figurative Expressions:Benchmark of Idioms and Proverbs in the Persian Language Authors: Faezeh Hosseini, Mohammadali Yousefzadeh, Yadollah Yaghoobzadeh Venue: arXiv (2026)\nFigurative language, particularly fixed figurative expressions (FFEs) such as idioms and proverbs, poses persistent challenges for large language models (LLMs). Unlike literal phrases, FFEs are culturally grounded, largely non-compositional, and conventionally fixed, making them especially vulnerable to figurative hallucination. We define figurative hallucination as the generation or endorsement of expressions that sound idiomatic and plausible but do not exist as authentic figurative expressions in the target language. We introduce FFEHallu, the first comprehensive benchmark for evaluating figurative hallucination in LLMs, with a focus on Persian, a linguistically rich yet underrepresented language. FFEHallu consists of 600 carefully curated instances spanning three complementary tasks: (i) FFE generation from meaning, (ii) detection of fabricated FFEs across four controlled construction categories, and (iii) FFE to FFE translation from English to Persian. Evaluating six state of the art multilingual LLMs, we find systematic weaknesses in figurative competence and cultural grounding. While models such as GPT4.1 demonstrate relatively strong performance in rejecting fabricated FFEs and retrieving authentic ones, most models struggle to reliably distinguish real expressions from high quality fabrications and frequently hallucinate during cross lingual translation. These findings reveal substantial gaps in current LLMs handling of figurative language and underscore the need for targeted benchmarks to assess and mitigate figurative hallucination.\nüìÑ Download PDF\nLinguaMap: Which Layers of LLMs Speak Your Language and How to Tune Them? Authors: J. Ben Tamo, Daniel Carlander-Reuterfelt, Jonathan Rubin, Dezhi Hong, Mingxian Wang, Oleg Poliannikov Venue: arXiv (2026)\nDespite multilingual pretraining, large language models often struggle with non-English tasks, particularly in language control, the ability to respond in the intended language. We identify and characterize two key failure modes: the multilingual transfer bottleneck (correct language, incorrect task response) and the language consistency bottleneck (correct task response, wrong language). To systematically surface these issues, we design a four-scenario evaluation protocol spanning MMLU, MGSM, and XQuAD benchmarks. To probe these issues with interpretability, we extend logit lens analysis to track language probabilities layer by layer and compute cross-lingual semantic similarity of hidden states. The results reveal a three-phase internal structure: early layers align inputs into a shared semantic space, middle layers perform task reasoning, and late layers drive language-specific generation. Guided by these insights, we introduce selective fine-tuning of only the final layers responsible for language control. On Qwen-3-32B and Bloom-7.1B, this method achieves over 98 percent language consistency across six languages while fine-tuning only 3-5 percent of parameters, without sacrificing task accuracy. Importantly, this result is nearly identical to that of full-scope fine-tuning (for example, above 98 percent language consistency for both methods across all prompt scenarios) but uses a fraction of the computational resources. To the best of our knowledge, this is the first approach to leverage layer-localization of language control for efficient multilingual adaptation.\nüìÑ Download PDF\nDynamic Multi-Expert Projectors with Stabilized Routing for Multilingual Speech Recognition Authors: Isha Pandey, Ashish Mittal, Vartul Bahuguna, Ganesh Ramakrishnan Venue: arXiv (2026)\nRecent advances in LLM-based ASR connect frozen speech encoders with Large Language Models (LLMs) via lightweight projectors. While effective in monolingual settings, a single projector struggles to capture the diverse acoustic-to-semantic mappings required for multilingual ASR. To address this, we propose SMEAR-MoE, a stabilized Mixture-of-Experts projector that ensures dense gradient flow to all experts, preventing expert collapse while enabling cross-lingual sharing. We systematically compare monolithic, static multi-projector, and dynamic MoE designs across four Indic languages (Hindi, Marathi, Tamil, Telugu). Our SMEAR-MoE achieves strong performance, delivering upto a 7.6% relative WER reduction over the single-projector baseline, while maintaining comparable runtime efficiency. Analysis of expert routing further shows linguistically meaningful specialization, with related languages sharing experts. These results demonstrate that stable multi-expert projectors are key to scalable and robust multilingual ASR.\nüìÑ Download PDF\nLLM-ForcedAligner: A Non-Autoregressive and Accurate LLM-Based Forced Aligner for Multilingual and Long-Form Speech Authors: Bingshen Mu, Xian Shi, Xiong Wang, Hexin Liu, Jin Xu, Lei Xie Venue: arXiv (2026)\nForced alignment (FA) predicts start and end timestamps for words or characters in speech, but existing methods are language-specific and prone to cumulative temporal shifts. The multilingual speech understanding and long-sequence processing abilities of speech large language models (SLLMs) make them promising for FA in multilingual, crosslingual, and long-form speech settings. However, directly applying the next-token prediction paradigm of SLLMs to FA results in hallucinations and slow inference. To bridge the gap, we propose LLM-ForcedAligner, reformulating FA as a slot-filling paradigm: timestamps are treated as discrete indices, and special timestamp tokens are inserted as slots into the transcript. Conditioned on the speech embeddings and the transcript with slots, the SLLM directly predicts the time indices at slots. During training, causal attention masking with non-shifted input and label sequences allows each slot to predict its own timestamp index based on itself and preceding context, with loss computed only at slot positions. Dynamic slot insertion enables FA at arbitrary positions. Moreover, non-autoregressive inference is supported, avoiding hallucinations and improving speed. Experiments across multilingual, crosslingual, and long-form speech scenarios show that LLM-ForcedAligner achieves a 69%~78% relative reduction in accumulated averaging shift compared with prior methods. The checkpoint and inference code will be released later.\nüìÑ Download PDF\nEvaluating ChatGPT on Medical Information Extraction Tasks: Performance, Explainability and Beyond Authors: Wei Zhu Venue: arXiv (2026)\nLarge Language Models (LLMs) like ChatGPT have demonstrated amazing capabilities in comprehending user intents and generate reasonable and useful responses. Beside their ability to chat, their capabilities in various natural language processing (NLP) tasks are of interest to the research community. In this paper, we focus on assessing the overall ability of ChatGPT in 4 different medical information extraction (MedIE) tasks across 6 benchmark datasets. We present the systematically analysis by measuring ChatGPT‚Äôs performance, explainability, confidence, faithfulness, and uncertainty. Our experiments reveal that: (a) ChatGPT‚Äôs performance scores on MedIE tasks fall behind those of the fine-tuned baseline models. (b) ChatGPT can provide high-quality explanations for its decisions, however, ChatGPT is over-confident in its predcitions. (c) ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. (d) The uncertainty in generation causes uncertainty in information extraction results, thus may hinder its applications in MedIE tasks.\nüìÑ Download PDF\nCE-GOCD: Central Entity-Guided Graph Optimization for Community Detection to Augment LLM Scientific Question Answering Authors: Jiayin Lan, Jiaqi Li, Baoxin Wang, Ming Liu, Dayong Wu, Shijin Wang, Bing Qin, Guoping Hu Venue: arXiv (2026)\nLarge Language Models (LLMs) are increasingly used for question answering over scientific research papers. Existing retrieval augmentation methods often rely on isolated text chunks or concepts, but overlook deeper semantic connections between papers. This impairs the LLM‚Äôs comprehension of scientific literature, hindering the comprehensiveness and specificity of its responses. To address this, we propose Central Entity-Guided Graph Optimization for Community Detection (CE-GOCD), a method that augments LLMs‚Äô scientific question answering by explicitly modeling and leveraging semantic substructures within academic knowledge graphs. Our approach operates by: (1) leveraging paper titles as central entities for targeted subgraph retrieval, (2) enhancing implicit semantic discovery via subgraph pruning and completion, and (3) applying community detection to distill coherent paper groups with shared themes. We evaluated the proposed method on three NLP literature-based question-answering datasets, and the results demonstrate its superiority over other retrieval-augmented baseline approaches, confirming the effectiveness of our framework.\nüìÑ Download PDF\nEnhancing Language Models for Robust Greenwashing Detection Authors: Neil Heinrich Braun, Keane Ong, Rui Mao, Erik Cambria, Gianmarco Mengaldo Venue: arXiv (2026)\nSustainability reports are critical for ESG assessment, yet greenwashing and vague claims often undermine their reliability. Existing NLP models lack robustness to these practices, typically relying on surface-level patterns that generalize poorly. We propose a parameter-efficient framework that structures LLM latent spaces by combining contrastive learning with an ordinal ranking objective to capture graded distinctions between concrete actions and ambiguous claims. Our approach incorporates gated feature modulation to filter disclosure noise and utilizes MetaGradNorm to stabilize multi-objective optimization. Experiments in cross-category settings demonstrate superior robustness over standard baselines while revealing a trade-off between representational rigidity and generalization.\nüìÑ Download PDF\nLanguage Models as Artificial Learners: Investigating Crosslinguistic Influence Authors: Abderrahmane Issam, Yusuf Can Semerci, Jan Scholtes, Gerasimos Spanakis Venue: arXiv (2026)\nDespite the centrality of crosslinguistic influence (CLI) to bilingualism research, human studies often yield conflicting results due to inherent experimental variance. We address these inconsistencies by using language models (LMs) as controlled statistical learners to systematically simulate CLI and isolate its underlying drivers. Specifically, we study the effect of varying the L1 language dominance and the L2 language proficiency, which we manipulate by controlling the L2 age of exposure ‚Äì defined as the training step at which the L2 is introduced. Furthermore, we investigate the impact of pretraining on L1 languages with varying syntactic distance from the L2. Using cross-linguistic priming, we analyze how activating L1 structures impacts L2 processing. Our results align with evidence from psycholinguistic studies, confirming that language dominance and proficiency are strong predictors of CLI. We further find that while priming of grammatical structures is bidirectional, the priming of ungrammatical structures is sensitive to language dominance. Finally, we provide mechanistic evidence of CLI in LMs, demonstrating that the L1 is co-activated during L2 processing and directly influences the neural circuitry recruited for the L2. More broadly, our work demonstrates that LMs can serve as a computational framework to inform theories of human CLI.\nüìÑ Download PDF\nICL-EVADER: Zero-Query Black-Box Evasion Attacks on In-Context Learning and Their Defenses Authors: Ningyuan He, Ronghong Huang, Qianqian Tang, Hongyu Wang, Xianghang Mi, Shanqing Guo Venue: arXiv (2026)\nIn-context learning (ICL) has become a powerful, data-efficient paradigm for text classification using large language models. However, its robustness against realistic adversarial threats remains largely unexplored. We introduce ICL-Evader, a novel black-box evasion attack framework that operates under a highly practical zero-query threat model, requiring no access to model parameters, gradients, or query-based feedback during attack generation. We design three novel attacks, Fake Claim, Template, and Needle-in-a-Haystack, that exploit inherent limitations of LLMs in processing in-context prompts. Evaluated across sentiment analysis, toxicity, and illicit promotion tasks, our attacks significantly degrade classifier performance (e.g., achieving up to 95.3% attack success rate), drastically outperforming traditional NLP attacks which prove ineffective under the same constraints. To counter these vulnerabilities, we systematically investigate defense strategies and identify a joint defense recipe that effectively mitigates all attacks with minimal utility loss (\u003c5% accuracy degradation). Finally, we translate our defensive insights into an automated tool that proactively fortifies standard ICL prompts against adversarial evasion. This work provides a comprehensive security assessment of ICL, revealing critical vulnerabilities and offering practical solutions for building more robust systems. Our source code and evaluation datasets are publicly available at: https://github.com/ChaseSecurity/ICL-Evader .\nüìÑ Download PDF\nICON: Intent-Context Coupling for Efficient Multi-Turn Jailbreak Attack Authors: Xingwei Lin, Wenhao Lin, Sicong Cao, Jiahao Yu, Renke Huang, Lei Xue, Chunming Wu Venue: arXiv (2026)\nMulti-turn jailbreak attacks have emerged as a critical threat to Large Language Models (LLMs), bypassing safety mechanisms by progressively constructing adversarial contexts from scratch and incrementally refining prompts. However, existing methods suffer from the inefficiency of incremental context construction that requires step-by-step LLM interaction, and often stagnate in suboptimal regions due to surface-level optimization. In this paper, we characterize the Intent-Context Coupling phenomenon, revealing that LLM safety constraints are significantly relaxed when a malicious intent is coupled with a semantically congruent context pattern. Driven by this insight, we propose ICON, an automated multi-turn jailbreak framework that efficiently constructs an authoritative-style context via prior-guided semantic routing. Specifically, ICON first routes the malicious intent to a congruent context pattern (e.g., Scientific Research) and instantiates it into an attack prompt sequence. This sequence progressively builds the authoritative-style context and ultimately elicits prohibited content. In addition, ICON incorporates a Hierarchical Optimization Strategy that combines local prompt refinement with global context switching, preventing the attack from stagnating in ineffective contexts. Experimental results across eight SOTA LLMs demonstrate the effectiveness of ICON, achieving a state-of-the-art average Attack Success Rate (ASR) of 97.1%. Code is available at https://github.com/xwlin-roy/ICON.\nüìÑ Download PDF\nBeyond Speedup ‚Äì Utilizing KV Cache for Sampling and Reasoning Authors: Zeyu Xing, Xing Li, Hui-Ling Zhen, Mingxuan Yuan, Sinno Jialin Pan Venue: arXiv (2026)\nKV caches, typically used only to speed up autoregressive decoding, encode contextual information that can be reused for downstream tasks at no extra cost. We propose treating the KV cache as a lightweight representation, eliminating the need to recompute or store full hidden states. Despite being weaker than dedicated embeddings, KV-derived representations are shown to be sufficient for two key applications: \\textbf{(i) Chain-of-Embedding}, where they achieve competitive or superior performance on Llama-3.1-8B-Instruct and Qwen2-7B-Instruct; and \\textbf{(ii) Fast/Slow Thinking Switching}, where they enable adaptive reasoning on Qwen3-8B and DeepSeek-R1-Distil-Qwen-14B, reducing token generation by up to $5.7\\times$ with minimal accuracy loss. Our findings establish KV caches as a free, effective substrate for sampling and reasoning, opening new directions for representation reuse in LLM inference. Code: https://github.com/cmd2001/ICLR2026_KV-Embedding.\nüìÑ Download PDF\nProbing the Future of Meta-Analysis: Eliciting Design Principles via an Agentic Research IDE Authors: Sizhe Cheng, Feng Liang, Yuhan Wen, Xipei Yu, Yong Wang Venue: arXiv (2026)\nMeta-analyses and systematic reviews demand rigorous abductive reasoning to build, test, and refine hypotheses across vast, heterogeneous literature. While NLP advancements have automated parts of this pipeline, existing tools often detach researchers from the cognitive loop or function merely as retrieval engines, leading to loss of intellectual ownership and frequent context switching. We present Research IDE, a prototype reimagining authoring environments through the ‚ÄúResearch as Code‚Äù metaphor. Research IDE embeds a multi-agent backend into the writing flow, enabling in-situ verification via ‚Äúhypothesis breakpoints.‚Äù A one-week field deployment with 8 domain experts, followed by a reflective workshop, as a Research through Design (RtD) probe, reveals that users strongly preferred this verification workflow, actively leveraged prior knowledge for confirmation, and reported that breakpoints sparked insights. Drawing from participant feedback and suggestions, we derive design implications for future AI-assisted research tools that fully preserve researcher autonomy and intellectual ownership while harnessing computational scale.\nüìÑ Download PDF\nVIBEVOICE-ASR Technical Report Authors: Zhiliang Peng, Jianwei Yu, Yaoyao Chang, Zilong Wang, Li Dong, Yingbo Hao, Yujie Tu, Chenyu Yang, Wenhui Wang, Songchen Xu, Yutao Sun, Hangbo Bao, Weijiang Xu, Yi Zhu, Zehua Wang, Ting Song, Yan Xia, Zewen Chi, Shaohan Huang, Liang Wang, Chuang Ding, Shuai Wang, Xie Chen, Furu Wei Venue: arXiv (2026)\nThis report presents VibeVoice-ASR, a general-purpose speech understanding framework built upon VibeVoice, designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VibeVoice-ASRsupports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into a single end-to-end generation task. In addition, VibeVoice-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce a prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation.\nüìÑ Download PDF\nThe augmented NLP bound for maximum-entropy remote sampling Authors: Gabriel Ponte, Marcia Fampa, Jon Lee Venue: arXiv (2026)\nThe maximum-entropy remote sampling problem (MERSP) is to select a subset of s random variables from a set of n random variables, so as to maximize the information concerning a set of target random variables that are not directly observable. We assume throughout that the set of all of these random variables follows a joint Gaussian distribution, and that we have the covariance matrix available. Finally, we measure information using Shannon‚Äôs differential entropy. The main approach for exact solution of moderate-sized instances of MERSP has been branch-and-bound, and so previous work concentrated on upper bounds. Prior to our work, there were two upper-bounding methods for MERSP: the so-called NLP bound and the spectral bound, both introduced 25 years ago. We are able now to establish domination results between these two upper bounds. We propose an ``augmented NLP bound‚Äô‚Äô based on a subtle convex relaxation. We provide theoretical guarantees, giving sufficient conditions under which the augmented NLP bound strictly dominates the ordinary NLP bound. In addition, the augmented NLP formulation allows us to derive upper bounds for rank-deficient covariance matrices when they satisfy a technical condition. This is in contrast to the earlier work on the ordinary NLP bound that worked with only positive definite covariance matrices. Finally, we introduce a novel and very effective diagonal-scaling technique for MERSP, employing a positive vector of parameters. Numerical experiments on benchmark instances demonstrate the effectiveness of our approaches in advancing the state of the art for calculating upper bounds on MERSP.\nüìÑ Download PDF\nRedSage: A Cybersecurity Generalist LLM Authors: Naufal Suryanto, Muzammal Naseer, Pengfei Li, Syed Talal Wasim, Jinhui Yi, Juergen Gall, Paolo Ceravolo, Ernesto Damiani Venue: arXiv (2026)\nCybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q\u0026A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.\nüìÑ Download PDF\nDiscovering Hidden Gems in Model Repositories Authors: Jonathan Kahana, Eliahu Horwitz, Yedid Hoshen Venue: arXiv (2026)\nPublic repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of ‚Äúhidden gems‚Äù, unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.\nüìÑ Download PDF\nHybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts Authors: Yingfa Chen, Zhen Leng Thai, Zihan Zhou, Zhu Zhang, Xingyu Shen, Shuo Wang, Chaojun Xiao, Xu Han, Zhiyuan Liu Venue: arXiv (2026)\nHybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data\nüìÑ Download PDF\nUEval: A Benchmark for Unified Multimodal Generation Authors: Bo Li, Yida Yin, Wenhao Chai, Xingyu Fu, Zhuang Liu Venue: arXiv (2026)\nWe introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.\nüìÑ Download PDF\nExploring Reasoning Reward Model for Agents Authors: Kaixuan Fan, Kaituo Feng, Manyuan Zhang, Tianshuo Peng, Zhixun Li, Yilei Jiang, Shuang Chen, Peng Pei, Xunliang Cai, Xiangyu Yue Venue: arXiv (2026)\nAgentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.\nüìÑ Download PDF\nDynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation Authors: Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong, Haiwen Diao, Ziwei Liu Venue: arXiv (2026)\nManipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.\nüìÑ Download PDF\nReactEMG Stroke: Healthy-to-Stroke Few-shot Adaptation for sEMG-Based Intent Detection Authors: Runsheng Wang, Katelyn Lee, Xinyue Zhu, Lauren Winterbottom, Dawn M. Nilsen, Joel Stein, Matei Ciocarlie Venue: arXiv (2026)\nSurface electromyography (sEMG) is a promising control signal for assist-as-needed hand rehabilitation after stroke, but detecting intent from paretic muscles often requires lengthy, subject-specific calibration and remains brittle to variability. We propose a healthy-to-stroke adaptation pipeline that initializes an intent detector from a model pretrained on large-scale able-bodied sEMG, then fine-tunes it for each stroke participant using only a small amount of subject-specific data. Using a newly collected dataset from three individuals with chronic stroke, we compare adaptation strategies (head-only tuning, parameter-efficient LoRA adapters, and full end-to-end fine-tuning) and evaluate on held-out test sets that include realistic distribution shifts such as within-session drift, posture changes, and armband repositioning. Across conditions, healthy-pretrained adaptation consistently improves stroke intent detection relative to both zero-shot transfer and stroke-only training under the same data budget; the best adaptation methods improve average transition accuracy from 0.42 to 0.61 and raw accuracy from 0.69 to 0.78. These results suggest that transferring a reusable healthy-domain EMG representation can reduce calibration burden while improving robustness for real-time post-stroke intent detection.\nüìÑ Download PDF\nLearning Transient Convective Heat Transfer with Geometry Aware World Models Authors: Onur T. Doganay, Alexander Klawonn, Martin Eigel, Hanno Gottschalk Venue: arXiv (2026)\nPartial differential equation (PDE) simulations are fundamental to engineering and physics but are often computationally prohibitive for real-time applications. While generative AI offers a promising avenue for surrogate modeling, standard video generation architectures lack the specific control and data compatibility required for physical simulations. This paper introduces a geometry aware world model architecture, derived from a video generation architecture (LongVideoGAN), designed to learn transient physics. We introduce two key architecture elements: (1) a twofold conditioning mechanism incorporating global physical parameters and local geometric masks, and (2) an architectural adaptation to support arbitrary channel dimensions, moving beyond standard RGB constraints. We evaluate this approach on a 2D transient computational fluid dynamics (CFD) problem involving convective heat transfer from buoyancy-driven flow coupled to a heat flow in a solid structure. We demonstrate that the conditioned model successfully reproduces complex temporal dynamics and spatial correlations of the training data. Furthermore, we assess the model‚Äôs generalization capabilities on unseen geometric configurations, highlighting both its potential for controlled simulation synthesis and current limitations in spatial precision for out-of-distribution samples.\nüìÑ Download PDF\nLearning to Communicate Across Modalities: Perceptual Heterogeneity in Multi-Agent Systems Authors: Naomi Pitzer, Daniela Mihai Venue: arXiv (2026)\nEmergent communication offers insight into how agents develop shared structured representations, yet most research assumes homogeneous modalities or aligned representational spaces, overlooking the perceptual heterogeneity of real-world settings. We study a heterogeneous multi-step binary communication game where agents differ in modality and lack perceptual grounding. Despite perceptual misalignment, multimodal systems converge to class-consistent messages grounded in perceptual input. Unimodal systems communicate more efficiently, using fewer bits and achieving lower classification entropy, while multimodal agents require greater information exchange and exhibit higher uncertainty. Bit perturbation experiments provide strong evidence that meaning is encoded in a distributional rather than compositional manner, as each bit‚Äôs contribution depends on its surrounding pattern. Finally, interoperability analyses show that systems trained in different perceptual worlds fail to directly communicate, but limited fine-tuning enables successful cross-system communication. This work positions emergent communication as a framework for studying how agents adapt and transfer representations across heterogeneous modalities, opening new directions for both theory and experimentation.\nüìÑ Download PDF\nRouting the Lottery: Adaptive Subnetworks for Heterogeneous Data Authors: Grzegorz Stefanski, Alberto Presta, Michal Byra Venue: arXiv (2026)\nIn pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.\nüìÑ Download PDF\nReasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers Authors: Xin Chen, Feng Jiang, Yiqian Zhang, Hardy Chen, Shuo Yan, Wenya Xie, Min Yang, Shujian Huang Venue: arXiv (2026)\nReasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \\emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70% higher accuracy, 22.90% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \\href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}\nüìÑ Download PDF\nValue-Based Pre-Training with Downstream Feedback Authors: Shuqi Ke, Giulia Fanti Venue: arXiv (2026)\nCan a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B‚Äì7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining.\nüìÑ Download PDF\nSINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence Authors: Saoud Aldowaish, Yashwanth Karumanchi, Kai-Chen Chiang, Soroosh Noorzad, Morteza Fayazi Venue: arXiv (2026)\nCurrent methods for converting circuit schematic images into machine-readable netlists struggle with component recognition and connectivity inference. In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator. SINA integrates deep learning for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity extraction, and Optical Character Recognition (OCR) for component reference designator retrieval, while employing a Vision-Language Model (VLM) for reliable reference designator assignments. In our experiments, SINA achieves 96.47% overall netlist-generation accuracy, which is 2.72x higher than state-of-the-art approaches.\nüìÑ Download PDF\nVision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models Authors: Wenxuan Huang, Yu Zeng, Qiuchen Wang, Zhen Fang, Shaosheng Cao, Zheng Chu, Qingyu Yin, Shuang Chen, Zhenfei Yin, Lin Chen, Zehui Chen, Yao Hu, Philip Torr, Feng Zhao, Wanli Ouyang Venue: arXiv (2026)\nMultimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call‚Äô‚Äô for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.\nüìÑ Download PDF\n$G^2$-Reader: Dual Evolving Graphs for Multimodal Document QA Authors: Yaxin Du, Junru Song, Yifan Zhou, Cheng Wang, Jiahao Gu, Zimeng Chen, Menglan Chen, Wen Yao, Yang Yang, Ying Wen, Siheng Chen Venue: arXiv (2026)\nRetrieval-augmented generation is a practical paradigm for question answering over long documents, but it remains brittle for multimodal reading where text, tables, and figures are interleaved across many pages. First, flat chunking breaks document-native structure and cross-modal alignment, yielding semantic fragments that are hard to interpret in isolation. Second, even iterative retrieval can fail in long contexts by looping on partial evidence or drifting into irrelevant sections as noise accumulates, since each step is guided only by the current snippet without a persistent global search state. We introduce $G^2$-Reader, a dual-graph system, to address both issues. It evolves a Content Graph to preserve document-native structure and cross-modal semantics, and maintains a Planning Graph, an agentic directed acyclic graph of sub-questions, to track intermediate findings and guide stepwise navigation for evidence completion. On VisDoMBench across five multimodal domains, $G^2$-Reader with Qwen3-VL-32B-Instruct reaches 66.21% average accuracy, outperforming strong baselines and a standalone GPT-5 (53.08%).\nüìÑ Download PDF\nWhen ‚ÄúBetter‚Äù Prompts Hurt: Evaluation-Driven Iteration for LLM Applications Authors: Daniel Commey Venue: arXiv (2026)\nEvaluating Large Language Model (LLM) applications differs from traditional software testing because outputs are stochastic, high-dimensional, and sensitive to prompt and model changes. We present an evaluation-driven workflow - Define, Test, Diagnose, Fix - that turns these challenges into a repeatable engineering loop. We introduce the Minimum Viable Evaluation Suite (MVES), a tiered set of recommended evaluation components for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows. We also synthesize common evaluation methods (automated checks, human rubrics, and LLM-as-judge) and discuss known judge failure modes. In reproducible local experiments (Ollama; Llama 3 8B Instruct and Qwen 2.5 7B Instruct), we observe that a generic ‚Äúimproved‚Äù prompt template can trade off behaviors: on our small structured suites, extraction pass rate decreased from 100% to 90% and RAG compliance from 93.3% to 80% for Llama 3 when replacing task-specific prompts with generic rules, while instruction-following improved. These findings motivate evaluation-driven prompt iteration and careful claim calibration rather than universal prompt recipes. All test suites, harnesses, and results are included for reproducibility.\nüìÑ Download PDF\nLANCER: LLM Reranking for Nugget Coverage Authors: Jia-Huei Ju, Fran√ßois G. Landry, Eugene Yang, Suzan Verberne, Andrew Yates Venue: arXiv (2026)\nUnlike short-form retrieval-augmented generation (RAG), such as factoid question answering, long-form RAG requires retrieval to provide documents covering a wide range of relevant information. Automated report generation exemplifies this setting: it requires not only relevant information but also a more elaborate response with comprehensive information. Yet, existing retrieval methods are primarily optimized for relevance ranking rather than information coverage. To address this limitation, we propose LANCER, an LLM-based reranking method for nugget coverage. LANCER predicts what sub-questions should be answered to satisfy an information need, predicts which documents answer these sub-questions, and reranks documents in order to provide a ranked list covering as many information nuggets as possible. Our empirical results show that LANCER enhances the quality of retrieval as measured by nugget coverage metrics, achieving higher $Œ±$-nDCG and information coverage than other LLM-based reranking methods. Our oracle analysis further reveals that sub-question generation plays an essential role.\nüìÑ Download PDF\nüîç linguistics One-step Latent-free Image Generation with Pixel Mean Flows Authors: Yiyang Lu, Susie Lu, Qiao Sun, Hanhong Zhao, Zhicheng Jiang, Xianbang Wang, Tianhong Li, Zhengyang Geng, Kaiming He Venue: arXiv (2026)\nModern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose ‚Äúpixel MeanFlow‚Äù (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.\nüìÑ Download PDF\nDynaWeb: Model-Based Reinforcement Learning of Web Agents Authors: Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao, Rongzhao Zhang, Lynn Ai, Eric Yang, Tianyu Shi, Lei Yu Venue: arXiv (2026)\nThe development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.\nüìÑ Download PDF\nCobordism and Concordance of Surfaces in 4-Manifolds Authors: Simeon Hellsten Venue: arXiv (2026)\nWe show that two properly embedded compact surfaces in an orientable 4-manifold are cobordant if and only if they are $\\mathbb{Z}/2$-homologous and either the 4-manifold has boundary or the surfaces have the same normal Euler number. If the 4-manifold is simply-connected and the surfaces are closed, non-orientable, and cobordant, we show that they are in fact concordant. This completes the classification of closed surfaces in simply-connected 4-manifolds up to concordance. Our methods give new constructions of cobordisms with prescribed boundaries, and completely determine when a given cobordism between the boundaries extends to a cobordism or concordance between the surfaces. We obtain our concordance results by extending Sunukjian‚Äôs method of ambient surgery to the unoriented case using Pin$^-$-structures. We also discuss conditions for an arbitrary codimension 2 properly embedded submanifold to admit an unoriented spanning manifold with prescribed boundary. All results hold in both the smooth and topological categories.\nüìÑ Download PDF\nLate Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing Authors: Daniel Stein, Shaoyi Huang, Rolf Drechsler, Bing Li, Grace Li Zhang Venue: arXiv (2026)\nNeural networks have been successfully applied in various resource-constrained edge devices, where usually central processing units (CPUs) instead of graphics processing units exist due to limited power availability. State-of-the-art research still focuses on efficiently executing enormous numbers of multiply-accumulate (MAC) operations. However, CPUs themselves are not good at executing such mathematical operations on a large scale, since they are more suited to execute control flow logic, i.e., computer algorithms. To enhance the computation efficiency of neural networks on CPUs, in this paper, we propose to convert them into logic flows for execution. Specifically, neural networks are first converted into equivalent decision trees, from which decision paths with constant leaves are then selected and compressed into logic flows. Such logic flows consist of if and else structures and a reduced number of MAC operations. Experimental results demonstrate that the latency can be reduced by up to 14.9 % on a simulated RISC-V CPU without any accuracy degradation. The code is open source at https://github.com/TUDa-HWAI/NN2Logic\nüìÑ Download PDF\nNeural S-matrix bootstrap II: solvable 4d amplitudes with particle production Authors: Mehmet Asim Gumus, Damien Leflot, Piotr Tourkine, Alexander Zhiboedov Venue: arXiv (2026)\nWe study a model for nonperturbative unitarization of the four-point contact scalar amplitude in four dimensions. It is defined through an infinite sum of planar diagrams, constructed using two-particle unitarity and crossing symmetry. We reformulate the problem in terms of a set of nonlinear integral equations obeyed by the single and double discontinuities of the amplitude. We then solve them using a neural-network ansatz trained by minimizing a physics-informed loss functional. We obtain a one-parameter family of amplitudes, which exhibit rich structure: sizeable particle production, nontrivial emergent Regge behavior, Landau curves, a logarithmic decay at high energy and fixed angle. Finally, we go beyond the two-particle-reducible setup by treating the multi-particle data ‚Äì supported above the multi-particle Landau curves due to multi-particle unitarity ‚Äì as a dynamical variable. We demonstrate that it can be tuned to suppress low-spin particle production ‚Äì a phenomenon we call Aks screening ‚Äì at the cost of generating larger and oscillatory double spectral density in the multi-particle region.\nüìÑ Download PDF\nAll-order prescription for facet regions in massless wide-angle scattering Authors: Yao Ma Venue: arXiv (2026)\nWe take a step toward answering a long-standing question in the asymptotic expansion of Feynman integrals: how to systematically determine the regions in the Expansion-by-Regions technique for multiscale processes? Focusing on generic massless wide-angle scattering, we provide an all-order momentum-space prescription for facet regions, which generally dominate ‚Äì and in most cases exhaust ‚Äì the contributions in a given asymptotic expansion. This extends the Euclidean-space picture, where regions correspond to specific subgraphs, to the complexities of Minkowski space. Our results are derived from a novel analytical approach combining graph theory and convex geometry; as a key byproduct, we uncover for the first time the algebraic structure underlying momentum modes (collinear, soft, and their hierarchies).\nüìÑ Download PDF\nTowards Universal Urban Patterns-of-Life Simulation Authors: Sandro M. Reia, Henrique F. de Arruda, Shiyang Ruan, Taylor Anderson, Hamdi Kavak, Dieter Pfoser Venue: arXiv (2026)\nUnderstanding urban mobility requires models that capture how people interact with and navigate the built environment. We present a scalable, generalizable agent-based framework in which daily schedules emerge from the interplay between mandatory (e.g., work, school) and flexible (e.g., errands, food, leisure) activities, driven by evolving individual needs. The results of our model are validated against empirical patterns from the 2017 U.S. National Household Travel Survey, including activity distributions, origin-destination flows, and trip-chain length distributions. We introduce a normalized similarity metric to quantify agreement between simulated and empirical patterns. Most cities achieve scores above 0.80, demonstrating strong alignment without the need for city-specific calibration. The model scales efficiently to over 20 million agents, enabling full-population simulations of large metropolitan areas. This combination of universality and scalability enables scenario analysis for infrastructure stress testing, disaster recovery, innovation diffusion, and disease spread in urban systems.\nüìÑ Download PDF\nConstraining Black Hole Parameters from Shadow and Inner-Shadow Morphology Considering Effects from Thick Disk Accretion Flows Authors: Julien A. Kearns, Dominic O. Chang, Daniel C. M. Palumbo, Shane W. Davis Venue: arXiv (2026)\nWe study the effects of emission geometry on the capability to constrain black hole parameters from measurements of the shadow and inner-shadow of a Reissner-Nordstr√∂m black hole. We investigate the capability to constrain mass, charge, observer inclination, and emission co-latitude from images of black hole accretion flows that would arise from thick and thin accretion disks. We confirm previous studies that have shown that independent radii measurements of the shadow and inner-shadow can constrain black hole parameters if the viewing inclination is known, but find that it is only possible if the true emission geometry is also assumed. We study the constraining capabilities of the shadow and inner-shadow observations of M87* and Sgr A* like systems within the context of the BHEX and NgEHT future observatories.\nüìÑ Download PDF\nSecure Group Key Agreement on Cyber-Physical System Buses Authors: Sebastian N. Peters, Lukas Lautenschlager, David Emeis, Jason Lochert Venue: arXiv (2026)\nCyber-Physical Systems (CPSs) rely on distributed embedded devices that often must communicate securely over buses. Ensuring message integrity and authenticity on these buses typically requires group-shared keys for Message Authentication Codes (MACs). To avoid insecure fixed pre-shared keys and trust-on-first-use concepts, a Group Key Agreement (GKA) protocol is needed to dynamically agree on a key amongst the devices. Yet existing GKA protocols lack adaptability to constrained CPS buses. This paper targets authenticated, fully distributed GKA suitable for bus topologies under constraints of industrial and cyber-physical systems, including broadcast-only links, half-duplex operation, resource limits, dynamic membership (including unannounced leaves), a long device lifetime, and a strong Dolev-Yao adversary capable of partitioning the bus. We first systematise existing protocols, then derive the requirements necessary for an authenticated and fully distributed GKA on bus systems. Finally, we design, implement, and evaluate a custom GKA protocol based on TreeKEM.\nüìÑ Download PDF\nInvestigation of Wake Dynamics of a Slender Symmetric Trailing Edge Hydrofoil Authors: Gabriele Gaiti, Chirag Trivedi, Kristian F. Sagmo Venue: arXiv (2026)\nAccurate prediction of wake dynamics behind hydrofoils is critical for mitigating vortex-induced vibrations and improving the performance of hydraulic machinery. Conventional turbulence modeling approaches often struggle to capture the unsteady, coherent structures governing wake behavior, particularly for slender hydrofoils operating at high Reynolds numbers. This study addresses this limitation by combining scale-resolving numerical simulations, including high-resolution Large Eddy Simulation (LES), with Particle Image Velocimetry (PIV) measurements to investigate the turbulent wake of a symmetric, blunt trailing-edge hydrofoil operating at zero angle of attack. The flow was analyzed at a Reynolds number of approximately 7.5x10e5, i.e. close to the onset of wake-structure interaction effects. LES was performed using a fine mesh of approximately 500 million nodes to resolve near-wall and wake dynamics beyond the experimental field of view, while PIV measurements provided time-resolved velocity fields downstream of the trailing edge. Proper Orthogonal Decomposition (POD) was applied to the PIV data to extract dominant coherent structures and quantify their contribution to the turbulent kinetic energy. POD analysis reveals that energy is distributed across many modes, with the leading mode capturing the primary wake dynamics and higher modes forming coupled oscillatory pairs associated with von Karman vortex shedding. PIV-LES agreement shows that central wake measurements combined with numerical simulations enables full wake reconstruction and validates modeling for vibration-relevant hydrofoil dynamics.\nüìÑ Download PDF\nA Judge-Aware Ranking Framework for Evaluating Large Language Models without Ground Truth Authors: Mingyuan Xu, Xinzi Tan, Jiawei Wu, Doudou Zhou Venue: arXiv (2026)\nEvaluating large language models (LLMs) on open-ended tasks without ground-truth labels is increasingly done via the LLM-as-a-judge paradigm. A critical but under-modeled issue is that judge LLMs differ substantially in reliability; treating all judges equally can yield biased leaderboards and misleading uncertainty estimates. More data can make evaluation more confidently wrong under misspecified aggregation. We propose a judge-aware ranking framework that extends the Bradley-Terry-Luce model by introducing judge-specific discrimination parameters, jointly estimating latent model quality and judge reliability from pairwise comparisons without reference labels. We establish identifiability up to natural normalizations and prove consistency and asymptotic normality of the maximum likelihood estimator, enabling confidence intervals for score differences and rank comparisons. Across multiple public benchmarks and a newly collected dataset, our method improves agreement with human preferences, achieves higher data efficiency than unweighted baselines, and produces calibrated uncertainty quantification for LLM rankings.\nüìÑ Download PDF\nEWOCS-V: Is Wd1-72 a recent post-interaction WR+O binary? Authors: C. J. K. Larkin, J. Mackey, H. Jin, A. A. C. Sander, B. Reville, K. Anastasopoulou, M. Andersen, A. Bayo, J. J. Drake, E. K. Grebel, M. G. Guarcello, T. J. Haworth, V. M. Kalari, R. R. Lefever, F. Najarro, B. W. Ritchie, E. Sabbi Venue: arXiv (2026)\nThe evolutionary origin of Wolf-Rayet (WR) stars at Solar metallicity is unclear. Single-star evolution from massive O stars, possibly via a Luminous Blue Variable phase, is challenged by binary period distributions of different WR subtypes. Wd1-72 is a WN7b+O binary embedded in the collective wind of the Galactic young massive cluster Westerlund 1 (Wd 1). It is surrounded by highly structured nebulosity, with cometary tails pointing away from Wd 1 and quasi-spherical droplets towards it. In this letter, we demonstrate that this morphology can be qualitatively reproduced by a hydrodynamic simulation of non-conservative Roche Lobe Overflow (RLOF) mass-loss into a cluster wind. Our model is based on a detailed binary evolution track consistent with key known properties of Wd1-72. Our work suggests Wd1-72 could be only ~10 kyr post-RLOF, and the hydrogen-free nature of Wd1-72 favours this being a second or subsequent RLOF episode. Follow-up observations could make Wd1-72 a valuable benchmark for probing mass-loss and mass-transfer in forming gravitational-wave binary-progenitor systems.\nüìÑ Download PDF\nFineInstructions: Scaling Synthetic Instructions to Pre-Training Scale Authors: Ajay Patel, Colin Raffel, Chris Callison-Burch Venue: arXiv (2026)\nDue to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised ‚Äúpredict the next word‚Äù objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of ‚Äúinstruction-tuning‚Äù data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With ‚Äúsupervised‚Äù synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .\nüìÑ Download PDF\nPRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training Authors: Shenghao Yang, Zhichao Wang, Oleg Balabanov, N. Benjamin Erichson, Michael W. Mahoney Venue: arXiv (2026)\nMatrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for neural network training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation), a general framework for accelerating iterative algorithms for computing matrix functions. PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, adapting to the instance at hand with minimal overhead. We apply PRISM to accelerate Newton-Schulz-like iterations for matrix square roots and orthogonalization, which are core primitives in machine learning. Unlike prior methods, PRISM requires no explicit spectral bounds or singular value estimates; and it adapts automatically to the evolving spectrum. Empirically, PRISM accelerates training when integrated into Shampoo and Muon optimizers.\nüìÑ Download PDF\nThe Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR Authors: Irsyad Adam, Zekai Chen, David Laprade, Shaun Porwal, David Laub, Erik Reinertsen, Arda Pekis, Kevin Brown Venue: arXiv (2026)\nLarge language models (LLMs) trained with next-word-prediction have achieved success as clinical foundation models. Representations from these language backbones yield strong linear probe performance across biomedical tasks, suggesting that patient semantics emerge from next-token prediction at scale. However, this paradigm treats patients as a document to be summarized rather than a dynamical system to be simulated; a patient‚Äôs trajectory emerges from their state evolving under interventions and time, requiring models that simulate dynamics rather than predict tokens. To address this, we introduce SMB-Structure, a world model for structured EHR that grounds a joint-embedding prediction architecture (JEPA) with next-token prediction (SFT). SFT grounds our model to reconstruct future patient states in token space, while JEPA predicts those futures in latent space from the initial patient representation alone, forcing trajectory dynamics to be encoded before the next state is observed. We validate across two large-scale cohorts: Memorial Sloan Kettering (23,319 oncology patients; 323,000+ patient-years) and INSPECT (19,402 pulmonary embolism patients). Using a linear probe evaluated at multiple points along the disease trajectory, we demonstrate that our training paradigm learns embeddings that capture disease dynamics not recoverable by autoregressive baselines, enabling SMB-Structure to achieve competitive performance on complex tasks characterized by high patient heterogeneity. Model weights are available at https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure.\nüìÑ Download PDF\nAlpha Discovery via Grammar-Guided Learning and Search Authors: Han Yang, Dong Hao, Zhuohan Wang, Qi Shi, Xingtong Li Venue: arXiv (2026)\nAutomatically discovering formulaic alpha factors is a central problem in quantitative finance. Existing methods often ignore syntactic and semantic constraints, relying on exhaustive search over unstructured and unbounded spaces. We present AlphaCFG, a grammar-based framework for defining and discovering alpha factors that are syntactically valid, financially interpretable, and computationally efficient. AlphaCFG uses an alpha-oriented context-free grammar to define a tree-structured, size-controlled search space, and formulates alpha discovery as a tree-structured linguistic Markov decision process, which is then solved using a grammar-aware Monte Carlo Tree Search guided by syntax-sensitive value and policy networks. Experiments on Chinese and U.S. stock market datasets show that AlphaCFG outperforms state-of-the-art baselines in both search efficiency and trading profitability. Beyond trading strategies, AlphaCFG serves as a general framework for symbolic factor discovery and refinement across quantitative finance, including asset pricing and portfolio construction.\nüìÑ Download PDF\nPaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing Authors: Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou, Hongen Liu, Manhui Lin, Yue Zhang, Yubo Zhang, Yi Liu, Dianhai Yu, Yanjun Ma Venue: arXiv (2026)\nWe introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-art (SOTA) accuracy of 94.5% on OmniDocBench v1.5. To rigorously evaluate robustness against real-world physical distortions, including scanning, skew, warping, screen-photography, and illumination, we propose the Real5-OmniDocBench benchmark. Experimental results demonstrate that this enhanced model attains SOTA performance on the newly curated benchmark. Furthermore, we extend the model‚Äôs capabilities by incorporating seal recognition and text spotting tasks, while remaining a 0.9B ultra-compact VLM with high efficiency. Code: https://github.com/PaddlePaddle/PaddleOCR\nüìÑ Download PDF\nFrom Future of Work to Future of Workers: Addressing Asymptomatic AI Harms for Dignified Human-AI Interaction Authors: Upol Ehsan, Samir Passi, Koustuv Saha, Todd McNutt, Mark O. Riedl, Sara Alcorn Venue: arXiv (2026)\nIn the future of work discourse, AI is touted as the ultimate productivity amplifier. Yet, beneath the efficiency gains lie subtle erosions of human expertise and agency. This paper shifts focus from the future of work to the future of workers by navigating the AI-as-Amplifier Paradox: AI‚Äôs dual role as enhancer and eroder, simultaneously strengthening performance while eroding underlying expertise. We present a year-long study on the longitudinal use of AI in a high-stakes workplace among cancer specialists. Initial operational gains hid ``intuition rust‚Äô‚Äô: the gradual dulling of expert judgment. These asymptomatic effects evolved into chronic harms, such as skill atrophy and identity commoditization. Building on these findings, we offer a framework for dignified Human-AI interaction co-constructed with professional knowledge workers facing AI-induced skill erosion without traditional labor protections. The framework operationalizes sociotechnical immunity through dual-purpose mechanisms that serve institutional quality goals while building worker power to detect, contain, and recover from skill erosion, and preserve human identity. Evaluated across healthcare and software engineering, our work takes a foundational step toward dignified human-AI interaction futures by balancing productivity with the preservation of human expertise.\nüìÑ Download PDF\nMURAD: A Large-Scale Multi-Domain Unified Reverse Arabic Dictionary Dataset Authors: Serry Sibaee, Yasser Alhabashi, Nadia Sibai, Yara Farouk, Adel Ammar, Sawsan AlHalawani, Wadii Boulila Venue: arXiv (2026)\nArabic is a linguistically and culturally rich language with a vast vocabulary that spans scientific, religious, and literary domains. Yet, large-scale lexical datasets linking Arabic words to precise definitions remain limited. We present MURAD (Multi-domain Unified Reverse Arabic Dictionary), an open lexical dataset with 96,243 word-definition pairs. The data come from trusted reference works and educational sources. Extraction used a hybrid pipeline integrating direct text parsing, optical character recognition, and automated reconstruction. This ensures accuracy and clarity. Each record aligns a target word with its standardized Arabic definition and metadata that identifies the source domain. The dataset covers terms from linguistics, Islamic studies, mathematics, physics, psychology, and engineering. It supports computational linguistics and lexicographic research. Applications include reverse dictionary modeling, semantic retrieval, and educational tools. By releasing this resource, we aim to advance Arabic natural language processing and promote reproducible research on Arabic lexical semantics.\nüìÑ Download PDF\nSMART: A Social Movement Analysis \u0026 Reasoning Tool with Case Studies on #MeToo and #BlackLivesMatter Authors: Valerio La Gatta, Marco Postiglione, Jeremy Gilbert, Daniel W. Linna, Morgan Manella Greenfield, Aaron Shaw, V. S. Subrahmanian Venue: arXiv (2026)\nSocial movements supporting the UN‚Äôs Sustainable Development Goals (SDGs) play a vital role in improving human lives. If journalists were aware of the relationship between social movements and external events, they could provide more precise, time-sensitive reporting about movement issues and SDGs. Our SMART system achieves this goal by collecting data from multiple sources, extracting emotions on various themes, and then using a transformer-based forecasting engine (DEEP) to predict quantity and intensity of emotions in future posts. This paper demonstrates SMART‚Äôs Retrospective capabilities required by journalists via case studies analyzing social media discussions of the #MeToo and #BlackLivesMatter before and after the 2024 U.S. election. We create a novel 1-year dataset which we will release upon publication. It contains over 2.7M Reddit posts and over 1M news articles. We show that SMART enables early detection of discourse shifts around key political events, providing journalists with actionable insights to inform editorial planning. SMART was developed through multiple interactions with a panel of over 20 journalists from a variety of news organizations over a 2-year period, including an author of this paper.\nüìÑ Download PDF\nSokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models Authors: Sebastiano Monti, Carlo Nicolini, Gianni Pellegrini, Jacopo Staiano, Bruno Lepri Venue: arXiv (2026)\nAlthough the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.\nüìÑ Download PDF\nLike a Therapist, But Not: Reddit Narratives of AI in Mental Health Contexts Authors: Elham Aghakhani, Rezvaneh Rezapour Venue: arXiv (2026)\nLarge language models (LLMs) are increasingly used for emotional support and mental health-related interactions outside clinical settings, yet little is known about how people evaluate and relate to these systems in everyday use. We analyze 5,126 Reddit posts from 47 mental health communities describing experiential or exploratory use of AI for emotional support or therapy. Grounded in the Technology Acceptance Model and therapeutic alliance theory, we develop a theory-informed annotation framework and apply a hybrid LLM-human pipeline to analyze evaluative language, adoption-related attitudes, and relational alignment at scale. Our results show that engagement is shaped primarily by narrated outcomes, trust, and response quality, rather than emotional bond alone. Positive sentiment is most strongly associated with task and goal alignment, while companionship-oriented use more often involves misaligned alliances and reported risks such as dependence and symptom escalation. Overall, this work demonstrates how theory-grounded constructs can be operationalized in large-scale discourse analysis and highlights the importance of studying how users interpret language technologies in sensitive, real-world contexts.\nüìÑ Download PDF\nDo VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions Authors: Xiaoxiao Sun, Mingyang Li, Kun yuan, Min Woo Sun, Mark Endo, Shengguang Wu, Changlin Li, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy Venue: arXiv (2026)\nLarge Vision-Language Models (VLMs) often answer classic visual illusions ‚Äúcorrectly‚Äù on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely recall memorized patterns? While several studies have noted this phenomenon, the underlying causes remain unclear. To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion framework with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven recall. Unlike prior work that focuses on averaged accuracy, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls. Experiments across different families reveal that response persistence arises from heterogeneous causes rather than a single mechanism. For instance, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, while Qwen variants suggest visual-processing limits. Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change. Data and code are available at https://sites.google.com/view/vi-probe/.\nüìÑ Download PDF\nProbing the Sound Speed of Dark Energy with a Lunar Laser Interferometer Authors: Alfredo Gurrola, Robert J. Scherrer, Oem Trivedi Venue: arXiv (2026)\nThe sound speed of dark energy encodes fundamental information about the microphysics underlying cosmic acceleration, yet remains essentially unconstrained by existing observations. We demonstrate that a lunar-based laser interferometer, such as the proposed Laser Interferometer Lunar Antenna (LILA), can directly probe the sound speed of dark energy by measuring the real-time evolution of horizon-scale gravitational potentials. Operating in the ultra-low-frequency gravitational band inaccessible from Earth, LILA is sensitive to scalar metric perturbations sourced by dark energy dynamics. Using both fluid and effective field theory descriptions, we develop a complete framework linking dark energy sound speed to observable strain signatures. We construct a likelihood pipeline and Fisher forecasts, showing that LILA can either detect clustering dark energy or exclude broad classes of models with unprecedented sensitivity. This establishes lunar interferometry as a novel and powerful probe of the physics driving cosmic acceleration.\nüìÑ Download PDF\nA Reverse Black Hole Information Problem Authors: Jan de Boer, Andrew Rolph, Jildou Hollander Venue: arXiv (2026)\nWe study the formation, detection and coarse-graining of black holes in AdS/CFT, with an emphasis on the tension between boundary unitarity and the production of mixed state Hawking radiation in the bulk. We construct CFT states dual to black hole formation and evaporation by colliding bulk particle wavepackets at trans-Planckian energy. We propose boundary probes which are able to distinguish small AdS black holes from other states within the microcanonical ensemble. We investigate different coarse-graining prescriptions acting on the evolving CFT state, including averaging over CFT data, Hamiltonians and time windows, and compare their purities to those expected from the bulk semiclassical description. Our results clarify how semiclassical black hole behaviour can arise from an ensemble-averaging of the exact unitary dynamics, and take a step towards a better understanding of coarse-graining in the single-sided black hole information problem.\nüìÑ Download PDF\nUniversal Multifractality at the Topological Anderson Insulator Transition Authors: Ksenija Kovalenka, Ahmad Ranjbar, Sam Azadi, Rodion Vladimirovich Belosludov, Thomas D. K√ºhne, Mohammad Saeed Bahramy Venue: arXiv (2026)\nDisorder is ubiquitous in quantum materials, and its interplay with topology can generate phases absent in the clean limit. Using the Haldane model as a minimal setting, we show that disorder not only shifts topological boundaries but also stabilizes a topological Anderson insulator (TAI) between trivial and Chern insulating regimes. Employing the local Chern marker as a real-space topological probe, we map the full phase diagram and demonstrate that the TAI forms a finite domain bounded by trivial and Anderson insulators. Multifractal analysis of low-energy eigenstates at the boundary reveals universal critical spectra, independent of whether disorder generates or destroys topology. These results place topology, localization, and criticality within a unified framework and provide clear benchmarks for real-space diagnostics of disordered topological phases.\nüìÑ Download PDF\nSymmetries of regular $q$-graphs Authors: Daniel R Hawtin, Padraig √ì Cath√°in Venue: arXiv (2026)\nGiven a finite vector space $V=\\mathbb{F}_q^n$, the $q$-analogue of a graph, called a $q$-graph, is a pair $Œì=(\\mathcal{V},\\mathcal{E})$, where $\\mathcal{V}$ is the set of $1$-dimensional subspaces of $V$ and $\\mathcal{E}$ is a subset of the $2$-dimensional subspaces of $V$. Elements of $\\mathcal{V}$ and $\\mathcal{E}$ are called vertices and edges, respectively. If the edges through a vertex $X$ consist of all $2$-spaces of a $(k+1)$-dimensional space which contain $X$, regardless of the choice of vertex, then $Œì$ is $k$-regular. Moreover, $Œì$ is flag-transitive if there is a subgroup of $Œì{\\rm L}_n(q)$ preserving $\\mathcal{E}$ and acting transitively on the set of all incident vertex-edge pairs; and symmetric if there is a subgroup of $Œì{\\rm L}_n(q)$ preserving $\\mathcal{E}$ and acting transitively on the set of all ordered pairs of adjacent vertices. This paper classifies all $k$-regular $q$-graphs that are either flag-transitive or symmetric. The $q$-graphs in the classification are constructed from familiar objects in finite geometry, including spreads, symplectic polar spaces, and generalised hexagons. The classification depends essentially on the classification of transitive linear groups, and thus ultimately on the classification of finite simple groups.\nüìÑ Download PDF\nOptimal cross-correlation technique to search for strongly lensed gravitational waves Authors: Anirban Kopty, Sanjit Mitra, Anupreeta More Venue: arXiv (2026)\nAs the number of detected gravitational wave (GW) events increases with the improved sensitivity of the observatories, detecting strongly lensed pairs of events is becoming a real possibility. Identifying such lensed pairs, however, remains challenging due to the computational cost and/or the reliance on prior knowledge of source parameters in existing methods. This study investigates a novel approach, Optimal Cross-Correlation Analysis for Multiplets (OCCAM), applied to strain data from one or more detectors for Compact Binary Coalescence (CBC) events identified by GW searches, using an optimal, mildly model-dependent, low computation cost approach to identify strongly lensed candidates. This technique efficiently narrows the search space, allowing for more sensitive, but (much) higher latency, algorithms to refine the results further. We demonstrate that our method performs significantly better than other computationally inexpensive methods. In particular, we achieve 97 percent (80 percent) lensed event detection at a pairwise false positive probability of approximately 13 percent (7 percent) for a single detector with LIGO design sensitivity, assuming an SNR greater than or equal to 10 astrophysically motivated lensed and unlensed populations. Thus, this method, using a network of detectors and in conjunction with sky-localisation information, can enormously reduce the false positive probability, making it highly viable to efficiently and quickly search for lensing pairs among thousands of events, including the sub-threshold candidates.\nüìÑ Download PDF\nAn Invitation to Higher-Order Riemannian Optimization: Optimal and Implementable Methods Authors: David Huckleberry Gutman, George Lobo Venue: arXiv (2026)\nThis paper presents the first optimal-rate $p$-th order methods with $p\\geq 1$ for finding first and second-order stationary points of non-convex smooth objective functions over Riemannian manifolds. In contrast to the geodesically convex setting, we definitively establish that the optimal oracle complexity of non-convex optimization over manifolds matches that over Euclidean space. In parallel with the complexity analysis, we introduce a general framework for systematically studying higher-order regularity on Riemannian manifolds that characterizes its joint dependence on the objective function and the chosen retraction. To the best of our knowledge, this framework constitutes the first known application in optimization of pullback connections and the Sasaki metric to the study of retraction-based pullbacks of the objective function. We provide clean derivative bounds based on a new covariant Fa√† di Bruno formula derived within our framework. For $p=3$, our methods are fully implementable via a new Krylov-based framework for minimizing quartically regularized cubic polynomials. This is the first Krylov method for this class of polynomials and may be of independent interest beyond Riemannian optimization.\nüìÑ Download PDF\nFirewalls in the non-perturbative bulk Hilbert space of JT gravity Authors: Hamed Zolfi Venue: arXiv (2026)\nIt has been shown that a very old black hole can tunnel into a white hole through the emission of a large baby universe. This process can be modeled by a genus-one geometry corresponding to a single baby universe emission, with a tunneling probability proportional to ( t^{2} e^{-2S(E)} ), where ( t ) denotes the black hole age and ( S(E) ) its entropy at energy ( E ). The growth of this probability at late times raises the question of its behavior near ( t \\sim e^{S} ). A natural possibility is that the full genus expansion, together with its non-perturbative completion, leads to saturation of the tunneling probability. Motivated by this idea, the present analysis employs a non-perturbative bulk inner product in place of the perturbative one and shows that, at late times, the probabilities of realizing firewall geometries and smooth geometries approach constant values.\nüìÑ Download PDF\nSuperdiffusion and anomalous regularization in self-similar random incompressible flows Authors: Scott Armstrong, Ahmed Bou-Rabee, Tuomo Kuusi Venue: arXiv (2026)\nWe study the long-time behavior of a particle in $\\mathbb{R}^d$, $d \\geq 2$, subject to molecular diffusion and advection by a random incompressible flow. The velocity field is the divergence of a stationary random stream matrix $\\mathbf{k} $ with positive Hurst exponent $Œ≥\u003e 0$, so the resulting random environment is multiscale and self-similar. In the perturbative regime $Œ≥\\ll 1$, we prove quenched power-law superdiffusion: for a typical realization of the environment, the displacement variance at time $t$ grows like $t^{2/(2-Œ≥)}$, the scaling predicted by renormalization group heuristics. We also identify the leading prefactor up to a random (quenched) relative error of order $Œ≥^{\\frac12}\\left| \\log Œ≥\\right|^3$. The proof implements a Wilsonian renormalization group scheme at the level of the infinitesimal generator $\\nabla \\cdot (ŒΩI_d + \\mathbf{k} ) \\nabla$, based on a self-similar induction across scales. We demonstrate that the coarse-grained generator is well-approximated, at each scale $r$, by a constant-coefficient Laplacian with effective diffusivity growing like $r^Œ≥$. This approximation is inherently scale-local: reflecting the multifractal nature of the environment, the relative error does not decay with the scale, but remains of order $Œ≥^{\\frac12}\\left| \\log Œ≥\\right|^2$. We also prove anomalous regularization under the quenched law: for almost every realization of the drift, solutions of the associated elliptic equation are H√∂lder continuous with exponent $1 - CŒ≥^{\\frac12}$ and satisfy estimates which are uniform in the molecular diffusivity $ŒΩ$ and the scale.\nüìÑ Download PDF\nMechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Authors: Jianhui Chen, Yuzhang Luo, Liangming Pan Venue: arXiv (2026)\nWhile Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention‚Äìremoving or augmenting a small fraction of high-influence samples‚Äìsignificantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model‚Äôs in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose a mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs.\nüìÑ Download PDF\nSpecTran: Spectral-Aware Transformer-based Adapter for LLM-Enhanced Sequential Recommendation Authors: Yu Cui, Feng Liu, Zhaoxiang Wang, Changwang Zhang, Jun Wang, Can Wang, Jiawei Chen Venue: arXiv (2026)\nTraditional sequential recommendation (SR) models learn low-dimensional item ID embeddings from user-item interactions, often overlooking textual information such as item titles or descriptions. Recent advances in Large Language Models (LLMs) have inspired a surge of research that encodes item textual information with high-dimensional semantic embeddings, and designs transformation methods to inject such embeddings into SR models. These embedding transformation strategies can be categorized into two types, both of which exhibits notable drawbacks: 1) adapter-based methods suffer from pronounced dimension collapse, concentrating information into a few dominant dimensions; 2) SVD-based methods are rigid and manual, considering only a few principal spectral components while discarding rich information in the remaining spectrum. To address these limitations, we propose SpecTran, a spectral-aware transformer-based adapter that operates in the spectral domain, attending to the full spectrum to select and aggregates informative components. A learnable spectral-position encoding injects singular-value cues as an inductive bias, guiding attention toward salient spectral components and promoting diversity across embedding dimensions. Across four real-world datasets and three SR backbones, it consistently outperforms strong baselines, achieving an average improvement of 9.17%.\nüìÑ Download PDF\nBreaking the Regional Barrier: Inductive Semantic Topology Learning for Worldwide Air Quality Forecasting Authors: Zhiqing Cui, Siru Zhong, Ming Jin, Shirui Pan, Qingsong Wen, Yuxuan Liang Venue: arXiv (2026)\nGlobal air quality forecasting grapples with extreme spatial heterogeneity and the poor generalization of existing transductive models to unseen regions. To tackle this, we propose OmniAir, a semantic topology learning framework tailored for global station-level prediction. By encoding invariant physical environmental attributes into generalizable station identities and dynamically constructing adaptive sparse topologies, our approach effectively captures long-range non-Euclidean correlations and physical diffusion patterns across unevenly distributed global networks. We further curate WorldAir, a massive dataset covering over 7,800 stations worldwide. Extensive experiments show that OmniAir achieves state-of-the-art performance against 18 baselines, maintaining high efficiency and scalability with speeds nearly 10 times faster than existing models, while effectively bridging the monitoring gap in data-sparse regions.\nüìÑ Download PDF\nQuantum LEGO Learning: A Modular Design Principle for Hybrid Artificial Intelligence Authors: Jun Qi, Chao-Han Huck Yang, Pin-Yu Chen, Min-Hsiu Hsieh, Hector Zenil, Jesper Tegner Venue: arXiv (2026)\nHybrid quantum-classical learning models increasingly integrate neural networks with variational quantum circuits (VQCs) to exploit complementary inductive biases. However, many existing approaches rely on tightly coupled architectures or task-specific encoders, limiting conceptual clarity, generality, and transferability across learning settings. In this work, we introduce Quantum LEGO Learning, a modular and architecture-agnostic learning framework that treats classical and quantum components as reusable, composable learning blocks with well-defined roles. Within this framework, a pre-trained classical neural network serves as a frozen feature block, while a VQC acts as a trainable adaptive module that operates on structured representations rather than raw inputs. This separation enables efficient learning under constrained quantum resources and provides a principled abstraction for analyzing hybrid models. We develop a block-wise generalization theory that decomposes learning error into approximation and estimation components, explicitly characterizing how the complexity and training status of each block influence overall performance. Our analysis generalizes prior tensor-network-specific results and identifies conditions under which quantum modules provide representational advantages over comparably sized classical heads. Empirically, we validate the framework through systematic block-swap experiments across frozen feature extractors and both quantum and classical adaptive heads. Experiments on quantum dot classification demonstrate stable optimization, reduced sensitivity to qubit count, and robustness to realistic noise.\nüìÑ Download PDF\nüîç psycholinguistics Variance component score test for multivariate change point detection with applications to mobile health Authors: Melissa Lynne Martin, Juliette Brook, Sage Rush, Theodore D. Satterthwaite, Ian J. Barnett Venue: arXiv (2026)\nMultivariate change point detection is the process of identifying distributional shifts in time-ordered data across multiple features. This task is particularly challenging when the number of features is large relative to the number of observations. This problem is often present in mobile health, where behavioral changes in at-risk patients must be detected in real time in order to prompt timely interventions. We propose a variance component score test (VC*) for detecting changes in feature means and/or variances using only pre-change point data to estimate distributional parameters. Through simulation studies, we show that VC* has higher power than existing methods. Moreover, we demonstrate that reducing bias by using only pre-change point days to estimate parameters outweighs the increased estimator variances in most scenarios. Lastly, we apply VC* and competing methods to passively collected smartphone data in adolescents and young adults with affective instability.\nüìÑ Download PDF\nSMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization Authors: Leonard Papenmeier, Petru Tighineanu Venue: arXiv (2026)\nMulti-objective optimization aims to solve problems with competing objectives, often with only black-box access to a problem and a limited budget of measurements. In many applications, historical data from related optimization tasks is available, creating an opportunity for meta-learning to accelerate the optimization. Bayesian optimization, as a promising technique for black-box optimization, has been extended to meta-learning and multi-objective optimization independently, but methods that simultaneously address both settings - meta-learned priors for multi-objective Bayesian optimization - remain largely unexplored. We propose SMOG, a scalable and modular meta-learning model based on a multi-output Gaussian process that explicitly learns correlations between objectives. SMOG builds a structured joint Gaussian process prior across meta- and target tasks and, after conditioning on metadata, yields a closed-form target-task prior augmented by a flexible residual multi-output kernel. This construction propagates metadata uncertainty into the target surrogate in a principled way. SMOG supports hierarchical, parallel training: meta-task Gaussian processes are fit once and then cached, achieving linear scaling with the number of meta-tasks. The resulting surrogate integrates seamlessly with standard multi-objective Bayesian optimization acquisition functions.\nüìÑ Download PDF\nFrom Particles to Agents: Hallucination as a Metric for Cognitive Friction in Spatial Simulation Authors: Javier Argota S√°nchez-Vaquerizo, Luis Borunda Monsivais Venue: arXiv (2026)\nTraditional architectural simulations (e.g. Computational Fluid Dynamics, evacuation, structural analysis) model elements as deterministic physics-based ‚Äúparticles‚Äù rather than cognitive ‚Äúagents‚Äù. To bridge this, we introduce \\textbf{Agentic Environmental Simulations}, where Large Multimodal generative models actively predict the next state of spatial environments based on semantic expectation. Drawing on examples from accessibility-oriented AR pipelines and multimodal digital twins, we propose a shift from chronological time-steps to Episodic Spatial Reasoning, where simulations advance through meaningful, surprisal-triggered events. Within this framework we posit AI hallucinations as diagnostic tools. By formalizing the \\textbf{Cognitive Friction} ($C_f$) it is possible to reveal ‚ÄúPhantom Affordances‚Äù, i.e. semiotic ambiguities in built space. Finally, we challenge current HCI paradigms by treating environments as dynamic cognitive partners and propose a human-centered framework of cognitive orchestration for designing AI-driven simulations that preserve autonomy, affective clarity, and cognitive integrity.\nüìÑ Download PDF\nUnique Continuation Property for Stochastic Wave Equations Authors: Qi L√º, Zhonghua Liao Venue: arXiv (2026)\nThis paper establishes a fundamental and surprising phenomenon in the theory of stochastic wave equations: the restoration of the unique continuation property (UCP) across characteristic hypersurfaces, a property that is known to fail generically in the deterministic setting. We prove that if a solution to a linear stochastic wave equation vanishes on one side of a characteristic surface $Œì$, then it must vanish in a full neighborhood of any point on $Œì$, provided the stochastic diffusion coefficient is non-degenerate. This result stands in sharp contrast to the classical H√∂rmander-type counterexamples for deterministic waves. Furthermore, we extend the UCP to equations with non-homogeneous stochastic sources and establish a global unique continuation result from the interior of an arbitrarily narrow characteristic cone. Our proofs rely on a novel stochastic Carleman estimate, where the It√¥ diffusion term introduces a crucial positive energy contribution that is absent in deterministic models. These findings demonstrate a qualitative difference between deterministic and stochastic hyperbolic dynamics and open new avenues for control theory and inverse problems in stochastic setting.\nüìÑ Download PDF\nThe Effectiveness of Style Vectors for Steering Large Language Models: A Human Evaluation Authors: Diaoul√© Diallo, Katharina Dworatzyk, Sophie Jentzsch, Peer Sch√ºtt, Sabine Theis, Tobias Hecking Venue: arXiv (2026)\nControlling the behavior of large language models (LLMs) at inference time is essential for aligning outputs with human abilities and safety requirements. \\emph{Activation steering} provides a lightweight alternative to prompt engineering and fine-tuning by directly modifying internal activations to guide generation. This research advances the literature in three significant directions. First, while previous work demonstrated the technical feasibility of steering emotional tone using automated classifiers, this paper presents the first human evaluation of activation steering concerning the emotional tone of LLM outputs, collecting over 7,000 crowd-sourced ratings from 190 participants via Prolific ($n=190$). These ratings assess both perceived emotional intensity and overall text quality. Second, we find strong alignment between human and model-based quality ratings (mean $r=0.776$, range $0.157$‚Äì$0.985$), indicating automatic scoring can proxy perceived quality. Moderate steering strengths ($Œª\\approx 0.15$) reliably amplify target emotions while preserving comprehensibility, with the strongest effects for disgust ($Œ∑_p^2 = 0.616$) and fear ($Œ∑_p^2 = 0.540$), and minimal effects for surprise ($Œ∑_p^2 = 0.042$). Finally, upgrading from Alpaca to LlaMA-3 yielded more consistent steering with significant effects across emotions and strengths (all $p \u003c 0.001$). Inter-rater reliability was high (ICC $= 0.71$‚Äì$0.87$), underscoring the robustness of the findings. These findings support activation-based control as a scalable method for steering LLM behavior across affective dimensions.\nüìÑ Download PDF\nThe Surprising Difficulty of Search in Model-Based Reinforcement Learning Authors: Wei-Di Chang, Mikael Henaff, Brandon Amos, Gregory Dudek, Scott Fujimoto Venue: arXiv (2026)\nThis paper investigates search in model-based reinforcement learning (RL). Conventional wisdom holds that long-term predictions and compounding errors are the primary obstacles for model-based RL. We challenge this view, showing that search is not a plug-and-play replacement for a learned policy. Surprisingly, we find that search can harm performance even when the model is highly accurate. Instead, we show that mitigating distribution shift matters more than improving model or value function accuracy. Building on this insight, we identify key techniques for enabling effective search, achieving state-of-the-art performance across multiple popular benchmark domains.\nüìÑ Download PDF\nMaxwait: A Generalized Mechanism for Distributed Time-Sensitive Systems Authors: Francesco Paladino, Shulu Li, Edward A. Lee Venue: arXiv (2026)\nDistributed time-sensitive systems must balance timing requirements (availability) and consistency in the presence of communication delays and synchronization uncertainty. This paper presents maxwait, a simple coordination mechanism with surprising generality that makes these tradeoffs explicit and configurable. We demonstrate that this mechanism subsumes classical distributed system methods such as PTIDES, Chandy-and-Misra with or without null messages, Jefferson‚Äôs Time-Warp, and Lamport‚Äôs time-based fault detection, while enabling real-time behavior in distributed cyber-physical applications. The mechanism can also realize many commonly used distributed system patterns, including logical execution time (LET), publish and subscribe, actors, conflict-free replicated data types (CRDTs), and remote procedure calls with futures. More importantly, it adds to these mechanisms better control over timing, bounded time fault detection, and the option of making them more deterministic, all within a single semantic framework. Implemented as an extension of the Lingua Franca coordination language, maxwait enforces logical-time consistency when communication latencies are bounded and provides structured fault handling when bounds are violated.\nüìÑ Download PDF\nThe Tensionless Lives of Null Strings Authors: Arjun Bagchi, Aritra Banerjee, Ritankar Chatterjee, Priyadarshini Pandit Venue: arXiv (2026)\nThe tensionless limit probes the very high energy regime of string theory in contrast to the well studied point-particle limit which reduces to Einstein gravity. Tensionless strings sweep out null worldsheets in the target space and hence are also called null strings. This article aims to provide a comprehensive review of tensionless null string theory beginning with the initial work of Schild, and continuing to the foundational work of Isberg et al (ILST) and then focussing on developments in the past decade. Recent work centres on the emergence of the Carrollian Conformal Algebra as residual worldsheet symmetries of the ILST action and the identification of tensionless limit as a worldsheet Carrollian limit on the string worldsheet. Carrollian structures are used to address the classical and quantum aspects of the null string. In the classical theory, the aforementioned limit agrees with the analysis from the ILST action. Symmetries, constraints, mode expansions computed from both perspectives match nicely providing a robust cross-check of the analyses. We discuss closed and open null strings as well as their supersymmetric cousins. The quantum null string comes with several surprises, the foremost of which is the emergence of three consistent quantum theories from the ILST action. We detail the canonical quantisation and the spectrum of the triumvirate of theories. We discuss the novelties of the quantum null theories and the effect compactifaction has on them. We also discuss Carroll strings, applications of these ideas to strings approaching black holes and give a quick overview of other related developments.\nüìÑ Download PDF\nLearning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics Authors: Winfried Ripken, Michael Plainer, Gregor Lied, Thorben Frank, Oliver T. Unke, Stefan Chmiela, Frank No√©, Klaus Robert M√ºller Venue: arXiv (2026)\nSimulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a framework to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span $Œît$, enabling stable large-timestep updates far beyond the stability limits of classical integrators. To this end, we impose a Mean Flow consistency condition for time-averaged Hamiltonian dynamics. Unlike prior approaches, this allows training on independent phase-space samples without access to future states, avoiding expensive trajectory generation. Validated across diverse Hamiltonian systems, our method in particular improves upon molecular dynamics simulations using machine-learned force fields (MLFF). Our models maintain comparable training and inference cost, but support significantly larger integration timesteps while trained directly on widely-available trajectory-free MLFF datasets.\nüìÑ Download PDF\nDesigning quantum technologies with a quantum computer Authors: Juan Naranjo, Thi Ha Kyaw, Gaurav Saxena, Kevin Ferreira, Jack S. Baker Venue: arXiv (2026)\nInteracting spin systems in solids underpin a wide range of quantum technologies, from quantum sensors and single-photon sources to spin-defect-based quantum registers and processors. We develop a quantum-computer-aided framework for simulating such devices using a general electron spin resonance Hamiltonian incorporating zero-field splitting, the Zeeman effect, hyperfine interactions, dipole-dipole spin-spin terms, and electron-phonon decoherence. Within this model, we combine Gray-encoded qudit-to-qubit mappings, qubit-wise commuting aggregation, and a multi-reference selected quantum Krylov fast-forwarding (sQKFF) hybrid algorithm to access long-time dynamics while remaining compatible with NISQ and early fault-tolerant hardware constraints. Numerical simulations demonstrate the computation of autocorrelation functions up to $\\sim100$ ns, together with microwave absorption spectra and the $\\ell_1$-norm of coherence, achieving 18-30$%$ reductions in gate counts and circuit depth for Trotterized time-evolution circuits compared to unoptimized implementations. Using the nitrogen vacancy center in diamond as a testbed, we benchmark the framework against classical simulations and identify the reference-state selection in sQKFF as the primary factor governing accuracy at fixed hardware cost. This methodology provides a flexible blueprint for using quantum computers to design, compare, and optimize solid-state spin-qubit technologies under experimentally realistic conditions.\nüìÑ Download PDF\nAccessibility-Driven Information Transformations in Mixed-Visual Ability Work Teams Authors: Yichun Zhao, Miguel A. Nacenta, Mahadeo A. Sukhai, Sowmya Somanath Venue: arXiv (2026)\nBlind and low-vision (BLV) employees in mixed-visual ability teams often encounter information (e.g., PDFs, diagrams) in inaccessible formats. To enable teamwork, teams must transform these representations by modifying or re-creating them into accessible forms. However, these transformations are frequently overlooked, lack infrastructural support, and cause additional labour. To design systems that move beyond one-off accommodations to effective mixed-ability collaboration, we need a deeper understanding of the representations, their transformations and how they occur. We conducted a week-long diary study with follow-up interviews with 23 BLV and sighted professionals from five legal, non-profit, and consulting teams, documenting 36 transformation cases. Our analysis characterizes how teams perform representational transformations for accessibility: how they are triggered proactively or reactively, how they simplify or enhance, and four common patterns in which workers coordinate with each other to address representational incompatibility. Our findings uncover opportunities for designing systems that can better support mixed-visual ability work.\nüìÑ Download PDF\nmjlab: A Lightweight Framework for GPU-Accelerated Robot Learning Authors: Kevin Zakka, Qiayuan Liao, Brent Yi, Louis Le Lay, Koushil Sreenath, Pieter Abbeel Venue: arXiv (2026)\nWe present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.\nüìÑ Download PDF\nCognitive Load Estimation Using Brain Foundation Models and Interpretability for BCIs Authors: Deeksha M. Shama, Dimitra Emmanouilidou, Ivan J. Tashev Venue: arXiv (2026)\nAccurately monitoring cognitive load in real time is critical for Brain-Computer Interfaces (BCIs) that adapt to user engagement and support personalized learning. Electroencephalography (EEG) offers a non-invasive, cost-effective modality for capturing neural activity, though traditional methods often struggle with cross-subject variability and task-specific preprocessing. We propose leveraging Brain Foundation Models (BFMs), large pre-trained neural networks, to extract generalizable EEG features for cognitive load estimation. We adapt BFMs for long-term EEG monitoring and show that fine-tuning a small subset of layers yields improved accuracy over the state-of-the-art. Despite their scale, BFMs allow for real-time inference with a longer context window. To address often-overlooked interpretability challenges, we apply Partition SHAP (SHapley Additive exPlanations) to quantify feature importance. Our findings reveal consistent emphasis on prefrontal regions linked to cognitive control, while longitudinal trends suggest learning progression. These results position BFMs as efficient and interpretable tools for continuous cognitive load monitoring in real-world BCIs.\nüìÑ Download PDF\nFrom Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning Authors: Shaojie Wang, Liang Zhang Venue: arXiv (2026)\nCurrent LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19% and 4.63% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.\nüìÑ Download PDF\nScale-Dependent Semantic Dynamics Revealed by Allan Deviation Authors: Debayan Dasgupta Venue: arXiv (2026)\nWhile language progresses through a sequence of semantic states, the underlying dynamics of this progression remain elusive. Here, we treat the semantic progression of written text as a stochastic trajectory in a high-dimensional state space. We utilize Allan deviation, a tool from precision metrology, to analyze the stability of meaning by treating ordered sentence embeddings as a displacement signal. Our analysis reveals two distinct dynamical regimes: short-time power-law scaling, which differentiates creative literature from technical texts, and a long-time crossover to a stability-limited noise floor. We find that while large language models successfully mimic the local scaling statistics of human text, they exhibit a systematic reduction in their stability horizon. These results establish semantic coherence as a measurable physical property, offering a framework to differentiate the nuanced dynamics of human cognition from the patterns generated by algorithmic models.\nüìÑ Download PDF\nAge Matters: Analyzing Age-Related Discussions in App Reviews Authors: Shashiwadana Nirmania, Garima Sharma, Hourieh Khalajzadeh, Mojtaba Shahin Venue: arXiv (2026)\nIn recent years, mobile applications have become indispensable tools for managing various aspects of life. From enhancing productivity to providing personalized entertainment, mobile apps have revolutionized people‚Äôs daily routines. Despite this rapid growth and popularity, gaps remain in how these apps address the needs of users from different age groups. Users of varying ages face distinct challenges when interacting with mobile apps, from younger users dealing with inappropriate content to older users having difficulty with usability due to age-related vision and cognition impairments. Although there have been initiatives to create age-inclusive apps, a limited understanding of user perspectives on age-related issues may hinder developers from recognizing specific challenges and implementing effective solutions. In this study, we explore age discussions in app reviews to gain insights into how mobile apps should cater to users across different age groups.We manually curated a dataset of 4,163 app reviews from the Google Play Store and identified 1,429 age-related reviews and 2,734 non-age-related reviews. We employed eight machine learning, deep learning, and large language models to automatically detect age discussions, with RoBERTa performing the best, achieving a precision of 92.46%. Additionally, a qualitative analysis of the 1,429 age-related reviews uncovers six dominant themes reflecting user concerns.\nüìÑ Download PDF\nComparative Assessment of Look-Ahead Economic Dispatch and Ramp Products for Grid Flexibility Authors: Qian Zhang, Le Xie, Long Zhao, Congcong Wang Venue: arXiv (2026)\nHigh renewable penetration increases the frequency and magnitude of net-load ramps, stressing real-time flexibility. Two commonly deployed remedies are look-ahead economic dispatch (LAED) and ramp products (RPs), yet their operational equivalence under the industry-standard rolling-window dispatch implementation is not well understood. This paper develops linear optimization models for multi-interval LAED and RP-based co-optimization, and proves that an enhanced RP formulation can match LAED‚Äôs dispatch feasible region at a single time step when additional intertemporal deliverability constraints are enforced. We then show that this equivalence does not generally persist under rolling-window operation because LAED and RP formulations optimize different intertemporal objectives, leading to divergent end-of-window states. Using different test systems under stressed ramping conditions and multiple load levels, we show LAED achieves similar or lower load shedding than RP implementations with the same look-ahead horizon, with the most pronounced differences under high-load, ramp-limited conditions. The study highlights the limitations of current ramp product implementations and suggests enhancements, such as introducing more mid-duration RPs.\nüìÑ Download PDF\nA Gradient-Based Capacity Accreditation Framework in Resource Adequacy: Formulation, Computation, and Practical Implications Authors: Qian Zhang, Feng Zhao, Gord Stephen, Chanan Singh, Le Xie Venue: arXiv (2026)\nProbabilistic resource adequacy assessment is a cornerstone of modern capacity accreditation. This paper develops a gradient-based framework, in which capacity accreditation is interpreted as the directional derivative of a probabilistic resource adequacy metric with respect to resource capacity, that unifies two widely used accreditation approaches: Effective Load Carrying Capability (ELCC) and Marginal Reliability Impact (MRI). Under mild regularity conditions, we show that marginal ELCC and MRI yield equivalent accreditation factors, while their numerical implementations exhibit markedly different computational characteristics. Building on this framework, we demonstrate how infinitesimal perturbation analysis enables up to a $1000\\times$ speedup in gradient estimation for capacity accreditation, and we implement gradient-informed search algorithms that significantly accelerate ELCC computations relative to standard bisection methods. Large-scale Monte Carlo experiments show that MRI achieves substantial runtime reductions compared to ELCC and exhibits greater robustness to perturbation step-size selection. These results provide practical guidance for implementing efficient and scalable capacity accreditation in large-scale power systems.\nüìÑ Download PDF\nVolt/VAR Optimization in Transmission Networks with Discrete-Control Devices Authors: Shuaicheng Tong, Michael A. Boateng, Mathieu Tanneau, Pascal Van Hentenryck Venue: arXiv (2026)\nVoltage (Volt) and reactive-power (VAR) control in transmission networks is critical for reliability and increasingly needs fast, implementable decisions. This paper presents a transmission Volt/VAR Optimization (VVO) framework that co-optimizes discrete control of on-load tap-changing transformers (OLTCs) and capacitor banks (CBs) with AC power flow (ACPF) physics to improve voltage stability and minimize VAR generation. The framework follows a relax-round-resolve pipeline: a continuous relaxation proposes targets, a rounding step selects feasible discrete settings, and a final solve enforces AC power flow physics. Extensive experiments on IEEE, PEGASE, and RTE systems show consistent improvements in voltage and VAR quality metrics with modest generator redispatch while preserving economic operation and achieving compatible runtimes with real-time transmission operations.\nüìÑ Download PDF\nLoRIF: Low-Rank Influence Functions for Scalable Training Data Attribution Authors: Shuangqi Li, Hieu Le, Jingyi Xu, Mathieu Salzmann Venue: arXiv (2026)\nTraining data attribution (TDA) identifies which training examples most influenced a model‚Äôs prediction. The best-performing TDA methods exploits gradients to define an influence function. To overcome the scalability challenge arising from gradient computation, the most popular strategy is random projection (e.g., TRAK, LoGRA). However, this still faces two bottlenecks when scaling to large training sets and high-quality attribution: \\emph{(i)} storing and loading projected per-example gradients for all $N$ training examples, where query latency is dominated by I/O; and \\emph{(ii)} forming the $D \\times D$ inverse Hessian approximation, which costs $O(D^2)$ memory. Both bottlenecks scale with the projection dimension $D$, yet increasing $D$ is necessary for attribution quality ‚Äì creating a quality-scalability tradeoff. We introduce \\textbf{LoRIF (Low-Rank Influence Functions)}, which exploits low-rank structures of gradient to address both bottlenecks. First, we store rank-$c$ factors of the projected per-example gradients rather than full matrices, reducing storage and query-time I/O from $O(D)$ to $O(c\\sqrt{D})$ per layer per sample. Second, we use truncated SVD with the Woodbury identity to approximate the Hessian term in an $r$-dimensional subspace, reducing memory from $O(D^2)$ to $O(Dr)$. On models from 0.1B to 70B parameters trained on datasets with millions of examples, LoRIF achieves up to 20$\\times$ storage reduction and query-time speedup compared to LoGRA, while matching or exceeding its attribution quality. LoRIF makes gradient-based TDA practical at frontier scale.\nüìÑ Download PDF\nüîç llm World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems Authors: Lakshya Gupta, Litao Li, Yizhe Liu, Sriram Ganapathi Subramanian, Kaheer Suleman, Zichen Zhang, Haoye Lu, Sumit Pasupalak Venue: arXiv (2026)\nFrontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.\nüìÑ Download PDF\nCreative Image Generation with Diffusion Model Authors: Kunpeng Song, Ahmed Elgammal Venue: arXiv (2026)\nCreative image generation has emerged as a compelling area of research, driven by the need to produce novel and high-quality images that expand the boundaries of imagination. In this work, we propose a novel framework for creative generation using diffusion models, where creativity is associated with the inverse probability of an image‚Äôs existence in the CLIP embedding space. Unlike prior approaches that rely on a manual blending of concepts or exclusion of subcategories, our method calculates the probability distribution of generated images and drives it towards low-probability regions to produce rare, imaginative, and visually captivating outputs. We also introduce pullback mechanisms, achieving high creativity without sacrificing visual fidelity. Extensive experiments on text-to-image diffusion models demonstrate the effectiveness and efficiency of our creative generation framework, showcasing its ability to produce unique, novel, and thought-provoking images. This work provides a new perspective on creativity in generative models, offering a principled method to foster innovation in visual content synthesis.\nüìÑ Download PDF\nPrior-Informed Flow Matching for Graph Reconstruction Authors: Harvey Chen, Nicolas Zilberstein, Santiago Segarra Venue: arXiv (2026)\nWe introduce Prior-Informed Flow Matching (PIFM), a conditional flow model for graph reconstruction. Reconstructing graphs from partial observations remains a key challenge; classical embedding methods often lack global consistency, while modern generative models struggle to incorporate structural priors. PIFM bridges this gap by integrating embedding-based priors with continuous-time flow matching. Grounded in a permutation equivariant version of the distortion-perception theory, our method first uses a prior, such as graphons or GraphSAGE/node2vec, to form an informed initial estimate of the adjacency matrix based on local information. It then applies rectified flow matching to refine this estimate, transporting it toward the true distribution of clean graphs and learning a global coupling. Experiments on different datasets demonstrate that PIFM consistently enhances classical embeddings, outperforming them and state-of-the-art generative baselines in reconstruction accuracy.\nüìÑ Download PDF\nüîç neuroscience AIRPET: Virtual Positron Emission Tomography Authors: J. Renner, J. J. G√≥mez-Cadenas, R. Soleti Venue: arXiv (2026)\nPositron Emission Tomography (PET) is a powerful medical imaging technique, but the design and evaluation of new PET scanner technologies present significant challenges. The process is typically divided into three major stages: 1. detector design and simulation, 2. image reconstruction, and 3. image interpretation. Each of these stages requires significant expertise, making it difficult for individuals or small teams to manage all three at once. AIRPET (AI-driven Revolution in Positron Emission Tomography) is a web-based platform designed to address this challenge by integrating all phases of PET design into a single, accessible, and AI-assisted workflow. AIRPET provides an interface to large language models (LLMs) for assisted geometry creation and an interface for basic PET image reconstruction with the potential for further expansion. Here we introduce AIRPET and outline its current functionality and proposed additions.\nüìÑ Download PDF\nMetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources Authors: Baorui Ma, Jiahui Yang, Donglin Di, Xuancheng Zhang, Jianxun Cui, Hao Li, Yan Xie, Wei Chen Venue: arXiv (2026)\nScaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.\nüìÑ Download PDF\nMolecular structure, binding, and disorder in TDBC-Ag plexcitonic assemblies Authors: J. Ba√±os-Guti√©rrez, R. Bercy, Y. Garc√≠a Jomaso, S. Balci, G. Pirruccio, J. Halldin Stenlid, M. J. Llansola-Portoles, D. Finkelstein-Shapiro Venue: arXiv (2026)\nPlexcitonic assemblies are hybrid materials composed of a plasmonic nanoparticle and molecular or semiconducting emitters whose electronic transitions are strongly coupled to the plasmonic mode. This coupling hybridizes the system modes into upper and lower polariton branches. The strength of the interaction depends on the number of emitters and on their orientation and spatial arrangement relative to the metallic surface. These structural factors have profound consequences for the ensuing photoexcited dynamics. Despite the extensive spectroscopic work on plexcitonic systems, direct understanding of the molecular geometry at the metal interface remains limited. In this work, we present a comprehensive structural characterization of one of the most widely studied plexcitons formed by the cyanine dye 5,5‚Äô,6,6‚Äô-tetrachloro-1,1‚Äô-diethyl-3,3‚Äô-di(4-sulfobutyl)-benzimidazolocarbocyanine (TDBC) and silver nanoprisms using a combination of NMR, THz-Raman spectroscopy, and DFT calculations. By comparing the signals from the monomeric and aggregated forms of TDBC with that of the plexciton, we identify shared spectral fingerprints that reveal how molecular packing is modified when the aggregate adsorbs on the silver surface. We observe Raman modes specific to plexciton systems, and identify NOESY cross-peaks in the aliphatic region, that along with THz-Raman modes in the 10-400 cm$^{-1}$ region are sensitive indicators of aggregation geometry and adsorption. We find that isolated TDBC monomers adopt an asymmetric conformation in which both sulfobutyl chains lie on the same side of the chromophore, while J-aggregates adopt a symmetric up-down alternation of the chains from molecule to molecule. This work establishes the molecular geometry of a prototypical TDBC-silver plexciton, providing a structural benchmark for understanding geometry-dependent photophysics in hybrid exciton-plasmon systems.\nüìÑ Download PDF\nLiquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems Authors: Dhiogo de S√°, Carlos Schmiedel, Carlos Pereira Lopes Venue: arXiv (2026)\nContemporary software architectures struggle to support autonomous agents whose reasoning is adaptive, probabilistic, and context-dependent, while system integration remains dominated by static interfaces and deterministic contracts. This paper introduces Liquid Interfaces, a coordination paradigm in which interfaces are not persistent technical artifacts, but ephemeral relational events that emerge through intention articulation and semantic negotiation at runtime.We formalize this model and present the Liquid Interface Protocol (LIP),which governs intention-driven interaction, negotiated execution, and enforce ephemerality under semantic uncertainty. We further discuss the governance implications of this approach and describe a reference architecture that demonstrates practical feasibility. Liquid Interfaces provide a principled foundation for adaptive coordination in agent-based systems\nüìÑ Download PDF\nHow do Visual Attributes Influence Web Agents? A Comprehensive Evaluation of User Interface Design Factors Authors: Kuai Yu, Naicheng Yu, Han Wang, Rui Yang, Huan Zhang Venue: arXiv (2026)\nWeb agents have demonstrated strong performance on a wide range of web-based tasks. However, existing research on the effect of environmental variation has mostly focused on robustness to adversarial attacks, with less attention to agents‚Äô preferences in benign scenarios. Although early studies have examined how textual attributes influence agent behavior, a systematic understanding of how visual attributes shape agent decision-making remains limited. To address this, we introduce VAF, a controlled evaluation pipeline for quantifying how webpage Visual Attribute Factors influence web-agent decision-making. Specifically, VAF consists of three stages: (i) variant generation, which ensures the variants share identical semantics as the original item while only differ in visual attributes; (ii) browsing interaction, where agents navigate the page via scrolling and clicking the interested item, mirroring how human users browse online; (iii) validating through both click action and reasoning from agents, which we use the Target Click Rate and Target Mention Rate to jointly evaluate the effect of visual attributes. By quantitatively measuring the decision-making difference between the original and variant, we identify which visual attributes influence agents‚Äô behavior most. Extensive experiments, across 8 variant families (48 variants total), 5 real-world websites (including shopping, travel, and news browsing), and 4 representative web agents, show that background color contrast, item size, position, and card clarity have a strong influence on agents‚Äô actions, whereas font styling, text color, and item image clarity exhibit minor effects.\nüìÑ Download PDF\nüîç data_resources PI-Light: Physics-Inspired Diffusion for Full-Image Relighting Authors: Zhexin Liang, Zhaoxi Chen, Yongwei Chen, Tianyi Wei, Tengfei Wang, Xingang Pan Venue: arXiv (2026)\nFull-image relighting remains a challenging problem due to the difficulty of collecting large-scale structured paired data, the difficulty of maintaining physical plausibility, and the limited generalizability imposed by data-driven priors. Existing attempts to bridge the synthetic-to-real gap for full-scene relighting remain suboptimal. To tackle these challenges, we introduce Physics-Inspired diffusion for full-image reLight ($œÄ$-Light, or PI-Light), a two-stage framework that leverages physics-inspired diffusion models. Our design incorporates (i) batch-aware attention, which improves the consistency of intrinsic predictions across a collection of images, (ii) a physics-guided neural rendering module that enforces physically plausible light transport, (iii) physics-inspired losses that regularize training dynamics toward a physically meaningful landscape, thereby enhancing generalizability to real-world image editing, and (iv) a carefully curated dataset of diverse objects and scenes captured under controlled lighting conditions. Together, these components enable efficient finetuning of pretrained diffusion models while also providing a solid benchmark for downstream evaluation. Experiments demonstrate that $œÄ$-Light synthesizes specular highlights and diffuse reflections across a wide variety of materials, achieving superior generalization to real-world scenes compared with prior approaches.\nüìÑ Download PDF\nPreliminary Results of a Scoping Review on Assistive Technologies for Adults with ADHD Authors: Valerie Tan, Luisa Jost, Jens Gerken, Max Pascher Venue: arXiv (2026)\nAttention Deficit Hyperactivity Disorder (ADHD), characterized by inattention, hyperactivity, and impulsivity, is prevalent in the adult population. Long perceived and treated as a childhood condition, ADHD and its characteristics nonetheless impact a significant portion of adults today. In contrast to children with ADHD, adults with ADHD face unique challenges in the workplace and in higher education. In this work-in-progress paper, we present a scoping review as a foundation to understand and explore existing technology-based approaches to support adults with ADHD. In total, our search returned 3,538 papers upon which we selected, based on PRISMA-ScR, a total of 46 papers for in-depth analysis. Our initial findings highlight that most papers take on a therapeutic or intervention perspective instead of a more positive support perspective. Our analysis also found a tremendous increase in recent papers on the topic, which highlights that more and more researchers are becoming aware of the need to address ADHD with adults. For the future, we aim to further analyze the corpus and identify research gaps and potentials for further development of ADHD assistive technologies.\nüìÑ Download PDF\nStepShield: When, Not Whether to Intervene on Rogue Agents Authors: Gloria Felicia, Michael Eniolade, Jinfeng He, Zitha Sasindran, Hemant Kumar, Milan Hussain Angati, Sandeep Bandarupalli Venue: arXiv (2026)\nExisting agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.\nüìÑ Download PDF\nClarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models Authors: Konstantinos P. Panousis, Diego Marcos Venue: arXiv (2026)\nThe widespread adoption of Vision-Language Models (VLMs) across fields has amplified concerns about model interpretability. Distressingly, these models are often treated as black-boxes, with limited or non-existent investigation of their decision making process. Despite numerous post- and ante-hoc interepretability methods, systematic and objective evaluation of the learned representations remains limited, particularly for sparsity-aware methods that are increasingly considered to ‚Äúinduce interpretability‚Äù. In this work, we focus on Concept Bottleneck Models and investigate how different modeling decisions affect the emerging representations. We introduce the notion of clarity, a measure, capturing the interplay between the downstream performance and the sparsity and precision of the concept representation, while proposing an interpretability assessment framework using datasets with ground truth concept annotations. We consider both VLM- and attribute predictor-based CBMs, and three different sparsity-inducing strategies: per example $\\ell_1, \\ell_0$ and Bernoulli-based formulations. Our experiments reveal a critical trade-off between flexibility and interpretability, under which a given method can exhibit markedly different behaviors even at comparable performance levels. The code will be made publicly available upon publication.\nüìÑ Download PDF\nRetrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities Authors: Shuangshuang Ying, Zheyu Wang, Yunjian Peng, Jin Chen, Yuhao Wu, Hongbin Lin, Dingyu He, Siyi Liu, Gengchen Yu, YinZhu Piao, Yuchen Wu, Xin Gui, Zhongyuan Peng, Xin Li, Xeron Du, Libo Qin, YiXin Cao, Ge Zhang Venue: arXiv (2026)\nDespite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility. We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search: multi-step synthesis, denoising, and evidence-based conclusion making. DeR2 decouples evidence access from reasoning via four regimes‚ÄìInstruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)‚Äìyielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution. To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability. To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales. Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.\nüìÑ Download PDF\nWebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents Authors: Yao Zhang, Shijie Tang, Zeyu Li, Zhen Han, Volker Tresp Venue: arXiv (2026)\nWeb agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.\nüìÑ Download PDF\nMMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Authors: Honglin Lin, Zheng Liu, Yun Zhu, Chonghan Qin, Juekai Lin, Xiaoran Shang, Conghui He, Wentao Zhang, Lijun Wu Venue: arXiv (2026)\nRecent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a ‚Äúless is more‚Äù phenomenon via our difficulty-aware filtering strategy: a subset of just 7% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.\nüìÑ Download PDF\nüîç emotion_language The algebraic and geometric classification of derived Jordan and bicommutative algebras Authors: Hani Abdelwahab, Ivan Kaygorodov, Roman Lubkov Venue: arXiv (2026)\nWe developed a new proper method for classifying $n$-dimensional derived Jordan algebras, and apply it to the classification of $3$-dimensional derived Jordan algebras. As a byproduct, we have the algebraic classification of $3$-dimensional metabelian commutative algebras and $3$-dimensional derived commutative associative algebras. After that, we introduced a method of classifying $n$-dimensional bicommutative algebras, based on the classification of $n$-dimensional derived commutative associative algebras, and applied it to the classification of $3$-dimensional bicommutative algebras. The second part of the paper is dedicated to the geometric classification of $3$-dimensional metabelian commutative, derived commutative associative, derived Jordan and bicommutative algebras.\nüìÑ Download PDF\nInvestigating Associational Biases in Inter-Model Communication of Large Generative Models Authors: Fethiye Irmak Dogan, Yuval Weiss, Kajal Patel, Jiaee Cheong, Hatice Gunes Venue: arXiv (2026)\nSocial bias in generative AI can manifest not only as performance disparities but also as associational bias, whereby models learn and reproduce stereotypical associations between concepts and demographic groups, even in the absence of explicit demographic information (e.g., associating doctors with men). These associations can persist, propagate, and potentially amplify across repeated exchanges in inter-model communication pipelines, where one generative model‚Äôs output becomes another‚Äôs input. This is especially salient for human-centred perception tasks, such as human activity recognition and affect prediction, where inferences about behaviour and internal states can lead to errors or stereotypical associations that propagate into unequal treatment. In this work, focusing on human activity and affective expression, we study how such associations evolve within an inter-model communication pipeline that alternates between image generation and image description. Using the RAF-DB and PHASE datasets, we quantify demographic distribution drift induced by model-to-model information exchange and assess whether these drifts are systematic using an explainability pipeline. Our results reveal demographic drifts toward younger representations for both actions and emotions, as well as toward more female-presenting representations, primarily for emotions. We further find evidence that some predictions are supported by spurious visual regions (e.g., background or hair) rather than concept-relevant cues (e.g., body or face). We also examine whether these demographic drifts translate into measurable differences in downstream behaviour, i.e., while predicting activity and emotion labels. Finally, we outline mitigation strategies spanning data-centric, training and deployment interventions, and emphasise the need for careful safeguards when deploying interconnected models in human-centred AI systems.\nüìÑ Download PDF\nOn set-theoretic solutions of pentagon equation and positive basis Hopf algebras Authors: Ilaria Colazzo, Geoffrey Janssens Venue: arXiv (2026)\nWe investigate the connection between bijective, not necessarily finite, set-theoretic solutions of the pentagon equation and Hopf algebras. Firstly, we prove that finite solutions correspond to Hopf algebras with the positive basis property. As a corollary we generalise Lu-Yan-Zhu classification to arbitrary characteristic $0$ fields $k$. Secondly, we study the general problem of when a Hopf algebra has a basis yielding a set-theoretic solution. Finally, we classify all (co)commutative bijective solutions. This result requires to obtain a description of all bases of a group algebra $k[G]$ yielding a set-theoretic solution. We namely show that such bases correspond, through a Fourier transform, to splittings $A \\rtimes N$ of $G$ with $A$ a finite abelian group.\nüìÑ Download PDF\nOn homogeneous involutions on matrix algebras Authors: Micael Said Garcia, Cassia Ferreira Sampaio Venue: arXiv (2026)\nWe study the homogeneous involutions on the full square matrices over an algebraically closed field endowed with a division grading with commutative support. We obtain the classification of the isomorphism and equivalence classes for the Pauli grading. We also investigate the homogeneous involutions on the full square matrices with entries in a finite-dimensional graded-division algebra over an algebraically closed field of characteristic not $2$ endowed with an arbitrary grading by an arbitrary group.\nüìÑ Download PDF\nQuantum fluctuations in hydrodynamics and quantum long-time tails Authors: Akash Jain Venue: arXiv (2026)\nWe construct a quantum Schwinger-Keldysh (SK) effective field theory for the diffusive hydrodynamics of a conserved scalar field. Quantum corrections within the SK framework are guided by fluctuation-dissipation relations, enforced via a dynamical Kubo-Martin-Schwinger (KMS) symmetry. We find that the KMS symmetry necessarily generates fluctuation contributions in the SK effective action at all orders in the noise field, thereby giving rise to intrinsically non-Gaussian noise. We use our results to compute one-loop quantum corrections to the two-point density-density retarded correlation function, leading to a quantum generalization of hydrodynamic long-time tails. Our results apply at arbitrarily high orders in $\\hbar$. The one-loop results for retarded correlation functions have been expressed in terms of a family of polynomials. We also provide a closed-form expression for the one-loop results at leading order in the wavevector expansion.\nüìÑ Download PDF\nMicrolocal maximal hypoellipticity from the geometric viewpoint: I Authors: Omar Mohsen Venue: arXiv (2026)\nGiven some vector fields on a smooth manifold satisfying H√∂rmander‚Äôs condition, we define a bi-graded pseudo-differential calculus which contains the classical pseudo-differential calculus and a pseudo-differential calculus adapted to the sub-Riemannian structure induced by the vector fields. Our approach is based on geometric constructions (resolution of singularities) together with methods from operators algebras. We develop this calculus in full generality, including Sobolev spaces, the wavefront set, and the principal symbol, etc. In particular, using this calculus, we prove that invertibility of the principal symbol implies microlocal maximal hypoellipticity. This allows us to resolve affirmatively the microlocal version of a conjecture of Helffer and Nourrigat.\nüìÑ Download PDF\nPay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference Authors: Ziming Dong, Hardik Sharma, Evan O‚ÄôToole, Jaya Prakash Champati, Kui Wu Venue: arXiv (2026)\nLarge Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in accuracy. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a complete response at full cost. We introduce LLM Shepherding, a framework that requests only a short prefix (a hint) from the LLM and provides it to SLM. This simple mechanism is surprisingly effective for math and coding tasks: even hints comprising 10-30% of the full LLM response improve SLM accuracy significantly. Shepherding generalizes both routing and cascading, and it achieves lower cost under oracle decision-making. We develop a two-stage predictor that jointly determines whether a hint is needed and how many tokens to request. On the widely-used mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks, Shepherding reduces costs by 42-94% relative to LLM-only inference. Compared to state-of-the-art routing and cascading baselines, shepherding delivers up to 2.8x cost reduction while matching accuracy. To our knowledge, this is the first work to exploit token-level budget control for SLM-LLM collaboration.\nüìÑ Download PDF\nSocial Media Data for Population Mapping: A Bayesian Approach to Address Representativeness and Privacy Challenges Authors: Paolo Andrich, Shengjie Lai, Halim Jun, Qianwen Duan, Zhifeng Cheng, Seth R. Flaxman, Andrew J. Tatem Venue: arXiv (2026)\nAccurate and timely population data are essential for disaster response and humanitarian planning, but traditional censuses often cannot capture rapid demographic changes. Social media data offer a promising alternative for dynamic population monitoring, but their representativeness remains poorly understood and stringent privacy requirements limit their reliability. Here, we address these limitations in the context of the Philippines by calibrating Facebook user counts with the country‚Äôs 2020 census figures. First, we find that differential privacy techniques commonly applied to social media-based population datasets disproportionately mask low-population areas. To address this, we propose a Bayesian imputation approach to recover missing values, restoring data coverage for $5.5%$ of rural areas. Further, using the imputed social media data and leveraging predictors such as urbanisation level, demographic composition, and socio-economic status, we develop a statistical model for the proportion of Facebook users in each municipality, which links observed Facebook user numbers to the true population levels. Out-of-sample validation demonstrates strong result generalisability, with errors as low as ${\\approx}18%$ and ${\\approx}24%$ for urban and rural Facebook user proportions, respectively. We further demonstrate that accounting for overdispersion and spatial correlations in the data is crucial to obtain accurate estimates and appropriate credible intervals. Crucially, as predictors change over time, the models can be used to regularly update the population predictions, providing a dynamic complement to census-based estimates. These results have direct implications for humanitarian response in disaster-prone regions and offer a general framework for using biased social media signals to generate reliable and timely population data.\nüìÑ Download PDF\nInverted anisotropy of the partially screened magnetic impurity Authors: Krzysztof P. W√≥jcik, Micha≈Ç P. Kwasigroch Venue: arXiv (2026)\nWe investigate a single magnetic impurity in the presence of strong spin-orbit coupling and single-ion anisotropy. We show that at sufficiently strong coupling there exists a finite temperature window, before the moment is completely screened, where the magnetic anisotropy of the system flips: the hard-axis becomes the easy-axis or vice versa. We derive this rigorously for a single impurity using numerical renormalization group calculations as well as Nozieres‚Äô strong-coupling limit and discuss its relevance to heavy-fermion compounds which order magnetically along the hard-direction. We show that the coexistence of Curie-like response and Kondo fluctuations is stabilized along the initially hard direction leading to the anisotropy switch.\nüìÑ Download PDF\nEarly and Prediagnostic Detection of Pancreatic Cancer from Computed Tomography Authors: Wenxuan Li, Pedro R. A. S. Bassi, Lizhou Wu, Xinze Zhou, Yuxuan Zhao, Qi Chen, Szymon Plotka, Tianyu Lin, Zheren Zhu, Marisa Martin, Justin Caskey, Shanshan Jiang, Xiaoxi Chen, Jaroslaw B. ƒÜwikla, Artur Sankowski, Yaping Wu, Sergio Decherchi, Andrea Cavalli, Chandana Lall, Cristian Tomasetti, Yaxing Guo, Xuan Yu, Yuqing Cai, Hualin Qiao, Jie Bao, Chenhan Hu, Ximing Wang, Arkadiusz Sitek, Kai Ding, Heng Li, Meiyun Wang, Dexin Yu, Guang Zhang, Yang Yang, Kang Wang, Alan L. Yuille, Zongwei Zhou Venue: arXiv (2026)\nPancreatic ductal adenocarcinoma (PDAC), one of the deadliest solid malignancies, is often detected at a late and inoperable stage. Retrospective reviews of prediagnostic CT scans, when conducted by expert radiologists aware that the patient later developed PDAC, frequently reveal lesions that were previously overlooked. To help detecting these lesions earlier, we developed an automated system named ePAI (early Pancreatic cancer detection with Artificial Intelligence). It was trained on data from 1,598 patients from a single medical center. In the internal test involving 1,009 patients, ePAI achieved an area under the receiver operating characteristic curve (AUC) of 0.939-0.999, a sensitivity of 95.3%, and a specificity of 98.7% for detecting small PDAC less than 2 cm in diameter, precisely localizing PDAC as small as 2 mm. In an external test involving 7,158 patients across 6 centers, ePAI achieved an AUC of 0.918-0.945, a sensitivity of 91.5%, and a specificity of 88.0%, precisely localizing PDAC as small as 5 mm. Importantly, ePAI detected PDACs on prediagnostic CT scans obtained 3 to 36 months before clinical diagnosis that had originally been overlooked by radiologists. It successfully detected and localized PDACs in 75 of 159 patients, with a median lead time of 347 days before clinical diagnosis. Our multi-reader study showed that ePAI significantly outperformed 30 board-certified radiologists by 50.3% (P \u003c 0.05) in sensitivity while maintaining a comparable specificity of 95.4% in detecting PDACs early and prediagnostic. These findings suggest its potential of ePAI as an assistive tool to improve early detection of pancreatic cancer.\nüìÑ Download PDF\nInformation-geometry-driven graph sequential growth Authors: Harry T. Bond, Bertrand Gauthier, Kirstin Strokorb Venue: arXiv (2026)\nWe investigate the properties of a class of regularisation-free approaches for Gaussian graphical inference based on the information-geometry-driven sequential growth of initially edgeless graphs. Relating the growth of a graph to a coordinate descent process, we characterise the fully-corrective descents corresponding to information-optimal growths, and propose numerically efficient strategies for their approximation. We demonstrate the ability of the proposed procedures to reliably extract sparse graphical models while limiting the number of false detections, and illustrate how activation ranks can provide insight into the informational relevance of edge sets. The considered approaches are tuning-parameter-free and have complexities akin to coordinate descents.\nüìÑ Download PDF\nAuditorily Embodied Conversational Agents: Effects of Spatialization and Situated Audio Cues on Presence and Social Perception Authors: Yi Fei Cheng, Jarod Bloch, Alexander Wang, Andrea Bianchi, Anusha Withana, Anhong Guo, Laurie M. Heller, David Lindlbauer Venue: arXiv (2026)\nEmbodiment can enhance conversational agents, such as increasing their perceived presence. This is typically achieved through visual representations of a virtual body; however, visual modalities are not always available, such as when users interact with agents using headphones or display-less glasses. In this work, we explore auditory embodiment. By introducing auditory cues of bodily presence - through spatially localized voice and situated Foley audio from environmental interactions - we investigate how audio alone can convey embodiment and influence perceptions of a conversational agent. We conducted a 2 (spatialization: monaural vs. spatialized) x 2 (Foley: none vs. Foley) within-subjects study, where participants (n=24) engaged in conversations with agents. Our results show that spatialization and Foley increase co-presence, but reduce users‚Äô perceptions of the agent‚Äôs attention and other social attributes.\nüìÑ Download PDF\nIndustrialized Deception: The Collateral Effects of LLM-Generated Misinformation on Digital Ecosystems Authors: Alexander Loth, Martin Kappes, Marc-Oliver Pahl Venue: arXiv (2026)\nGenerative AI and misinformation research has evolved since our 2024 survey. This paper presents an updated perspective, transitioning from literature review to practical countermeasures. We report on changes in the threat landscape, including improved AI-generated content through Large Language Models (LLMs) and multimodal systems. Central to this work are our practical contributions: JudgeGPT, a platform for evaluating human perception of AI-generated news, and RogueGPT, a controlled stimulus generation engine for research. Together, these tools form an experimental pipeline for studying how humans perceive and detect AI-generated misinformation. Our findings show that detection capabilities have improved, but the competition between generation and detection continues. We discuss mitigation strategies including LLM-based detection, inoculation approaches, and the dual-use nature of generative AI. This work contributes to research addressing the adverse impacts of AI on information quality.\nüìÑ Download PDF\nUser Acceptance Model for Smart Incentives in Sustainable Video Streaming towards 6G Authors: Konstantinos Varsos, Adamantia Stamou, George D. Stamoulis, Vasillios A. Siris Venue: arXiv (2026)\nThe rapid growth of 5G video streaming is intensifying energy consumption across access, core, and data-center networks, underscoring the critical need for energy and carbon-efficient solutions. While reducing streaming bitrates improves energy efficiency, its success hinges on user acceptance‚Äìparticularly when lower bitrates may be perceived as reduced quality of experience (QoE). Therefore, there is a need to develop transparent, user-centric incentive models that balance sustainability with perceived value. We propose a user-acceptance model that combines diverse environmental awareness, personalized responsiveness to incentives, and varying levels of altruism into a unified probabilistic framework. The model incorporates dynamic, individualized incentives that adapt over time. We further enhance the framework by incorporating (i) social well-being as a motivator for altruistic choices, (ii) provider-driven education strategies that gradually adjust user acceptance thresholds, and (iii) data-driven learning of user traits from historical offer‚Äìresponse interactions. Extensive synthetic-data experiments reveal the trade-offs between provider cost and network flexibility, showing that personalized incentives and gradual behavioral adaptation can advance sustainability targets without compromising stakeholder requirements.\nüìÑ Download PDF\nKnowBias: Mitigating Social Bias in LLMs via Know-Bias Neuron Enhancement Authors: Jinhao Pan, Chahat Raj, Anjishnu Mukherjee, Sina Mansouri, Bowen Wei, Shloka Yada, Ziwei Zhu Venue: arXiv (2026)\nLarge language models (LLMs) exhibit social biases that reinforce harmful stereotypes, limiting their safe deployment. Most existing debiasing methods adopt a suppressive paradigm by modifying parameters, prompts, or neurons associated with biased behavior; however, such approaches are often brittle, weakly generalizable, data-inefficient, and prone to degrading general capability. We propose \\textbf{KnowBias}, a lightweight and conceptually distinct framework that mitigates bias by strengthening, rather than suppressing, neurons encoding bias-knowledge. KnowBias identifies neurons encoding bias knowledge using a small set of bias-knowledge questions via attribution-based analysis, and selectively enhances them at inference time. This design enables strong debiasing while preserving general capabilities, generalizes across bias types and demographics, and is highly data efficient, requiring only a handful of simple yes/no questions and no retraining. Experiments across multiple benchmarks and LLMs demonstrate consistent state-of-the-art debiasing performance with minimal utility degradation. Data and code are available at https://github.com/JP-25/KnowBias.\nüìÑ Download PDF\nPutting Pressure Under Pressure: On the Status of Classical Pressure in Special Relativity Authors: Eugene Y. S. Chua Venue: arXiv (2026)\nMuch of the century-old debate surrounding the status of thermodynamics in relativity has centered on the search for a suitably relativistic temperature; recent works by Chua (2023) and Chua and Callender (forthcoming) have suggested that the classical temperature concept ‚Äì consilient as it is in classical settings ‚Äì ‚Äòfalls apart‚Äô in relativity. However, these discussions typically assume an unproblematic Lorentz transformation for ‚Äì specifically, the Lorentz invariance of ‚Äì the pressure concept. Here I argue that, just like the classical temperature, the classical concept of pressure breaks down in relativistic settings. I discuss how this might suggest a new thermodynamic limit ‚Äì a u ‚Äì\u003e 0 limit ‚Äì without which an unambiguous thermodynamic description of systems doesn‚Äôt emerge.\nüìÑ Download PDF\nA Federated and Parameter-Efficient Framework for Large Language Model Training in Medicine Authors: Anran Li, Yuanyuan Chen, Wenjun Long, Yu Yin, Yan Hu, Hyunjae Kim, Weipeng Zhou, Yujia Zhou, Hongyi Peng, Yang Ren, Xuguang Ai, Zhenyue Qin, Ming Hu, Xiaoxiao Li, Han Yu, Yih-Chung Tham, Lucila Ohno-Machado, Hua Xu, Qingyu Chen Venue: arXiv (2026)\nLarge language models (LLMs) have demonstrated strong performance on medical benchmarks, including question answering and diagnosis. To enable their use in clinical settings, LLMs are typically further adapted through continued pretraining or post-training using clinical data. However, most medical LLMs are trained on data from a single institution, which faces limitations in generalizability and safety in heterogeneous systems. Federated learning (FL) is a promising solution for enabling collaborative model development across healthcare institutions. Yet applying FL to LLMs in medicine remains fundamentally limited. First, conventional FL requires transmitting the full model during each communication round, which becomes impractical for multi-billion-parameter LLMs given the limited computational resources. Second, many FL algorithms implicitly assume data homogeneity, whereas real-world clinical data are highly heterogeneous across patients, diseases, and institutional practices. We introduce the model-agnostic and parameter-efficient federated learning framework for adapting LLMs to medical applications. Fed-MedLoRA transmits only low-rank adapter parameters, reducing communication and computation overhead, while Fed-MedLoRA+ further incorporates adaptive, data-aware aggregation to improve convergence under cross-site heterogeneity. We apply the framework to clinical information extraction (IE), which transforms patient narratives into structured medical entities and relations. Accuracy was assessed across five patient cohorts through comparisons with BERT models, and LLaMA-3 and DeepSeek-R1, GPT-4o models. Evaluation settings included (1) in-domain training and testing, (2) external validation on independent cohorts, and (3) a low-resource new-site adaptation scenario using real-world clinical notes from the Yale New Haven Health System.\nüìÑ Download PDF\n","wordCount":"24538","inLanguage":"en","datePublished":"2026-02-01T15:30:48.807773Z","dateModified":"2026-02-01T15:30:48.807773Z","author":{"@type":"Person","name":"Gary"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://garyforreal.me/en/posts/paper/paper-2026-02-01-weekly/"},"publisher":{"@type":"Organization","name":"Gary's House","logo":{"@type":"ImageObject","url":"https://garyforreal.me/img/Q.jpg"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><script src=https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://garyforreal.me/en/ accesskey=h title="Gary's Blog (Alt + H)"><img src=https://garyforreal.me/img/me.jpg alt aria-label=logo height=35>Gary's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://garyforreal.me/zh/ title=‰∏≠Êñá aria-label=‰∏≠Êñá>‰∏≠Êñá</a></li></ul></div></div><ul id=menu><li><a href=https://garyforreal.me/en/search title="üîçSearch (Alt + /)" accesskey=/><span>üîçSearch</span></a></li><li><a href=https://garyforreal.me/en/ title=üè†Homepage><span>üè†Homepage</span></a></li><li><a href=https://garyforreal.me/en/posts title=üìöArticle><span>üìöArticle</span></a></li><li><a href=https://garyforreal.me/en/archives/ title=‚è±Archives><span>‚è±Archives</span></a></li><li><a href=https://garyforreal.me/en/music/ title=üéµmusic><span>üéµmusic</span></a></li><li><a href=https://garyforreal.me/en/about title=üôãüèª‚Äç‚ôÇÔ∏èAbout><span>üôãüèª‚Äç‚ôÇÔ∏èAbout</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://garyforreal.me/en/>Home</a>&nbsp;¬ª&nbsp;<a href=https://garyforreal.me/en/posts/>Posts</a>&nbsp;¬ª&nbsp;<a href=https://garyforreal.me/en/posts/paper/>Paper</a></div><h1 class="post-title entry-hint-parent">Weekly Paper Notes - 2026-02-01</h1><div class=post-meta><span title='2026-02-01 15:30:48.807773 +0000 UTC'>2026-02-01</span>&nbsp;¬∑&nbsp;116 min&nbsp;¬∑&nbsp;116 min&nbsp;¬∑&nbsp;Gary&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://garyforreal.me/zh/posts/paper/paper-2026-02-01-weekly/>‰∏≠Êñá</a></li></ul><div class=meta-item>&nbsp¬∑&nbsp
        <span id=busuanzi_container_page_pv>Êú¨ÊñáÈòÖËØªÈáè<span id=busuanzi_value_page_pv></span>Ê¨°</span></div></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#weekly-paper-notes aria-label="Weekly Paper Notes">Weekly Paper Notes</a><ul><li><a href=#-multilingual aria-label="üîç multilingual">üîç multilingual</a><ul><li><a href=#just-dub-it-video-dubbing-via-joint-audio-visual-diffusionhttpsarxivorgabs260122143v1 aria-label="JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion"><a href=https://arxiv.org/abs/2601.22143v1>JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion</a></a></li><li><a href=#swe-replay-efficient-test-time-scaling-for-software-engineering-agentshttpsarxivorgabs260122129v1 aria-label="SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents"><a href=https://arxiv.org/abs/2601.22129v1>SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents</a></a></li><li><a href=#making-foundation-models-probabilistic-via-singular-value-ensembleshttpsarxivorgabs260122068v1 aria-label="Making Foundation Models Probabilistic via Singular Value Ensembles"><a href=https://arxiv.org/abs/2601.22068v1>Making Foundation Models Probabilistic via Singular Value Ensembles</a></a></li><li><a href=#masalbench-a-benchmark-for-contextual-and-cross-cultural-understanding-of-persian-proverbs-in-llmshttpsarxivorgabs260122050v1 aria-label="MasalBench: A Benchmark for Contextual and Cross-Cultural Understanding of Persian Proverbs in LLMs"><a href=https://arxiv.org/abs/2601.22050v1>MasalBench: A Benchmark for Contextual and Cross-Cultural Understanding of Persian Proverbs in LLMs</a></a></li><li><a href=#tidyvoice-2026-challenge-evaluation-planhttpsarxivorgabs260121960v1 aria-label="TidyVoice 2026 Challenge Evaluation Plan"><a href=https://arxiv.org/abs/2601.21960v1>TidyVoice 2026 Challenge Evaluation Plan</a></a></li><li><a href=#kid-knowledge-injected-dual-head-learning-for-knowledge-grounded-harmful-meme-detectionhttpsarxivorgabs260121796v1 aria-label="KID: Knowledge-Injected Dual-Head Learning for Knowledge-Grounded Harmful Meme Detection"><a href=https://arxiv.org/abs/2601.21796v1>KID: Knowledge-Injected Dual-Head Learning for Knowledge-Grounded Harmful Meme Detection</a></a></li><li><a href=#dimstance-multilingual-datasets-for-dimensional-stance-analysishttpsarxivorgabs260121483v1 aria-label="DimStance: Multilingual Datasets for Dimensional Stance Analysis"><a href=https://arxiv.org/abs/2601.21483v1>DimStance: Multilingual Datasets for Dimensional Stance Analysis</a></a></li><li><a href=#milore-ssl-scaling-multilingual-capabilities-in-self-supervised-models-without-forgettinghttpsarxivorgabs260120300v1 aria-label="MiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without Forgetting"><a href=https://arxiv.org/abs/2601.20300v1>MiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without Forgetting</a></a></li><li><a href=#ffe-halluhallucinations-in-fixed-figurative-expressionsbenchmark-of-idioms-and-proverbs-in-the-persian-languagehttpsarxivorgabs260120105v1 aria-label="FFE-Hallu:Hallucinations in Fixed Figurative Expressions:Benchmark of Idioms and Proverbs in the Persian Language"><a href=https://arxiv.org/abs/2601.20105v1>FFE-Hallu:Hallucinations in Fixed Figurative Expressions:Benchmark of Idioms and Proverbs in the Persian Language</a></a></li><li><a href=#linguamap-which-layers-of-llms-speak-your-language-and-how-to-tune-themhttpsarxivorgabs260120009v1 aria-label="LinguaMap: Which Layers of LLMs Speak Your Language and How to Tune Them?"><a href=https://arxiv.org/abs/2601.20009v1>LinguaMap: Which Layers of LLMs Speak Your Language and How to Tune Them?</a></a></li><li><a href=#dynamic-multi-expert-projectors-with-stabilized-routing-for-multilingual-speech-recognitionhttpsarxivorgabs260119451v1 aria-label="Dynamic Multi-Expert Projectors with Stabilized Routing for Multilingual Speech Recognition"><a href=https://arxiv.org/abs/2601.19451v1>Dynamic Multi-Expert Projectors with Stabilized Routing for Multilingual Speech Recognition</a></a></li><li><a href=#llm-forcedaligner-a-non-autoregressive-and-accurate-llm-based-forced-aligner-for-multilingual-and-long-form-speechhttpsarxivorgabs260118220v1 aria-label="LLM-ForcedAligner: A Non-Autoregressive and Accurate LLM-Based Forced Aligner for Multilingual and Long-Form Speech"><a href=https://arxiv.org/abs/2601.18220v1>LLM-ForcedAligner: A Non-Autoregressive and Accurate LLM-Based Forced Aligner for Multilingual and Long-Form Speech</a></a></li><li><a href=#evaluating-chatgpt-on-medical-information-extraction-tasks-performance-explainability-and-beyondhttpsarxivorgabs260121767v1 aria-label="Evaluating ChatGPT on Medical Information Extraction Tasks: Performance, Explainability and Beyond"><a href=https://arxiv.org/abs/2601.21767v1>Evaluating ChatGPT on Medical Information Extraction Tasks: Performance, Explainability and Beyond</a></a></li><li><a href=#ce-gocd-central-entity-guided-graph-optimization-for-community-detection-to-augment-llm-scientific-question-answeringhttpsarxivorgabs260121733v1 aria-label="CE-GOCD: Central Entity-Guided Graph Optimization for Community Detection to Augment LLM Scientific Question Answering"><a href=https://arxiv.org/abs/2601.21733v1>CE-GOCD: Central Entity-Guided Graph Optimization for Community Detection to Augment LLM Scientific Question Answering</a></a></li><li><a href=#enhancing-language-models-for-robust-greenwashing-detectionhttpsarxivorgabs260121722v1 aria-label="Enhancing Language Models for Robust Greenwashing Detection"><a href=https://arxiv.org/abs/2601.21722v1>Enhancing Language Models for Robust Greenwashing Detection</a></a></li><li><a href=#language-models-as-artificial-learners-investigating-crosslinguistic-influencehttpsarxivorgabs260121587v1 aria-label="Language Models as Artificial Learners: Investigating Crosslinguistic Influence"><a href=https://arxiv.org/abs/2601.21587v1>Language Models as Artificial Learners: Investigating Crosslinguistic Influence</a></a></li><li><a href=#icl-evader-zero-query-black-box-evasion-attacks-on-in-context-learning-and-their-defenseshttpsarxivorgabs260121586v1 aria-label="ICL-EVADER: Zero-Query Black-Box Evasion Attacks on In-Context Learning and Their Defenses"><a href=https://arxiv.org/abs/2601.21586v1>ICL-EVADER: Zero-Query Black-Box Evasion Attacks on In-Context Learning and Their Defenses</a></a></li><li><a href=#icon-intent-context-coupling-for-efficient-multi-turn-jailbreak-attackhttpsarxivorgabs260120903v1 aria-label="ICON: Intent-Context Coupling for Efficient Multi-Turn Jailbreak Attack"><a href=https://arxiv.org/abs/2601.20903v1>ICON: Intent-Context Coupling for Efficient Multi-Turn Jailbreak Attack</a></a></li><li><a href=#beyond-speedup----utilizing-kv-cache-for-sampling-and-reasoninghttpsarxivorgabs260120326v1 aria-label="Beyond Speedup &ndash; Utilizing KV Cache for Sampling and Reasoning"><a href=https://arxiv.org/abs/2601.20326v1>Beyond Speedup &ndash; Utilizing KV Cache for Sampling and Reasoning</a></a></li><li><a href=#probing-the-future-of-meta-analysis-eliciting-design-principles-via-an-agentic-research-idehttpsarxivorgabs260118239v1 aria-label="Probing the Future of Meta-Analysis: Eliciting Design Principles via an Agentic Research IDE"><a href=https://arxiv.org/abs/2601.18239v1>Probing the Future of Meta-Analysis: Eliciting Design Principles via an Agentic Research IDE</a></a></li><li><a href=#vibevoice-asr-technical-reporthttpsarxivorgabs260118184v1 aria-label="VIBEVOICE-ASR Technical Report"><a href=https://arxiv.org/abs/2601.18184v1>VIBEVOICE-ASR Technical Report</a></a></li><li><a href=#the-augmented-nlp-bound-for-maximum-entropy-remote-samplinghttpsarxivorgabs260120970v1 aria-label="The augmented NLP bound for maximum-entropy remote sampling"><a href=https://arxiv.org/abs/2601.20970v1>The augmented NLP bound for maximum-entropy remote sampling</a></a></li><li><a href=#redsage-a-cybersecurity-generalist-llmhttpsarxivorgabs260122159v1 aria-label="RedSage: A Cybersecurity Generalist LLM"><a href=https://arxiv.org/abs/2601.22159v1>RedSage: A Cybersecurity Generalist LLM</a></a></li><li><a href=#discovering-hidden-gems-in-model-repositorieshttpsarxivorgabs260122157v1 aria-label="Discovering Hidden Gems in Model Repositories"><a href=https://arxiv.org/abs/2601.22157v1>Discovering Hidden Gems in Model Repositories</a></a></li><li><a href=#hybrid-linear-attention-done-right-efficient-distillation-and-effective-architectures-for-extremely-long-contextshttpsarxivorgabs260122156v1 aria-label="Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts"><a href=https://arxiv.org/abs/2601.22156v1>Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts</a></a></li><li><a href=#ueval-a-benchmark-for-unified-multimodal-generationhttpsarxivorgabs260122155v1 aria-label="UEval: A Benchmark for Unified Multimodal Generation"><a href=https://arxiv.org/abs/2601.22155v1>UEval: A Benchmark for Unified Multimodal Generation</a></a></li><li><a href=#exploring-reasoning-reward-model-for-agentshttpsarxivorgabs260122154v1 aria-label="Exploring Reasoning Reward Model for Agents"><a href=https://arxiv.org/abs/2601.22154v1>Exploring Reasoning Reward Model for Agents</a></a></li><li><a href=#dynamicvla-a-vision-language-action-model-for-dynamic-object-manipulationhttpsarxivorgabs260122153v1 aria-label="DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation"><a href=https://arxiv.org/abs/2601.22153v1>DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation</a></a></li><li><a href=#reactemg-stroke-healthy-to-stroke-few-shot-adaptation-for-semg-based-intent-detectionhttpsarxivorgabs260122090v1 aria-label="ReactEMG Stroke: Healthy-to-Stroke Few-shot Adaptation for sEMG-Based Intent Detection"><a href=https://arxiv.org/abs/2601.22090v1>ReactEMG Stroke: Healthy-to-Stroke Few-shot Adaptation for sEMG-Based Intent Detection</a></a></li><li><a href=#learning-transient-convective-heat-transfer-with-geometry-aware-world-modelshttpsarxivorgabs260122086v1 aria-label="Learning Transient Convective Heat Transfer with Geometry Aware World Models"><a href=https://arxiv.org/abs/2601.22086v1>Learning Transient Convective Heat Transfer with Geometry Aware World Models</a></a></li><li><a href=#learning-to-communicate-across-modalities-perceptual-heterogeneity-in-multi-agent-systemshttpsarxivorgabs260122041v1 aria-label="Learning to Communicate Across Modalities: Perceptual Heterogeneity in Multi-Agent Systems"><a href=https://arxiv.org/abs/2601.22041v1>Learning to Communicate Across Modalities: Perceptual Heterogeneity in Multi-Agent Systems</a></a></li><li><a href=#routing-the-lottery-adaptive-subnetworks-for-heterogeneous-datahttpsarxivorgabs260122141v1 aria-label="Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data"><a href=https://arxiv.org/abs/2601.22141v1>Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data</a></a></li><li><a href=#reasoning-while-asking-transforming-reasoning-large-language-models-from-passive-solvers-to-proactive-inquirershttpsarxivorgabs260122139v1 aria-label="Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers"><a href=https://arxiv.org/abs/2601.22139v1>Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers</a></a></li><li><a href=#value-based-pre-training-with-downstream-feedbackhttpsarxivorgabs260122108v1 aria-label="Value-Based Pre-Training with Downstream Feedback"><a href=https://arxiv.org/abs/2601.22108v1>Value-Based Pre-Training with Downstream Feedback</a></a></li><li><a href=#sina-a-circuit-schematic-image-to-netlist-generator-using-artificial-intelligencehttpsarxivorgabs260122114v1 aria-label="SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence"><a href=https://arxiv.org/abs/2601.22114v1>SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence</a></a></li><li><a href=#vision-deepresearch-incentivizing-deepresearch-capability-in-multimodal-large-language-modelshttpsarxivorgabs260122060v1 aria-label="Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models"><a href=https://arxiv.org/abs/2601.22060v1>Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models</a></a></li><li><a href=#g2-reader-dual-evolving-graphs-for-multimodal-document-qahttpsarxivorgabs260122055v1 aria-label="$G^2$-Reader: Dual Evolving Graphs for Multimodal Document QA"><a href=https://arxiv.org/abs/2601.22055v1>$G^2$-Reader: Dual Evolving Graphs for Multimodal Document QA</a></a></li><li><a href=#when-better-prompts-hurt-evaluation-driven-iteration-for-llm-applicationshttpsarxivorgabs260122025v1 aria-label="When &ldquo;Better&rdquo; Prompts Hurt: Evaluation-Driven Iteration for LLM Applications"><a href=https://arxiv.org/abs/2601.22025v1>When &ldquo;Better&rdquo; Prompts Hurt: Evaluation-Driven Iteration for LLM Applications</a></a></li><li><a href=#lancer-llm-reranking-for-nugget-coveragehttpsarxivorgabs260122008v1 aria-label="LANCER: LLM Reranking for Nugget Coverage"><a href=https://arxiv.org/abs/2601.22008v1>LANCER: LLM Reranking for Nugget Coverage</a></a></li></ul></li><li><a href=#-linguistics aria-label="üîç linguistics">üîç linguistics</a><ul><li><a href=#one-step-latent-free-image-generation-with-pixel-mean-flowshttpsarxivorgabs260122158v1 aria-label="One-step Latent-free Image Generation with Pixel Mean Flows"><a href=https://arxiv.org/abs/2601.22158v1>One-step Latent-free Image Generation with Pixel Mean Flows</a></a></li><li><a href=#dynaweb-model-based-reinforcement-learning-of-web-agentshttpsarxivorgabs260122149v1 aria-label="DynaWeb: Model-Based Reinforcement Learning of Web Agents"><a href=https://arxiv.org/abs/2601.22149v1>DynaWeb: Model-Based Reinforcement Learning of Web Agents</a></a></li><li><a href=#cobordism-and-concordance-of-surfaces-in-4-manifoldshttpsarxivorgabs260122152v1 aria-label="Cobordism and Concordance of Surfaces in 4-Manifolds"><a href=https://arxiv.org/abs/2601.22152v1>Cobordism and Concordance of Surfaces in 4-Manifolds</a></a></li><li><a href=#late-breaking-results-conversion-of-neural-networks-into-logic-flows-for-edge-computinghttpsarxivorgabs260122151v1 aria-label="Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing"><a href=https://arxiv.org/abs/2601.22151v1>Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing</a></a></li><li><a href=#neural-s-matrix-bootstrap-ii-solvable-4d-amplitudes-with-particle-productionhttpsarxivorgabs260122145v1 aria-label="Neural S-matrix bootstrap II: solvable 4d amplitudes with particle production"><a href=https://arxiv.org/abs/2601.22145v1>Neural S-matrix bootstrap II: solvable 4d amplitudes with particle production</a></a></li><li><a href=#all-order-prescription-for-facet-regions-in-massless-wide-angle-scatteringhttpsarxivorgabs260122144v1 aria-label="All-order prescription for facet regions in massless wide-angle scattering"><a href=https://arxiv.org/abs/2601.22144v1>All-order prescription for facet regions in massless wide-angle scattering</a></a></li><li><a href=#towards-universal-urban-patterns-of-life-simulationhttpsarxivorgabs260122099v1 aria-label="Towards Universal Urban Patterns-of-Life Simulation"><a href=https://arxiv.org/abs/2601.22099v1>Towards Universal Urban Patterns-of-Life Simulation</a></a></li><li><a href=#constraining-black-hole-parameters-from-shadow-and-inner-shadow-morphology-considering-effects-from-thick-disk-accretion-flowshttpsarxivorgabs260121995v1 aria-label="Constraining Black Hole Parameters from Shadow and Inner-Shadow Morphology Considering Effects from Thick Disk Accretion Flows"><a href=https://arxiv.org/abs/2601.21995v1>Constraining Black Hole Parameters from Shadow and Inner-Shadow Morphology Considering Effects from Thick Disk Accretion Flows</a></a></li><li><a href=#secure-group-key-agreement-on-cyber-physical-system-buseshttpsarxivorgabs260121966v1 aria-label="Secure Group Key Agreement on Cyber-Physical System Buses"><a href=https://arxiv.org/abs/2601.21966v1>Secure Group Key Agreement on Cyber-Physical System Buses</a></a></li><li><a href=#investigation-of-wake-dynamics-of-a-slender-symmetric-trailing-edge-hydrofoilhttpsarxivorgabs260121939v1 aria-label="Investigation of Wake Dynamics of a Slender Symmetric Trailing Edge Hydrofoil"><a href=https://arxiv.org/abs/2601.21939v1>Investigation of Wake Dynamics of a Slender Symmetric Trailing Edge Hydrofoil</a></a></li><li><a href=#a-judge-aware-ranking-framework-for-evaluating-large-language-models-without-ground-truthhttpsarxivorgabs260121817v1 aria-label="A Judge-Aware Ranking Framework for Evaluating Large Language Models without Ground Truth"><a href=https://arxiv.org/abs/2601.21817v1>A Judge-Aware Ranking Framework for Evaluating Large Language Models without Ground Truth</a></a></li><li><a href=#ewocs-v-is-wd1-72-a-recent-post-interaction-wro-binaryhttpsarxivorgabs260121788v1 aria-label="EWOCS-V: Is Wd1-72 a recent post-interaction WR+O binary?"><a href=https://arxiv.org/abs/2601.21788v1>EWOCS-V: Is Wd1-72 a recent post-interaction WR+O binary?</a></a></li><li><a href=#fineinstructions-scaling-synthetic-instructions-to-pre-training-scalehttpsarxivorgabs260122146v1 aria-label="FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale"><a href=https://arxiv.org/abs/2601.22146v1>FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale</a></a></li><li><a href=#prism-distribution-free-adaptive-computation-of-matrix-functions-for-accelerating-neural-network-traininghttpsarxivorgabs260122137v1 aria-label="PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training"><a href=https://arxiv.org/abs/2601.22137v1>PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training</a></a></li><li><a href=#the-patient-is-not-a-moving-document-a-world-model-training-paradigm-for-longitudinal-ehrhttpsarxivorgabs260122128v1 aria-label="The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR"><a href=https://arxiv.org/abs/2601.22128v1>The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR</a></a></li><li><a href=#alpha-discovery-via-grammar-guided-learning-and-searchhttpsarxivorgabs260122119v1 aria-label="Alpha Discovery via Grammar-Guided Learning and Search"><a href=https://arxiv.org/abs/2601.22119v1>Alpha Discovery via Grammar-Guided Learning and Search</a></a></li><li><a href=#paddleocr-vl-15-towards-a-multi-task-09b-vlm-for-robust-in-the-wild-document-parsinghttpsarxivorgabs260121957v1 aria-label="PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing"><a href=https://arxiv.org/abs/2601.21957v1>PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing</a></a></li><li><a href=#from-future-of-work-to-future-of-workers-addressing-asymptomatic-ai-harms-for-dignified-human-ai-interactionhttpsarxivorgabs260121920v1 aria-label="From Future of Work to Future of Workers: Addressing Asymptomatic AI Harms for Dignified Human-AI Interaction"><a href=https://arxiv.org/abs/2601.21920v1>From Future of Work to Future of Workers: Addressing Asymptomatic AI Harms for Dignified Human-AI Interaction</a></a></li><li><a href=#murad-a-large-scale-multi-domain-unified-reverse-arabic-dictionary-datasethttpsarxivorgabs260121512v1 aria-label="MURAD: A Large-Scale Multi-Domain Unified Reverse Arabic Dictionary Dataset"><a href=https://arxiv.org/abs/2601.21512v1>MURAD: A Large-Scale Multi-Domain Unified Reverse Arabic Dictionary Dataset</a></a></li><li><a href=#smart-a-social-movement-analysis--reasoning-tool-with-case-studies-on-metoo-and-blacklivesmatterhttpsarxivorgabs260120986v1 aria-label="SMART: A Social Movement Analysis & Reasoning Tool with Case Studies on #MeToo and #BlackLivesMatter"><a href=https://arxiv.org/abs/2601.20986v1>SMART: A Social Movement Analysis & Reasoning Tool with Case Studies on #MeToo and #BlackLivesMatter</a></a></li><li><a href=#sokobench-evaluating-long-horizon-planning-and-reasoning-in-large-language-modelshttpsarxivorgabs260120856v1 aria-label="SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models"><a href=https://arxiv.org/abs/2601.20856v1>SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models</a></a></li><li><a href=#like-a-therapist-but-not-reddit-narratives-of-ai-in-mental-health-contextshttpsarxivorgabs260120747v1 aria-label="Like a Therapist, But Not: Reddit Narratives of AI in Mental Health Contexts"><a href=https://arxiv.org/abs/2601.20747v1>Like a Therapist, But Not: Reddit Narratives of AI in Mental Health Contexts</a></a></li><li><a href=#do-vlms-perceive-or-recall-probing-visual-perception-vs-memory-with-classic-visual-illusionshttpsarxivorgabs260122150v1 aria-label="Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions"><a href=https://arxiv.org/abs/2601.22150v1>Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions</a></a></li><li><a href=#probing-the-sound-speed-of-dark-energy-with-a-lunar-laser-interferometerhttpsarxivorgabs260122084v1 aria-label="Probing the Sound Speed of Dark Energy with a Lunar Laser Interferometer"><a href=https://arxiv.org/abs/2601.22084v1>Probing the Sound Speed of Dark Energy with a Lunar Laser Interferometer</a></a></li><li><a href=#a-reverse-black-hole-information-problemhttpsarxivorgabs260122077v1 aria-label="A Reverse Black Hole Information Problem"><a href=https://arxiv.org/abs/2601.22077v1>A Reverse Black Hole Information Problem</a></a></li><li><a href=#universal-multifractality-at-the-topological-anderson-insulator-transitionhttpsarxivorgabs260122065v1 aria-label="Universal Multifractality at the Topological Anderson Insulator Transition"><a href=https://arxiv.org/abs/2601.22065v1>Universal Multifractality at the Topological Anderson Insulator Transition</a></a></li><li><a href=#symmetries-of-regular-q-graphshttpsarxivorgabs260122148v1 aria-label="Symmetries of regular $q$-graphs"><a href=https://arxiv.org/abs/2601.22148v1>Symmetries of regular $q$-graphs</a></a></li><li><a href=#optimal-cross-correlation-technique-to-search-for-strongly-lensed-gravitational-waveshttpsarxivorgabs260122138v1 aria-label="Optimal cross-correlation technique to search for strongly lensed gravitational waves"><a href=https://arxiv.org/abs/2601.22138v1>Optimal cross-correlation technique to search for strongly lensed gravitational waves</a></a></li><li><a href=#an-invitation-to-higher-order-riemannian-optimization-optimal-and-implementable-methodshttpsarxivorgabs260122126v1 aria-label="An Invitation to Higher-Order Riemannian Optimization: Optimal and Implementable Methods"><a href=https://arxiv.org/abs/2601.22126v1>An Invitation to Higher-Order Riemannian Optimization: Optimal and Implementable Methods</a></a></li><li><a href=#firewalls-in-the-non-perturbative-bulk-hilbert-space-of-jt-gravityhttpsarxivorgabs260122121v1 aria-label="Firewalls in the non-perturbative bulk Hilbert space of JT gravity"><a href=https://arxiv.org/abs/2601.22121v1>Firewalls in the non-perturbative bulk Hilbert space of JT gravity</a></a></li><li><a href=#superdiffusion-and-anomalous-regularization-in-self-similar-random-incompressible-flowshttpsarxivorgabs260122142v1 aria-label="Superdiffusion and anomalous regularization in self-similar random incompressible flows"><a href=https://arxiv.org/abs/2601.22142v1>Superdiffusion and anomalous regularization in self-similar random incompressible flows</a></a></li><li><a href=#mechanistic-data-attribution-tracing-the-training-origins-of-interpretable-llm-unitshttpsarxivorgabs260121996v1 aria-label="Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units"><a href=https://arxiv.org/abs/2601.21996v1>Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units</a></a></li><li><a href=#spectran-spectral-aware-transformer-based-adapter-for-llm-enhanced-sequential-recommendationhttpsarxivorgabs260121986v1 aria-label="SpecTran: Spectral-Aware Transformer-based Adapter for LLM-Enhanced Sequential Recommendation"><a href=https://arxiv.org/abs/2601.21986v1>SpecTran: Spectral-Aware Transformer-based Adapter for LLM-Enhanced Sequential Recommendation</a></a></li><li><a href=#breaking-the-regional-barrier-inductive-semantic-topology-learning-for-worldwide-air-quality-forecastinghttpsarxivorgabs260121899v1 aria-label="Breaking the Regional Barrier: Inductive Semantic Topology Learning for Worldwide Air Quality Forecasting"><a href=https://arxiv.org/abs/2601.21899v1>Breaking the Regional Barrier: Inductive Semantic Topology Learning for Worldwide Air Quality Forecasting</a></a></li><li><a href=#quantum-lego-learning-a-modular-design-principle-for-hybrid-artificial-intelligencehttpsarxivorgabs260121780v1 aria-label="Quantum LEGO Learning: A Modular Design Principle for Hybrid Artificial Intelligence"><a href=https://arxiv.org/abs/2601.21780v1>Quantum LEGO Learning: A Modular Design Principle for Hybrid Artificial Intelligence</a></a></li></ul></li><li><a href=#-psycholinguistics aria-label="üîç psycholinguistics">üîç psycholinguistics</a><ul><li><a href=#variance-component-score-test-for-multivariate-change-point-detection-with-applications-to-mobile-healthhttpsarxivorgabs260122147v1 aria-label="Variance component score test for multivariate change point detection with applications to mobile health"><a href=https://arxiv.org/abs/2601.22147v1>Variance component score test for multivariate change point detection with applications to mobile health</a></a></li><li><a href=#smog-scalable-meta-learning-for-multi-objective-bayesian-optimizationhttpsarxivorgabs260122131v1 aria-label="SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization"><a href=https://arxiv.org/abs/2601.22131v1>SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization</a></a></li><li><a href=#from-particles-to-agents-hallucination-as-a-metric-for-cognitive-friction-in-spatial-simulationhttpsarxivorgabs260121977v1 aria-label="From Particles to Agents: Hallucination as a Metric for Cognitive Friction in Spatial Simulation"><a href=https://arxiv.org/abs/2601.21977v1>From Particles to Agents: Hallucination as a Metric for Cognitive Friction in Spatial Simulation</a></a></li><li><a href=#unique-continuation-property-for-stochastic-wave-equationshttpsarxivorgabs260121854v1 aria-label="Unique Continuation Property for Stochastic Wave Equations"><a href=https://arxiv.org/abs/2601.21854v1>Unique Continuation Property for Stochastic Wave Equations</a></a></li><li><a href=#the-effectiveness-of-style-vectors-for-steering-large-language-models-a-human-evaluationhttpsarxivorgabs260121505v1 aria-label="The Effectiveness of Style Vectors for Steering Large Language Models: A Human Evaluation"><a href=https://arxiv.org/abs/2601.21505v1>The Effectiveness of Style Vectors for Steering Large Language Models: A Human Evaluation</a></a></li><li><a href=#the-surprising-difficulty-of-search-in-model-based-reinforcement-learninghttpsarxivorgabs260121306v1 aria-label="The Surprising Difficulty of Search in Model-Based Reinforcement Learning"><a href=https://arxiv.org/abs/2601.21306v1>The Surprising Difficulty of Search in Model-Based Reinforcement Learning</a></a></li><li><a href=#maxwait-a-generalized-mechanism-for-distributed-time-sensitive-systemshttpsarxivorgabs260121146v1 aria-label="Maxwait: A Generalized Mechanism for Distributed Time-Sensitive Systems"><a href=https://arxiv.org/abs/2601.21146v1>Maxwait: A Generalized Mechanism for Distributed Time-Sensitive Systems</a></a></li><li><a href=#the-tensionless-lives-of-null-stringshttpsarxivorgabs260120959v1 aria-label="The Tensionless Lives of Null Strings"><a href=https://arxiv.org/abs/2601.20959v1>The Tensionless Lives of Null Strings</a></a></li><li><a href=#learning-hamiltonian-flow-maps-mean-flow-consistency-for-large-timestep-molecular-dynamicshttpsarxivorgabs260122123v1 aria-label="Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics"><a href=https://arxiv.org/abs/2601.22123v1>Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics</a></a></li><li><a href=#designing-quantum-technologies-with-a-quantum-computerhttpsarxivorgabs260122091v1 aria-label="Designing quantum technologies with a quantum computer"><a href=https://arxiv.org/abs/2601.22091v1>Designing quantum technologies with a quantum computer</a></a></li><li><a href=#accessibility-driven-information-transformations-in-mixed-visual-ability-work-teamshttpsarxivorgabs260122081v1 aria-label="Accessibility-Driven Information Transformations in Mixed-Visual Ability Work Teams"><a href=https://arxiv.org/abs/2601.22081v1>Accessibility-Driven Information Transformations in Mixed-Visual Ability Work Teams</a></a></li><li><a href=#mjlab-a-lightweight-framework-for-gpu-accelerated-robot-learninghttpsarxivorgabs260122074v1 aria-label="mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning"><a href=https://arxiv.org/abs/2601.22074v1>mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning</a></a></li><li><a href=#cognitive-load-estimation-using-brain-foundation-models-and-interpretability-for-bcishttpsarxivorgabs260121965v1 aria-label="Cognitive Load Estimation Using Brain Foundation Models and Interpretability for BCIs"><a href=https://arxiv.org/abs/2601.21965v1>Cognitive Load Estimation Using Brain Foundation Models and Interpretability for BCIs</a></a></li><li><a href=#from-meta-thought-to-execution-cognitively-aligned-post-training-for-generalizable-and-reliable-llm-reasoninghttpsarxivorgabs260121909v1 aria-label="From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning"><a href=https://arxiv.org/abs/2601.21909v1>From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning</a></a></li><li><a href=#scale-dependent-semantic-dynamics-revealed-by-allan-deviationhttpsarxivorgabs260121678v1 aria-label="Scale-Dependent Semantic Dynamics Revealed by Allan Deviation"><a href=https://arxiv.org/abs/2601.21678v1>Scale-Dependent Semantic Dynamics Revealed by Allan Deviation</a></a></li><li><a href=#age-matters-analyzing-age-related-discussions-in-app-reviewshttpsarxivorgabs260121605v1 aria-label="Age Matters: Analyzing Age-Related Discussions in App Reviews"><a href=https://arxiv.org/abs/2601.21605v1>Age Matters: Analyzing Age-Related Discussions in App Reviews</a></a></li><li><a href=#comparative-assessment-of-look-ahead-economic-dispatch-and-ramp-products-for-grid-flexibilityhttpsarxivorgabs260122120v1 aria-label="Comparative Assessment of Look-Ahead Economic Dispatch and Ramp Products for Grid Flexibility"><a href=https://arxiv.org/abs/2601.22120v1>Comparative Assessment of Look-Ahead Economic Dispatch and Ramp Products for Grid Flexibility</a></a></li><li><a href=#a-gradient-based-capacity-accreditation-framework-in-resource-adequacy-formulation-computation-and-practical-implicationshttpsarxivorgabs260122087v1 aria-label="A Gradient-Based Capacity Accreditation Framework in Resource Adequacy: Formulation, Computation, and Practical Implications"><a href=https://arxiv.org/abs/2601.22087v1>A Gradient-Based Capacity Accreditation Framework in Resource Adequacy: Formulation, Computation, and Practical Implications</a></a></li><li><a href=#voltvar-optimization-in-transmission-networks-with-discrete-control-deviceshttpsarxivorgabs260122080v1 aria-label="Volt/VAR Optimization in Transmission Networks with Discrete-Control Devices"><a href=https://arxiv.org/abs/2601.22080v1>Volt/VAR Optimization in Transmission Networks with Discrete-Control Devices</a></a></li><li><a href=#lorif-low-rank-influence-functions-for-scalable-training-data-attributionhttpsarxivorgabs260121929v1 aria-label="LoRIF: Low-Rank Influence Functions for Scalable Training Data Attribution"><a href=https://arxiv.org/abs/2601.21929v1>LoRIF: Low-Rank Influence Functions for Scalable Training Data Attribution</a></a></li></ul></li><li><a href=#-llm aria-label="üîç llm">üîç llm</a><ul><li><a href=#world-of-workflows-a-benchmark-for-bringing-world-models-to-enterprise-systemshttpsarxivorgabs260122130v1 aria-label="World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems"><a href=https://arxiv.org/abs/2601.22130v1>World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems</a></a></li><li><a href=#creative-image-generation-with-diffusion-modelhttpsarxivorgabs260122125v1 aria-label="Creative Image Generation with Diffusion Model"><a href=https://arxiv.org/abs/2601.22125v1>Creative Image Generation with Diffusion Model</a></a></li><li><a href=#prior-informed-flow-matching-for-graph-reconstructionhttpsarxivorgabs260122107v1 aria-label="Prior-Informed Flow Matching for Graph Reconstruction"><a href=https://arxiv.org/abs/2601.22107v1>Prior-Informed Flow Matching for Graph Reconstruction</a></a></li></ul></li><li><a href=#-neuroscience aria-label="üîç neuroscience">üîç neuroscience</a><ul><li><a href=#airpet-virtual-positron-emission-tomographyhttpsarxivorgabs260122059v1 aria-label="AIRPET: Virtual Positron Emission Tomography"><a href=https://arxiv.org/abs/2601.22059v1>AIRPET: Virtual Positron Emission Tomography</a></a></li><li><a href=#metricanything-scaling-metric-depth-pretraining-with-noisy-heterogeneous-sourceshttpsarxivorgabs260122054v1 aria-label="MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources"><a href=https://arxiv.org/abs/2601.22054v1>MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources</a></a></li><li><a href=#molecular-structure-binding-and-disorder-in-tdbc-ag-plexcitonic-assemblieshttpsarxivorgabs260122022v1 aria-label="Molecular structure, binding, and disorder in TDBC-Ag plexcitonic assemblies"><a href=https://arxiv.org/abs/2601.22022v1>Molecular structure, binding, and disorder in TDBC-Ag plexcitonic assemblies</a></a></li><li><a href=#liquid-interfaces-a-dynamic-ontology-for-the-interoperability-of-autonomous-systemshttpsarxivorgabs260121993v1 aria-label="Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems"><a href=https://arxiv.org/abs/2601.21993v1>Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems</a></a></li><li><a href=#how-do-visual-attributes-influence-web-agents-a-comprehensive-evaluation-of-user-interface-design-factorshttpsarxivorgabs260121961v1 aria-label="How do Visual Attributes Influence Web Agents? A Comprehensive Evaluation of User Interface Design Factors"><a href=https://arxiv.org/abs/2601.21961v1>How do Visual Attributes Influence Web Agents? A Comprehensive Evaluation of User Interface Design Factors</a></a></li></ul></li><li><a href=#-data_resources aria-label="üîç data_resources">üîç data_resources</a><ul><li><a href=#pi-light-physics-inspired-diffusion-for-full-image-relightinghttpsarxivorgabs260122135v1 aria-label="PI-Light: Physics-Inspired Diffusion for Full-Image Relighting"><a href=https://arxiv.org/abs/2601.22135v1>PI-Light: Physics-Inspired Diffusion for Full-Image Relighting</a></a></li><li><a href=#preliminary-results-of-a-scoping-review-on-assistive-technologies-for-adults-with-adhdhttpsarxivorgabs260121791v1 aria-label="Preliminary Results of a Scoping Review on Assistive Technologies for Adults with ADHD"><a href=https://arxiv.org/abs/2601.21791v1>Preliminary Results of a Scoping Review on Assistive Technologies for Adults with ADHD</a></a></li><li><a href=#stepshield-when-not-whether-to-intervene-on-rogue-agentshttpsarxivorgabs260122136v1 aria-label="StepShield: When, Not Whether to Intervene on Rogue Agents"><a href=https://arxiv.org/abs/2601.22136v1>StepShield: When, Not Whether to Intervene on Rogue Agents</a></a></li><li><a href=#clarity-the-flexibility-interpretability-trade-off-in-sparsity-aware-concept-bottleneck-modelshttpsarxivorgabs260121944v1 aria-label="Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models"><a href=https://arxiv.org/abs/2601.21944v1>Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models</a></a></li><li><a href=#retrieval-infused-reasoning-sandbox-a-benchmark-for-decoupling-retrieval-and-reasoning-capabilitieshttpsarxivorgabs260121937v1 aria-label="Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities"><a href=https://arxiv.org/abs/2601.21937v1>Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities</a></a></li><li><a href=#webarbiter-a-principle-guided-reasoning-process-reward-model-for-web-agentshttpsarxivorgabs260121872v1 aria-label="WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents"><a href=https://arxiv.org/abs/2601.21872v1>WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents</a></a></li><li><a href=#mmfinereason-closing-the-multimodal-reasoning-gap-via-open-data-centric-methodshttpsarxivorgabs260121821v1 aria-label="MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods"><a href=https://arxiv.org/abs/2601.21821v1>MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods</a></a></li></ul></li><li><a href=#-emotion_language aria-label="üîç emotion_language">üîç emotion_language</a><ul><li><a href=#the-algebraic-and-geometric-classification-of-derived-jordan-and-bicommutative-algebrashttpsarxivorgabs260122110v1 aria-label="The algebraic and geometric classification of derived Jordan and bicommutative algebras"><a href=https://arxiv.org/abs/2601.22110v1>The algebraic and geometric classification of derived Jordan and bicommutative algebras</a></a></li><li><a href=#investigating-associational-biases-in-inter-model-communication-of-large-generative-modelshttpsarxivorgabs260122093v1 aria-label="Investigating Associational Biases in Inter-Model Communication of Large Generative Models"><a href=https://arxiv.org/abs/2601.22093v1>Investigating Associational Biases in Inter-Model Communication of Large Generative Models</a></a></li><li><a href=#on-set-theoretic-solutions-of-pentagon-equation-and-positive-basis-hopf-algebrashttpsarxivorgabs260122089v1 aria-label="On set-theoretic solutions of pentagon equation and positive basis Hopf algebras"><a href=https://arxiv.org/abs/2601.22089v1>On set-theoretic solutions of pentagon equation and positive basis Hopf algebras</a></a></li><li><a href=#on-homogeneous-involutions-on-matrix-algebrashttpsarxivorgabs260122049v1 aria-label="On homogeneous involutions on matrix algebras"><a href=https://arxiv.org/abs/2601.22049v1>On homogeneous involutions on matrix algebras</a></a></li><li><a href=#quantum-fluctuations-in-hydrodynamics-and-quantum-long-time-tailshttpsarxivorgabs260122140v1 aria-label="Quantum fluctuations in hydrodynamics and quantum long-time tails"><a href=https://arxiv.org/abs/2601.22140v1>Quantum fluctuations in hydrodynamics and quantum long-time tails</a></a></li><li><a href=#microlocal-maximal-hypoellipticity-from-the-geometric-viewpoint-ihttpsarxivorgabs260122122v1 aria-label="Microlocal maximal hypoellipticity from the geometric viewpoint: I"><a href=https://arxiv.org/abs/2601.22122v1>Microlocal maximal hypoellipticity from the geometric viewpoint: I</a></a></li><li><a href=#pay-for-hints-not-answers-llm-shepherding-for-cost-efficient-inferencehttpsarxivorgabs260122132v1 aria-label="Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference"><a href=https://arxiv.org/abs/2601.22132v1>Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference</a></a></li><li><a href=#social-media-data-for-population-mapping-a-bayesian-approach-to-address-representativeness-and-privacy-challengeshttpsarxivorgabs260122104v1 aria-label="Social Media Data for Population Mapping: A Bayesian Approach to Address Representativeness and Privacy Challenges"><a href=https://arxiv.org/abs/2601.22104v1>Social Media Data for Population Mapping: A Bayesian Approach to Address Representativeness and Privacy Challenges</a></a></li><li><a href=#inverted-anisotropy-of-the-partially-screened-magnetic-impurityhttpsarxivorgabs260122078v1 aria-label="Inverted anisotropy of the partially screened magnetic impurity"><a href=https://arxiv.org/abs/2601.22078v1>Inverted anisotropy of the partially screened magnetic impurity</a></a></li><li><a href=#early-and-prediagnostic-detection-of-pancreatic-cancer-from-computed-tomographyhttpsarxivorgabs260122134v1 aria-label="Early and Prediagnostic Detection of Pancreatic Cancer from Computed Tomography"><a href=https://arxiv.org/abs/2601.22134v1>Early and Prediagnostic Detection of Pancreatic Cancer from Computed Tomography</a></a></li><li><a href=#information-geometry-driven-graph-sequential-growthhttpsarxivorgabs260122106v1 aria-label="Information-geometry-driven graph sequential growth"><a href=https://arxiv.org/abs/2601.22106v1>Information-geometry-driven graph sequential growth</a></a></li><li><a href=#auditorily-embodied-conversational-agents-effects-of-spatialization-and-situated-audio-cues-on-presence-and-social-perceptionhttpsarxivorgabs260122082v1 aria-label="Auditorily Embodied Conversational Agents: Effects of Spatialization and Situated Audio Cues on Presence and Social Perception"><a href=https://arxiv.org/abs/2601.22082v1>Auditorily Embodied Conversational Agents: Effects of Spatialization and Situated Audio Cues on Presence and Social Perception</a></a></li><li><a href=#industrialized-deception-the-collateral-effects-of-llm-generated-misinformation-on-digital-ecosystemshttpsarxivorgabs260121963v1 aria-label="Industrialized Deception: The Collateral Effects of LLM-Generated Misinformation on Digital Ecosystems"><a href=https://arxiv.org/abs/2601.21963v1>Industrialized Deception: The Collateral Effects of LLM-Generated Misinformation on Digital Ecosystems</a></a></li><li><a href=#user-acceptance-model-for-smart-incentives-in-sustainable-video-streaming-towards-6ghttpsarxivorgabs260121903v1 aria-label="User Acceptance Model for Smart Incentives in Sustainable Video Streaming towards 6G"><a href=https://arxiv.org/abs/2601.21903v1>User Acceptance Model for Smart Incentives in Sustainable Video Streaming towards 6G</a></a></li><li><a href=#knowbias-mitigating-social-bias-in-llms-via-know-bias-neuron-enhancementhttpsarxivorgabs260121864v1 aria-label="KnowBias: Mitigating Social Bias in LLMs via Know-Bias Neuron Enhancement"><a href=https://arxiv.org/abs/2601.21864v1>KnowBias: Mitigating Social Bias in LLMs via Know-Bias Neuron Enhancement</a></a></li><li><a href=#putting-pressure-under-pressure-on-the-status-of-classical-pressure-in-special-relativityhttpsarxivorgabs260122133v1 aria-label="Putting Pressure Under Pressure: On the Status of Classical Pressure in Special Relativity"><a href=https://arxiv.org/abs/2601.22133v1>Putting Pressure Under Pressure: On the Status of Classical Pressure in Special Relativity</a></a></li><li><a href=#a-federated-and-parameter-efficient-framework-for-large-language-model-training-in-medicinehttpsarxivorgabs260122124v1 aria-label="A Federated and Parameter-Efficient Framework for Large Language Model Training in Medicine"><a href=https://arxiv.org/abs/2601.22124v1>A Federated and Parameter-Efficient Framework for Large Language Model Training in Medicine</a></a></li></ul></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h1 id=weekly-paper-notes>Weekly Paper Notes<a hidden class=anchor aria-hidden=true href=#weekly-paper-notes>#</a></h1><h2 id=-multilingual>üîç multilingual<a hidden class=anchor aria-hidden=true href=#-multilingual>#</a></h2><h3 id=just-dub-it-video-dubbing-via-joint-audio-visual-diffusionhttpsarxivorgabs260122143v1><a href=https://arxiv.org/abs/2601.22143v1>JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion</a><a hidden class=anchor aria-hidden=true href=#just-dub-it-video-dubbing-via-joint-audio-visual-diffusionhttpsarxivorgabs260122143v1>#</a></h3><p><strong>Authors:</strong> Anthony Chen, Naomi Ken Korem, Tavi Halperin, Matan Ben Yosef, Urska Jelercic, Ofir Bibi, Or Patashnik, Daniel Cohen-Or
<strong>Venue:</strong> arXiv (2026)</p><p>Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22143v1">üìÑ Download PDF</a></p><hr><h3 id=swe-replay-efficient-test-time-scaling-for-software-engineering-agentshttpsarxivorgabs260122129v1><a href=https://arxiv.org/abs/2601.22129v1>SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents</a><a hidden class=anchor aria-hidden=true href=#swe-replay-efficient-test-time-scaling-for-software-engineering-agentshttpsarxivorgabs260122129v1>#</a></h3><p><strong>Authors:</strong> Yifeng Ding, Lingming Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22129v1">üìÑ Download PDF</a></p><hr><h3 id=making-foundation-models-probabilistic-via-singular-value-ensembleshttpsarxivorgabs260122068v1><a href=https://arxiv.org/abs/2601.22068v1>Making Foundation Models Probabilistic via Singular Value Ensembles</a><a hidden class=anchor aria-hidden=true href=#making-foundation-models-probabilistic-via-singular-value-ensembleshttpsarxivorgabs260122068v1>#</a></h3><p><strong>Authors:</strong> Mehmet Ozgur Turkoglu, Dominik J. M√ºhlematter, Alexander Becker, Konrad Schindler, Helge Aasen
<strong>Venue:</strong> arXiv (2026)</p><p>Foundation models have become a dominant paradigm in machine learning, achieving remarkable performance across diverse tasks through large-scale pretraining. However, these models often yield overconfident, uncalibrated predictions. The standard approach to quantifying epistemic uncertainty, training an ensemble of independent models, incurs prohibitive computational costs that scale linearly with ensemble size, making it impractical for large foundation models. We propose Singular Value Ensemble (SVE), a parameter-efficient implicit ensemble method that builds on a simple, but powerful core assumption: namely, that the singular vectors of the weight matrices constitute meaningful subspaces of the model&rsquo;s knowledge. Pretrained foundation models encode rich, transferable information in their weight matrices. If the singular vectors are indeed meaningful (orthogonal) &ldquo;knowledge directions&rdquo;. To obtain a model ensemble, we modulate only how strongly each direction contributes to the output. Rather than learning entirely new parameters, we freeze the singular vectors and only train per-member singular values that rescale the contribution of each direction in that shared knowledge basis. Ensemble diversity emerges naturally as stochastic initialization and random sampling of mini-batches during joint training cause different members to converge to different combinations of the same underlying knowledge. SVE achieves uncertainty quantification comparable to explicit deep ensembles while increasing the parameter count of the base model by less than 1%, making principled uncertainty estimation accessible in resource-constrained settings. We validate SVE on NLP and vision tasks with various different backbones and show that it improves calibration while maintaining predictive accuracy.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22068v1">üìÑ Download PDF</a></p><hr><h3 id=masalbench-a-benchmark-for-contextual-and-cross-cultural-understanding-of-persian-proverbs-in-llmshttpsarxivorgabs260122050v1><a href=https://arxiv.org/abs/2601.22050v1>MasalBench: A Benchmark for Contextual and Cross-Cultural Understanding of Persian Proverbs in LLMs</a><a hidden class=anchor aria-hidden=true href=#masalbench-a-benchmark-for-contextual-and-cross-cultural-understanding-of-persian-proverbs-in-llmshttpsarxivorgabs260122050v1>#</a></h3><p><strong>Authors:</strong> Ghazal Kalhor, Behnam Bahrak
<strong>Venue:</strong> arXiv (2026)</p><p>In recent years, multilingual Large Language Models (LLMs) have become an inseparable part of daily life, making it crucial for them to master the rules of conversational language in order to communicate effectively with users. While previous work has evaluated LLMs&rsquo; understanding of figurative language in high-resource languages, their performance in low-resource languages remains underexplored. In this paper, we introduce MasalBench, a comprehensive benchmark for assessing LLMs&rsquo; contextual and cross-cultural understanding of Persian proverbs, which are a key component of conversation in this low-resource language. We evaluate eight state-of-the-art LLMs on MasalBench and find that they perform well in identifying Persian proverbs in context, achieving accuracies above 0.90. However, their performance drops considerably when tasked with identifying equivalent English proverbs, with the best model achieving 0.79 accuracy. Our findings highlight the limitations of current LLMs in cultural knowledge and analogical reasoning, and they provide a framework for assessing cross-cultural understanding in other low-resource languages. MasalBench is available at <a href=https://github.com/kalhorghazal/MasalBench>https://github.com/kalhorghazal/MasalBench</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22050v1">üìÑ Download PDF</a></p><hr><h3 id=tidyvoice-2026-challenge-evaluation-planhttpsarxivorgabs260121960v1><a href=https://arxiv.org/abs/2601.21960v1>TidyVoice 2026 Challenge Evaluation Plan</a><a hidden class=anchor aria-hidden=true href=#tidyvoice-2026-challenge-evaluation-planhttpsarxivorgabs260121960v1>#</a></h3><p><strong>Authors:</strong> Aref Farhadipour, Jan Marquenie, Srikanth Madikeri, Teodora Vukovic, Volker Dellwo, Kathy Reid, Francis M. Tyers, Ingo Siegert, Eleanor Chodroff
<strong>Venue:</strong> arXiv (2026)</p><p>The performance of speaker verification systems degrades significantly under language mismatch, a critical challenge exacerbated by the field&rsquo;s reliance on English-centric data. To address this, we propose the TidyVoice Challenge for cross-lingual speaker verification. The challenge leverages the TidyVoiceX dataset from the novel TidyVoice benchmark, a large-scale, multilingual corpus derived from Mozilla Common Voice, and specifically curated to isolate the effect of language switching across approximately 40 languages. Participants will be tasked with building systems robust to this mismatch, with performance primarily evaluated using the Equal Error Rate on cross-language trials. By providing standardized data, open-source baselines, and a rigorous evaluation protocol, this challenge aims to drive research towards fairer, more inclusive, and language-independent speaker recognition technologies, directly aligning with the Interspeech 2026 theme, &ldquo;Speaking Together.&rdquo;</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21960v1">üìÑ Download PDF</a></p><hr><h3 id=kid-knowledge-injected-dual-head-learning-for-knowledge-grounded-harmful-meme-detectionhttpsarxivorgabs260121796v1><a href=https://arxiv.org/abs/2601.21796v1>KID: Knowledge-Injected Dual-Head Learning for Knowledge-Grounded Harmful Meme Detection</a><a hidden class=anchor aria-hidden=true href=#kid-knowledge-injected-dual-head-learning-for-knowledge-grounded-harmful-meme-detectionhttpsarxivorgabs260121796v1>#</a></h3><p><strong>Authors:</strong> Yaocong Li, Leihan Zhang, Le Zhang, Qiang Yan
<strong>Venue:</strong> arXiv (2026)</p><p>Internet memes have become pervasive carriers of digital culture on social platforms. However, their heavy reliance on metaphors and sociocultural context also makes them subtle vehicles for harmful content, posing significant challenges for automated content moderation. Existing approaches primarily focus on intra-modal and inter-modal signal analysis, while the understanding of implicit toxicity often depends on background knowledge that is not explicitly present in the meme itself. To address this challenge, we propose KID, a Knowledge-Injected Dual-Head Learning framework for knowledge-grounded harmful meme detection. KID adopts a label-constrained distillation paradigm to decompose complex meme understanding into structured reasoning chains that explicitly link visual evidence, background knowledge, and classification labels. These chains guide the learning process by grounding external knowledge in meme-specific contexts. In addition, KID employs a dual-head architecture that jointly optimizes semantic generation and classification objectives, enabling aligned linguistic reasoning while maintaining stable decision boundaries. Extensive experiments on five multilingual datasets spanning English, Chinese, and low-resource Bengali demonstrate that KID achieves SOTA performance on both binary and multi-label harmful meme detection tasks, improving over previous best methods by 2.1%&ndash;19.7% across primary evaluation metrics. Ablation studies further confirm the effectiveness of knowledge injection and dual-head joint learning, highlighting their complementary contributions to robust and generalizable meme understanding. The code and data are available at <a href=https://github.com/PotatoDog1669/KID>https://github.com/PotatoDog1669/KID</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21796v1">üìÑ Download PDF</a></p><hr><h3 id=dimstance-multilingual-datasets-for-dimensional-stance-analysishttpsarxivorgabs260121483v1><a href=https://arxiv.org/abs/2601.21483v1>DimStance: Multilingual Datasets for Dimensional Stance Analysis</a><a hidden class=anchor aria-hidden=true href=#dimstance-multilingual-datasets-for-dimensional-stance-analysishttpsarxivorgabs260121483v1>#</a></h3><p><strong>Authors:</strong> Jonas Becker, Liang-Chih Yu, Shamsuddeen Hassan Muhammad, Jan Philip Wahle, Terry Ruas, Idris Abdulmumin, Lung-Hao Lee, Wen-Ni Liu, Tzu-Mi Lin, Zhe-Yu Xu, Ying-Lung Lin, Jin Wang, Maryam Ibrahim Mukhtar, Bela Gipp, Saif M. Mohammed
<strong>Venue:</strong> arXiv (2026)</p><p>Stance detection is an established task that classifies an author&rsquo;s attitude toward a specific target into categories such as Favor, Neutral, and Against. Beyond categorical stance labels, we leverage a long-established affective science framework to model stance along real-valued dimensions of valence (negative-positive) and arousal (calm-active). This dimensional approach captures nuanced affective states underlying stance expressions, enabling fine-grained stance analysis. To this end, we introduce DimStance, the first dimensional stance resource with valence-arousal (VA) annotations. This resource comprises 11,746 target aspects in 7,365 texts across five languages (English, German, Chinese, Nigerian Pidgin, and Swahili) and two domains (politics and environmental protection). To facilitate the evaluation of stance VA prediction, we formulate the dimensional stance regression task, analyze cross-lingual VA patterns, and benchmark pretrained and large language models under regression and prompting settings. Results show competitive performance of fine-tuned LLM regressors, persistent challenges in low-resource languages, and limitations of token-based generation. DimStance provides a foundation for multilingual, emotion-aware, stance analysis and benchmarking.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21483v1">üìÑ Download PDF</a></p><hr><h3 id=milore-ssl-scaling-multilingual-capabilities-in-self-supervised-models-without-forgettinghttpsarxivorgabs260120300v1><a href=https://arxiv.org/abs/2601.20300v1>MiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without Forgetting</a><a hidden class=anchor aria-hidden=true href=#milore-ssl-scaling-multilingual-capabilities-in-self-supervised-models-without-forgettinghttpsarxivorgabs260120300v1>#</a></h3><p><strong>Authors:</strong> Jing Xu, Minglin Wu, Xueyuan Chen, Xixin Wu, Helen Meng
<strong>Venue:</strong> arXiv (2026)</p><p>Self-supervised learning (SSL) has greatly advanced speech representation learning, but multilingual SSL models remain constrained to languages encountered during pretraining. Retraining from scratch to incorporate new languages is computationally expensive, while sequential training without migitation strategies often leads to catastrophic forgetting. To address this, we propose MiLorE-SSL, a lightweight framework that combines LoRA modules with a soft mixture-of-experts (MoE) mechanism for efficient continual multilingual training. LoRA provides efficient low-rank adaptation, while soft MoE promotes flexible expert sharing across languages, reducing cross-lingual interference. To further mitigate forgetting, we introduce limited replay data from existing languages, avoiding reliance on large historical corpora. Experiments on ML-SUPERB demonstrate that MiLorE-SSL achieves strong performance in new languages and improves the ability in existing ones with only 2.14% trainable parameters.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.20300v1">üìÑ Download PDF</a></p><hr><h3 id=ffe-halluhallucinations-in-fixed-figurative-expressionsbenchmark-of-idioms-and-proverbs-in-the-persian-languagehttpsarxivorgabs260120105v1><a href=https://arxiv.org/abs/2601.20105v1>FFE-Hallu:Hallucinations in Fixed Figurative Expressions:Benchmark of Idioms and Proverbs in the Persian Language</a><a hidden class=anchor aria-hidden=true href=#ffe-halluhallucinations-in-fixed-figurative-expressionsbenchmark-of-idioms-and-proverbs-in-the-persian-languagehttpsarxivorgabs260120105v1>#</a></h3><p><strong>Authors:</strong> Faezeh Hosseini, Mohammadali Yousefzadeh, Yadollah Yaghoobzadeh
<strong>Venue:</strong> arXiv (2026)</p><p>Figurative language, particularly fixed figurative expressions (FFEs) such as idioms and proverbs, poses persistent challenges for large language models (LLMs). Unlike literal phrases, FFEs are culturally grounded, largely non-compositional, and conventionally fixed, making them especially vulnerable to figurative hallucination. We define figurative hallucination as the generation or endorsement of expressions that sound idiomatic and plausible but do not exist as authentic figurative expressions in the target language. We introduce FFEHallu, the first comprehensive benchmark for evaluating figurative hallucination in LLMs, with a focus on Persian, a linguistically rich yet underrepresented language. FFEHallu consists of 600 carefully curated instances spanning three complementary tasks: (i) FFE generation from meaning, (ii) detection of fabricated FFEs across four controlled construction categories, and (iii) FFE to FFE translation from English to Persian. Evaluating six state of the art multilingual LLMs, we find systematic weaknesses in figurative competence and cultural grounding. While models such as GPT4.1 demonstrate relatively strong performance in rejecting fabricated FFEs and retrieving authentic ones, most models struggle to reliably distinguish real expressions from high quality fabrications and frequently hallucinate during cross lingual translation. These findings reveal substantial gaps in current LLMs handling of figurative language and underscore the need for targeted benchmarks to assess and mitigate figurative hallucination.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.20105v1">üìÑ Download PDF</a></p><hr><h3 id=linguamap-which-layers-of-llms-speak-your-language-and-how-to-tune-themhttpsarxivorgabs260120009v1><a href=https://arxiv.org/abs/2601.20009v1>LinguaMap: Which Layers of LLMs Speak Your Language and How to Tune Them?</a><a hidden class=anchor aria-hidden=true href=#linguamap-which-layers-of-llms-speak-your-language-and-how-to-tune-themhttpsarxivorgabs260120009v1>#</a></h3><p><strong>Authors:</strong> J. Ben Tamo, Daniel Carlander-Reuterfelt, Jonathan Rubin, Dezhi Hong, Mingxian Wang, Oleg Poliannikov
<strong>Venue:</strong> arXiv (2026)</p><p>Despite multilingual pretraining, large language models often struggle with non-English tasks, particularly in language control, the ability to respond in the intended language. We identify and characterize two key failure modes: the multilingual transfer bottleneck (correct language, incorrect task response) and the language consistency bottleneck (correct task response, wrong language). To systematically surface these issues, we design a four-scenario evaluation protocol spanning MMLU, MGSM, and XQuAD benchmarks. To probe these issues with interpretability, we extend logit lens analysis to track language probabilities layer by layer and compute cross-lingual semantic similarity of hidden states. The results reveal a three-phase internal structure: early layers align inputs into a shared semantic space, middle layers perform task reasoning, and late layers drive language-specific generation. Guided by these insights, we introduce selective fine-tuning of only the final layers responsible for language control. On Qwen-3-32B and Bloom-7.1B, this method achieves over 98 percent language consistency across six languages while fine-tuning only 3-5 percent of parameters, without sacrificing task accuracy. Importantly, this result is nearly identical to that of full-scope fine-tuning (for example, above 98 percent language consistency for both methods across all prompt scenarios) but uses a fraction of the computational resources. To the best of our knowledge, this is the first approach to leverage layer-localization of language control for efficient multilingual adaptation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.20009v1">üìÑ Download PDF</a></p><hr><h3 id=dynamic-multi-expert-projectors-with-stabilized-routing-for-multilingual-speech-recognitionhttpsarxivorgabs260119451v1><a href=https://arxiv.org/abs/2601.19451v1>Dynamic Multi-Expert Projectors with Stabilized Routing for Multilingual Speech Recognition</a><a hidden class=anchor aria-hidden=true href=#dynamic-multi-expert-projectors-with-stabilized-routing-for-multilingual-speech-recognitionhttpsarxivorgabs260119451v1>#</a></h3><p><strong>Authors:</strong> Isha Pandey, Ashish Mittal, Vartul Bahuguna, Ganesh Ramakrishnan
<strong>Venue:</strong> arXiv (2026)</p><p>Recent advances in LLM-based ASR connect frozen speech encoders with Large Language Models (LLMs) via lightweight projectors. While effective in monolingual settings, a single projector struggles to capture the diverse acoustic-to-semantic mappings required for multilingual ASR. To address this, we propose SMEAR-MoE, a stabilized Mixture-of-Experts projector that ensures dense gradient flow to all experts, preventing expert collapse while enabling cross-lingual sharing. We systematically compare monolithic, static multi-projector, and dynamic MoE designs across four Indic languages (Hindi, Marathi, Tamil, Telugu). Our SMEAR-MoE achieves strong performance, delivering upto a 7.6% relative WER reduction over the single-projector baseline, while maintaining comparable runtime efficiency. Analysis of expert routing further shows linguistically meaningful specialization, with related languages sharing experts. These results demonstrate that stable multi-expert projectors are key to scalable and robust multilingual ASR.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.19451v1">üìÑ Download PDF</a></p><hr><h3 id=llm-forcedaligner-a-non-autoregressive-and-accurate-llm-based-forced-aligner-for-multilingual-and-long-form-speechhttpsarxivorgabs260118220v1><a href=https://arxiv.org/abs/2601.18220v1>LLM-ForcedAligner: A Non-Autoregressive and Accurate LLM-Based Forced Aligner for Multilingual and Long-Form Speech</a><a hidden class=anchor aria-hidden=true href=#llm-forcedaligner-a-non-autoregressive-and-accurate-llm-based-forced-aligner-for-multilingual-and-long-form-speechhttpsarxivorgabs260118220v1>#</a></h3><p><strong>Authors:</strong> Bingshen Mu, Xian Shi, Xiong Wang, Hexin Liu, Jin Xu, Lei Xie
<strong>Venue:</strong> arXiv (2026)</p><p>Forced alignment (FA) predicts start and end timestamps for words or characters in speech, but existing methods are language-specific and prone to cumulative temporal shifts. The multilingual speech understanding and long-sequence processing abilities of speech large language models (SLLMs) make them promising for FA in multilingual, crosslingual, and long-form speech settings. However, directly applying the next-token prediction paradigm of SLLMs to FA results in hallucinations and slow inference. To bridge the gap, we propose LLM-ForcedAligner, reformulating FA as a slot-filling paradigm: timestamps are treated as discrete indices, and special timestamp tokens are inserted as slots into the transcript. Conditioned on the speech embeddings and the transcript with slots, the SLLM directly predicts the time indices at slots. During training, causal attention masking with non-shifted input and label sequences allows each slot to predict its own timestamp index based on itself and preceding context, with loss computed only at slot positions. Dynamic slot insertion enables FA at arbitrary positions. Moreover, non-autoregressive inference is supported, avoiding hallucinations and improving speed. Experiments across multilingual, crosslingual, and long-form speech scenarios show that LLM-ForcedAligner achieves a 69%~78% relative reduction in accumulated averaging shift compared with prior methods. The checkpoint and inference code will be released later.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.18220v1">üìÑ Download PDF</a></p><hr><h3 id=evaluating-chatgpt-on-medical-information-extraction-tasks-performance-explainability-and-beyondhttpsarxivorgabs260121767v1><a href=https://arxiv.org/abs/2601.21767v1>Evaluating ChatGPT on Medical Information Extraction Tasks: Performance, Explainability and Beyond</a><a hidden class=anchor aria-hidden=true href=#evaluating-chatgpt-on-medical-information-extraction-tasks-performance-explainability-and-beyondhttpsarxivorgabs260121767v1>#</a></h3><p><strong>Authors:</strong> Wei Zhu
<strong>Venue:</strong> arXiv (2026)</p><p>Large Language Models (LLMs) like ChatGPT have demonstrated amazing capabilities in comprehending user intents and generate reasonable and useful responses. Beside their ability to chat, their capabilities in various natural language processing (NLP) tasks are of interest to the research community. In this paper, we focus on assessing the overall ability of ChatGPT in 4 different medical information extraction (MedIE) tasks across 6 benchmark datasets. We present the systematically analysis by measuring ChatGPT&rsquo;s performance, explainability, confidence, faithfulness, and uncertainty. Our experiments reveal that: (a) ChatGPT&rsquo;s performance scores on MedIE tasks fall behind those of the fine-tuned baseline models. (b) ChatGPT can provide high-quality explanations for its decisions, however, ChatGPT is over-confident in its predcitions. (c) ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. (d) The uncertainty in generation causes uncertainty in information extraction results, thus may hinder its applications in MedIE tasks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21767v1">üìÑ Download PDF</a></p><hr><h3 id=ce-gocd-central-entity-guided-graph-optimization-for-community-detection-to-augment-llm-scientific-question-answeringhttpsarxivorgabs260121733v1><a href=https://arxiv.org/abs/2601.21733v1>CE-GOCD: Central Entity-Guided Graph Optimization for Community Detection to Augment LLM Scientific Question Answering</a><a hidden class=anchor aria-hidden=true href=#ce-gocd-central-entity-guided-graph-optimization-for-community-detection-to-augment-llm-scientific-question-answeringhttpsarxivorgabs260121733v1>#</a></h3><p><strong>Authors:</strong> Jiayin Lan, Jiaqi Li, Baoxin Wang, Ming Liu, Dayong Wu, Shijin Wang, Bing Qin, Guoping Hu
<strong>Venue:</strong> arXiv (2026)</p><p>Large Language Models (LLMs) are increasingly used for question answering over scientific research papers. Existing retrieval augmentation methods often rely on isolated text chunks or concepts, but overlook deeper semantic connections between papers. This impairs the LLM&rsquo;s comprehension of scientific literature, hindering the comprehensiveness and specificity of its responses. To address this, we propose Central Entity-Guided Graph Optimization for Community Detection (CE-GOCD), a method that augments LLMs&rsquo; scientific question answering by explicitly modeling and leveraging semantic substructures within academic knowledge graphs. Our approach operates by: (1) leveraging paper titles as central entities for targeted subgraph retrieval, (2) enhancing implicit semantic discovery via subgraph pruning and completion, and (3) applying community detection to distill coherent paper groups with shared themes. We evaluated the proposed method on three NLP literature-based question-answering datasets, and the results demonstrate its superiority over other retrieval-augmented baseline approaches, confirming the effectiveness of our framework.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21733v1">üìÑ Download PDF</a></p><hr><h3 id=enhancing-language-models-for-robust-greenwashing-detectionhttpsarxivorgabs260121722v1><a href=https://arxiv.org/abs/2601.21722v1>Enhancing Language Models for Robust Greenwashing Detection</a><a hidden class=anchor aria-hidden=true href=#enhancing-language-models-for-robust-greenwashing-detectionhttpsarxivorgabs260121722v1>#</a></h3><p><strong>Authors:</strong> Neil Heinrich Braun, Keane Ong, Rui Mao, Erik Cambria, Gianmarco Mengaldo
<strong>Venue:</strong> arXiv (2026)</p><p>Sustainability reports are critical for ESG assessment, yet greenwashing and vague claims often undermine their reliability. Existing NLP models lack robustness to these practices, typically relying on surface-level patterns that generalize poorly. We propose a parameter-efficient framework that structures LLM latent spaces by combining contrastive learning with an ordinal ranking objective to capture graded distinctions between concrete actions and ambiguous claims. Our approach incorporates gated feature modulation to filter disclosure noise and utilizes MetaGradNorm to stabilize multi-objective optimization. Experiments in cross-category settings demonstrate superior robustness over standard baselines while revealing a trade-off between representational rigidity and generalization.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21722v1">üìÑ Download PDF</a></p><hr><h3 id=language-models-as-artificial-learners-investigating-crosslinguistic-influencehttpsarxivorgabs260121587v1><a href=https://arxiv.org/abs/2601.21587v1>Language Models as Artificial Learners: Investigating Crosslinguistic Influence</a><a hidden class=anchor aria-hidden=true href=#language-models-as-artificial-learners-investigating-crosslinguistic-influencehttpsarxivorgabs260121587v1>#</a></h3><p><strong>Authors:</strong> Abderrahmane Issam, Yusuf Can Semerci, Jan Scholtes, Gerasimos Spanakis
<strong>Venue:</strong> arXiv (2026)</p><p>Despite the centrality of crosslinguistic influence (CLI) to bilingualism research, human studies often yield conflicting results due to inherent experimental variance. We address these inconsistencies by using language models (LMs) as controlled statistical learners to systematically simulate CLI and isolate its underlying drivers. Specifically, we study the effect of varying the L1 language dominance and the L2 language proficiency, which we manipulate by controlling the L2 age of exposure &ndash; defined as the training step at which the L2 is introduced. Furthermore, we investigate the impact of pretraining on L1 languages with varying syntactic distance from the L2. Using cross-linguistic priming, we analyze how activating L1 structures impacts L2 processing. Our results align with evidence from psycholinguistic studies, confirming that language dominance and proficiency are strong predictors of CLI. We further find that while priming of grammatical structures is bidirectional, the priming of ungrammatical structures is sensitive to language dominance. Finally, we provide mechanistic evidence of CLI in LMs, demonstrating that the L1 is co-activated during L2 processing and directly influences the neural circuitry recruited for the L2. More broadly, our work demonstrates that LMs can serve as a computational framework to inform theories of human CLI.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21587v1">üìÑ Download PDF</a></p><hr><h3 id=icl-evader-zero-query-black-box-evasion-attacks-on-in-context-learning-and-their-defenseshttpsarxivorgabs260121586v1><a href=https://arxiv.org/abs/2601.21586v1>ICL-EVADER: Zero-Query Black-Box Evasion Attacks on In-Context Learning and Their Defenses</a><a hidden class=anchor aria-hidden=true href=#icl-evader-zero-query-black-box-evasion-attacks-on-in-context-learning-and-their-defenseshttpsarxivorgabs260121586v1>#</a></h3><p><strong>Authors:</strong> Ningyuan He, Ronghong Huang, Qianqian Tang, Hongyu Wang, Xianghang Mi, Shanqing Guo
<strong>Venue:</strong> arXiv (2026)</p><p>In-context learning (ICL) has become a powerful, data-efficient paradigm for text classification using large language models. However, its robustness against realistic adversarial threats remains largely unexplored. We introduce ICL-Evader, a novel black-box evasion attack framework that operates under a highly practical zero-query threat model, requiring no access to model parameters, gradients, or query-based feedback during attack generation. We design three novel attacks, Fake Claim, Template, and Needle-in-a-Haystack, that exploit inherent limitations of LLMs in processing in-context prompts. Evaluated across sentiment analysis, toxicity, and illicit promotion tasks, our attacks significantly degrade classifier performance (e.g., achieving up to 95.3% attack success rate), drastically outperforming traditional NLP attacks which prove ineffective under the same constraints. To counter these vulnerabilities, we systematically investigate defense strategies and identify a joint defense recipe that effectively mitigates all attacks with minimal utility loss (&lt;5% accuracy degradation). Finally, we translate our defensive insights into an automated tool that proactively fortifies standard ICL prompts against adversarial evasion. This work provides a comprehensive security assessment of ICL, revealing critical vulnerabilities and offering practical solutions for building more robust systems. Our source code and evaluation datasets are publicly available at: <a href=https://github.com/ChaseSecurity/ICL-Evader>https://github.com/ChaseSecurity/ICL-Evader</a> .</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21586v1">üìÑ Download PDF</a></p><hr><h3 id=icon-intent-context-coupling-for-efficient-multi-turn-jailbreak-attackhttpsarxivorgabs260120903v1><a href=https://arxiv.org/abs/2601.20903v1>ICON: Intent-Context Coupling for Efficient Multi-Turn Jailbreak Attack</a><a hidden class=anchor aria-hidden=true href=#icon-intent-context-coupling-for-efficient-multi-turn-jailbreak-attackhttpsarxivorgabs260120903v1>#</a></h3><p><strong>Authors:</strong> Xingwei Lin, Wenhao Lin, Sicong Cao, Jiahao Yu, Renke Huang, Lei Xue, Chunming Wu
<strong>Venue:</strong> arXiv (2026)</p><p>Multi-turn jailbreak attacks have emerged as a critical threat to Large Language Models (LLMs), bypassing safety mechanisms by progressively constructing adversarial contexts from scratch and incrementally refining prompts. However, existing methods suffer from the inefficiency of incremental context construction that requires step-by-step LLM interaction, and often stagnate in suboptimal regions due to surface-level optimization. In this paper, we characterize the Intent-Context Coupling phenomenon, revealing that LLM safety constraints are significantly relaxed when a malicious intent is coupled with a semantically congruent context pattern. Driven by this insight, we propose ICON, an automated multi-turn jailbreak framework that efficiently constructs an authoritative-style context via prior-guided semantic routing. Specifically, ICON first routes the malicious intent to a congruent context pattern (e.g., Scientific Research) and instantiates it into an attack prompt sequence. This sequence progressively builds the authoritative-style context and ultimately elicits prohibited content. In addition, ICON incorporates a Hierarchical Optimization Strategy that combines local prompt refinement with global context switching, preventing the attack from stagnating in ineffective contexts. Experimental results across eight SOTA LLMs demonstrate the effectiveness of ICON, achieving a state-of-the-art average Attack Success Rate (ASR) of 97.1%. Code is available at <a href=https://github.com/xwlin-roy/ICON>https://github.com/xwlin-roy/ICON</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.20903v1">üìÑ Download PDF</a></p><hr><h3 id=beyond-speedup----utilizing-kv-cache-for-sampling-and-reasoninghttpsarxivorgabs260120326v1><a href=https://arxiv.org/abs/2601.20326v1>Beyond Speedup &ndash; Utilizing KV Cache for Sampling and Reasoning</a><a hidden class=anchor aria-hidden=true href=#beyond-speedup----utilizing-kv-cache-for-sampling-and-reasoninghttpsarxivorgabs260120326v1>#</a></h3><p><strong>Authors:</strong> Zeyu Xing, Xing Li, Hui-Ling Zhen, Mingxuan Yuan, Sinno Jialin Pan
<strong>Venue:</strong> arXiv (2026)</p><p>KV caches, typically used only to speed up autoregressive decoding, encode contextual information that can be reused for downstream tasks at no extra cost. We propose treating the KV cache as a lightweight representation, eliminating the need to recompute or store full hidden states. Despite being weaker than dedicated embeddings, KV-derived representations are shown to be sufficient for two key applications: \textbf{(i) Chain-of-Embedding}, where they achieve competitive or superior performance on Llama-3.1-8B-Instruct and Qwen2-7B-Instruct; and \textbf{(ii) Fast/Slow Thinking Switching}, where they enable adaptive reasoning on Qwen3-8B and DeepSeek-R1-Distil-Qwen-14B, reducing token generation by up to $5.7\times$ with minimal accuracy loss. Our findings establish KV caches as a free, effective substrate for sampling and reasoning, opening new directions for representation reuse in LLM inference. Code: <a href=https://github.com/cmd2001/ICLR2026_KV-Embedding>https://github.com/cmd2001/ICLR2026_KV-Embedding</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.20326v1">üìÑ Download PDF</a></p><hr><h3 id=probing-the-future-of-meta-analysis-eliciting-design-principles-via-an-agentic-research-idehttpsarxivorgabs260118239v1><a href=https://arxiv.org/abs/2601.18239v1>Probing the Future of Meta-Analysis: Eliciting Design Principles via an Agentic Research IDE</a><a hidden class=anchor aria-hidden=true href=#probing-the-future-of-meta-analysis-eliciting-design-principles-via-an-agentic-research-idehttpsarxivorgabs260118239v1>#</a></h3><p><strong>Authors:</strong> Sizhe Cheng, Feng Liang, Yuhan Wen, Xipei Yu, Yong Wang
<strong>Venue:</strong> arXiv (2026)</p><p>Meta-analyses and systematic reviews demand rigorous abductive reasoning to build, test, and refine hypotheses across vast, heterogeneous literature. While NLP advancements have automated parts of this pipeline, existing tools often detach researchers from the cognitive loop or function merely as retrieval engines, leading to loss of intellectual ownership and frequent context switching. We present Research IDE, a prototype reimagining authoring environments through the &ldquo;Research as Code&rdquo; metaphor. Research IDE embeds a multi-agent backend into the writing flow, enabling in-situ verification via &ldquo;hypothesis breakpoints.&rdquo; A one-week field deployment with 8 domain experts, followed by a reflective workshop, as a Research through Design (RtD) probe, reveals that users strongly preferred this verification workflow, actively leveraged prior knowledge for confirmation, and reported that breakpoints sparked insights. Drawing from participant feedback and suggestions, we derive design implications for future AI-assisted research tools that fully preserve researcher autonomy and intellectual ownership while harnessing computational scale.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.18239v1">üìÑ Download PDF</a></p><hr><h3 id=vibevoice-asr-technical-reporthttpsarxivorgabs260118184v1><a href=https://arxiv.org/abs/2601.18184v1>VIBEVOICE-ASR Technical Report</a><a hidden class=anchor aria-hidden=true href=#vibevoice-asr-technical-reporthttpsarxivorgabs260118184v1>#</a></h3><p><strong>Authors:</strong> Zhiliang Peng, Jianwei Yu, Yaoyao Chang, Zilong Wang, Li Dong, Yingbo Hao, Yujie Tu, Chenyu Yang, Wenhui Wang, Songchen Xu, Yutao Sun, Hangbo Bao, Weijiang Xu, Yi Zhu, Zehua Wang, Ting Song, Yan Xia, Zewen Chi, Shaohan Huang, Liang Wang, Chuang Ding, Shuai Wang, Xie Chen, Furu Wei
<strong>Venue:</strong> arXiv (2026)</p><p>This report presents VibeVoice-ASR, a general-purpose speech understanding framework built upon VibeVoice, designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VibeVoice-ASRsupports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into a single end-to-end generation task. In addition, VibeVoice-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce a prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.18184v1">üìÑ Download PDF</a></p><hr><h3 id=the-augmented-nlp-bound-for-maximum-entropy-remote-samplinghttpsarxivorgabs260120970v1><a href=https://arxiv.org/abs/2601.20970v1>The augmented NLP bound for maximum-entropy remote sampling</a><a hidden class=anchor aria-hidden=true href=#the-augmented-nlp-bound-for-maximum-entropy-remote-samplinghttpsarxivorgabs260120970v1>#</a></h3><p><strong>Authors:</strong> Gabriel Ponte, Marcia Fampa, Jon Lee
<strong>Venue:</strong> arXiv (2026)</p><p>The maximum-entropy remote sampling problem (MERSP) is to select a subset of s random variables from a set of n random variables, so as to maximize the information concerning a set of target random variables that are not directly observable. We assume throughout that the set of all of these random variables follows a joint Gaussian distribution, and that we have the covariance matrix available. Finally, we measure information using Shannon&rsquo;s differential entropy.
The main approach for exact solution of moderate-sized instances of MERSP has been branch-and-bound, and so previous work concentrated on upper bounds. Prior to our work, there were two upper-bounding methods for MERSP: the so-called NLP bound and the spectral bound, both introduced 25 years ago. We are able now to establish domination results between these two upper bounds. We propose an ``augmented NLP bound&rsquo;&rsquo; based on a subtle convex relaxation. We provide theoretical guarantees, giving sufficient conditions under which the augmented NLP bound strictly dominates the ordinary NLP bound. In addition, the augmented NLP formulation allows us to derive upper bounds for rank-deficient covariance matrices when they satisfy a technical condition. This is in contrast to the earlier work on the ordinary NLP bound that worked with only positive definite covariance matrices. Finally, we introduce a novel and very effective diagonal-scaling technique for MERSP, employing a positive vector of parameters. Numerical experiments on benchmark instances demonstrate the effectiveness of our approaches in advancing the state of the art for calculating upper bounds on MERSP.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.20970v1">üìÑ Download PDF</a></p><hr><h3 id=redsage-a-cybersecurity-generalist-llmhttpsarxivorgabs260122159v1><a href=https://arxiv.org/abs/2601.22159v1>RedSage: A Cybersecurity Generalist LLM</a><a hidden class=anchor aria-hidden=true href=#redsage-a-cybersecurity-generalist-llmhttpsarxivorgabs260122159v1>#</a></h3><p><strong>Authors:</strong> Naufal Suryanto, Muzammal Naseer, Pengfei Li, Syed Talal Wasim, Jinhui Yi, Juergen Gall, Paolo Ceravolo, Ernesto Damiani
<strong>Venue:</strong> arXiv (2026)</p><p>Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&amp;A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22159v1">üìÑ Download PDF</a></p><hr><h3 id=discovering-hidden-gems-in-model-repositorieshttpsarxivorgabs260122157v1><a href=https://arxiv.org/abs/2601.22157v1>Discovering Hidden Gems in Model Repositories</a><a hidden class=anchor aria-hidden=true href=#discovering-hidden-gems-in-model-repositorieshttpsarxivorgabs260122157v1>#</a></h3><p><strong>Authors:</strong> Jonathan Kahana, Eliahu Horwitz, Yedid Hoshen
<strong>Venue:</strong> arXiv (2026)</p><p>Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of &ldquo;hidden gems&rdquo;, unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22157v1">üìÑ Download PDF</a></p><hr><h3 id=hybrid-linear-attention-done-right-efficient-distillation-and-effective-architectures-for-extremely-long-contextshttpsarxivorgabs260122156v1><a href=https://arxiv.org/abs/2601.22156v1>Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts</a><a hidden class=anchor aria-hidden=true href=#hybrid-linear-attention-done-right-efficient-distillation-and-effective-architectures-for-extremely-long-contextshttpsarxivorgabs260122156v1>#</a></h3><p><strong>Authors:</strong> Yingfa Chen, Zhen Leng Thai, Zihan Zhou, Zhu Zhang, Xingyu Shen, Shuo Wang, Chaojun Xiao, Xu Han, Zhiyuan Liu
<strong>Venue:</strong> arXiv (2026)</p><p>Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22156v1">üìÑ Download PDF</a></p><hr><h3 id=ueval-a-benchmark-for-unified-multimodal-generationhttpsarxivorgabs260122155v1><a href=https://arxiv.org/abs/2601.22155v1>UEval: A Benchmark for Unified Multimodal Generation</a><a hidden class=anchor aria-hidden=true href=#ueval-a-benchmark-for-unified-multimodal-generationhttpsarxivorgabs260122155v1>#</a></h3><p><strong>Authors:</strong> Bo Li, Yida Yin, Wenhao Chai, Xingyu Fu, Zhuang Liu
<strong>Venue:</strong> arXiv (2026)</p><p>We introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22155v1">üìÑ Download PDF</a></p><hr><h3 id=exploring-reasoning-reward-model-for-agentshttpsarxivorgabs260122154v1><a href=https://arxiv.org/abs/2601.22154v1>Exploring Reasoning Reward Model for Agents</a><a hidden class=anchor aria-hidden=true href=#exploring-reasoning-reward-model-for-agentshttpsarxivorgabs260122154v1>#</a></h3><p><strong>Authors:</strong> Kaixuan Fan, Kaituo Feng, Manyuan Zhang, Tianshuo Peng, Zhixun Li, Yilei Jiang, Shuang Chen, Peng Pei, Xunliang Cai, Xiangyu Yue
<strong>Venue:</strong> arXiv (2026)</p><p>Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22154v1">üìÑ Download PDF</a></p><hr><h3 id=dynamicvla-a-vision-language-action-model-for-dynamic-object-manipulationhttpsarxivorgabs260122153v1><a href=https://arxiv.org/abs/2601.22153v1>DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation</a><a hidden class=anchor aria-hidden=true href=#dynamicvla-a-vision-language-action-model-for-dynamic-object-manipulationhttpsarxivorgabs260122153v1>#</a></h3><p><strong>Authors:</strong> Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong, Haiwen Diao, Ziwei Liu
<strong>Venue:</strong> arXiv (2026)</p><p>Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22153v1">üìÑ Download PDF</a></p><hr><h3 id=reactemg-stroke-healthy-to-stroke-few-shot-adaptation-for-semg-based-intent-detectionhttpsarxivorgabs260122090v1><a href=https://arxiv.org/abs/2601.22090v1>ReactEMG Stroke: Healthy-to-Stroke Few-shot Adaptation for sEMG-Based Intent Detection</a><a hidden class=anchor aria-hidden=true href=#reactemg-stroke-healthy-to-stroke-few-shot-adaptation-for-semg-based-intent-detectionhttpsarxivorgabs260122090v1>#</a></h3><p><strong>Authors:</strong> Runsheng Wang, Katelyn Lee, Xinyue Zhu, Lauren Winterbottom, Dawn M. Nilsen, Joel Stein, Matei Ciocarlie
<strong>Venue:</strong> arXiv (2026)</p><p>Surface electromyography (sEMG) is a promising control signal for assist-as-needed hand rehabilitation after stroke, but detecting intent from paretic muscles often requires lengthy, subject-specific calibration and remains brittle to variability. We propose a healthy-to-stroke adaptation pipeline that initializes an intent detector from a model pretrained on large-scale able-bodied sEMG, then fine-tunes it for each stroke participant using only a small amount of subject-specific data. Using a newly collected dataset from three individuals with chronic stroke, we compare adaptation strategies (head-only tuning, parameter-efficient LoRA adapters, and full end-to-end fine-tuning) and evaluate on held-out test sets that include realistic distribution shifts such as within-session drift, posture changes, and armband repositioning. Across conditions, healthy-pretrained adaptation consistently improves stroke intent detection relative to both zero-shot transfer and stroke-only training under the same data budget; the best adaptation methods improve average transition accuracy from 0.42 to 0.61 and raw accuracy from 0.69 to 0.78. These results suggest that transferring a reusable healthy-domain EMG representation can reduce calibration burden while improving robustness for real-time post-stroke intent detection.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22090v1">üìÑ Download PDF</a></p><hr><h3 id=learning-transient-convective-heat-transfer-with-geometry-aware-world-modelshttpsarxivorgabs260122086v1><a href=https://arxiv.org/abs/2601.22086v1>Learning Transient Convective Heat Transfer with Geometry Aware World Models</a><a hidden class=anchor aria-hidden=true href=#learning-transient-convective-heat-transfer-with-geometry-aware-world-modelshttpsarxivorgabs260122086v1>#</a></h3><p><strong>Authors:</strong> Onur T. Doganay, Alexander Klawonn, Martin Eigel, Hanno Gottschalk
<strong>Venue:</strong> arXiv (2026)</p><p>Partial differential equation (PDE) simulations are fundamental to engineering and physics but are often computationally prohibitive for real-time applications. While generative AI offers a promising avenue for surrogate modeling, standard video generation architectures lack the specific control and data compatibility required for physical simulations. This paper introduces a geometry aware world model architecture, derived from a video generation architecture (LongVideoGAN), designed to learn transient physics. We introduce two key architecture elements: (1) a twofold conditioning mechanism incorporating global physical parameters and local geometric masks, and (2) an architectural adaptation to support arbitrary channel dimensions, moving beyond standard RGB constraints. We evaluate this approach on a 2D transient computational fluid dynamics (CFD) problem involving convective heat transfer from buoyancy-driven flow coupled to a heat flow in a solid structure. We demonstrate that the conditioned model successfully reproduces complex temporal dynamics and spatial correlations of the training data. Furthermore, we assess the model&rsquo;s generalization capabilities on unseen geometric configurations, highlighting both its potential for controlled simulation synthesis and current limitations in spatial precision for out-of-distribution samples.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22086v1">üìÑ Download PDF</a></p><hr><h3 id=learning-to-communicate-across-modalities-perceptual-heterogeneity-in-multi-agent-systemshttpsarxivorgabs260122041v1><a href=https://arxiv.org/abs/2601.22041v1>Learning to Communicate Across Modalities: Perceptual Heterogeneity in Multi-Agent Systems</a><a hidden class=anchor aria-hidden=true href=#learning-to-communicate-across-modalities-perceptual-heterogeneity-in-multi-agent-systemshttpsarxivorgabs260122041v1>#</a></h3><p><strong>Authors:</strong> Naomi Pitzer, Daniela Mihai
<strong>Venue:</strong> arXiv (2026)</p><p>Emergent communication offers insight into how agents develop shared structured representations, yet most research assumes homogeneous modalities or aligned representational spaces, overlooking the perceptual heterogeneity of real-world settings. We study a heterogeneous multi-step binary communication game where agents differ in modality and lack perceptual grounding. Despite perceptual misalignment, multimodal systems converge to class-consistent messages grounded in perceptual input. Unimodal systems communicate more efficiently, using fewer bits and achieving lower classification entropy, while multimodal agents require greater information exchange and exhibit higher uncertainty. Bit perturbation experiments provide strong evidence that meaning is encoded in a distributional rather than compositional manner, as each bit&rsquo;s contribution depends on its surrounding pattern. Finally, interoperability analyses show that systems trained in different perceptual worlds fail to directly communicate, but limited fine-tuning enables successful cross-system communication. This work positions emergent communication as a framework for studying how agents adapt and transfer representations across heterogeneous modalities, opening new directions for both theory and experimentation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22041v1">üìÑ Download PDF</a></p><hr><h3 id=routing-the-lottery-adaptive-subnetworks-for-heterogeneous-datahttpsarxivorgabs260122141v1><a href=https://arxiv.org/abs/2601.22141v1>Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data</a><a hidden class=anchor aria-hidden=true href=#routing-the-lottery-adaptive-subnetworks-for-heterogeneous-datahttpsarxivorgabs260122141v1>#</a></h3><p><strong>Authors:</strong> Grzegorz Stefanski, Alberto Presta, Michal Byra
<strong>Venue:</strong> arXiv (2026)</p><p>In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22141v1">üìÑ Download PDF</a></p><hr><h3 id=reasoning-while-asking-transforming-reasoning-large-language-models-from-passive-solvers-to-proactive-inquirershttpsarxivorgabs260122139v1><a href=https://arxiv.org/abs/2601.22139v1>Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers</a><a hidden class=anchor aria-hidden=true href=#reasoning-while-asking-transforming-reasoning-large-language-models-from-passive-solvers-to-proactive-inquirershttpsarxivorgabs260122139v1>#</a></h3><p><strong>Authors:</strong> Xin Chen, Feng Jiang, Yiqian Zhang, Hardy Chen, Shuo Yan, Wenya Xie, Min Yang, Shujian Huang
<strong>Venue:</strong> arXiv (2026)</p><p>Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70% higher accuracy, 22.90% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22139v1">üìÑ Download PDF</a></p><hr><h3 id=value-based-pre-training-with-downstream-feedbackhttpsarxivorgabs260122108v1><a href=https://arxiv.org/abs/2601.22108v1>Value-Based Pre-Training with Downstream Feedback</a><a hidden class=anchor aria-hidden=true href=#value-based-pre-training-with-downstream-feedbackhttpsarxivorgabs260122108v1>#</a></h3><p><strong>Authors:</strong> Shuqi Ke, Giulia Fanti
<strong>Venue:</strong> arXiv (2026)</p><p>Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B&ndash;7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22108v1">üìÑ Download PDF</a></p><hr><h3 id=sina-a-circuit-schematic-image-to-netlist-generator-using-artificial-intelligencehttpsarxivorgabs260122114v1><a href=https://arxiv.org/abs/2601.22114v1>SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence</a><a hidden class=anchor aria-hidden=true href=#sina-a-circuit-schematic-image-to-netlist-generator-using-artificial-intelligencehttpsarxivorgabs260122114v1>#</a></h3><p><strong>Authors:</strong> Saoud Aldowaish, Yashwanth Karumanchi, Kai-Chen Chiang, Soroosh Noorzad, Morteza Fayazi
<strong>Venue:</strong> arXiv (2026)</p><p>Current methods for converting circuit schematic images into machine-readable netlists struggle with component recognition and connectivity inference. In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator. SINA integrates deep learning for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity extraction, and Optical Character Recognition (OCR) for component reference designator retrieval, while employing a Vision-Language Model (VLM) for reliable reference designator assignments. In our experiments, SINA achieves 96.47% overall netlist-generation accuracy, which is 2.72x higher than state-of-the-art approaches.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22114v1">üìÑ Download PDF</a></p><hr><h3 id=vision-deepresearch-incentivizing-deepresearch-capability-in-multimodal-large-language-modelshttpsarxivorgabs260122060v1><a href=https://arxiv.org/abs/2601.22060v1>Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models</a><a hidden class=anchor aria-hidden=true href=#vision-deepresearch-incentivizing-deepresearch-capability-in-multimodal-large-language-modelshttpsarxivorgabs260122060v1>#</a></h3><p><strong>Authors:</strong> Wenxuan Huang, Yu Zeng, Qiuchen Wang, Zhen Fang, Shaosheng Cao, Zheng Chu, Qingyu Yin, Shuang Chen, Zhenfei Yin, Lin Chen, Zehui Chen, Yao Hu, Philip Torr, Feng Zhao, Wanli Ouyang
<strong>Venue:</strong> arXiv (2026)</p><p>Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call&rsquo;&rsquo; for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in <a href=https://github.com/Osilly/Vision-DeepResearch>https://github.com/Osilly/Vision-DeepResearch</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22060v1">üìÑ Download PDF</a></p><hr><h3 id=g2-reader-dual-evolving-graphs-for-multimodal-document-qahttpsarxivorgabs260122055v1><a href=https://arxiv.org/abs/2601.22055v1>$G^2$-Reader: Dual Evolving Graphs for Multimodal Document QA</a><a hidden class=anchor aria-hidden=true href=#g2-reader-dual-evolving-graphs-for-multimodal-document-qahttpsarxivorgabs260122055v1>#</a></h3><p><strong>Authors:</strong> Yaxin Du, Junru Song, Yifan Zhou, Cheng Wang, Jiahao Gu, Zimeng Chen, Menglan Chen, Wen Yao, Yang Yang, Ying Wen, Siheng Chen
<strong>Venue:</strong> arXiv (2026)</p><p>Retrieval-augmented generation is a practical paradigm for question answering over long documents, but it remains brittle for multimodal reading where text, tables, and figures are interleaved across many pages. First, flat chunking breaks document-native structure and cross-modal alignment, yielding semantic fragments that are hard to interpret in isolation. Second, even iterative retrieval can fail in long contexts by looping on partial evidence or drifting into irrelevant sections as noise accumulates, since each step is guided only by the current snippet without a persistent global search state. We introduce $G^2$-Reader, a dual-graph system, to address both issues. It evolves a Content Graph to preserve document-native structure and cross-modal semantics, and maintains a Planning Graph, an agentic directed acyclic graph of sub-questions, to track intermediate findings and guide stepwise navigation for evidence completion. On VisDoMBench across five multimodal domains, $G^2$-Reader with Qwen3-VL-32B-Instruct reaches 66.21% average accuracy, outperforming strong baselines and a standalone GPT-5 (53.08%).</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22055v1">üìÑ Download PDF</a></p><hr><h3 id=when-better-prompts-hurt-evaluation-driven-iteration-for-llm-applicationshttpsarxivorgabs260122025v1><a href=https://arxiv.org/abs/2601.22025v1>When &ldquo;Better&rdquo; Prompts Hurt: Evaluation-Driven Iteration for LLM Applications</a><a hidden class=anchor aria-hidden=true href=#when-better-prompts-hurt-evaluation-driven-iteration-for-llm-applicationshttpsarxivorgabs260122025v1>#</a></h3><p><strong>Authors:</strong> Daniel Commey
<strong>Venue:</strong> arXiv (2026)</p><p>Evaluating Large Language Model (LLM) applications differs from traditional software testing because outputs are stochastic, high-dimensional, and sensitive to prompt and model changes. We present an evaluation-driven workflow - Define, Test, Diagnose, Fix - that turns these challenges into a repeatable engineering loop.
We introduce the Minimum Viable Evaluation Suite (MVES), a tiered set of recommended evaluation components for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows. We also synthesize common evaluation methods (automated checks, human rubrics, and LLM-as-judge) and discuss known judge failure modes.
In reproducible local experiments (Ollama; Llama 3 8B Instruct and Qwen 2.5 7B Instruct), we observe that a generic &ldquo;improved&rdquo; prompt template can trade off behaviors: on our small structured suites, extraction pass rate decreased from 100% to 90% and RAG compliance from 93.3% to 80% for Llama 3 when replacing task-specific prompts with generic rules, while instruction-following improved. These findings motivate evaluation-driven prompt iteration and careful claim calibration rather than universal prompt recipes.
All test suites, harnesses, and results are included for reproducibility.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22025v1">üìÑ Download PDF</a></p><hr><h3 id=lancer-llm-reranking-for-nugget-coveragehttpsarxivorgabs260122008v1><a href=https://arxiv.org/abs/2601.22008v1>LANCER: LLM Reranking for Nugget Coverage</a><a hidden class=anchor aria-hidden=true href=#lancer-llm-reranking-for-nugget-coveragehttpsarxivorgabs260122008v1>#</a></h3><p><strong>Authors:</strong> Jia-Huei Ju, Fran√ßois G. Landry, Eugene Yang, Suzan Verberne, Andrew Yates
<strong>Venue:</strong> arXiv (2026)</p><p>Unlike short-form retrieval-augmented generation (RAG), such as factoid question answering, long-form RAG requires retrieval to provide documents covering a wide range of relevant information. Automated report generation exemplifies this setting: it requires not only relevant information but also a more elaborate response with comprehensive information. Yet, existing retrieval methods are primarily optimized for relevance ranking rather than information coverage. To address this limitation, we propose LANCER, an LLM-based reranking method for nugget coverage. LANCER predicts what sub-questions should be answered to satisfy an information need, predicts which documents answer these sub-questions, and reranks documents in order to provide a ranked list covering as many information nuggets as possible. Our empirical results show that LANCER enhances the quality of retrieval as measured by nugget coverage metrics, achieving higher $Œ±$-nDCG and information coverage than other LLM-based reranking methods. Our oracle analysis further reveals that sub-question generation plays an essential role.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22008v1">üìÑ Download PDF</a></p><hr><h2 id=-linguistics>üîç linguistics<a hidden class=anchor aria-hidden=true href=#-linguistics>#</a></h2><h3 id=one-step-latent-free-image-generation-with-pixel-mean-flowshttpsarxivorgabs260122158v1><a href=https://arxiv.org/abs/2601.22158v1>One-step Latent-free Image Generation with Pixel Mean Flows</a><a hidden class=anchor aria-hidden=true href=#one-step-latent-free-image-generation-with-pixel-mean-flowshttpsarxivorgabs260122158v1>#</a></h3><p><strong>Authors:</strong> Yiyang Lu, Susie Lu, Qiao Sun, Hanhong Zhao, Zhicheng Jiang, Xianbang Wang, Tianhong Li, Zhengyang Geng, Kaiming He
<strong>Venue:</strong> arXiv (2026)</p><p>Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose &ldquo;pixel MeanFlow&rdquo; (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22158v1">üìÑ Download PDF</a></p><hr><h3 id=dynaweb-model-based-reinforcement-learning-of-web-agentshttpsarxivorgabs260122149v1><a href=https://arxiv.org/abs/2601.22149v1>DynaWeb: Model-Based Reinforcement Learning of Web Agents</a><a hidden class=anchor aria-hidden=true href=#dynaweb-model-based-reinforcement-learning-of-web-agentshttpsarxivorgabs260122149v1>#</a></h3><p><strong>Authors:</strong> Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao, Rongzhao Zhang, Lynn Ai, Eric Yang, Tianyu Shi, Lei Yu
<strong>Venue:</strong> arXiv (2026)</p><p>The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22149v1">üìÑ Download PDF</a></p><hr><h3 id=cobordism-and-concordance-of-surfaces-in-4-manifoldshttpsarxivorgabs260122152v1><a href=https://arxiv.org/abs/2601.22152v1>Cobordism and Concordance of Surfaces in 4-Manifolds</a><a hidden class=anchor aria-hidden=true href=#cobordism-and-concordance-of-surfaces-in-4-manifoldshttpsarxivorgabs260122152v1>#</a></h3><p><strong>Authors:</strong> Simeon Hellsten
<strong>Venue:</strong> arXiv (2026)</p><p>We show that two properly embedded compact surfaces in an orientable 4-manifold are cobordant if and only if they are $\mathbb{Z}/2$-homologous and either the 4-manifold has boundary or the surfaces have the same normal Euler number. If the 4-manifold is simply-connected and the surfaces are closed, non-orientable, and cobordant, we show that they are in fact concordant. This completes the classification of closed surfaces in simply-connected 4-manifolds up to concordance. Our methods give new constructions of cobordisms with prescribed boundaries, and completely determine when a given cobordism between the boundaries extends to a cobordism or concordance between the surfaces. We obtain our concordance results by extending Sunukjian&rsquo;s method of ambient surgery to the unoriented case using Pin$^-$-structures. We also discuss conditions for an arbitrary codimension 2 properly embedded submanifold to admit an unoriented spanning manifold with prescribed boundary. All results hold in both the smooth and topological categories.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22152v1">üìÑ Download PDF</a></p><hr><h3 id=late-breaking-results-conversion-of-neural-networks-into-logic-flows-for-edge-computinghttpsarxivorgabs260122151v1><a href=https://arxiv.org/abs/2601.22151v1>Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing</a><a hidden class=anchor aria-hidden=true href=#late-breaking-results-conversion-of-neural-networks-into-logic-flows-for-edge-computinghttpsarxivorgabs260122151v1>#</a></h3><p><strong>Authors:</strong> Daniel Stein, Shaoyi Huang, Rolf Drechsler, Bing Li, Grace Li Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>Neural networks have been successfully applied in various resource-constrained edge devices, where usually central processing units (CPUs) instead of graphics processing units exist due to limited power availability. State-of-the-art research still focuses on efficiently executing enormous numbers of multiply-accumulate (MAC) operations. However, CPUs themselves are not good at executing such mathematical operations on a large scale, since they are more suited to execute control flow logic, i.e., computer algorithms. To enhance the computation efficiency of neural networks on CPUs, in this paper, we propose to convert them into logic flows for execution. Specifically, neural networks are first converted into equivalent decision trees, from which decision paths with constant leaves are then selected and compressed into logic flows. Such logic flows consist of if and else structures and a reduced number of MAC operations. Experimental results demonstrate that the latency can be reduced by up to 14.9 % on a simulated RISC-V CPU without any accuracy degradation.
The code is open source at <a href=https://github.com/TUDa-HWAI/NN2Logic>https://github.com/TUDa-HWAI/NN2Logic</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22151v1">üìÑ Download PDF</a></p><hr><h3 id=neural-s-matrix-bootstrap-ii-solvable-4d-amplitudes-with-particle-productionhttpsarxivorgabs260122145v1><a href=https://arxiv.org/abs/2601.22145v1>Neural S-matrix bootstrap II: solvable 4d amplitudes with particle production</a><a hidden class=anchor aria-hidden=true href=#neural-s-matrix-bootstrap-ii-solvable-4d-amplitudes-with-particle-productionhttpsarxivorgabs260122145v1>#</a></h3><p><strong>Authors:</strong> Mehmet Asim Gumus, Damien Leflot, Piotr Tourkine, Alexander Zhiboedov
<strong>Venue:</strong> arXiv (2026)</p><p>We study a model for nonperturbative unitarization of the four-point contact scalar amplitude in four dimensions. It is defined through an infinite sum of planar diagrams, constructed using two-particle unitarity and crossing symmetry. We reformulate the problem in terms of a set of nonlinear integral equations obeyed by the single and double discontinuities of the amplitude. We then solve them using a neural-network ansatz trained by minimizing a physics-informed loss functional. We obtain a one-parameter family of amplitudes, which exhibit rich structure: sizeable particle production, nontrivial emergent Regge behavior, Landau curves, a logarithmic decay at high energy and fixed angle. Finally, we go beyond the two-particle-reducible setup by treating the multi-particle data &ndash; supported above the multi-particle Landau curves due to multi-particle unitarity &ndash; as a dynamical variable. We demonstrate that it can be tuned to suppress low-spin particle production &ndash; a phenomenon we call Aks screening &ndash; at the cost of generating larger and oscillatory double spectral density in the multi-particle region.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22145v1">üìÑ Download PDF</a></p><hr><h3 id=all-order-prescription-for-facet-regions-in-massless-wide-angle-scatteringhttpsarxivorgabs260122144v1><a href=https://arxiv.org/abs/2601.22144v1>All-order prescription for facet regions in massless wide-angle scattering</a><a hidden class=anchor aria-hidden=true href=#all-order-prescription-for-facet-regions-in-massless-wide-angle-scatteringhttpsarxivorgabs260122144v1>#</a></h3><p><strong>Authors:</strong> Yao Ma
<strong>Venue:</strong> arXiv (2026)</p><p>We take a step toward answering a long-standing question in the asymptotic expansion of Feynman integrals: how to systematically determine the regions in the Expansion-by-Regions technique for multiscale processes? Focusing on generic massless wide-angle scattering, we provide an all-order momentum-space prescription for facet regions, which generally dominate &ndash; and in most cases exhaust &ndash; the contributions in a given asymptotic expansion. This extends the Euclidean-space picture, where regions correspond to specific subgraphs, to the complexities of Minkowski space. Our results are derived from a novel analytical approach combining graph theory and convex geometry; as a key byproduct, we uncover for the first time the algebraic structure underlying momentum modes (collinear, soft, and their hierarchies).</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22144v1">üìÑ Download PDF</a></p><hr><h3 id=towards-universal-urban-patterns-of-life-simulationhttpsarxivorgabs260122099v1><a href=https://arxiv.org/abs/2601.22099v1>Towards Universal Urban Patterns-of-Life Simulation</a><a hidden class=anchor aria-hidden=true href=#towards-universal-urban-patterns-of-life-simulationhttpsarxivorgabs260122099v1>#</a></h3><p><strong>Authors:</strong> Sandro M. Reia, Henrique F. de Arruda, Shiyang Ruan, Taylor Anderson, Hamdi Kavak, Dieter Pfoser
<strong>Venue:</strong> arXiv (2026)</p><p>Understanding urban mobility requires models that capture how people interact with and navigate the built environment. We present a scalable, generalizable agent-based framework in which daily schedules emerge from the interplay between mandatory (e.g., work, school) and flexible (e.g., errands, food, leisure) activities, driven by evolving individual needs. The results of our model are validated against empirical patterns from the 2017 U.S. National Household Travel Survey, including activity distributions, origin-destination flows, and trip-chain length distributions. We introduce a normalized similarity metric to quantify agreement between simulated and empirical patterns. Most cities achieve scores above 0.80, demonstrating strong alignment without the need for city-specific calibration. The model scales efficiently to over 20 million agents, enabling full-population simulations of large metropolitan areas. This combination of universality and scalability enables scenario analysis for infrastructure stress testing, disaster recovery, innovation diffusion, and disease spread in urban systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22099v1">üìÑ Download PDF</a></p><hr><h3 id=constraining-black-hole-parameters-from-shadow-and-inner-shadow-morphology-considering-effects-from-thick-disk-accretion-flowshttpsarxivorgabs260121995v1><a href=https://arxiv.org/abs/2601.21995v1>Constraining Black Hole Parameters from Shadow and Inner-Shadow Morphology Considering Effects from Thick Disk Accretion Flows</a><a hidden class=anchor aria-hidden=true href=#constraining-black-hole-parameters-from-shadow-and-inner-shadow-morphology-considering-effects-from-thick-disk-accretion-flowshttpsarxivorgabs260121995v1>#</a></h3><p><strong>Authors:</strong> Julien A. Kearns, Dominic O. Chang, Daniel C. M. Palumbo, Shane W. Davis
<strong>Venue:</strong> arXiv (2026)</p><p>We study the effects of emission geometry on the capability to constrain black hole parameters from measurements of the shadow and inner-shadow of a Reissner-Nordstr√∂m black hole. We investigate the capability to constrain mass, charge, observer inclination, and emission co-latitude from images of black hole accretion flows that would arise from thick and thin accretion disks. We confirm previous studies that have shown that independent radii measurements of the shadow and inner-shadow can constrain black hole parameters if the viewing inclination is known, but find that it is only possible if the true emission geometry is also assumed. We study the constraining capabilities of the shadow and inner-shadow observations of M87* and Sgr A* like systems within the context of the BHEX and NgEHT future observatories.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21995v1">üìÑ Download PDF</a></p><hr><h3 id=secure-group-key-agreement-on-cyber-physical-system-buseshttpsarxivorgabs260121966v1><a href=https://arxiv.org/abs/2601.21966v1>Secure Group Key Agreement on Cyber-Physical System Buses</a><a hidden class=anchor aria-hidden=true href=#secure-group-key-agreement-on-cyber-physical-system-buseshttpsarxivorgabs260121966v1>#</a></h3><p><strong>Authors:</strong> Sebastian N. Peters, Lukas Lautenschlager, David Emeis, Jason Lochert
<strong>Venue:</strong> arXiv (2026)</p><p>Cyber-Physical Systems (CPSs) rely on distributed embedded devices that often must communicate securely over buses. Ensuring message integrity and authenticity on these buses typically requires group-shared keys for Message Authentication Codes (MACs). To avoid insecure fixed pre-shared keys and trust-on-first-use concepts, a Group Key Agreement (GKA) protocol is needed to dynamically agree on a key amongst the devices. Yet existing GKA protocols lack adaptability to constrained CPS buses. This paper targets authenticated, fully distributed GKA suitable for bus topologies under constraints of industrial and cyber-physical systems, including broadcast-only links, half-duplex operation, resource limits, dynamic membership (including unannounced leaves), a long device lifetime, and a strong Dolev-Yao adversary capable of partitioning the bus. We first systematise existing protocols, then derive the requirements necessary for an authenticated and fully distributed GKA on bus systems. Finally, we design, implement, and evaluate a custom GKA protocol based on TreeKEM.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21966v1">üìÑ Download PDF</a></p><hr><h3 id=investigation-of-wake-dynamics-of-a-slender-symmetric-trailing-edge-hydrofoilhttpsarxivorgabs260121939v1><a href=https://arxiv.org/abs/2601.21939v1>Investigation of Wake Dynamics of a Slender Symmetric Trailing Edge Hydrofoil</a><a hidden class=anchor aria-hidden=true href=#investigation-of-wake-dynamics-of-a-slender-symmetric-trailing-edge-hydrofoilhttpsarxivorgabs260121939v1>#</a></h3><p><strong>Authors:</strong> Gabriele Gaiti, Chirag Trivedi, Kristian F. Sagmo
<strong>Venue:</strong> arXiv (2026)</p><p>Accurate prediction of wake dynamics behind hydrofoils is critical for mitigating vortex-induced vibrations and improving the performance of hydraulic machinery. Conventional turbulence modeling approaches often struggle to capture the unsteady, coherent structures governing wake behavior, particularly for slender hydrofoils operating at high Reynolds numbers. This study addresses this limitation by combining scale-resolving numerical simulations, including high-resolution Large Eddy Simulation (LES), with Particle Image Velocimetry (PIV) measurements to investigate the turbulent wake of a symmetric, blunt trailing-edge hydrofoil operating at zero angle of attack. The flow was analyzed at a Reynolds number of approximately 7.5x10e5, i.e. close to the onset of wake-structure interaction effects. LES was performed using a fine mesh of approximately 500 million nodes to resolve near-wall and wake dynamics beyond the experimental field of view, while PIV measurements provided time-resolved velocity fields downstream of the trailing edge. Proper Orthogonal Decomposition (POD) was applied to the PIV data to extract dominant coherent structures and quantify their contribution to the turbulent kinetic energy. POD analysis reveals that energy is distributed across many modes, with the leading mode capturing the primary wake dynamics and higher modes forming coupled oscillatory pairs associated with von Karman vortex shedding. PIV-LES agreement shows that central wake measurements combined with numerical simulations enables full wake reconstruction and validates modeling for vibration-relevant hydrofoil dynamics.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21939v1">üìÑ Download PDF</a></p><hr><h3 id=a-judge-aware-ranking-framework-for-evaluating-large-language-models-without-ground-truthhttpsarxivorgabs260121817v1><a href=https://arxiv.org/abs/2601.21817v1>A Judge-Aware Ranking Framework for Evaluating Large Language Models without Ground Truth</a><a hidden class=anchor aria-hidden=true href=#a-judge-aware-ranking-framework-for-evaluating-large-language-models-without-ground-truthhttpsarxivorgabs260121817v1>#</a></h3><p><strong>Authors:</strong> Mingyuan Xu, Xinzi Tan, Jiawei Wu, Doudou Zhou
<strong>Venue:</strong> arXiv (2026)</p><p>Evaluating large language models (LLMs) on open-ended tasks without ground-truth labels is increasingly done via the LLM-as-a-judge paradigm. A critical but under-modeled issue is that judge LLMs differ substantially in reliability; treating all judges equally can yield biased leaderboards and misleading uncertainty estimates. More data can make evaluation more confidently wrong under misspecified aggregation. We propose a judge-aware ranking framework that extends the Bradley-Terry-Luce model by introducing judge-specific discrimination parameters, jointly estimating latent model quality and judge reliability from pairwise comparisons without reference labels. We establish identifiability up to natural normalizations and prove consistency and asymptotic normality of the maximum likelihood estimator, enabling confidence intervals for score differences and rank comparisons. Across multiple public benchmarks and a newly collected dataset, our method improves agreement with human preferences, achieves higher data efficiency than unweighted baselines, and produces calibrated uncertainty quantification for LLM rankings.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21817v1">üìÑ Download PDF</a></p><hr><h3 id=ewocs-v-is-wd1-72-a-recent-post-interaction-wro-binaryhttpsarxivorgabs260121788v1><a href=https://arxiv.org/abs/2601.21788v1>EWOCS-V: Is Wd1-72 a recent post-interaction WR+O binary?</a><a hidden class=anchor aria-hidden=true href=#ewocs-v-is-wd1-72-a-recent-post-interaction-wro-binaryhttpsarxivorgabs260121788v1>#</a></h3><p><strong>Authors:</strong> C. J. K. Larkin, J. Mackey, H. Jin, A. A. C. Sander, B. Reville, K. Anastasopoulou, M. Andersen, A. Bayo, J. J. Drake, E. K. Grebel, M. G. Guarcello, T. J. Haworth, V. M. Kalari, R. R. Lefever, F. Najarro, B. W. Ritchie, E. Sabbi
<strong>Venue:</strong> arXiv (2026)</p><p>The evolutionary origin of Wolf-Rayet (WR) stars at Solar metallicity is unclear. Single-star evolution from massive O stars, possibly via a Luminous Blue Variable phase, is challenged by binary period distributions of different WR subtypes. Wd1-72 is a WN7b+O binary embedded in the collective wind of the Galactic young massive cluster Westerlund 1 (Wd 1). It is surrounded by highly structured nebulosity, with cometary tails pointing away from Wd 1 and quasi-spherical droplets towards it. In this letter, we demonstrate that this morphology can be qualitatively reproduced by a hydrodynamic simulation of non-conservative Roche Lobe Overflow (RLOF) mass-loss into a cluster wind. Our model is based on a detailed binary evolution track consistent with key known properties of Wd1-72. Our work suggests Wd1-72 could be only ~10 kyr post-RLOF, and the hydrogen-free nature of Wd1-72 favours this being a second or subsequent RLOF episode. Follow-up observations could make Wd1-72 a valuable benchmark for probing mass-loss and mass-transfer in forming gravitational-wave binary-progenitor systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21788v1">üìÑ Download PDF</a></p><hr><h3 id=fineinstructions-scaling-synthetic-instructions-to-pre-training-scalehttpsarxivorgabs260122146v1><a href=https://arxiv.org/abs/2601.22146v1>FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale</a><a hidden class=anchor aria-hidden=true href=#fineinstructions-scaling-synthetic-instructions-to-pre-training-scalehttpsarxivorgabs260122146v1>#</a></h3><p><strong>Authors:</strong> Ajay Patel, Colin Raffel, Chris Callison-Burch
<strong>Venue:</strong> arXiv (2026)</p><p>Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised &ldquo;predict the next word&rdquo; objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of &ldquo;instruction-tuning&rdquo; data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With &ldquo;supervised&rdquo; synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at <a href=https://huggingface.co/fineinstructions>https://huggingface.co/fineinstructions</a> .</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22146v1">üìÑ Download PDF</a></p><hr><h3 id=prism-distribution-free-adaptive-computation-of-matrix-functions-for-accelerating-neural-network-traininghttpsarxivorgabs260122137v1><a href=https://arxiv.org/abs/2601.22137v1>PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training</a><a hidden class=anchor aria-hidden=true href=#prism-distribution-free-adaptive-computation-of-matrix-functions-for-accelerating-neural-network-traininghttpsarxivorgabs260122137v1>#</a></h3><p><strong>Authors:</strong> Shenghao Yang, Zhichao Wang, Oleg Balabanov, N. Benjamin Erichson, Michael W. Mahoney
<strong>Venue:</strong> arXiv (2026)</p><p>Matrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for neural network training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation), a general framework for accelerating iterative algorithms for computing matrix functions. PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, adapting to the instance at hand with minimal overhead. We apply PRISM to accelerate Newton-Schulz-like iterations for matrix square roots and orthogonalization, which are core primitives in machine learning. Unlike prior methods, PRISM requires no explicit spectral bounds or singular value estimates; and it adapts automatically to the evolving spectrum. Empirically, PRISM accelerates training when integrated into Shampoo and Muon optimizers.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22137v1">üìÑ Download PDF</a></p><hr><h3 id=the-patient-is-not-a-moving-document-a-world-model-training-paradigm-for-longitudinal-ehrhttpsarxivorgabs260122128v1><a href=https://arxiv.org/abs/2601.22128v1>The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR</a><a hidden class=anchor aria-hidden=true href=#the-patient-is-not-a-moving-document-a-world-model-training-paradigm-for-longitudinal-ehrhttpsarxivorgabs260122128v1>#</a></h3><p><strong>Authors:</strong> Irsyad Adam, Zekai Chen, David Laprade, Shaun Porwal, David Laub, Erik Reinertsen, Arda Pekis, Kevin Brown
<strong>Venue:</strong> arXiv (2026)</p><p>Large language models (LLMs) trained with next-word-prediction have achieved success as clinical foundation models. Representations from these language backbones yield strong linear probe performance across biomedical tasks, suggesting that patient semantics emerge from next-token prediction at scale. However, this paradigm treats patients as a document to be summarized rather than a dynamical system to be simulated; a patient&rsquo;s trajectory emerges from their state evolving under interventions and time, requiring models that simulate dynamics rather than predict tokens. To address this, we introduce SMB-Structure, a world model for structured EHR that grounds a joint-embedding prediction architecture (JEPA) with next-token prediction (SFT). SFT grounds our model to reconstruct future patient states in token space, while JEPA predicts those futures in latent space from the initial patient representation alone, forcing trajectory dynamics to be encoded before the next state is observed. We validate across two large-scale cohorts: Memorial Sloan Kettering (23,319 oncology patients; 323,000+ patient-years) and INSPECT (19,402 pulmonary embolism patients). Using a linear probe evaluated at multiple points along the disease trajectory, we demonstrate that our training paradigm learns embeddings that capture disease dynamics not recoverable by autoregressive baselines, enabling SMB-Structure to achieve competitive performance on complex tasks characterized by high patient heterogeneity. Model weights are available at <a href=https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure>https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22128v1">üìÑ Download PDF</a></p><hr><h3 id=alpha-discovery-via-grammar-guided-learning-and-searchhttpsarxivorgabs260122119v1><a href=https://arxiv.org/abs/2601.22119v1>Alpha Discovery via Grammar-Guided Learning and Search</a><a hidden class=anchor aria-hidden=true href=#alpha-discovery-via-grammar-guided-learning-and-searchhttpsarxivorgabs260122119v1>#</a></h3><p><strong>Authors:</strong> Han Yang, Dong Hao, Zhuohan Wang, Qi Shi, Xingtong Li
<strong>Venue:</strong> arXiv (2026)</p><p>Automatically discovering formulaic alpha factors is a central problem in quantitative finance. Existing methods often ignore syntactic and semantic constraints, relying on exhaustive search over unstructured and unbounded spaces. We present AlphaCFG, a grammar-based framework for defining and discovering alpha factors that are syntactically valid, financially interpretable, and computationally efficient. AlphaCFG uses an alpha-oriented context-free grammar to define a tree-structured, size-controlled search space, and formulates alpha discovery as a tree-structured linguistic Markov decision process, which is then solved using a grammar-aware Monte Carlo Tree Search guided by syntax-sensitive value and policy networks. Experiments on Chinese and U.S. stock market datasets show that AlphaCFG outperforms state-of-the-art baselines in both search efficiency and trading profitability. Beyond trading strategies, AlphaCFG serves as a general framework for symbolic factor discovery and refinement across quantitative finance, including asset pricing and portfolio construction.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22119v1">üìÑ Download PDF</a></p><hr><h3 id=paddleocr-vl-15-towards-a-multi-task-09b-vlm-for-robust-in-the-wild-document-parsinghttpsarxivorgabs260121957v1><a href=https://arxiv.org/abs/2601.21957v1>PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing</a><a hidden class=anchor aria-hidden=true href=#paddleocr-vl-15-towards-a-multi-task-09b-vlm-for-robust-in-the-wild-document-parsinghttpsarxivorgabs260121957v1>#</a></h3><p><strong>Authors:</strong> Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou, Hongen Liu, Manhui Lin, Yue Zhang, Yubo Zhang, Yi Liu, Dianhai Yu, Yanjun Ma
<strong>Venue:</strong> arXiv (2026)</p><p>We introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-art (SOTA) accuracy of 94.5% on OmniDocBench v1.5. To rigorously evaluate robustness against real-world physical distortions, including scanning, skew, warping, screen-photography, and illumination, we propose the Real5-OmniDocBench benchmark. Experimental results demonstrate that this enhanced model attains SOTA performance on the newly curated benchmark. Furthermore, we extend the model&rsquo;s capabilities by incorporating seal recognition and text spotting tasks, while remaining a 0.9B ultra-compact VLM with high efficiency. Code: <a href=https://github.com/PaddlePaddle/PaddleOCR>https://github.com/PaddlePaddle/PaddleOCR</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21957v1">üìÑ Download PDF</a></p><hr><h3 id=from-future-of-work-to-future-of-workers-addressing-asymptomatic-ai-harms-for-dignified-human-ai-interactionhttpsarxivorgabs260121920v1><a href=https://arxiv.org/abs/2601.21920v1>From Future of Work to Future of Workers: Addressing Asymptomatic AI Harms for Dignified Human-AI Interaction</a><a hidden class=anchor aria-hidden=true href=#from-future-of-work-to-future-of-workers-addressing-asymptomatic-ai-harms-for-dignified-human-ai-interactionhttpsarxivorgabs260121920v1>#</a></h3><p><strong>Authors:</strong> Upol Ehsan, Samir Passi, Koustuv Saha, Todd McNutt, Mark O. Riedl, Sara Alcorn
<strong>Venue:</strong> arXiv (2026)</p><p>In the future of work discourse, AI is touted as the ultimate productivity amplifier. Yet, beneath the efficiency gains lie subtle erosions of human expertise and agency. This paper shifts focus from the future of work to the future of workers by navigating the AI-as-Amplifier Paradox: AI&rsquo;s dual role as enhancer and eroder, simultaneously strengthening performance while eroding underlying expertise. We present a year-long study on the longitudinal use of AI in a high-stakes workplace among cancer specialists. Initial operational gains hid ``intuition rust&rsquo;&rsquo;: the gradual dulling of expert judgment. These asymptomatic effects evolved into chronic harms, such as skill atrophy and identity commoditization. Building on these findings, we offer a framework for dignified Human-AI interaction co-constructed with professional knowledge workers facing AI-induced skill erosion without traditional labor protections. The framework operationalizes sociotechnical immunity through dual-purpose mechanisms that serve institutional quality goals while building worker power to detect, contain, and recover from skill erosion, and preserve human identity. Evaluated across healthcare and software engineering, our work takes a foundational step toward dignified human-AI interaction futures by balancing productivity with the preservation of human expertise.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21920v1">üìÑ Download PDF</a></p><hr><h3 id=murad-a-large-scale-multi-domain-unified-reverse-arabic-dictionary-datasethttpsarxivorgabs260121512v1><a href=https://arxiv.org/abs/2601.21512v1>MURAD: A Large-Scale Multi-Domain Unified Reverse Arabic Dictionary Dataset</a><a hidden class=anchor aria-hidden=true href=#murad-a-large-scale-multi-domain-unified-reverse-arabic-dictionary-datasethttpsarxivorgabs260121512v1>#</a></h3><p><strong>Authors:</strong> Serry Sibaee, Yasser Alhabashi, Nadia Sibai, Yara Farouk, Adel Ammar, Sawsan AlHalawani, Wadii Boulila
<strong>Venue:</strong> arXiv (2026)</p><p>Arabic is a linguistically and culturally rich language with a vast vocabulary that spans scientific, religious, and literary domains. Yet, large-scale lexical datasets linking Arabic words to precise definitions remain limited. We present MURAD (Multi-domain Unified Reverse Arabic Dictionary), an open lexical dataset with 96,243 word-definition pairs. The data come from trusted reference works and educational sources. Extraction used a hybrid pipeline integrating direct text parsing, optical character recognition, and automated reconstruction. This ensures accuracy and clarity. Each record aligns a target word with its standardized Arabic definition and metadata that identifies the source domain. The dataset covers terms from linguistics, Islamic studies, mathematics, physics, psychology, and engineering. It supports computational linguistics and lexicographic research. Applications include reverse dictionary modeling, semantic retrieval, and educational tools. By releasing this resource, we aim to advance Arabic natural language processing and promote reproducible research on Arabic lexical semantics.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21512v1">üìÑ Download PDF</a></p><hr><h3 id=smart-a-social-movement-analysis--reasoning-tool-with-case-studies-on-metoo-and-blacklivesmatterhttpsarxivorgabs260120986v1><a href=https://arxiv.org/abs/2601.20986v1>SMART: A Social Movement Analysis & Reasoning Tool with Case Studies on #MeToo and #BlackLivesMatter</a><a hidden class=anchor aria-hidden=true href=#smart-a-social-movement-analysis--reasoning-tool-with-case-studies-on-metoo-and-blacklivesmatterhttpsarxivorgabs260120986v1>#</a></h3><p><strong>Authors:</strong> Valerio La Gatta, Marco Postiglione, Jeremy Gilbert, Daniel W. Linna, Morgan Manella Greenfield, Aaron Shaw, V. S. Subrahmanian
<strong>Venue:</strong> arXiv (2026)</p><p>Social movements supporting the UN&rsquo;s Sustainable Development Goals (SDGs) play a vital role in improving human lives. If journalists were aware of the relationship between social movements and external events, they could provide more precise, time-sensitive reporting about movement issues and SDGs. Our SMART system achieves this goal by collecting data from multiple sources, extracting emotions on various themes, and then using a transformer-based forecasting engine (DEEP) to predict quantity and intensity of emotions in future posts. This paper demonstrates SMART&rsquo;s Retrospective capabilities required by journalists via case studies analyzing social media discussions of the #MeToo and #BlackLivesMatter before and after the 2024 U.S. election. We create a novel 1-year dataset which we will release upon publication. It contains over 2.7M Reddit posts and over 1M news articles. We show that SMART enables early detection of discourse shifts around key political events, providing journalists with actionable insights to inform editorial planning. SMART was developed through multiple interactions with a panel of over 20 journalists from a variety of news organizations over a 2-year period, including an author of this paper.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.20986v1">üìÑ Download PDF</a></p><hr><h3 id=sokobench-evaluating-long-horizon-planning-and-reasoning-in-large-language-modelshttpsarxivorgabs260120856v1><a href=https://arxiv.org/abs/2601.20856v1>SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models</a><a hidden class=anchor aria-hidden=true href=#sokobench-evaluating-long-horizon-planning-and-reasoning-in-large-language-modelshttpsarxivorgabs260120856v1>#</a></h3><p><strong>Authors:</strong> Sebastiano Monti, Carlo Nicolini, Gianni Pellegrini, Jacopo Staiano, Bruno Lepri
<strong>Venue:</strong> arXiv (2026)</p><p>Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.20856v1">üìÑ Download PDF</a></p><hr><h3 id=like-a-therapist-but-not-reddit-narratives-of-ai-in-mental-health-contextshttpsarxivorgabs260120747v1><a href=https://arxiv.org/abs/2601.20747v1>Like a Therapist, But Not: Reddit Narratives of AI in Mental Health Contexts</a><a hidden class=anchor aria-hidden=true href=#like-a-therapist-but-not-reddit-narratives-of-ai-in-mental-health-contextshttpsarxivorgabs260120747v1>#</a></h3><p><strong>Authors:</strong> Elham Aghakhani, Rezvaneh Rezapour
<strong>Venue:</strong> arXiv (2026)</p><p>Large language models (LLMs) are increasingly used for emotional support and mental health-related interactions outside clinical settings, yet little is known about how people evaluate and relate to these systems in everyday use. We analyze 5,126 Reddit posts from 47 mental health communities describing experiential or exploratory use of AI for emotional support or therapy. Grounded in the Technology Acceptance Model and therapeutic alliance theory, we develop a theory-informed annotation framework and apply a hybrid LLM-human pipeline to analyze evaluative language, adoption-related attitudes, and relational alignment at scale. Our results show that engagement is shaped primarily by narrated outcomes, trust, and response quality, rather than emotional bond alone. Positive sentiment is most strongly associated with task and goal alignment, while companionship-oriented use more often involves misaligned alliances and reported risks such as dependence and symptom escalation. Overall, this work demonstrates how theory-grounded constructs can be operationalized in large-scale discourse analysis and highlights the importance of studying how users interpret language technologies in sensitive, real-world contexts.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.20747v1">üìÑ Download PDF</a></p><hr><h3 id=do-vlms-perceive-or-recall-probing-visual-perception-vs-memory-with-classic-visual-illusionshttpsarxivorgabs260122150v1><a href=https://arxiv.org/abs/2601.22150v1>Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions</a><a hidden class=anchor aria-hidden=true href=#do-vlms-perceive-or-recall-probing-visual-perception-vs-memory-with-classic-visual-illusionshttpsarxivorgabs260122150v1>#</a></h3><p><strong>Authors:</strong> Xiaoxiao Sun, Mingyang Li, Kun yuan, Min Woo Sun, Mark Endo, Shengguang Wu, Changlin Li, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy
<strong>Venue:</strong> arXiv (2026)</p><p>Large Vision-Language Models (VLMs) often answer classic visual illusions &ldquo;correctly&rdquo; on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely recall memorized patterns? While several studies have noted this phenomenon, the underlying causes remain unclear. To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion framework with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven recall. Unlike prior work that focuses on averaged accuracy, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls. Experiments across different families reveal that response persistence arises from heterogeneous causes rather than a single mechanism. For instance, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, while Qwen variants suggest visual-processing limits. Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change. Data and code are available at <a href=https://sites.google.com/view/vi-probe/>https://sites.google.com/view/vi-probe/</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22150v1">üìÑ Download PDF</a></p><hr><h3 id=probing-the-sound-speed-of-dark-energy-with-a-lunar-laser-interferometerhttpsarxivorgabs260122084v1><a href=https://arxiv.org/abs/2601.22084v1>Probing the Sound Speed of Dark Energy with a Lunar Laser Interferometer</a><a hidden class=anchor aria-hidden=true href=#probing-the-sound-speed-of-dark-energy-with-a-lunar-laser-interferometerhttpsarxivorgabs260122084v1>#</a></h3><p><strong>Authors:</strong> Alfredo Gurrola, Robert J. Scherrer, Oem Trivedi
<strong>Venue:</strong> arXiv (2026)</p><p>The sound speed of dark energy encodes fundamental information about the microphysics underlying cosmic acceleration, yet remains essentially unconstrained by existing observations. We demonstrate that a lunar-based laser interferometer, such as the proposed Laser Interferometer Lunar Antenna (LILA), can directly probe the sound speed of dark energy by measuring the real-time evolution of horizon-scale gravitational potentials. Operating in the ultra-low-frequency gravitational band inaccessible from Earth, LILA is sensitive to scalar metric perturbations sourced by dark energy dynamics. Using both fluid and effective field theory descriptions, we develop a complete framework linking dark energy sound speed to observable strain signatures. We construct a likelihood pipeline and Fisher forecasts, showing that LILA can either detect clustering dark energy or exclude broad classes of models with unprecedented sensitivity. This establishes lunar interferometry as a novel and powerful probe of the physics driving cosmic acceleration.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22084v1">üìÑ Download PDF</a></p><hr><h3 id=a-reverse-black-hole-information-problemhttpsarxivorgabs260122077v1><a href=https://arxiv.org/abs/2601.22077v1>A Reverse Black Hole Information Problem</a><a hidden class=anchor aria-hidden=true href=#a-reverse-black-hole-information-problemhttpsarxivorgabs260122077v1>#</a></h3><p><strong>Authors:</strong> Jan de Boer, Andrew Rolph, Jildou Hollander
<strong>Venue:</strong> arXiv (2026)</p><p>We study the formation, detection and coarse-graining of black holes in AdS/CFT, with an emphasis on the tension between boundary unitarity and the production of mixed state Hawking radiation in the bulk. We construct CFT states dual to black hole formation and evaporation by colliding bulk particle wavepackets at trans-Planckian energy. We propose boundary probes which are able to distinguish small AdS black holes from other states within the microcanonical ensemble. We investigate different coarse-graining prescriptions acting on the evolving CFT state, including averaging over CFT data, Hamiltonians and time windows, and compare their purities to those expected from the bulk semiclassical description. Our results clarify how semiclassical black hole behaviour can arise from an ensemble-averaging of the exact unitary dynamics, and take a step towards a better understanding of coarse-graining in the single-sided black hole information problem.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22077v1">üìÑ Download PDF</a></p><hr><h3 id=universal-multifractality-at-the-topological-anderson-insulator-transitionhttpsarxivorgabs260122065v1><a href=https://arxiv.org/abs/2601.22065v1>Universal Multifractality at the Topological Anderson Insulator Transition</a><a hidden class=anchor aria-hidden=true href=#universal-multifractality-at-the-topological-anderson-insulator-transitionhttpsarxivorgabs260122065v1>#</a></h3><p><strong>Authors:</strong> Ksenija Kovalenka, Ahmad Ranjbar, Sam Azadi, Rodion Vladimirovich Belosludov, Thomas D. K√ºhne, Mohammad Saeed Bahramy
<strong>Venue:</strong> arXiv (2026)</p><p>Disorder is ubiquitous in quantum materials, and its interplay with topology can generate phases absent in the clean limit. Using the Haldane model as a minimal setting, we show that disorder not only shifts topological boundaries but also stabilizes a topological Anderson insulator (TAI) between trivial and Chern insulating regimes. Employing the local Chern marker as a real-space topological probe, we map the full phase diagram and demonstrate that the TAI forms a finite domain bounded by trivial and Anderson insulators. Multifractal analysis of low-energy eigenstates at the boundary reveals universal critical spectra, independent of whether disorder generates or destroys topology. These results place topology, localization, and criticality within a unified framework and provide clear benchmarks for real-space diagnostics of disordered topological phases.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22065v1">üìÑ Download PDF</a></p><hr><h3 id=symmetries-of-regular-q-graphshttpsarxivorgabs260122148v1><a href=https://arxiv.org/abs/2601.22148v1>Symmetries of regular $q$-graphs</a><a hidden class=anchor aria-hidden=true href=#symmetries-of-regular-q-graphshttpsarxivorgabs260122148v1>#</a></h3><p><strong>Authors:</strong> Daniel R Hawtin, Padraig √ì Cath√°in
<strong>Venue:</strong> arXiv (2026)</p><p>Given a finite vector space $V=\mathbb{F}_q^n$, the $q$-analogue of a graph, called a $q$-graph, is a pair $Œì=(\mathcal{V},\mathcal{E})$, where $\mathcal{V}$ is the set of $1$-dimensional subspaces of $V$ and $\mathcal{E}$ is a subset of the $2$-dimensional subspaces of $V$. Elements of $\mathcal{V}$ and $\mathcal{E}$ are called vertices and edges, respectively. If the edges through a vertex $X$ consist of all $2$-spaces of a $(k+1)$-dimensional space which contain $X$, regardless of the choice of vertex, then $Œì$ is $k$-regular. Moreover, $Œì$ is flag-transitive if there is a subgroup of $Œì{\rm L}_n(q)$ preserving $\mathcal{E}$ and acting transitively on the set of all incident vertex-edge pairs; and symmetric if there is a subgroup of $Œì{\rm L}_n(q)$ preserving $\mathcal{E}$ and acting transitively on the set of all ordered pairs of adjacent vertices.
This paper classifies all $k$-regular $q$-graphs that are either flag-transitive or symmetric. The $q$-graphs in the classification are constructed from familiar objects in finite geometry, including spreads, symplectic polar spaces, and generalised hexagons. The classification depends essentially on the classification of transitive linear groups, and thus ultimately on the classification of finite simple groups.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22148v1">üìÑ Download PDF</a></p><hr><h3 id=optimal-cross-correlation-technique-to-search-for-strongly-lensed-gravitational-waveshttpsarxivorgabs260122138v1><a href=https://arxiv.org/abs/2601.22138v1>Optimal cross-correlation technique to search for strongly lensed gravitational waves</a><a hidden class=anchor aria-hidden=true href=#optimal-cross-correlation-technique-to-search-for-strongly-lensed-gravitational-waveshttpsarxivorgabs260122138v1>#</a></h3><p><strong>Authors:</strong> Anirban Kopty, Sanjit Mitra, Anupreeta More
<strong>Venue:</strong> arXiv (2026)</p><p>As the number of detected gravitational wave (GW) events increases with the improved sensitivity of the observatories, detecting strongly lensed pairs of events is becoming a real possibility. Identifying such lensed pairs, however, remains challenging due to the computational cost and/or the reliance on prior knowledge of source parameters in existing methods. This study investigates a novel approach, Optimal Cross-Correlation Analysis for Multiplets (OCCAM), applied to strain data from one or more detectors for Compact Binary Coalescence (CBC) events identified by GW searches, using an optimal, mildly model-dependent, low computation cost approach to identify strongly lensed candidates. This technique efficiently narrows the search space, allowing for more sensitive, but (much) higher latency, algorithms to refine the results further. We demonstrate that our method performs significantly better than other computationally inexpensive methods. In particular, we achieve 97 percent (80 percent) lensed event detection at a pairwise false positive probability of approximately 13 percent (7 percent) for a single detector with LIGO design sensitivity, assuming an SNR greater than or equal to 10 astrophysically motivated lensed and unlensed populations. Thus, this method, using a network of detectors and in conjunction with sky-localisation information, can enormously reduce the false positive probability, making it highly viable to efficiently and quickly search for lensing pairs among thousands of events, including the sub-threshold candidates.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22138v1">üìÑ Download PDF</a></p><hr><h3 id=an-invitation-to-higher-order-riemannian-optimization-optimal-and-implementable-methodshttpsarxivorgabs260122126v1><a href=https://arxiv.org/abs/2601.22126v1>An Invitation to Higher-Order Riemannian Optimization: Optimal and Implementable Methods</a><a hidden class=anchor aria-hidden=true href=#an-invitation-to-higher-order-riemannian-optimization-optimal-and-implementable-methodshttpsarxivorgabs260122126v1>#</a></h3><p><strong>Authors:</strong> David Huckleberry Gutman, George Lobo
<strong>Venue:</strong> arXiv (2026)</p><p>This paper presents the first optimal-rate $p$-th order methods with $p\geq 1$ for finding first and second-order stationary points of non-convex smooth objective functions over Riemannian manifolds. In contrast to the geodesically convex setting, we definitively establish that the optimal oracle complexity of non-convex optimization over manifolds matches that over Euclidean space. In parallel with the complexity analysis, we introduce a general framework for systematically studying higher-order regularity on Riemannian manifolds that characterizes its joint dependence on the objective function and the chosen retraction. To the best of our knowledge, this framework constitutes the first known application in optimization of pullback connections and the Sasaki metric to the study of retraction-based pullbacks of the objective function. We provide clean derivative bounds based on a new covariant Fa√† di Bruno formula derived within our framework. For $p=3$, our methods are fully implementable via a new Krylov-based framework for minimizing quartically regularized cubic polynomials. This is the first Krylov method for this class of polynomials and may be of independent interest beyond Riemannian optimization.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22126v1">üìÑ Download PDF</a></p><hr><h3 id=firewalls-in-the-non-perturbative-bulk-hilbert-space-of-jt-gravityhttpsarxivorgabs260122121v1><a href=https://arxiv.org/abs/2601.22121v1>Firewalls in the non-perturbative bulk Hilbert space of JT gravity</a><a hidden class=anchor aria-hidden=true href=#firewalls-in-the-non-perturbative-bulk-hilbert-space-of-jt-gravityhttpsarxivorgabs260122121v1>#</a></h3><p><strong>Authors:</strong> Hamed Zolfi
<strong>Venue:</strong> arXiv (2026)</p><p>It has been shown that a very old black hole can tunnel into a white hole through the emission of a large baby universe. This process can be modeled by a genus-one geometry corresponding to a single baby universe emission, with a tunneling probability proportional to ( t^{2} e^{-2S(E)} ), where ( t ) denotes the black hole age and ( S(E) ) its entropy at energy ( E ). The growth of this probability at late times raises the question of its behavior near ( t \sim e^{S} ). A natural possibility is that the full genus expansion, together with its non-perturbative completion, leads to saturation of the tunneling probability. Motivated by this idea, the present analysis employs a non-perturbative bulk inner product in place of the perturbative one and shows that, at late times, the probabilities of realizing firewall geometries and smooth geometries approach constant values.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22121v1">üìÑ Download PDF</a></p><hr><h3 id=superdiffusion-and-anomalous-regularization-in-self-similar-random-incompressible-flowshttpsarxivorgabs260122142v1><a href=https://arxiv.org/abs/2601.22142v1>Superdiffusion and anomalous regularization in self-similar random incompressible flows</a><a hidden class=anchor aria-hidden=true href=#superdiffusion-and-anomalous-regularization-in-self-similar-random-incompressible-flowshttpsarxivorgabs260122142v1>#</a></h3><p><strong>Authors:</strong> Scott Armstrong, Ahmed Bou-Rabee, Tuomo Kuusi
<strong>Venue:</strong> arXiv (2026)</p><p>We study the long-time behavior of a particle in $\mathbb{R}^d$, $d \geq 2$, subject to molecular diffusion and advection by a random incompressible flow. The velocity field is the divergence of a stationary random stream matrix $\mathbf{k} $ with positive Hurst exponent $Œ≥> 0$, so the resulting random environment is multiscale and self-similar. In the perturbative regime $Œ≥\ll 1$, we prove quenched power-law superdiffusion: for a typical realization of the environment, the displacement variance at time $t$ grows like $t^{2/(2-Œ≥)}$, the scaling predicted by renormalization group heuristics. We also identify the leading prefactor up to a random (quenched) relative error of order $Œ≥^{\frac12}\left| \log Œ≥\right|^3$. The proof implements a Wilsonian renormalization group scheme at the level of the infinitesimal generator $\nabla \cdot (ŒΩI_d + \mathbf{k} ) \nabla$, based on a self-similar induction across scales. We demonstrate that the coarse-grained generator is well-approximated, at each scale $r$, by a constant-coefficient Laplacian with effective diffusivity growing like $r^Œ≥$. This approximation is inherently scale-local: reflecting the multifractal nature of the environment, the relative error does not decay with the scale, but remains of order $Œ≥^{\frac12}\left| \log Œ≥\right|^2$. We also prove anomalous regularization under the quenched law: for almost every realization of the drift, solutions of the associated elliptic equation are H√∂lder continuous with exponent $1 - CŒ≥^{\frac12}$ and satisfy estimates which are uniform in the molecular diffusivity $ŒΩ$ and the scale.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22142v1">üìÑ Download PDF</a></p><hr><h3 id=mechanistic-data-attribution-tracing-the-training-origins-of-interpretable-llm-unitshttpsarxivorgabs260121996v1><a href=https://arxiv.org/abs/2601.21996v1>Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units</a><a hidden class=anchor aria-hidden=true href=#mechanistic-data-attribution-tracing-the-training-origins-of-interpretable-llm-unitshttpsarxivorgabs260121996v1>#</a></h3><p><strong>Authors:</strong> Jianhui Chen, Yuzhang Luo, Liangming Pan
<strong>Venue:</strong> arXiv (2026)</p><p>While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention&ndash;removing or augmenting a small fraction of high-influence samples&ndash;significantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model&rsquo;s in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose a mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21996v1">üìÑ Download PDF</a></p><hr><h3 id=spectran-spectral-aware-transformer-based-adapter-for-llm-enhanced-sequential-recommendationhttpsarxivorgabs260121986v1><a href=https://arxiv.org/abs/2601.21986v1>SpecTran: Spectral-Aware Transformer-based Adapter for LLM-Enhanced Sequential Recommendation</a><a hidden class=anchor aria-hidden=true href=#spectran-spectral-aware-transformer-based-adapter-for-llm-enhanced-sequential-recommendationhttpsarxivorgabs260121986v1>#</a></h3><p><strong>Authors:</strong> Yu Cui, Feng Liu, Zhaoxiang Wang, Changwang Zhang, Jun Wang, Can Wang, Jiawei Chen
<strong>Venue:</strong> arXiv (2026)</p><p>Traditional sequential recommendation (SR) models learn low-dimensional item ID embeddings from user-item interactions, often overlooking textual information such as item titles or descriptions. Recent advances in Large Language Models (LLMs) have inspired a surge of research that encodes item textual information with high-dimensional semantic embeddings, and designs transformation methods to inject such embeddings into SR models. These embedding transformation strategies can be categorized into two types, both of which exhibits notable drawbacks: 1) adapter-based methods suffer from pronounced dimension collapse, concentrating information into a few dominant dimensions; 2) SVD-based methods are rigid and manual, considering only a few principal spectral components while discarding rich information in the remaining spectrum.
To address these limitations, we propose SpecTran, a spectral-aware transformer-based adapter that operates in the spectral domain, attending to the full spectrum to select and aggregates informative components. A learnable spectral-position encoding injects singular-value cues as an inductive bias, guiding attention toward salient spectral components and promoting diversity across embedding dimensions. Across four real-world datasets and three SR backbones, it consistently outperforms strong baselines, achieving an average improvement of 9.17%.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21986v1">üìÑ Download PDF</a></p><hr><h3 id=breaking-the-regional-barrier-inductive-semantic-topology-learning-for-worldwide-air-quality-forecastinghttpsarxivorgabs260121899v1><a href=https://arxiv.org/abs/2601.21899v1>Breaking the Regional Barrier: Inductive Semantic Topology Learning for Worldwide Air Quality Forecasting</a><a hidden class=anchor aria-hidden=true href=#breaking-the-regional-barrier-inductive-semantic-topology-learning-for-worldwide-air-quality-forecastinghttpsarxivorgabs260121899v1>#</a></h3><p><strong>Authors:</strong> Zhiqing Cui, Siru Zhong, Ming Jin, Shirui Pan, Qingsong Wen, Yuxuan Liang
<strong>Venue:</strong> arXiv (2026)</p><p>Global air quality forecasting grapples with extreme spatial heterogeneity and the poor generalization of existing transductive models to unseen regions. To tackle this, we propose OmniAir, a semantic topology learning framework tailored for global station-level prediction. By encoding invariant physical environmental attributes into generalizable station identities and dynamically constructing adaptive sparse topologies, our approach effectively captures long-range non-Euclidean correlations and physical diffusion patterns across unevenly distributed global networks. We further curate WorldAir, a massive dataset covering over 7,800 stations worldwide. Extensive experiments show that OmniAir achieves state-of-the-art performance against 18 baselines, maintaining high efficiency and scalability with speeds nearly 10 times faster than existing models, while effectively bridging the monitoring gap in data-sparse regions.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21899v1">üìÑ Download PDF</a></p><hr><h3 id=quantum-lego-learning-a-modular-design-principle-for-hybrid-artificial-intelligencehttpsarxivorgabs260121780v1><a href=https://arxiv.org/abs/2601.21780v1>Quantum LEGO Learning: A Modular Design Principle for Hybrid Artificial Intelligence</a><a hidden class=anchor aria-hidden=true href=#quantum-lego-learning-a-modular-design-principle-for-hybrid-artificial-intelligencehttpsarxivorgabs260121780v1>#</a></h3><p><strong>Authors:</strong> Jun Qi, Chao-Han Huck Yang, Pin-Yu Chen, Min-Hsiu Hsieh, Hector Zenil, Jesper Tegner
<strong>Venue:</strong> arXiv (2026)</p><p>Hybrid quantum-classical learning models increasingly integrate neural networks with variational quantum circuits (VQCs) to exploit complementary inductive biases. However, many existing approaches rely on tightly coupled architectures or task-specific encoders, limiting conceptual clarity, generality, and transferability across learning settings. In this work, we introduce Quantum LEGO Learning, a modular and architecture-agnostic learning framework that treats classical and quantum components as reusable, composable learning blocks with well-defined roles. Within this framework, a pre-trained classical neural network serves as a frozen feature block, while a VQC acts as a trainable adaptive module that operates on structured representations rather than raw inputs. This separation enables efficient learning under constrained quantum resources and provides a principled abstraction for analyzing hybrid models. We develop a block-wise generalization theory that decomposes learning error into approximation and estimation components, explicitly characterizing how the complexity and training status of each block influence overall performance. Our analysis generalizes prior tensor-network-specific results and identifies conditions under which quantum modules provide representational advantages over comparably sized classical heads. Empirically, we validate the framework through systematic block-swap experiments across frozen feature extractors and both quantum and classical adaptive heads. Experiments on quantum dot classification demonstrate stable optimization, reduced sensitivity to qubit count, and robustness to realistic noise.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21780v1">üìÑ Download PDF</a></p><hr><h2 id=-psycholinguistics>üîç psycholinguistics<a hidden class=anchor aria-hidden=true href=#-psycholinguistics>#</a></h2><h3 id=variance-component-score-test-for-multivariate-change-point-detection-with-applications-to-mobile-healthhttpsarxivorgabs260122147v1><a href=https://arxiv.org/abs/2601.22147v1>Variance component score test for multivariate change point detection with applications to mobile health</a><a hidden class=anchor aria-hidden=true href=#variance-component-score-test-for-multivariate-change-point-detection-with-applications-to-mobile-healthhttpsarxivorgabs260122147v1>#</a></h3><p><strong>Authors:</strong> Melissa Lynne Martin, Juliette Brook, Sage Rush, Theodore D. Satterthwaite, Ian J. Barnett
<strong>Venue:</strong> arXiv (2026)</p><p>Multivariate change point detection is the process of identifying distributional shifts in time-ordered data across multiple features. This task is particularly challenging when the number of features is large relative to the number of observations. This problem is often present in mobile health, where behavioral changes in at-risk patients must be detected in real time in order to prompt timely interventions. We propose a variance component score test (VC*) for detecting changes in feature means and/or variances using only pre-change point data to estimate distributional parameters. Through simulation studies, we show that VC* has higher power than existing methods. Moreover, we demonstrate that reducing bias by using only pre-change point days to estimate parameters outweighs the increased estimator variances in most scenarios. Lastly, we apply VC* and competing methods to passively collected smartphone data in adolescents and young adults with affective instability.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22147v1">üìÑ Download PDF</a></p><hr><h3 id=smog-scalable-meta-learning-for-multi-objective-bayesian-optimizationhttpsarxivorgabs260122131v1><a href=https://arxiv.org/abs/2601.22131v1>SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization</a><a hidden class=anchor aria-hidden=true href=#smog-scalable-meta-learning-for-multi-objective-bayesian-optimizationhttpsarxivorgabs260122131v1>#</a></h3><p><strong>Authors:</strong> Leonard Papenmeier, Petru Tighineanu
<strong>Venue:</strong> arXiv (2026)</p><p>Multi-objective optimization aims to solve problems with competing objectives, often with only black-box access to a problem and a limited budget of measurements. In many applications, historical data from related optimization tasks is available, creating an opportunity for meta-learning to accelerate the optimization. Bayesian optimization, as a promising technique for black-box optimization, has been extended to meta-learning and multi-objective optimization independently, but methods that simultaneously address both settings - meta-learned priors for multi-objective Bayesian optimization - remain largely unexplored. We propose SMOG, a scalable and modular meta-learning model based on a multi-output Gaussian process that explicitly learns correlations between objectives. SMOG builds a structured joint Gaussian process prior across meta- and target tasks and, after conditioning on metadata, yields a closed-form target-task prior augmented by a flexible residual multi-output kernel. This construction propagates metadata uncertainty into the target surrogate in a principled way. SMOG supports hierarchical, parallel training: meta-task Gaussian processes are fit once and then cached, achieving linear scaling with the number of meta-tasks. The resulting surrogate integrates seamlessly with standard multi-objective Bayesian optimization acquisition functions.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22131v1">üìÑ Download PDF</a></p><hr><h3 id=from-particles-to-agents-hallucination-as-a-metric-for-cognitive-friction-in-spatial-simulationhttpsarxivorgabs260121977v1><a href=https://arxiv.org/abs/2601.21977v1>From Particles to Agents: Hallucination as a Metric for Cognitive Friction in Spatial Simulation</a><a hidden class=anchor aria-hidden=true href=#from-particles-to-agents-hallucination-as-a-metric-for-cognitive-friction-in-spatial-simulationhttpsarxivorgabs260121977v1>#</a></h3><p><strong>Authors:</strong> Javier Argota S√°nchez-Vaquerizo, Luis Borunda Monsivais
<strong>Venue:</strong> arXiv (2026)</p><p>Traditional architectural simulations (e.g. Computational Fluid Dynamics, evacuation, structural analysis) model elements as deterministic physics-based &ldquo;particles&rdquo; rather than cognitive &ldquo;agents&rdquo;. To bridge this, we introduce \textbf{Agentic Environmental Simulations}, where Large Multimodal generative models actively predict the next state of spatial environments based on semantic expectation. Drawing on examples from accessibility-oriented AR pipelines and multimodal digital twins, we propose a shift from chronological time-steps to Episodic Spatial Reasoning, where simulations advance through meaningful, surprisal-triggered events. Within this framework we posit AI hallucinations as diagnostic tools. By formalizing the \textbf{Cognitive Friction} ($C_f$) it is possible to reveal &ldquo;Phantom Affordances&rdquo;, i.e. semiotic ambiguities in built space. Finally, we challenge current HCI paradigms by treating environments as dynamic cognitive partners and propose a human-centered framework of cognitive orchestration for designing AI-driven simulations that preserve autonomy, affective clarity, and cognitive integrity.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21977v1">üìÑ Download PDF</a></p><hr><h3 id=unique-continuation-property-for-stochastic-wave-equationshttpsarxivorgabs260121854v1><a href=https://arxiv.org/abs/2601.21854v1>Unique Continuation Property for Stochastic Wave Equations</a><a hidden class=anchor aria-hidden=true href=#unique-continuation-property-for-stochastic-wave-equationshttpsarxivorgabs260121854v1>#</a></h3><p><strong>Authors:</strong> Qi L√º, Zhonghua Liao
<strong>Venue:</strong> arXiv (2026)</p><p>This paper establishes a fundamental and surprising phenomenon in the theory of stochastic wave equations: the restoration of the unique continuation property (UCP) across characteristic hypersurfaces, a property that is known to fail generically in the deterministic setting. We prove that if a solution to a linear stochastic wave equation vanishes on one side of a characteristic surface $Œì$, then it must vanish in a full neighborhood of any point on $Œì$, provided the stochastic diffusion coefficient is non-degenerate. This result stands in sharp contrast to the classical H√∂rmander-type counterexamples for deterministic waves.
Furthermore, we extend the UCP to equations with non-homogeneous stochastic sources and establish a global unique continuation result from the interior of an arbitrarily narrow characteristic cone. Our proofs rely on a novel stochastic Carleman estimate, where the It√¥ diffusion term introduces a crucial positive energy contribution that is absent in deterministic models.
These findings demonstrate a qualitative difference between deterministic and stochastic hyperbolic dynamics and open new avenues for control theory and inverse problems in stochastic setting.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21854v1">üìÑ Download PDF</a></p><hr><h3 id=the-effectiveness-of-style-vectors-for-steering-large-language-models-a-human-evaluationhttpsarxivorgabs260121505v1><a href=https://arxiv.org/abs/2601.21505v1>The Effectiveness of Style Vectors for Steering Large Language Models: A Human Evaluation</a><a hidden class=anchor aria-hidden=true href=#the-effectiveness-of-style-vectors-for-steering-large-language-models-a-human-evaluationhttpsarxivorgabs260121505v1>#</a></h3><p><strong>Authors:</strong> Diaoul√© Diallo, Katharina Dworatzyk, Sophie Jentzsch, Peer Sch√ºtt, Sabine Theis, Tobias Hecking
<strong>Venue:</strong> arXiv (2026)</p><p>Controlling the behavior of large language models (LLMs) at inference time is essential for aligning outputs with human abilities and safety requirements. \emph{Activation steering} provides a lightweight alternative to prompt engineering and fine-tuning by directly modifying internal activations to guide generation. This research advances the literature in three significant directions. First, while previous work demonstrated the technical feasibility of steering emotional tone using automated classifiers, this paper presents the first human evaluation of activation steering concerning the emotional tone of LLM outputs, collecting over 7,000 crowd-sourced ratings from 190 participants via Prolific ($n=190$). These ratings assess both perceived emotional intensity and overall text quality. Second, we find strong alignment between human and model-based quality ratings (mean $r=0.776$, range $0.157$&ndash;$0.985$), indicating automatic scoring can proxy perceived quality. Moderate steering strengths ($Œª\approx 0.15$) reliably amplify target emotions while preserving comprehensibility, with the strongest effects for disgust ($Œ∑_p^2 = 0.616$) and fear ($Œ∑_p^2 = 0.540$), and minimal effects for surprise ($Œ∑_p^2 = 0.042$). Finally, upgrading from Alpaca to LlaMA-3 yielded more consistent steering with significant effects across emotions and strengths (all $p &lt; 0.001$). Inter-rater reliability was high (ICC $= 0.71$&ndash;$0.87$), underscoring the robustness of the findings. These findings support activation-based control as a scalable method for steering LLM behavior across affective dimensions.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21505v1">üìÑ Download PDF</a></p><hr><h3 id=the-surprising-difficulty-of-search-in-model-based-reinforcement-learninghttpsarxivorgabs260121306v1><a href=https://arxiv.org/abs/2601.21306v1>The Surprising Difficulty of Search in Model-Based Reinforcement Learning</a><a hidden class=anchor aria-hidden=true href=#the-surprising-difficulty-of-search-in-model-based-reinforcement-learninghttpsarxivorgabs260121306v1>#</a></h3><p><strong>Authors:</strong> Wei-Di Chang, Mikael Henaff, Brandon Amos, Gregory Dudek, Scott Fujimoto
<strong>Venue:</strong> arXiv (2026)</p><p>This paper investigates search in model-based reinforcement learning (RL). Conventional wisdom holds that long-term predictions and compounding errors are the primary obstacles for model-based RL. We challenge this view, showing that search is not a plug-and-play replacement for a learned policy. Surprisingly, we find that search can harm performance even when the model is highly accurate. Instead, we show that mitigating distribution shift matters more than improving model or value function accuracy. Building on this insight, we identify key techniques for enabling effective search, achieving state-of-the-art performance across multiple popular benchmark domains.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21306v1">üìÑ Download PDF</a></p><hr><h3 id=maxwait-a-generalized-mechanism-for-distributed-time-sensitive-systemshttpsarxivorgabs260121146v1><a href=https://arxiv.org/abs/2601.21146v1>Maxwait: A Generalized Mechanism for Distributed Time-Sensitive Systems</a><a hidden class=anchor aria-hidden=true href=#maxwait-a-generalized-mechanism-for-distributed-time-sensitive-systemshttpsarxivorgabs260121146v1>#</a></h3><p><strong>Authors:</strong> Francesco Paladino, Shulu Li, Edward A. Lee
<strong>Venue:</strong> arXiv (2026)</p><p>Distributed time-sensitive systems must balance timing requirements (availability) and consistency in the presence of communication delays and synchronization uncertainty. This paper presents maxwait, a simple coordination mechanism with surprising generality that makes these tradeoffs explicit and configurable. We demonstrate that this mechanism subsumes classical distributed system methods such as PTIDES, Chandy-and-Misra with or without null messages, Jefferson&rsquo;s Time-Warp, and Lamport&rsquo;s time-based fault detection, while enabling real-time behavior in distributed cyber-physical applications. The mechanism can also realize many commonly used distributed system patterns, including logical execution time (LET), publish and subscribe, actors, conflict-free replicated data types (CRDTs), and remote procedure calls with futures. More importantly, it adds to these mechanisms better control over timing, bounded time fault detection, and the option of making them more deterministic, all within a single semantic framework. Implemented as an extension of the Lingua Franca coordination language, maxwait enforces logical-time consistency when communication latencies are bounded and provides structured fault handling when bounds are violated.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21146v1">üìÑ Download PDF</a></p><hr><h3 id=the-tensionless-lives-of-null-stringshttpsarxivorgabs260120959v1><a href=https://arxiv.org/abs/2601.20959v1>The Tensionless Lives of Null Strings</a><a hidden class=anchor aria-hidden=true href=#the-tensionless-lives-of-null-stringshttpsarxivorgabs260120959v1>#</a></h3><p><strong>Authors:</strong> Arjun Bagchi, Aritra Banerjee, Ritankar Chatterjee, Priyadarshini Pandit
<strong>Venue:</strong> arXiv (2026)</p><p>The tensionless limit probes the very high energy regime of string theory in contrast to the well studied point-particle limit which reduces to Einstein gravity. Tensionless strings sweep out null worldsheets in the target space and hence are also called null strings. This article aims to provide a comprehensive review of tensionless null string theory beginning with the initial work of Schild, and continuing to the foundational work of Isberg et al (ILST) and then focussing on developments in the past decade.
Recent work centres on the emergence of the Carrollian Conformal Algebra as residual worldsheet symmetries of the ILST action and the identification of tensionless limit as a worldsheet Carrollian limit on the string worldsheet. Carrollian structures are used to address the classical and quantum aspects of the null string. In the classical theory, the aforementioned limit agrees with the analysis from the ILST action. Symmetries, constraints, mode expansions computed from both perspectives match nicely providing a robust cross-check of the analyses. We discuss closed and open null strings as well as their supersymmetric cousins.
The quantum null string comes with several surprises, the foremost of which is the emergence of three consistent quantum theories from the ILST action. We detail the canonical quantisation and the spectrum of the triumvirate of theories. We discuss the novelties of the quantum null theories and the effect compactifaction has on them. We also discuss Carroll strings, applications of these ideas to strings approaching black holes and give a quick overview of other related developments.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.20959v1">üìÑ Download PDF</a></p><hr><h3 id=learning-hamiltonian-flow-maps-mean-flow-consistency-for-large-timestep-molecular-dynamicshttpsarxivorgabs260122123v1><a href=https://arxiv.org/abs/2601.22123v1>Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics</a><a hidden class=anchor aria-hidden=true href=#learning-hamiltonian-flow-maps-mean-flow-consistency-for-large-timestep-molecular-dynamicshttpsarxivorgabs260122123v1>#</a></h3><p><strong>Authors:</strong> Winfried Ripken, Michael Plainer, Gregor Lied, Thorben Frank, Oliver T. Unke, Stefan Chmiela, Frank No√©, Klaus Robert M√ºller
<strong>Venue:</strong> arXiv (2026)</p><p>Simulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a framework to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span $Œît$, enabling stable large-timestep updates far beyond the stability limits of classical integrators. To this end, we impose a Mean Flow consistency condition for time-averaged Hamiltonian dynamics. Unlike prior approaches, this allows training on independent phase-space samples without access to future states, avoiding expensive trajectory generation. Validated across diverse Hamiltonian systems, our method in particular improves upon molecular dynamics simulations using machine-learned force fields (MLFF). Our models maintain comparable training and inference cost, but support significantly larger integration timesteps while trained directly on widely-available trajectory-free MLFF datasets.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22123v1">üìÑ Download PDF</a></p><hr><h3 id=designing-quantum-technologies-with-a-quantum-computerhttpsarxivorgabs260122091v1><a href=https://arxiv.org/abs/2601.22091v1>Designing quantum technologies with a quantum computer</a><a hidden class=anchor aria-hidden=true href=#designing-quantum-technologies-with-a-quantum-computerhttpsarxivorgabs260122091v1>#</a></h3><p><strong>Authors:</strong> Juan Naranjo, Thi Ha Kyaw, Gaurav Saxena, Kevin Ferreira, Jack S. Baker
<strong>Venue:</strong> arXiv (2026)</p><p>Interacting spin systems in solids underpin a wide range of quantum technologies, from quantum sensors and single-photon sources to spin-defect-based quantum registers and processors. We develop a quantum-computer-aided framework for simulating such devices using a general electron spin resonance Hamiltonian incorporating zero-field splitting, the Zeeman effect, hyperfine interactions, dipole-dipole spin-spin terms, and electron-phonon decoherence. Within this model, we combine Gray-encoded qudit-to-qubit mappings, qubit-wise commuting aggregation, and a multi-reference selected quantum Krylov fast-forwarding (sQKFF) hybrid algorithm to access long-time dynamics while remaining compatible with NISQ and early fault-tolerant hardware constraints. Numerical simulations demonstrate the computation of autocorrelation functions up to $\sim100$ ns, together with microwave absorption spectra and the $\ell_1$-norm of coherence, achieving 18-30$%$ reductions in gate counts and circuit depth for Trotterized time-evolution circuits compared to unoptimized implementations. Using the nitrogen vacancy center in diamond as a testbed, we benchmark the framework against classical simulations and identify the reference-state selection in sQKFF as the primary factor governing accuracy at fixed hardware cost. This methodology provides a flexible blueprint for using quantum computers to design, compare, and optimize solid-state spin-qubit technologies under experimentally realistic conditions.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22091v1">üìÑ Download PDF</a></p><hr><h3 id=accessibility-driven-information-transformations-in-mixed-visual-ability-work-teamshttpsarxivorgabs260122081v1><a href=https://arxiv.org/abs/2601.22081v1>Accessibility-Driven Information Transformations in Mixed-Visual Ability Work Teams</a><a hidden class=anchor aria-hidden=true href=#accessibility-driven-information-transformations-in-mixed-visual-ability-work-teamshttpsarxivorgabs260122081v1>#</a></h3><p><strong>Authors:</strong> Yichun Zhao, Miguel A. Nacenta, Mahadeo A. Sukhai, Sowmya Somanath
<strong>Venue:</strong> arXiv (2026)</p><p>Blind and low-vision (BLV) employees in mixed-visual ability teams often encounter information (e.g., PDFs, diagrams) in inaccessible formats. To enable teamwork, teams must transform these representations by modifying or re-creating them into accessible forms. However, these transformations are frequently overlooked, lack infrastructural support, and cause additional labour. To design systems that move beyond one-off accommodations to effective mixed-ability collaboration, we need a deeper understanding of the representations, their transformations and how they occur. We conducted a week-long diary study with follow-up interviews with 23 BLV and sighted professionals from five legal, non-profit, and consulting teams, documenting 36 transformation cases. Our analysis characterizes how teams perform representational transformations for accessibility: how they are triggered proactively or reactively, how they simplify or enhance, and four common patterns in which workers coordinate with each other to address representational incompatibility. Our findings uncover opportunities for designing systems that can better support mixed-visual ability work.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22081v1">üìÑ Download PDF</a></p><hr><h3 id=mjlab-a-lightweight-framework-for-gpu-accelerated-robot-learninghttpsarxivorgabs260122074v1><a href=https://arxiv.org/abs/2601.22074v1>mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning</a><a hidden class=anchor aria-hidden=true href=#mjlab-a-lightweight-framework-for-gpu-accelerated-robot-learninghttpsarxivorgabs260122074v1>#</a></h3><p><strong>Authors:</strong> Kevin Zakka, Qiayuan Liao, Brent Yi, Louis Le Lay, Koushil Sreenath, Pieter Abbeel
<strong>Venue:</strong> arXiv (2026)</p><p>We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22074v1">üìÑ Download PDF</a></p><hr><h3 id=cognitive-load-estimation-using-brain-foundation-models-and-interpretability-for-bcishttpsarxivorgabs260121965v1><a href=https://arxiv.org/abs/2601.21965v1>Cognitive Load Estimation Using Brain Foundation Models and Interpretability for BCIs</a><a hidden class=anchor aria-hidden=true href=#cognitive-load-estimation-using-brain-foundation-models-and-interpretability-for-bcishttpsarxivorgabs260121965v1>#</a></h3><p><strong>Authors:</strong> Deeksha M. Shama, Dimitra Emmanouilidou, Ivan J. Tashev
<strong>Venue:</strong> arXiv (2026)</p><p>Accurately monitoring cognitive load in real time is critical for Brain-Computer Interfaces (BCIs) that adapt to user engagement and support personalized learning. Electroencephalography (EEG) offers a non-invasive, cost-effective modality for capturing neural activity, though traditional methods often struggle with cross-subject variability and task-specific preprocessing. We propose leveraging Brain Foundation Models (BFMs), large pre-trained neural networks, to extract generalizable EEG features for cognitive load estimation. We adapt BFMs for long-term EEG monitoring and show that fine-tuning a small subset of layers yields improved accuracy over the state-of-the-art. Despite their scale, BFMs allow for real-time inference with a longer context window. To address often-overlooked interpretability challenges, we apply Partition SHAP (SHapley Additive exPlanations) to quantify feature importance. Our findings reveal consistent emphasis on prefrontal regions linked to cognitive control, while longitudinal trends suggest learning progression. These results position BFMs as efficient and interpretable tools for continuous cognitive load monitoring in real-world BCIs.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21965v1">üìÑ Download PDF</a></p><hr><h3 id=from-meta-thought-to-execution-cognitively-aligned-post-training-for-generalizable-and-reliable-llm-reasoninghttpsarxivorgabs260121909v1><a href=https://arxiv.org/abs/2601.21909v1>From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning</a><a hidden class=anchor aria-hidden=true href=#from-meta-thought-to-execution-cognitively-aligned-post-training-for-generalizable-and-reliable-llm-reasoninghttpsarxivorgabs260121909v1>#</a></h3><p><strong>Authors:</strong> Shaojie Wang, Liang Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19% and 4.63% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21909v1">üìÑ Download PDF</a></p><hr><h3 id=scale-dependent-semantic-dynamics-revealed-by-allan-deviationhttpsarxivorgabs260121678v1><a href=https://arxiv.org/abs/2601.21678v1>Scale-Dependent Semantic Dynamics Revealed by Allan Deviation</a><a hidden class=anchor aria-hidden=true href=#scale-dependent-semantic-dynamics-revealed-by-allan-deviationhttpsarxivorgabs260121678v1>#</a></h3><p><strong>Authors:</strong> Debayan Dasgupta
<strong>Venue:</strong> arXiv (2026)</p><p>While language progresses through a sequence of semantic states, the underlying dynamics of this progression remain elusive. Here, we treat the semantic progression of written text as a stochastic trajectory in a high-dimensional state space. We utilize Allan deviation, a tool from precision metrology, to analyze the stability of meaning by treating ordered sentence embeddings as a displacement signal. Our analysis reveals two distinct dynamical regimes: short-time power-law scaling, which differentiates creative literature from technical texts, and a long-time crossover to a stability-limited noise floor. We find that while large language models successfully mimic the local scaling statistics of human text, they exhibit a systematic reduction in their stability horizon. These results establish semantic coherence as a measurable physical property, offering a framework to differentiate the nuanced dynamics of human cognition from the patterns generated by algorithmic models.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21678v1">üìÑ Download PDF</a></p><hr><h3 id=age-matters-analyzing-age-related-discussions-in-app-reviewshttpsarxivorgabs260121605v1><a href=https://arxiv.org/abs/2601.21605v1>Age Matters: Analyzing Age-Related Discussions in App Reviews</a><a hidden class=anchor aria-hidden=true href=#age-matters-analyzing-age-related-discussions-in-app-reviewshttpsarxivorgabs260121605v1>#</a></h3><p><strong>Authors:</strong> Shashiwadana Nirmania, Garima Sharma, Hourieh Khalajzadeh, Mojtaba Shahin
<strong>Venue:</strong> arXiv (2026)</p><p>In recent years, mobile applications have become indispensable tools for managing various aspects of life. From enhancing productivity to providing personalized entertainment, mobile apps have revolutionized people&rsquo;s daily routines. Despite this rapid growth and popularity, gaps remain in how these apps address the needs of users from different age groups. Users of varying ages face distinct challenges when interacting with mobile apps, from younger users dealing with inappropriate content to older users having difficulty with usability due to age-related vision and cognition impairments. Although there have been initiatives to create age-inclusive apps, a limited understanding of user perspectives on age-related issues may hinder developers from recognizing specific challenges and implementing effective solutions. In this study, we explore age discussions in app reviews to gain insights into how mobile apps should cater to users across different age groups.We manually curated a dataset of 4,163 app reviews from the Google Play Store and identified 1,429 age-related reviews and 2,734 non-age-related reviews. We employed eight machine learning, deep learning, and large language models to automatically detect age discussions, with RoBERTa performing the best, achieving a precision of 92.46%. Additionally, a qualitative analysis of the 1,429 age-related reviews uncovers six dominant themes reflecting user concerns.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21605v1">üìÑ Download PDF</a></p><hr><h3 id=comparative-assessment-of-look-ahead-economic-dispatch-and-ramp-products-for-grid-flexibilityhttpsarxivorgabs260122120v1><a href=https://arxiv.org/abs/2601.22120v1>Comparative Assessment of Look-Ahead Economic Dispatch and Ramp Products for Grid Flexibility</a><a hidden class=anchor aria-hidden=true href=#comparative-assessment-of-look-ahead-economic-dispatch-and-ramp-products-for-grid-flexibilityhttpsarxivorgabs260122120v1>#</a></h3><p><strong>Authors:</strong> Qian Zhang, Le Xie, Long Zhao, Congcong Wang
<strong>Venue:</strong> arXiv (2026)</p><p>High renewable penetration increases the frequency and magnitude of net-load ramps, stressing real-time flexibility. Two commonly deployed remedies are look-ahead economic dispatch (LAED) and ramp products (RPs), yet their operational equivalence under the industry-standard rolling-window dispatch implementation is not well understood. This paper develops linear optimization models for multi-interval LAED and RP-based co-optimization, and proves that an enhanced RP formulation can match LAED&rsquo;s dispatch feasible region at a single time step when additional intertemporal deliverability constraints are enforced. We then show that this equivalence does not generally persist under rolling-window operation because LAED and RP formulations optimize different intertemporal objectives, leading to divergent end-of-window states. Using different test systems under stressed ramping conditions and multiple load levels, we show LAED achieves similar or lower load shedding than RP implementations with the same look-ahead horizon, with the most pronounced differences under high-load, ramp-limited conditions. The study highlights the limitations of current ramp product implementations and suggests enhancements, such as introducing more mid-duration RPs.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22120v1">üìÑ Download PDF</a></p><hr><h3 id=a-gradient-based-capacity-accreditation-framework-in-resource-adequacy-formulation-computation-and-practical-implicationshttpsarxivorgabs260122087v1><a href=https://arxiv.org/abs/2601.22087v1>A Gradient-Based Capacity Accreditation Framework in Resource Adequacy: Formulation, Computation, and Practical Implications</a><a hidden class=anchor aria-hidden=true href=#a-gradient-based-capacity-accreditation-framework-in-resource-adequacy-formulation-computation-and-practical-implicationshttpsarxivorgabs260122087v1>#</a></h3><p><strong>Authors:</strong> Qian Zhang, Feng Zhao, Gord Stephen, Chanan Singh, Le Xie
<strong>Venue:</strong> arXiv (2026)</p><p>Probabilistic resource adequacy assessment is a cornerstone of modern capacity accreditation. This paper develops a gradient-based framework, in which capacity accreditation is interpreted as the directional derivative of a probabilistic resource adequacy metric with respect to resource capacity, that unifies two widely used accreditation approaches: Effective Load Carrying Capability (ELCC) and Marginal Reliability Impact (MRI). Under mild regularity conditions, we show that marginal ELCC and MRI yield equivalent accreditation factors, while their numerical implementations exhibit markedly different computational characteristics. Building on this framework, we demonstrate how infinitesimal perturbation analysis enables up to a $1000\times$ speedup in gradient estimation for capacity accreditation, and we implement gradient-informed search algorithms that significantly accelerate ELCC computations relative to standard bisection methods. Large-scale Monte Carlo experiments show that MRI achieves substantial runtime reductions compared to ELCC and exhibits greater robustness to perturbation step-size selection. These results provide practical guidance for implementing efficient and scalable capacity accreditation in large-scale power systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22087v1">üìÑ Download PDF</a></p><hr><h3 id=voltvar-optimization-in-transmission-networks-with-discrete-control-deviceshttpsarxivorgabs260122080v1><a href=https://arxiv.org/abs/2601.22080v1>Volt/VAR Optimization in Transmission Networks with Discrete-Control Devices</a><a hidden class=anchor aria-hidden=true href=#voltvar-optimization-in-transmission-networks-with-discrete-control-deviceshttpsarxivorgabs260122080v1>#</a></h3><p><strong>Authors:</strong> Shuaicheng Tong, Michael A. Boateng, Mathieu Tanneau, Pascal Van Hentenryck
<strong>Venue:</strong> arXiv (2026)</p><p>Voltage (Volt) and reactive-power (VAR) control in transmission networks is critical for reliability and increasingly needs fast, implementable decisions. This paper presents a transmission Volt/VAR Optimization (VVO) framework that co-optimizes discrete control of on-load tap-changing transformers (OLTCs) and capacitor banks (CBs) with AC power flow (ACPF) physics to improve voltage stability and minimize VAR generation. The framework follows a relax-round-resolve pipeline: a continuous relaxation proposes targets, a rounding step selects feasible discrete settings, and a final solve enforces AC power flow physics. Extensive experiments on IEEE, PEGASE, and RTE systems show consistent improvements in voltage and VAR quality metrics with modest generator redispatch while preserving economic operation and achieving compatible runtimes with real-time transmission operations.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22080v1">üìÑ Download PDF</a></p><hr><h3 id=lorif-low-rank-influence-functions-for-scalable-training-data-attributionhttpsarxivorgabs260121929v1><a href=https://arxiv.org/abs/2601.21929v1>LoRIF: Low-Rank Influence Functions for Scalable Training Data Attribution</a><a hidden class=anchor aria-hidden=true href=#lorif-low-rank-influence-functions-for-scalable-training-data-attributionhttpsarxivorgabs260121929v1>#</a></h3><p><strong>Authors:</strong> Shuangqi Li, Hieu Le, Jingyi Xu, Mathieu Salzmann
<strong>Venue:</strong> arXiv (2026)</p><p>Training data attribution (TDA) identifies which training examples most influenced a model&rsquo;s prediction. The best-performing TDA methods exploits gradients to define an influence function. To overcome the scalability challenge arising from gradient computation, the most popular strategy is random projection (e.g., TRAK, LoGRA). However, this still faces two bottlenecks when scaling to large training sets and high-quality attribution: \emph{(i)} storing and loading projected per-example gradients for all $N$ training examples, where query latency is dominated by I/O; and \emph{(ii)} forming the $D \times D$ inverse Hessian approximation, which costs $O(D^2)$ memory. Both bottlenecks scale with the projection dimension $D$, yet increasing $D$ is necessary for attribution quality &ndash; creating a quality-scalability tradeoff. We introduce \textbf{LoRIF (Low-Rank Influence Functions)}, which exploits low-rank structures of gradient to address both bottlenecks. First, we store rank-$c$ factors of the projected per-example gradients rather than full matrices, reducing storage and query-time I/O from $O(D)$ to $O(c\sqrt{D})$ per layer per sample. Second, we use truncated SVD with the Woodbury identity to approximate the Hessian term in an $r$-dimensional subspace, reducing memory from $O(D^2)$ to $O(Dr)$. On models from 0.1B to 70B parameters trained on datasets with millions of examples, LoRIF achieves up to 20$\times$ storage reduction and query-time speedup compared to LoGRA, while matching or exceeding its attribution quality. LoRIF makes gradient-based TDA practical at frontier scale.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21929v1">üìÑ Download PDF</a></p><hr><h2 id=-llm>üîç llm<a hidden class=anchor aria-hidden=true href=#-llm>#</a></h2><h3 id=world-of-workflows-a-benchmark-for-bringing-world-models-to-enterprise-systemshttpsarxivorgabs260122130v1><a href=https://arxiv.org/abs/2601.22130v1>World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems</a><a hidden class=anchor aria-hidden=true href=#world-of-workflows-a-benchmark-for-bringing-world-models-to-enterprise-systemshttpsarxivorgabs260122130v1>#</a></h3><p><strong>Authors:</strong> Lakshya Gupta, Litao Li, Yizhe Liu, Sriram Ganapathi Subramanian, Kaheer Suleman, Zichen Zhang, Haoye Lu, Sumit Pasupalak
<strong>Venue:</strong> arXiv (2026)</p><p>Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22130v1">üìÑ Download PDF</a></p><hr><h3 id=creative-image-generation-with-diffusion-modelhttpsarxivorgabs260122125v1><a href=https://arxiv.org/abs/2601.22125v1>Creative Image Generation with Diffusion Model</a><a hidden class=anchor aria-hidden=true href=#creative-image-generation-with-diffusion-modelhttpsarxivorgabs260122125v1>#</a></h3><p><strong>Authors:</strong> Kunpeng Song, Ahmed Elgammal
<strong>Venue:</strong> arXiv (2026)</p><p>Creative image generation has emerged as a compelling area of research, driven by the need to produce novel and high-quality images that expand the boundaries of imagination. In this work, we propose a novel framework for creative generation using diffusion models, where creativity is associated with the inverse probability of an image&rsquo;s existence in the CLIP embedding space. Unlike prior approaches that rely on a manual blending of concepts or exclusion of subcategories, our method calculates the probability distribution of generated images and drives it towards low-probability regions to produce rare, imaginative, and visually captivating outputs. We also introduce pullback mechanisms, achieving high creativity without sacrificing visual fidelity. Extensive experiments on text-to-image diffusion models demonstrate the effectiveness and efficiency of our creative generation framework, showcasing its ability to produce unique, novel, and thought-provoking images. This work provides a new perspective on creativity in generative models, offering a principled method to foster innovation in visual content synthesis.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22125v1">üìÑ Download PDF</a></p><hr><h3 id=prior-informed-flow-matching-for-graph-reconstructionhttpsarxivorgabs260122107v1><a href=https://arxiv.org/abs/2601.22107v1>Prior-Informed Flow Matching for Graph Reconstruction</a><a hidden class=anchor aria-hidden=true href=#prior-informed-flow-matching-for-graph-reconstructionhttpsarxivorgabs260122107v1>#</a></h3><p><strong>Authors:</strong> Harvey Chen, Nicolas Zilberstein, Santiago Segarra
<strong>Venue:</strong> arXiv (2026)</p><p>We introduce Prior-Informed Flow Matching (PIFM), a conditional flow model for graph reconstruction. Reconstructing graphs from partial observations remains a key challenge; classical embedding methods often lack global consistency, while modern generative models struggle to incorporate structural priors. PIFM bridges this gap by integrating embedding-based priors with continuous-time flow matching. Grounded in a permutation equivariant version of the distortion-perception theory, our method first uses a prior, such as graphons or GraphSAGE/node2vec, to form an informed initial estimate of the adjacency matrix based on local information. It then applies rectified flow matching to refine this estimate, transporting it toward the true distribution of clean graphs and learning a global coupling. Experiments on different datasets demonstrate that PIFM consistently enhances classical embeddings, outperforming them and state-of-the-art generative baselines in reconstruction accuracy.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22107v1">üìÑ Download PDF</a></p><hr><h2 id=-neuroscience>üîç neuroscience<a hidden class=anchor aria-hidden=true href=#-neuroscience>#</a></h2><h3 id=airpet-virtual-positron-emission-tomographyhttpsarxivorgabs260122059v1><a href=https://arxiv.org/abs/2601.22059v1>AIRPET: Virtual Positron Emission Tomography</a><a hidden class=anchor aria-hidden=true href=#airpet-virtual-positron-emission-tomographyhttpsarxivorgabs260122059v1>#</a></h3><p><strong>Authors:</strong> J. Renner, J. J. G√≥mez-Cadenas, R. Soleti
<strong>Venue:</strong> arXiv (2026)</p><p>Positron Emission Tomography (PET) is a powerful medical imaging technique, but the design and evaluation of new PET scanner technologies present significant challenges. The process is typically divided into three major stages: 1. detector design and simulation, 2. image reconstruction, and 3. image interpretation. Each of these stages requires significant expertise, making it difficult for individuals or small teams to manage all three at once. AIRPET (AI-driven Revolution in Positron Emission Tomography) is a web-based platform designed to address this challenge by integrating all phases of PET design into a single, accessible, and AI-assisted workflow. AIRPET provides an interface to large language models (LLMs) for assisted geometry creation and an interface for basic PET image reconstruction with the potential for further expansion. Here we introduce AIRPET and outline its current functionality and proposed additions.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22059v1">üìÑ Download PDF</a></p><hr><h3 id=metricanything-scaling-metric-depth-pretraining-with-noisy-heterogeneous-sourceshttpsarxivorgabs260122054v1><a href=https://arxiv.org/abs/2601.22054v1>MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources</a><a hidden class=anchor aria-hidden=true href=#metricanything-scaling-metric-depth-pretraining-with-noisy-heterogeneous-sourceshttpsarxivorgabs260122054v1>#</a></h3><p><strong>Authors:</strong> Baorui Ma, Jiahui Yang, Donglin Di, Xuancheng Zhang, Jianxun Cui, Hao Li, Yan Xie, Wei Chen
<strong>Venue:</strong> arXiv (2026)</p><p>Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at <a href=http://metric-anything.github.io/metric-anything-io/>http://metric-anything.github.io/metric-anything-io/</a> to support community research.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22054v1">üìÑ Download PDF</a></p><hr><h3 id=molecular-structure-binding-and-disorder-in-tdbc-ag-plexcitonic-assemblieshttpsarxivorgabs260122022v1><a href=https://arxiv.org/abs/2601.22022v1>Molecular structure, binding, and disorder in TDBC-Ag plexcitonic assemblies</a><a hidden class=anchor aria-hidden=true href=#molecular-structure-binding-and-disorder-in-tdbc-ag-plexcitonic-assemblieshttpsarxivorgabs260122022v1>#</a></h3><p><strong>Authors:</strong> J. Ba√±os-Guti√©rrez, R. Bercy, Y. Garc√≠a Jomaso, S. Balci, G. Pirruccio, J. Halldin Stenlid, M. J. Llansola-Portoles, D. Finkelstein-Shapiro
<strong>Venue:</strong> arXiv (2026)</p><p>Plexcitonic assemblies are hybrid materials composed of a plasmonic nanoparticle and molecular or semiconducting emitters whose electronic transitions are strongly coupled to the plasmonic mode. This coupling hybridizes the system modes into upper and lower polariton branches. The strength of the interaction depends on the number of emitters and on their orientation and spatial arrangement relative to the metallic surface. These structural factors have profound consequences for the ensuing photoexcited dynamics. Despite the extensive spectroscopic work on plexcitonic systems, direct understanding of the molecular geometry at the metal interface remains limited. In this work, we present a comprehensive structural characterization of one of the most widely studied plexcitons formed by the cyanine dye 5,5&rsquo;,6,6&rsquo;-tetrachloro-1,1&rsquo;-diethyl-3,3&rsquo;-di(4-sulfobutyl)-benzimidazolocarbocyanine (TDBC) and silver nanoprisms using a combination of NMR, THz-Raman spectroscopy, and DFT calculations. By comparing the signals from the monomeric and aggregated forms of TDBC with that of the plexciton, we identify shared spectral fingerprints that reveal how molecular packing is modified when the aggregate adsorbs on the silver surface. We observe Raman modes specific to plexciton systems, and identify NOESY cross-peaks in the aliphatic region, that along with THz-Raman modes in the 10-400 cm$^{-1}$ region are sensitive indicators of aggregation geometry and adsorption. We find that isolated TDBC monomers adopt an asymmetric conformation in which both sulfobutyl chains lie on the same side of the chromophore, while J-aggregates adopt a symmetric up-down alternation of the chains from molecule to molecule. This work establishes the molecular geometry of a prototypical TDBC-silver plexciton, providing a structural benchmark for understanding geometry-dependent photophysics in hybrid exciton-plasmon systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22022v1">üìÑ Download PDF</a></p><hr><h3 id=liquid-interfaces-a-dynamic-ontology-for-the-interoperability-of-autonomous-systemshttpsarxivorgabs260121993v1><a href=https://arxiv.org/abs/2601.21993v1>Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems</a><a hidden class=anchor aria-hidden=true href=#liquid-interfaces-a-dynamic-ontology-for-the-interoperability-of-autonomous-systemshttpsarxivorgabs260121993v1>#</a></h3><p><strong>Authors:</strong> Dhiogo de S√°, Carlos Schmiedel, Carlos Pereira Lopes
<strong>Venue:</strong> arXiv (2026)</p><p>Contemporary software architectures struggle to support autonomous agents whose reasoning is adaptive, probabilistic, and context-dependent, while system integration remains dominated by static interfaces and deterministic contracts. This paper introduces Liquid Interfaces, a coordination paradigm in which interfaces are not persistent technical artifacts, but ephemeral relational events that emerge through intention articulation and semantic negotiation at runtime.We formalize this model and present the Liquid Interface Protocol (LIP),which governs intention-driven interaction, negotiated execution, and enforce ephemerality under semantic uncertainty. We further discuss the governance implications of this approach and describe a reference architecture that demonstrates practical feasibility. Liquid Interfaces provide a principled foundation for adaptive coordination in agent-based systems</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21993v1">üìÑ Download PDF</a></p><hr><h3 id=how-do-visual-attributes-influence-web-agents-a-comprehensive-evaluation-of-user-interface-design-factorshttpsarxivorgabs260121961v1><a href=https://arxiv.org/abs/2601.21961v1>How do Visual Attributes Influence Web Agents? A Comprehensive Evaluation of User Interface Design Factors</a><a hidden class=anchor aria-hidden=true href=#how-do-visual-attributes-influence-web-agents-a-comprehensive-evaluation-of-user-interface-design-factorshttpsarxivorgabs260121961v1>#</a></h3><p><strong>Authors:</strong> Kuai Yu, Naicheng Yu, Han Wang, Rui Yang, Huan Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>Web agents have demonstrated strong performance on a wide range of web-based tasks. However, existing research on the effect of environmental variation has mostly focused on robustness to adversarial attacks, with less attention to agents&rsquo; preferences in benign scenarios. Although early studies have examined how textual attributes influence agent behavior, a systematic understanding of how visual attributes shape agent decision-making remains limited. To address this, we introduce VAF, a controlled evaluation pipeline for quantifying how webpage Visual Attribute Factors influence web-agent decision-making. Specifically, VAF consists of three stages: (i) variant generation, which ensures the variants share identical semantics as the original item while only differ in visual attributes; (ii) browsing interaction, where agents navigate the page via scrolling and clicking the interested item, mirroring how human users browse online; (iii) validating through both click action and reasoning from agents, which we use the Target Click Rate and Target Mention Rate to jointly evaluate the effect of visual attributes. By quantitatively measuring the decision-making difference between the original and variant, we identify which visual attributes influence agents&rsquo; behavior most. Extensive experiments, across 8 variant families (48 variants total), 5 real-world websites (including shopping, travel, and news browsing), and 4 representative web agents, show that background color contrast, item size, position, and card clarity have a strong influence on agents&rsquo; actions, whereas font styling, text color, and item image clarity exhibit minor effects.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21961v1">üìÑ Download PDF</a></p><hr><h2 id=-data_resources>üîç data_resources<a hidden class=anchor aria-hidden=true href=#-data_resources>#</a></h2><h3 id=pi-light-physics-inspired-diffusion-for-full-image-relightinghttpsarxivorgabs260122135v1><a href=https://arxiv.org/abs/2601.22135v1>PI-Light: Physics-Inspired Diffusion for Full-Image Relighting</a><a hidden class=anchor aria-hidden=true href=#pi-light-physics-inspired-diffusion-for-full-image-relightinghttpsarxivorgabs260122135v1>#</a></h3><p><strong>Authors:</strong> Zhexin Liang, Zhaoxi Chen, Yongwei Chen, Tianyi Wei, Tengfei Wang, Xingang Pan
<strong>Venue:</strong> arXiv (2026)</p><p>Full-image relighting remains a challenging problem due to the difficulty of collecting large-scale structured paired data, the difficulty of maintaining physical plausibility, and the limited generalizability imposed by data-driven priors. Existing attempts to bridge the synthetic-to-real gap for full-scene relighting remain suboptimal. To tackle these challenges, we introduce Physics-Inspired diffusion for full-image reLight ($œÄ$-Light, or PI-Light), a two-stage framework that leverages physics-inspired diffusion models. Our design incorporates (i) batch-aware attention, which improves the consistency of intrinsic predictions across a collection of images, (ii) a physics-guided neural rendering module that enforces physically plausible light transport, (iii) physics-inspired losses that regularize training dynamics toward a physically meaningful landscape, thereby enhancing generalizability to real-world image editing, and (iv) a carefully curated dataset of diverse objects and scenes captured under controlled lighting conditions. Together, these components enable efficient finetuning of pretrained diffusion models while also providing a solid benchmark for downstream evaluation. Experiments demonstrate that $œÄ$-Light synthesizes specular highlights and diffuse reflections across a wide variety of materials, achieving superior generalization to real-world scenes compared with prior approaches.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22135v1">üìÑ Download PDF</a></p><hr><h3 id=preliminary-results-of-a-scoping-review-on-assistive-technologies-for-adults-with-adhdhttpsarxivorgabs260121791v1><a href=https://arxiv.org/abs/2601.21791v1>Preliminary Results of a Scoping Review on Assistive Technologies for Adults with ADHD</a><a hidden class=anchor aria-hidden=true href=#preliminary-results-of-a-scoping-review-on-assistive-technologies-for-adults-with-adhdhttpsarxivorgabs260121791v1>#</a></h3><p><strong>Authors:</strong> Valerie Tan, Luisa Jost, Jens Gerken, Max Pascher
<strong>Venue:</strong> arXiv (2026)</p><p>Attention Deficit Hyperactivity Disorder (ADHD), characterized by inattention, hyperactivity, and impulsivity, is prevalent in the adult population. Long perceived and treated as a childhood condition, ADHD and its characteristics nonetheless impact a significant portion of adults today. In contrast to children with ADHD, adults with ADHD face unique challenges in the workplace and in higher education. In this work-in-progress paper, we present a scoping review as a foundation to understand and explore existing technology-based approaches to support adults with ADHD. In total, our search returned 3,538 papers upon which we selected, based on PRISMA-ScR, a total of 46 papers for in-depth analysis. Our initial findings highlight that most papers take on a therapeutic or intervention perspective instead of a more positive support perspective. Our analysis also found a tremendous increase in recent papers on the topic, which highlights that more and more researchers are becoming aware of the need to address ADHD with adults. For the future, we aim to further analyze the corpus and identify research gaps and potentials for further development of ADHD assistive technologies.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21791v1">üìÑ Download PDF</a></p><hr><h3 id=stepshield-when-not-whether-to-intervene-on-rogue-agentshttpsarxivorgabs260122136v1><a href=https://arxiv.org/abs/2601.22136v1>StepShield: When, Not Whether to Intervene on Rogue Agents</a><a hidden class=anchor aria-hidden=true href=#stepshield-when-not-whether-to-intervene-on-rogue-agentshttpsarxivorgabs260122136v1>#</a></h3><p><strong>Authors:</strong> Gloria Felicia, Michael Eniolade, Jinfeng He, Zitha Sasindran, Hemant Kumar, Milan Hussain Angati, Sandeep Bandarupalli
<strong>Venue:</strong> arXiv (2026)</p><p>Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22136v1">üìÑ Download PDF</a></p><hr><h3 id=clarity-the-flexibility-interpretability-trade-off-in-sparsity-aware-concept-bottleneck-modelshttpsarxivorgabs260121944v1><a href=https://arxiv.org/abs/2601.21944v1>Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models</a><a hidden class=anchor aria-hidden=true href=#clarity-the-flexibility-interpretability-trade-off-in-sparsity-aware-concept-bottleneck-modelshttpsarxivorgabs260121944v1>#</a></h3><p><strong>Authors:</strong> Konstantinos P. Panousis, Diego Marcos
<strong>Venue:</strong> arXiv (2026)</p><p>The widespread adoption of Vision-Language Models (VLMs) across fields has amplified concerns about model interpretability. Distressingly, these models are often treated as black-boxes, with limited or non-existent investigation of their decision making process. Despite numerous post- and ante-hoc interepretability methods, systematic and objective evaluation of the learned representations remains limited, particularly for sparsity-aware methods that are increasingly considered to &ldquo;induce interpretability&rdquo;. In this work, we focus on Concept Bottleneck Models and investigate how different modeling decisions affect the emerging representations. We introduce the notion of clarity, a measure, capturing the interplay between the downstream performance and the sparsity and precision of the concept representation, while proposing an interpretability assessment framework using datasets with ground truth concept annotations. We consider both VLM- and attribute predictor-based CBMs, and three different sparsity-inducing strategies: per example $\ell_1, \ell_0$ and Bernoulli-based formulations. Our experiments reveal a critical trade-off between flexibility and interpretability, under which a given method can exhibit markedly different behaviors even at comparable performance levels. The code will be made publicly available upon publication.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21944v1">üìÑ Download PDF</a></p><hr><h3 id=retrieval-infused-reasoning-sandbox-a-benchmark-for-decoupling-retrieval-and-reasoning-capabilitieshttpsarxivorgabs260121937v1><a href=https://arxiv.org/abs/2601.21937v1>Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities</a><a hidden class=anchor aria-hidden=true href=#retrieval-infused-reasoning-sandbox-a-benchmark-for-decoupling-retrieval-and-reasoning-capabilitieshttpsarxivorgabs260121937v1>#</a></h3><p><strong>Authors:</strong> Shuangshuang Ying, Zheyu Wang, Yunjian Peng, Jin Chen, Yuhao Wu, Hongbin Lin, Dingyu He, Siyi Liu, Gengchen Yu, YinZhu Piao, Yuchen Wu, Xin Gui, Zhongyuan Peng, Xin Li, Xeron Du, Libo Qin, YiXin Cao, Ge Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility. We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search: multi-step synthesis, denoising, and evidence-based conclusion making. DeR2 decouples evidence access from reasoning via four regimes&ndash;Instruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)&ndash;yielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution. To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability. To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales. Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21937v1">üìÑ Download PDF</a></p><hr><h3 id=webarbiter-a-principle-guided-reasoning-process-reward-model-for-web-agentshttpsarxivorgabs260121872v1><a href=https://arxiv.org/abs/2601.21872v1>WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents</a><a hidden class=anchor aria-hidden=true href=#webarbiter-a-principle-guided-reasoning-process-reward-model-for-web-agentshttpsarxivorgabs260121872v1>#</a></h3><p><strong>Authors:</strong> Yao Zhang, Shijie Tang, Zeyu Li, Zhen Han, Volker Tresp
<strong>Venue:</strong> arXiv (2026)</p><p>Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21872v1">üìÑ Download PDF</a></p><hr><h3 id=mmfinereason-closing-the-multimodal-reasoning-gap-via-open-data-centric-methodshttpsarxivorgabs260121821v1><a href=https://arxiv.org/abs/2601.21821v1>MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods</a><a hidden class=anchor aria-hidden=true href=#mmfinereason-closing-the-multimodal-reasoning-gap-via-open-data-centric-methodshttpsarxivorgabs260121821v1>#</a></h3><p><strong>Authors:</strong> Honglin Lin, Zheng Liu, Yun Zhu, Chonghan Qin, Juekai Lin, Xiaoran Shang, Conghui He, Wentao Zhang, Lijun Wu
<strong>Venue:</strong> arXiv (2026)</p><p>Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a &ldquo;less is more&rdquo; phenomenon via our difficulty-aware filtering strategy: a subset of just 7% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21821v1">üìÑ Download PDF</a></p><hr><h2 id=-emotion_language>üîç emotion_language<a hidden class=anchor aria-hidden=true href=#-emotion_language>#</a></h2><h3 id=the-algebraic-and-geometric-classification-of-derived-jordan-and-bicommutative-algebrashttpsarxivorgabs260122110v1><a href=https://arxiv.org/abs/2601.22110v1>The algebraic and geometric classification of derived Jordan and bicommutative algebras</a><a hidden class=anchor aria-hidden=true href=#the-algebraic-and-geometric-classification-of-derived-jordan-and-bicommutative-algebrashttpsarxivorgabs260122110v1>#</a></h3><p><strong>Authors:</strong> Hani Abdelwahab, Ivan Kaygorodov, Roman Lubkov
<strong>Venue:</strong> arXiv (2026)</p><p>We developed a new proper method for classifying $n$-dimensional derived Jordan algebras, and apply it to the classification of $3$-dimensional derived Jordan algebras. As a byproduct, we have the algebraic classification of $3$-dimensional metabelian commutative algebras and $3$-dimensional derived commutative associative algebras. After that, we introduced a method of classifying $n$-dimensional bicommutative algebras, based on the classification of $n$-dimensional derived commutative associative algebras, and applied it to the classification of $3$-dimensional bicommutative algebras. The second part of the paper is dedicated to the geometric classification of $3$-dimensional metabelian commutative, derived commutative associative, derived Jordan and bicommutative algebras.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22110v1">üìÑ Download PDF</a></p><hr><h3 id=investigating-associational-biases-in-inter-model-communication-of-large-generative-modelshttpsarxivorgabs260122093v1><a href=https://arxiv.org/abs/2601.22093v1>Investigating Associational Biases in Inter-Model Communication of Large Generative Models</a><a hidden class=anchor aria-hidden=true href=#investigating-associational-biases-in-inter-model-communication-of-large-generative-modelshttpsarxivorgabs260122093v1>#</a></h3><p><strong>Authors:</strong> Fethiye Irmak Dogan, Yuval Weiss, Kajal Patel, Jiaee Cheong, Hatice Gunes
<strong>Venue:</strong> arXiv (2026)</p><p>Social bias in generative AI can manifest not only as performance disparities but also as associational bias, whereby models learn and reproduce stereotypical associations between concepts and demographic groups, even in the absence of explicit demographic information (e.g., associating doctors with men). These associations can persist, propagate, and potentially amplify across repeated exchanges in inter-model communication pipelines, where one generative model&rsquo;s output becomes another&rsquo;s input. This is especially salient for human-centred perception tasks, such as human activity recognition and affect prediction, where inferences about behaviour and internal states can lead to errors or stereotypical associations that propagate into unequal treatment. In this work, focusing on human activity and affective expression, we study how such associations evolve within an inter-model communication pipeline that alternates between image generation and image description. Using the RAF-DB and PHASE datasets, we quantify demographic distribution drift induced by model-to-model information exchange and assess whether these drifts are systematic using an explainability pipeline. Our results reveal demographic drifts toward younger representations for both actions and emotions, as well as toward more female-presenting representations, primarily for emotions. We further find evidence that some predictions are supported by spurious visual regions (e.g., background or hair) rather than concept-relevant cues (e.g., body or face). We also examine whether these demographic drifts translate into measurable differences in downstream behaviour, i.e., while predicting activity and emotion labels. Finally, we outline mitigation strategies spanning data-centric, training and deployment interventions, and emphasise the need for careful safeguards when deploying interconnected models in human-centred AI systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22093v1">üìÑ Download PDF</a></p><hr><h3 id=on-set-theoretic-solutions-of-pentagon-equation-and-positive-basis-hopf-algebrashttpsarxivorgabs260122089v1><a href=https://arxiv.org/abs/2601.22089v1>On set-theoretic solutions of pentagon equation and positive basis Hopf algebras</a><a hidden class=anchor aria-hidden=true href=#on-set-theoretic-solutions-of-pentagon-equation-and-positive-basis-hopf-algebrashttpsarxivorgabs260122089v1>#</a></h3><p><strong>Authors:</strong> Ilaria Colazzo, Geoffrey Janssens
<strong>Venue:</strong> arXiv (2026)</p><p>We investigate the connection between bijective, not necessarily finite, set-theoretic solutions of the pentagon equation and Hopf algebras. Firstly, we prove that finite solutions correspond to Hopf algebras with the positive basis property. As a corollary we generalise Lu-Yan-Zhu classification to arbitrary characteristic $0$ fields $k$. Secondly, we study the general problem of when a Hopf algebra has a basis yielding a set-theoretic solution. Finally, we classify all (co)commutative bijective solutions. This result requires to obtain a description of all bases of a group algebra $k[G]$ yielding a set-theoretic solution. We namely show that such bases correspond, through a Fourier transform, to splittings $A \rtimes N$ of $G$ with $A$ a finite abelian group.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22089v1">üìÑ Download PDF</a></p><hr><h3 id=on-homogeneous-involutions-on-matrix-algebrashttpsarxivorgabs260122049v1><a href=https://arxiv.org/abs/2601.22049v1>On homogeneous involutions on matrix algebras</a><a hidden class=anchor aria-hidden=true href=#on-homogeneous-involutions-on-matrix-algebrashttpsarxivorgabs260122049v1>#</a></h3><p><strong>Authors:</strong> Micael Said Garcia, Cassia Ferreira Sampaio
<strong>Venue:</strong> arXiv (2026)</p><p>We study the homogeneous involutions on the full square matrices over an algebraically closed field endowed with a division grading with commutative support. We obtain the classification of the isomorphism and equivalence classes for the Pauli grading. We also investigate the homogeneous involutions on the full square matrices with entries in a finite-dimensional graded-division algebra over an algebraically closed field of characteristic not $2$ endowed with an arbitrary grading by an arbitrary group.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22049v1">üìÑ Download PDF</a></p><hr><h3 id=quantum-fluctuations-in-hydrodynamics-and-quantum-long-time-tailshttpsarxivorgabs260122140v1><a href=https://arxiv.org/abs/2601.22140v1>Quantum fluctuations in hydrodynamics and quantum long-time tails</a><a hidden class=anchor aria-hidden=true href=#quantum-fluctuations-in-hydrodynamics-and-quantum-long-time-tailshttpsarxivorgabs260122140v1>#</a></h3><p><strong>Authors:</strong> Akash Jain
<strong>Venue:</strong> arXiv (2026)</p><p>We construct a quantum Schwinger-Keldysh (SK) effective field theory for the diffusive hydrodynamics of a conserved scalar field. Quantum corrections within the SK framework are guided by fluctuation-dissipation relations, enforced via a dynamical Kubo-Martin-Schwinger (KMS) symmetry. We find that the KMS symmetry necessarily generates fluctuation contributions in the SK effective action at all orders in the noise field, thereby giving rise to intrinsically non-Gaussian noise. We use our results to compute one-loop quantum corrections to the two-point density-density retarded correlation function, leading to a quantum generalization of hydrodynamic long-time tails. Our results apply at arbitrarily high orders in $\hbar$. The one-loop results for retarded correlation functions have been expressed in terms of a family of polynomials. We also provide a closed-form expression for the one-loop results at leading order in the wavevector expansion.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22140v1">üìÑ Download PDF</a></p><hr><h3 id=microlocal-maximal-hypoellipticity-from-the-geometric-viewpoint-ihttpsarxivorgabs260122122v1><a href=https://arxiv.org/abs/2601.22122v1>Microlocal maximal hypoellipticity from the geometric viewpoint: I</a><a hidden class=anchor aria-hidden=true href=#microlocal-maximal-hypoellipticity-from-the-geometric-viewpoint-ihttpsarxivorgabs260122122v1>#</a></h3><p><strong>Authors:</strong> Omar Mohsen
<strong>Venue:</strong> arXiv (2026)</p><p>Given some vector fields on a smooth manifold satisfying H√∂rmander&rsquo;s condition, we define a bi-graded pseudo-differential calculus which contains the classical pseudo-differential calculus and a pseudo-differential calculus adapted to the sub-Riemannian structure induced by the vector fields.
Our approach is based on geometric constructions (resolution of singularities) together with methods from operators algebras. We develop this calculus in full generality, including Sobolev spaces, the wavefront set, and the principal symbol, etc.
In particular, using this calculus, we prove that invertibility of the principal symbol implies microlocal maximal hypoellipticity. This allows us to resolve affirmatively the microlocal version of a conjecture of Helffer and Nourrigat.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22122v1">üìÑ Download PDF</a></p><hr><h3 id=pay-for-hints-not-answers-llm-shepherding-for-cost-efficient-inferencehttpsarxivorgabs260122132v1><a href=https://arxiv.org/abs/2601.22132v1>Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference</a><a hidden class=anchor aria-hidden=true href=#pay-for-hints-not-answers-llm-shepherding-for-cost-efficient-inferencehttpsarxivorgabs260122132v1>#</a></h3><p><strong>Authors:</strong> Ziming Dong, Hardik Sharma, Evan O&rsquo;Toole, Jaya Prakash Champati, Kui Wu
<strong>Venue:</strong> arXiv (2026)</p><p>Large Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in accuracy. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a complete response at full cost. We introduce LLM Shepherding, a framework that requests only a short prefix (a hint) from the LLM and provides it to SLM. This simple mechanism is surprisingly effective for math and coding tasks: even hints comprising 10-30% of the full LLM response improve SLM accuracy significantly. Shepherding generalizes both routing and cascading, and it achieves lower cost under oracle decision-making. We develop a two-stage predictor that jointly determines whether a hint is needed and how many tokens to request. On the widely-used mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks, Shepherding reduces costs by 42-94% relative to LLM-only inference. Compared to state-of-the-art routing and cascading baselines, shepherding delivers up to 2.8x cost reduction while matching accuracy. To our knowledge, this is the first work to exploit token-level budget control for SLM-LLM collaboration.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22132v1">üìÑ Download PDF</a></p><hr><h3 id=social-media-data-for-population-mapping-a-bayesian-approach-to-address-representativeness-and-privacy-challengeshttpsarxivorgabs260122104v1><a href=https://arxiv.org/abs/2601.22104v1>Social Media Data for Population Mapping: A Bayesian Approach to Address Representativeness and Privacy Challenges</a><a hidden class=anchor aria-hidden=true href=#social-media-data-for-population-mapping-a-bayesian-approach-to-address-representativeness-and-privacy-challengeshttpsarxivorgabs260122104v1>#</a></h3><p><strong>Authors:</strong> Paolo Andrich, Shengjie Lai, Halim Jun, Qianwen Duan, Zhifeng Cheng, Seth R. Flaxman, Andrew J. Tatem
<strong>Venue:</strong> arXiv (2026)</p><p>Accurate and timely population data are essential for disaster response and humanitarian planning, but traditional censuses often cannot capture rapid demographic changes. Social media data offer a promising alternative for dynamic population monitoring, but their representativeness remains poorly understood and stringent privacy requirements limit their reliability. Here, we address these limitations in the context of the Philippines by calibrating Facebook user counts with the country&rsquo;s 2020 census figures. First, we find that differential privacy techniques commonly applied to social media-based population datasets disproportionately mask low-population areas. To address this, we propose a Bayesian imputation approach to recover missing values, restoring data coverage for $5.5%$ of rural areas. Further, using the imputed social media data and leveraging predictors such as urbanisation level, demographic composition, and socio-economic status, we develop a statistical model for the proportion of Facebook users in each municipality, which links observed Facebook user numbers to the true population levels. Out-of-sample validation demonstrates strong result generalisability, with errors as low as ${\approx}18%$ and ${\approx}24%$ for urban and rural Facebook user proportions, respectively. We further demonstrate that accounting for overdispersion and spatial correlations in the data is crucial to obtain accurate estimates and appropriate credible intervals. Crucially, as predictors change over time, the models can be used to regularly update the population predictions, providing a dynamic complement to census-based estimates. These results have direct implications for humanitarian response in disaster-prone regions and offer a general framework for using biased social media signals to generate reliable and timely population data.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22104v1">üìÑ Download PDF</a></p><hr><h3 id=inverted-anisotropy-of-the-partially-screened-magnetic-impurityhttpsarxivorgabs260122078v1><a href=https://arxiv.org/abs/2601.22078v1>Inverted anisotropy of the partially screened magnetic impurity</a><a hidden class=anchor aria-hidden=true href=#inverted-anisotropy-of-the-partially-screened-magnetic-impurityhttpsarxivorgabs260122078v1>#</a></h3><p><strong>Authors:</strong> Krzysztof P. W√≥jcik, Micha≈Ç P. Kwasigroch
<strong>Venue:</strong> arXiv (2026)</p><p>We investigate a single magnetic impurity in the presence of strong spin-orbit coupling and single-ion anisotropy. We show that at sufficiently strong coupling there exists a finite temperature window, before the moment is completely screened, where the magnetic anisotropy of the system flips: the hard-axis becomes the easy-axis or vice versa. We derive this rigorously for a single impurity using numerical renormalization group calculations as well as Nozieres&rsquo; strong-coupling limit and discuss its relevance to heavy-fermion compounds which order magnetically along the hard-direction. We show that the coexistence of Curie-like response and Kondo fluctuations is stabilized along the initially hard direction leading to the anisotropy switch.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22078v1">üìÑ Download PDF</a></p><hr><h3 id=early-and-prediagnostic-detection-of-pancreatic-cancer-from-computed-tomographyhttpsarxivorgabs260122134v1><a href=https://arxiv.org/abs/2601.22134v1>Early and Prediagnostic Detection of Pancreatic Cancer from Computed Tomography</a><a hidden class=anchor aria-hidden=true href=#early-and-prediagnostic-detection-of-pancreatic-cancer-from-computed-tomographyhttpsarxivorgabs260122134v1>#</a></h3><p><strong>Authors:</strong> Wenxuan Li, Pedro R. A. S. Bassi, Lizhou Wu, Xinze Zhou, Yuxuan Zhao, Qi Chen, Szymon Plotka, Tianyu Lin, Zheren Zhu, Marisa Martin, Justin Caskey, Shanshan Jiang, Xiaoxi Chen, Jaroslaw B. ƒÜwikla, Artur Sankowski, Yaping Wu, Sergio Decherchi, Andrea Cavalli, Chandana Lall, Cristian Tomasetti, Yaxing Guo, Xuan Yu, Yuqing Cai, Hualin Qiao, Jie Bao, Chenhan Hu, Ximing Wang, Arkadiusz Sitek, Kai Ding, Heng Li, Meiyun Wang, Dexin Yu, Guang Zhang, Yang Yang, Kang Wang, Alan L. Yuille, Zongwei Zhou
<strong>Venue:</strong> arXiv (2026)</p><p>Pancreatic ductal adenocarcinoma (PDAC), one of the deadliest solid malignancies, is often detected at a late and inoperable stage. Retrospective reviews of prediagnostic CT scans, when conducted by expert radiologists aware that the patient later developed PDAC, frequently reveal lesions that were previously overlooked. To help detecting these lesions earlier, we developed an automated system named ePAI (early Pancreatic cancer detection with Artificial Intelligence). It was trained on data from 1,598 patients from a single medical center. In the internal test involving 1,009 patients, ePAI achieved an area under the receiver operating characteristic curve (AUC) of 0.939-0.999, a sensitivity of 95.3%, and a specificity of 98.7% for detecting small PDAC less than 2 cm in diameter, precisely localizing PDAC as small as 2 mm. In an external test involving 7,158 patients across 6 centers, ePAI achieved an AUC of 0.918-0.945, a sensitivity of 91.5%, and a specificity of 88.0%, precisely localizing PDAC as small as 5 mm. Importantly, ePAI detected PDACs on prediagnostic CT scans obtained 3 to 36 months before clinical diagnosis that had originally been overlooked by radiologists. It successfully detected and localized PDACs in 75 of 159 patients, with a median lead time of 347 days before clinical diagnosis. Our multi-reader study showed that ePAI significantly outperformed 30 board-certified radiologists by 50.3% (P &lt; 0.05) in sensitivity while maintaining a comparable specificity of 95.4% in detecting PDACs early and prediagnostic. These findings suggest its potential of ePAI as an assistive tool to improve early detection of pancreatic cancer.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22134v1">üìÑ Download PDF</a></p><hr><h3 id=information-geometry-driven-graph-sequential-growthhttpsarxivorgabs260122106v1><a href=https://arxiv.org/abs/2601.22106v1>Information-geometry-driven graph sequential growth</a><a hidden class=anchor aria-hidden=true href=#information-geometry-driven-graph-sequential-growthhttpsarxivorgabs260122106v1>#</a></h3><p><strong>Authors:</strong> Harry T. Bond, Bertrand Gauthier, Kirstin Strokorb
<strong>Venue:</strong> arXiv (2026)</p><p>We investigate the properties of a class of regularisation-free approaches for Gaussian graphical inference based on the information-geometry-driven sequential growth of initially edgeless graphs. Relating the growth of a graph to a coordinate descent process, we characterise the fully-corrective descents corresponding to information-optimal growths, and propose numerically efficient strategies for their approximation. We demonstrate the ability of the proposed procedures to reliably extract sparse graphical models while limiting the number of false detections, and illustrate how activation ranks can provide insight into the informational relevance of edge sets. The considered approaches are tuning-parameter-free and have complexities akin to coordinate descents.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22106v1">üìÑ Download PDF</a></p><hr><h3 id=auditorily-embodied-conversational-agents-effects-of-spatialization-and-situated-audio-cues-on-presence-and-social-perceptionhttpsarxivorgabs260122082v1><a href=https://arxiv.org/abs/2601.22082v1>Auditorily Embodied Conversational Agents: Effects of Spatialization and Situated Audio Cues on Presence and Social Perception</a><a hidden class=anchor aria-hidden=true href=#auditorily-embodied-conversational-agents-effects-of-spatialization-and-situated-audio-cues-on-presence-and-social-perceptionhttpsarxivorgabs260122082v1>#</a></h3><p><strong>Authors:</strong> Yi Fei Cheng, Jarod Bloch, Alexander Wang, Andrea Bianchi, Anusha Withana, Anhong Guo, Laurie M. Heller, David Lindlbauer
<strong>Venue:</strong> arXiv (2026)</p><p>Embodiment can enhance conversational agents, such as increasing their perceived presence. This is typically achieved through visual representations of a virtual body; however, visual modalities are not always available, such as when users interact with agents using headphones or display-less glasses. In this work, we explore auditory embodiment. By introducing auditory cues of bodily presence - through spatially localized voice and situated Foley audio from environmental interactions - we investigate how audio alone can convey embodiment and influence perceptions of a conversational agent. We conducted a 2 (spatialization: monaural vs. spatialized) x 2 (Foley: none vs. Foley) within-subjects study, where participants (n=24) engaged in conversations with agents. Our results show that spatialization and Foley increase co-presence, but reduce users&rsquo; perceptions of the agent&rsquo;s attention and other social attributes.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22082v1">üìÑ Download PDF</a></p><hr><h3 id=industrialized-deception-the-collateral-effects-of-llm-generated-misinformation-on-digital-ecosystemshttpsarxivorgabs260121963v1><a href=https://arxiv.org/abs/2601.21963v1>Industrialized Deception: The Collateral Effects of LLM-Generated Misinformation on Digital Ecosystems</a><a hidden class=anchor aria-hidden=true href=#industrialized-deception-the-collateral-effects-of-llm-generated-misinformation-on-digital-ecosystemshttpsarxivorgabs260121963v1>#</a></h3><p><strong>Authors:</strong> Alexander Loth, Martin Kappes, Marc-Oliver Pahl
<strong>Venue:</strong> arXiv (2026)</p><p>Generative AI and misinformation research has evolved since our 2024 survey. This paper presents an updated perspective, transitioning from literature review to practical countermeasures. We report on changes in the threat landscape, including improved AI-generated content through Large Language Models (LLMs) and multimodal systems. Central to this work are our practical contributions: JudgeGPT, a platform for evaluating human perception of AI-generated news, and RogueGPT, a controlled stimulus generation engine for research. Together, these tools form an experimental pipeline for studying how humans perceive and detect AI-generated misinformation. Our findings show that detection capabilities have improved, but the competition between generation and detection continues. We discuss mitigation strategies including LLM-based detection, inoculation approaches, and the dual-use nature of generative AI. This work contributes to research addressing the adverse impacts of AI on information quality.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21963v1">üìÑ Download PDF</a></p><hr><h3 id=user-acceptance-model-for-smart-incentives-in-sustainable-video-streaming-towards-6ghttpsarxivorgabs260121903v1><a href=https://arxiv.org/abs/2601.21903v1>User Acceptance Model for Smart Incentives in Sustainable Video Streaming towards 6G</a><a hidden class=anchor aria-hidden=true href=#user-acceptance-model-for-smart-incentives-in-sustainable-video-streaming-towards-6ghttpsarxivorgabs260121903v1>#</a></h3><p><strong>Authors:</strong> Konstantinos Varsos, Adamantia Stamou, George D. Stamoulis, Vasillios A. Siris
<strong>Venue:</strong> arXiv (2026)</p><p>The rapid growth of 5G video streaming is intensifying energy consumption across access, core, and data-center networks, underscoring the critical need for energy and carbon-efficient solutions. While reducing streaming bitrates improves energy efficiency, its success hinges on user acceptance&ndash;particularly when lower bitrates may be perceived as reduced quality of experience (QoE). Therefore, there is a need to develop transparent, user-centric incentive models that balance sustainability with perceived value. We propose a user-acceptance model that combines diverse environmental awareness, personalized responsiveness to incentives, and varying levels of altruism into a unified probabilistic framework. The model incorporates dynamic, individualized incentives that adapt over time. We further enhance the framework by incorporating (i) social well-being as a motivator for altruistic choices, (ii) provider-driven education strategies that gradually adjust user acceptance thresholds, and (iii) data-driven learning of user traits from historical offer&ndash;response interactions. Extensive synthetic-data experiments reveal the trade-offs between provider cost and network flexibility, showing that personalized incentives and gradual behavioral adaptation can advance sustainability targets without compromising stakeholder requirements.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21903v1">üìÑ Download PDF</a></p><hr><h3 id=knowbias-mitigating-social-bias-in-llms-via-know-bias-neuron-enhancementhttpsarxivorgabs260121864v1><a href=https://arxiv.org/abs/2601.21864v1>KnowBias: Mitigating Social Bias in LLMs via Know-Bias Neuron Enhancement</a><a hidden class=anchor aria-hidden=true href=#knowbias-mitigating-social-bias-in-llms-via-know-bias-neuron-enhancementhttpsarxivorgabs260121864v1>#</a></h3><p><strong>Authors:</strong> Jinhao Pan, Chahat Raj, Anjishnu Mukherjee, Sina Mansouri, Bowen Wei, Shloka Yada, Ziwei Zhu
<strong>Venue:</strong> arXiv (2026)</p><p>Large language models (LLMs) exhibit social biases that reinforce harmful stereotypes, limiting their safe deployment. Most existing debiasing methods adopt a suppressive paradigm by modifying parameters, prompts, or neurons associated with biased behavior; however, such approaches are often brittle, weakly generalizable, data-inefficient, and prone to degrading general capability. We propose \textbf{KnowBias}, a lightweight and conceptually distinct framework that mitigates bias by strengthening, rather than suppressing, neurons encoding bias-knowledge. KnowBias identifies neurons encoding bias knowledge using a small set of bias-knowledge questions via attribution-based analysis, and selectively enhances them at inference time. This design enables strong debiasing while preserving general capabilities, generalizes across bias types and demographics, and is highly data efficient, requiring only a handful of simple yes/no questions and no retraining. Experiments across multiple benchmarks and LLMs demonstrate consistent state-of-the-art debiasing performance with minimal utility degradation. Data and code are available at <a href=https://github.com/JP-25/KnowBias>https://github.com/JP-25/KnowBias</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.21864v1">üìÑ Download PDF</a></p><hr><h3 id=putting-pressure-under-pressure-on-the-status-of-classical-pressure-in-special-relativityhttpsarxivorgabs260122133v1><a href=https://arxiv.org/abs/2601.22133v1>Putting Pressure Under Pressure: On the Status of Classical Pressure in Special Relativity</a><a hidden class=anchor aria-hidden=true href=#putting-pressure-under-pressure-on-the-status-of-classical-pressure-in-special-relativityhttpsarxivorgabs260122133v1>#</a></h3><p><strong>Authors:</strong> Eugene Y. S. Chua
<strong>Venue:</strong> arXiv (2026)</p><p>Much of the century-old debate surrounding the status of thermodynamics in relativity has centered on the search for a suitably relativistic temperature; recent works by Chua (2023) and Chua and Callender (forthcoming) have suggested that the classical temperature concept &ndash; consilient as it is in classical settings &ndash; &lsquo;falls apart&rsquo; in relativity. However, these discussions typically assume an unproblematic Lorentz transformation for &ndash; specifically, the Lorentz invariance of &ndash; the pressure concept. Here I argue that, just like the classical temperature, the classical concept of pressure breaks down in relativistic settings. I discuss how this might suggest a new thermodynamic limit &ndash; a u &ndash;> 0 limit &ndash; without which an unambiguous thermodynamic description of systems doesn&rsquo;t emerge.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22133v1">üìÑ Download PDF</a></p><hr><h3 id=a-federated-and-parameter-efficient-framework-for-large-language-model-training-in-medicinehttpsarxivorgabs260122124v1><a href=https://arxiv.org/abs/2601.22124v1>A Federated and Parameter-Efficient Framework for Large Language Model Training in Medicine</a><a hidden class=anchor aria-hidden=true href=#a-federated-and-parameter-efficient-framework-for-large-language-model-training-in-medicinehttpsarxivorgabs260122124v1>#</a></h3><p><strong>Authors:</strong> Anran Li, Yuanyuan Chen, Wenjun Long, Yu Yin, Yan Hu, Hyunjae Kim, Weipeng Zhou, Yujia Zhou, Hongyi Peng, Yang Ren, Xuguang Ai, Zhenyue Qin, Ming Hu, Xiaoxiao Li, Han Yu, Yih-Chung Tham, Lucila Ohno-Machado, Hua Xu, Qingyu Chen
<strong>Venue:</strong> arXiv (2026)</p><p>Large language models (LLMs) have demonstrated strong performance on medical benchmarks, including question answering and diagnosis. To enable their use in clinical settings, LLMs are typically further adapted through continued pretraining or post-training using clinical data. However, most medical LLMs are trained on data from a single institution, which faces limitations in generalizability and safety in heterogeneous systems. Federated learning (FL) is a promising solution for enabling collaborative model development across healthcare institutions. Yet applying FL to LLMs in medicine remains fundamentally limited. First, conventional FL requires transmitting the full model during each communication round, which becomes impractical for multi-billion-parameter LLMs given the limited computational resources. Second, many FL algorithms implicitly assume data homogeneity, whereas real-world clinical data are highly heterogeneous across patients, diseases, and institutional practices. We introduce the model-agnostic and parameter-efficient federated learning framework for adapting LLMs to medical applications. Fed-MedLoRA transmits only low-rank adapter parameters, reducing communication and computation overhead, while Fed-MedLoRA+ further incorporates adaptive, data-aware aggregation to improve convergence under cross-site heterogeneity. We apply the framework to clinical information extraction (IE), which transforms patient narratives into structured medical entities and relations. Accuracy was assessed across five patient cohorts through comparisons with BERT models, and LLaMA-3 and DeepSeek-R1, GPT-4o models. Evaluation settings included (1) in-domain training and testing, (2) external validation on independent cohorts, and (3) a low-resource new-site adaptation scenario using real-world clinical notes from the Yale New Haven Health System.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.22124v1">üìÑ Download PDF</a></p><hr></div><footer class=post-footer><ul class=post-tags></ul></footer><div><div class=pagination__title><span class=pagination__title-h style=font-size:20px>Comments</span><hr></div><div id=tcomment></div><script src=https://cdn.staticfile.org/twikoo/1.6.39/twikoo.all.min.js></script><script>twikoo.init({envId:"https://mangodb-theta.vercel.app/",el:"#tcomment",lang:"en-US",path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://garyforreal.me/en/>Gary's House</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>Views: <span id=busuanzi_value_site_pv></span>
</span><span id=busuanzi_container_site_uv>Visitors: <span id=busuanzi_value_site_uv></span></span></div></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>