<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Weekly Paper Notes - 2026-02-15 | Gary's House</title>
<meta name=keywords content><meta name=description content="Weekly Paper Notes
üîç multilingual
A technical curriculum on language-oriented artificial intelligence in translation and specialised communication
Authors: Ralph Kr√ºger
Venue: arXiv (2026)
This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&amp;T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions."><meta name=author content="Gary"><link rel=canonical href=https://garyforreal.me/en/posts/paper/paper-2026-02-15-weekly/><meta name=google-site-verification content="XYZabc"><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><link crossorigin=anonymous href=/assets/css/stylesheet.7e5251d8716d933fafcf3df70d7ecd02729661d6d232fc5dd0b8a85ef75e3409.css integrity="sha256-flJR2HFtkz+vzz33DX7NAnKWYdbSMvxd0LioXvdeNAk=" rel="preload stylesheet" as=style><link rel=icon href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=16x16 href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=32x32 href=https://garyforreal.me/img/Q.jpg><link rel=apple-touch-icon href=https://garyforreal.me/Q.jpg><link rel=mask-icon href=https://garyforreal.me/Q.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://garyforreal.me/zh/posts/paper/paper-2026-02-15-weekly/><link rel=alternate hreflang=en href=https://garyforreal.me/en/posts/paper/paper-2026-02-15-weekly/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@200..900&display=swap" rel=stylesheet><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous referrerpolicy=no-referrer></script><meta property="og:title" content="Weekly Paper Notes - 2026-02-15"><meta property="og:description" content="Weekly Paper Notes
üîç multilingual
A technical curriculum on language-oriented artificial intelligence in translation and specialised communication
Authors: Ralph Kr√ºger
Venue: arXiv (2026)
This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&amp;T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions."><meta property="og:type" content="article"><meta property="og:url" content="https://garyforreal.me/en/posts/paper/paper-2026-02-15-weekly/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-15T15:30:24+00:00"><meta property="article:modified_time" content="2026-02-15T15:30:24+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Weekly Paper Notes - 2026-02-15"><meta name=twitter:description content="Weekly Paper Notes
üîç multilingual
A technical curriculum on language-oriented artificial intelligence in translation and specialised communication
Authors: Ralph Kr√ºger
Venue: arXiv (2026)
This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&amp;T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://garyforreal.me/en/posts/"},{"@type":"ListItem","position":2,"name":"Paper","item":"https://garyforreal.me/en/posts/paper/"},{"@type":"ListItem","position":3,"name":"Weekly Paper Notes - 2026-02-15","item":"https://garyforreal.me/en/posts/paper/paper-2026-02-15-weekly/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Weekly Paper Notes - 2026-02-15","name":"Weekly Paper Notes - 2026-02-15","description":"Weekly Paper Notes üîç multilingual A technical curriculum on language-oriented artificial intelligence in translation and specialised communication Authors: Ralph Kr√ºger Venue: arXiv (2026)\nThis paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L\u0026amp;T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions.\n","keywords":[],"articleBody":"Weekly Paper Notes üîç multilingual A technical curriculum on language-oriented artificial intelligence in translation and specialised communication Authors: Ralph Kr√ºger Venue: arXiv (2026)\nThis paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L\u0026T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions.\nüìÑ Download PDF\nStatistical Parsing for Logical Information Retrieval Authors: Greg Coppola Venue: arXiv (2026)\nIn previous work (Coppola, 2024) we introduced the Quantified Boolean Bayesian Network (QBBN), a logical graphical model that implements the forward fragment of natural deduction (Prawitz, 1965) as a probabilistic factor graph. That work left two gaps: no negation/backward reasoning, and no parser for natural language. This paper addresses both gaps across inference, semantics, and syntax. For inference, we extend the QBBN with NEG factors enforcing P(x) + P(neg x) = 1, enabling contrapositive reasoning (modus tollens) via backward lambda messages, completing Prawitz‚Äôs simple elimination rules. The engine handles 44/44 test cases spanning 22 reasoning patterns. For semantics, we present a typed logical language with role-labeled predicates, modal quantifiers, and three tiers of expressiveness following Prawitz: first-order quantification, propositions as arguments, and predicate quantification via lambda abstraction. For syntax, we present a typed slot grammar that deterministically compiles sentences to logical form (33/33 correct, zero ambiguity). LLMs handle disambiguation (95% PP attachment accuracy) but cannot produce structured parses directly (12.4% UAS), confirming grammars are necessary. The architecture: LLM preprocesses, grammar parses, LLM reranks, QBBN infers. We argue this reconciles formal semantics with Sutton‚Äôs ‚Äúbitter lesson‚Äù (2019): LLMs eliminate the annotation bottleneck that killed formal NLP, serving as annotator while the QBBN serves as verifier. Code: https://github.com/gregorycoppola/world\nüìÑ Download PDF\nCitiLink-Minutes: A Multilayer Annotated Dataset of Municipal Meeting Minutes Authors: Ricardo Campos, Ana Filipa Pacheco, Ana Lu√≠sa Fernandes, In√™s Cantante, Rute Rebou√ßas, Lu√≠s Filipe Cunha, Jos√© Miguel Isidro, Jos√© Pedro Evans, Miguel Marques, Rodrigo Batista, Evelin Amorim, Al√≠pio Jorge, Nuno Guimar√£es, S√©rgio Nunes, Ant√≥nio Leal, Purifica√ß√£o Silvano Venue: arXiv (2026)\nCity councils play a crucial role in local governance, directly influencing citizens‚Äô daily lives through decisions made during municipal meetings. These deliberations are formally documented in meeting minutes, which serve as official records of discussions, decisions, and voting outcomes. Despite their importance, municipal meeting records have received little attention in Information Retrieval (IR) and Natural Language Processing (NLP), largely due to the lack of annotated datasets, which ultimately limit the development of computational models. To address this gap, we introduce CitiLink-Minutes, a multilayer dataset of 120 European Portuguese municipal meeting minutes from six municipalities. Unlike prior annotated datasets of parliamentary or video records, CitiLink-Minutes provides multilayer annotations and structured linkage of official written minutes. The dataset contains over one million tokens, with all personal identifiers de-identified. Each minute was manually annotated by two trained annotators and curated by an experienced linguist across three complementary dimensions: (1) metadata, (2) subjects of discussion, and (3) voting outcomes, totaling over 38,000 individual annotations. Released under FAIR principles and accompanied by baseline results on metadata extraction, topic classification, and vote labeling, CitiLink-Minutes demonstrates its potential for downstream NLP and IR tasks, while promoting transparent access to municipal decisions.\nüìÑ Download PDF\nDHPLT: large-scale multilingual diachronic corpora and word representations for semantic change modelling Authors: Mariia Fedorova, Andrey Kutuzov, Khonzoda Umarova Venue: arXiv (2026)\nIn this resource paper, we present DHPLT, an open collection of diachronic corpora in 41 diverse languages. DHPLT is based on the web-crawled HPLT datasets; we use web crawl timestamps as the approximate signal of document creation time. The collection covers three time periods: 2011-2015, 2020-2021 and 2024-present (1 million documents per time period for each language). We additionally provide pre-computed word type and token embeddings and lexical substitutions for our chosen target words, while at the same time leaving it open for the other researchers to come up with their own target words using the same datasets. DHPLT aims at filling in the current lack of multilingual diachronic corpora for semantic change modelling (beyond a dozen of high-resource languages). It opens the way for a variety of new experimental setups in this field. All the resources described in this paper are available at https://data.hplt-project.org/three/diachronic/, sorted by language.\nüìÑ Download PDF\nScaling Model and Data for Multilingual Machine Translation with Open Large Language Models Authors: Yuzhe Shang, Pengzhi Gao, Wei Liu, Jian Luan, Jinsong Su Venue: arXiv (2026)\nOpen large language models (LLMs) have demonstrated improving multilingual capabilities in recent years. In this paper, we present a study of open LLMs for multilingual machine translation (MT) across a range of languages, and investigate the effects of model scaling and data scaling when adapting open LLMs to multilingual MT through continual pretraining and instruction finetuning. Based on the Gemma3 model family, we develop MiLMMT-46, which achieves top-tier multilingual translation performance across 46 languages. Extensive experiments show that MiLMMT-46 consistently outperforms recent state-of-the-art (SOTA) models, including Seed-X, HY-MT-1.5, and TranslateGemma, and achieves competitive performance with strong proprietary systems such as Google Translate and Gemini 3 Pro.\nüìÑ Download PDF\nA Subword Embedding Approach for Variation Detection in Luxembourgish User Comments Authors: Anne-Marie Lutgen, Alistair Plum, Christoph Purschke Venue: arXiv (2026)\nThis paper presents an embedding-based approach to detecting variation without relying on prior normalisation or predefined variant lists. The method trains subword embeddings on raw text and groups related forms through combined cosine and n-gram similarity. This allows spelling and morphological diversity to be examined and analysed as linguistic structure rather than treated as noise. Using a large corpus of Luxembourgish user comments, the approach uncovers extensive lexical and orthographic variation that aligns with patterns described in dialectal and sociolinguistic research. The induced families capture systematic correspondences and highlight areas of regional and stylistic differentiation. The procedure does not strictly require manual annotation, but does produce transparent clusters that support both quantitative and qualitative analysis. The results demonstrate that distributional modelling can reveal meaningful patterns of variation even in ‚Äò‚Äônoisy‚Äô‚Äô or low-resource settings, offering a reproducible methodological framework for studying language variety in multilingual and small-language contexts.\nüìÑ Download PDF\nTowards Reliable Machine Translation: Scaling LLMs for Critical Error Detection and Safety Authors: Muskaan Chopra, Lorenz Sparrenberg, Rafet Sifa Venue: arXiv (2026)\nMachine Translation (MT) plays a pivotal role in cross-lingual information access, public policy communication, and equitable knowledge dissemination. However, critical meaning errors, such as factual distortions, intent reversals, or biased translations, can undermine the reliability, fairness, and safety of multilingual systems. In this work, we explore the capacity of instruction-tuned Large Language Models (LLMs) to detect such critical errors, evaluating models across a range of parameters using the publicly accessible data sets. Our findings show that model scaling and adaptation strategies (zero-shot, few-shot, fine-tuning) yield consistent improvements, outperforming encoder-only baselines like XLM-R and ModernBERT. We argue that improving critical error detection in MT contributes to safer, more trustworthy, and socially accountable information systems by reducing the risk of disinformation, miscommunication, and linguistic harm, especially in high-stakes or underrepresented contexts. This work positions error detection not merely as a technical challenge, but as a necessary safeguard in the pursuit of just and responsible multilingual AI. The code will be made available at GitHub.\nüìÑ Download PDF\nOn the Robustness of Knowledge Editing for Detoxification Authors: Ming Dong, Shiyi Tang, Ziyan Peng, Guanyi Chen, Tingting He Venue: arXiv (2026)\nKnowledge-Editing-based (KE-based) detoxification has emerged as a promising approach for mitigating harmful behaviours in Large Language Models. Existing evaluations, however, largely rely on automatic toxicity classifiers, implicitly assuming that reduced toxicity scores reflect genuine behavioural suppression. In this work, we propose a robustness-oriented evaluation framework for KE-based detoxification that examines its reliability beyond standard classifier-based metrics along three dimensions: optimisation robustness, compositional robustness, and cross-lingual robustness. We identify pseudo-detoxification as a common failure mode, where apparent toxicity reductions arise from degenerate generation behaviours rather than meaningful suppression of unsafe content. We further show that detoxification effectiveness degrades when multiple unsafe behaviours are edited jointly, and that both monolingual and cross-lingual detoxification remain effective only under specific model-method combinations. Overall, our results indicate that KE-based detoxification is robust only for certain models, limited numbers of detoxification objectives, and a subset of languages.\nüìÑ Download PDF\nFrom FusHa to Folk: Exploring Cross-Lingual Transfer in Arabic Language Models Authors: Abdulmuizz Khalak, Abderrahmane Issam, Gerasimos Spanakis Venue: arXiv (2026)\nArabic Language Models (LMs) are pretrained predominately on Modern Standard Arabic (MSA) and are expected to transfer to its dialects. While MSA as the standard written variety is commonly used in formal settings, people speak and write online in various dialects that are spread across the Arab region. This poses limitations for Arabic LMs, since its dialects vary in their similarity to MSA. In this work we study cross-lingual transfer of Arabic models using probing on 3 Natural Language Processing (NLP) Tasks, and representational similarity. Our results indicate that transfer is possible but disproportionate across dialects, which we find to be partially explained by their geographic proximity. Furthermore, we find evidence for negative interference in models trained to support all Arabic dialects. This questions their degree of similarity, and raises concerns for cross-lingual transfer in Arabic models.\nüìÑ Download PDF\nLEMUR: A Corpus for Robust Fine-Tuning of Multilingual Law Embedding Models for Retrieval Authors: Narges Baba Ahmadi, Jan Strich, Martin Semmann, Chris Biemann Venue: arXiv (2026)\nLarge language models (LLMs) are increasingly used to access legal information. Yet, their deployment in multilingual legal settings is constrained by unreliable retrieval and the lack of domain-adapted, open-embedding models. In particular, existing multilingual legal corpora are not designed for semantic retrieval, and PDF-based legislative sources introduce substantial noise due to imperfect text extraction. To address these challenges, we introduce LEMUR, a large-scale multilingual corpus of EU environmental legislation constructed from 24,953 official EUR-Lex PDF documents covering 25 languages. We quantify the fidelity of PDF-to-text conversion by measuring lexical consistency against authoritative HTML versions using the Lexical Content Score (LCS). Building on LEMUR, we fine-tune three state-of-the-art multilingual embedding models using contrastive objectives in both monolingual and bilingual settings, reflecting realistic legal-retrieval scenarios. Experiments across low- and high-resource languages demonstrate that legal-domain fine-tuning consistently improves Top-k retrieval accuracy relative to strong baselines, with particularly pronounced gains for low-resource languages. Cross-lingual evaluations show that these improvements transfer to unseen languages, indicating that fine-tuning primarily enhances language-independent, content-level legal representations rather than language-specific cues. We publish code\\footnote{\\href{https://github.com/nargesbh/eur_lex}{GitHub Repository}} and data\\footnote{\\href{https://huggingface.co/datasets/G4KMU/LEMUR}{Hugging Face Dataset}}.\nüìÑ Download PDF\nUnsupervised Cross-Lingual Part-of-Speech Tagging with Monolingual Corpora Only Authors: Jianyu Zheng Venue: arXiv (2026)\nDue to the scarcity of part-of-speech annotated data, existing studies on low-resource languages typically adopt unsupervised approaches for POS tagging. Among these, POS tag projection with word alignment method transfers POS tags from a high-resource source language to a low-resource target language based on parallel corpora, making it particularly suitable for low-resource language settings. However, this approach relies heavily on parallel corpora, which are often unavailable for many low-resource languages. To overcome this limitation, we propose a fully unsupervised cross-lingual part-of-speech(POS) tagging framework that relies solely on monolingual corpora by leveraging unsupervised neural machine translation(UNMT) system. This UNMT system first translates sentences from a high-resource language into a low-resource one, thereby constructing pseudo-parallel sentence pairs. Then, we train a POS tagger for the target language following the standard projection procedure based on word alignments. Moreover, we propose a multi-source projection technique to calibrate the projected POS tags on the target side, enhancing to train a more effective POS tagger. We evaluate our framework on 28 language pairs, covering four source languages (English, German, Spanish and French) and seven target languages (Afrikaans, Basque, Finnis, Indonesian, Lithuanian, Portuguese and Turkish). Experimental results show that our method can achieve performance comparable to the baseline cross-lingual POS tagger with parallel sentence pairs, and even exceeds it for certain target languages. Furthermore, our proposed multi-source projection technique further boosts performance, yielding an average improvement of 1.3% over previous methods.\nüìÑ Download PDF\nSinFoS: A Parallel Dataset for Translating Sinhala Figures of Speech Authors: Johan Sofalas, Dilushri Pavithra, Nevidu Jayatilleke, Ruvan Weerasinghe Venue: arXiv (2026)\nFigures of Speech (FoS) consist of multi-word phrases that are deeply intertwined with culture. While Neural Machine Translation (NMT) performs relatively well with the figurative expressions of high-resource languages, it often faces challenges when dealing with low-resource languages like Sinhala due to limited available data. To address this limitation, we introduce a corpus of 2,344 Sinhala figures of speech with cultural and cross-lingual annotations. We examine this dataset to classify the cultural origins of the figures of speech and to identify their cross-lingual equivalents. Additionally, we have developed a binary classifier to differentiate between two types of FOS in the dataset, achieving an accuracy rate of approximately 92%. We also evaluate the performance of existing LLMs on this dataset. Our findings reveal significant shortcomings in the current capabilities of LLMs, as these models often struggle to accurately convey idiomatic meanings. By making this dataset publicly available, we offer a crucial benchmark for future research in low-resource NLP and culturally aware machine translation.\nüìÑ Download PDF\nDo Not Treat Code as Natural Language: Implications for Repository-Level Code Generation and Beyond Authors: Minh Le-Anh, Huyen Nguyen, Khanh An Tran, Nam Le Hai, Linh Ngo Van, Nghi D. Q. Bui, Bach Le Venue: arXiv (2026)\nLarge language models for code (CodeLLMs) have demonstrated remarkable success in standalone code completion and generation, sometimes even surpassing human performance, yet their effectiveness diminishes in repository-level settings where cross-file dependencies and structural context are essential. Existing Retrieval-Augmented Generation (RAG) approaches often borrow strategies from NLP, relying on chunking-based indexing and similarity-based retrieval. Chunking results in the loss of coherence between code units and overlooks structural relationships, while similarity-driven methods frequently miss functionally relevant dependencies such as helper functions, classes, or global variables. To address these limitations, we present Hydra, a repository-level code generation framework that treats code as structured code rather than natural language. Our approach introduces (i) a structure-aware indexing strategy that represents repositories as hierarchical trees of functions, classes, and variables, preserving code structure and dependencies, (ii) a lightweight dependency-aware retriever (DAR) that explicitly identifies and retrieves the true dependencies required by a target function, and (iii) a hybrid retrieval mechanism that combines DAR with similarity-based retrieval to provide both essential building blocks and practical usage examples. Extensive experiments on the challenging DevEval and RepoExec benchmarks, both requiring function implementation from real-world repositories with complex large repository context, show that Hydra achieves state-of-the-art performance across open- and closed-source CodeLLMs. Notably, our method establishes a new state of the art in repository-level code generation, surpassing strongest baseline by over 5% in Pass@1 and even enabling smaller models to match or exceed the performance of much larger ones that rely on existing retrievers.\nüìÑ Download PDF\nA Human-Centric Framework for Data Attribution in Large Language Models Authors: Amelie W√ºhrl, Mattes Ruckdeschel, Kyle Lo, Anna Rogers Venue: arXiv (2026)\nIn the current Large Language Model (LLM) ecosystem, creators have little agency over how their data is used, and LLM users may find themselves unknowingly plagiarizing existing sources. Attribution of LLM-generated text to LLM input data could help with these challenges, but so far we have more questions than answers: what elements of LLM outputs require attribution, what goals should it serve, how should it be implemented? We contribute a human-centric data attribution framework, which situates the attribution problem within the broader data economy. Specific use cases for attribution, such as creative writing assistance or fact-checking, can be specified via a set of parameters (including stakeholder objectives and implementation criteria). These criteria are up for negotiation by the relevant stakeholder groups: creators, LLM users, and their intermediaries (publishers, platforms, AI companies). The outcome of domain-specific negotiations can be implemented and tested for whether the stakeholder goals are achieved. The proposed approach provides a bridge between methodological NLP work on data attribution, governance work on policy interventions, and economic analysis of creator incentives for a sustainable equilibrium in the data economy.\nüìÑ Download PDF\nI can tell whether you are a Native Hawl√™ri Speaker! How ANN, CNN, and RNN perform in NLI-Native Language Identification Authors: Hardi Garari, Hossein Hassani Venue: arXiv (2026)\nNative Language Identification (NLI) is a task in Natural Language Processing (NLP) that typically determines the native language of an author through their writing or a speaker through their speaking. It has various applications in different areas, such as forensic linguistics and general linguistics studies. Although considerable research has been conducted on NLI regarding two different languages, such as English and German, the literature indicates a significant gap regarding NLI for dialects and subdialects. The gap becomes wider in less-resourced languages such as Kurdish. This research focuses on NLI within the context of a subdialect of Sorani (Central) Kurdish. It aims to investigate the NLI for Hewl√™ri, a subdialect spoken in Hewl√™r (Erbil), the Capital of the Kurdistan Region of Iraq. We collected about 24 hours of speech by recording interviews with 40 native or non-native Hewl√™ri speakers, 17 female and 23 male. We created three Neural Network-based models: Artificial Neural Network (ANN), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), which were evaluated through 66 experiments, covering various time-frames from 1 to 60 seconds, undersampling, oversampling, and cross-validation. The RNN model showed the highest accuracy of 95.92% for 5-second audio segmentation, using an 80:10:10 data splitting scheme. The created dataset is the first speech dataset for NLI on the Hewl√™ri subdialect in the Sorani Kurdish dialect, which can be of benefit to various research areas.\nüìÑ Download PDF\nAutonomous Continual Learning of Computer-Use Agents for Environment Adaptation Authors: Tianci Xue, Zeyi Liao, Tianneng Shi, Zilu Wang, Kai Zhang, Dawn Song, Yu Su, Huan Sun Venue: arXiv (2026)\nReal-world digital environments are highly diverse and dynamic. These characteristics cause agents to frequently encounter unseen scenarios and distribution shifts, making continual learning in specific environments essential for computer-use agents (CUAs). However, a key challenge lies in obtaining high-quality and environment-grounded agent data without relying on costly human annotation. In this work, we introduce ACuRL, an Autonomous Curriculum Reinforcement Learning framework that continually adapts agents to specific environments with zero human data. The agent first explores target environments to acquire initial experiences. During subsequent iterative training, a curriculum task generator leverages these experiences together with feedback from the previous iteration to synthesize new tasks tailored for the agent‚Äôs current capabilities. To provide reliable reward signals, we introduce CUAJudge, a robust automatic evaluator for CUAs that achieves 93% agreement with human judgments. Empirically, our method effectively enables both intra-environment and cross-environment continual learning, yielding 4-22% performance gains without catastrophic forgetting on existing environments. Further analyses show highly sparse updates (e.g., 20% parameters), which helps explain the effective and robust adaptation. Our data and code are available at https://github.com/OSU-NLP-Group/ACuRL.\nüìÑ Download PDF\nCalliope: A TTS-based Narrated E-book Creator Ensuring Exact Synchronization, Privacy, and Layout Fidelity Authors: Hugo L. Hammer, Vajira Thambawita, P√•l Halvorsen Venue: arXiv (2026)\nA narrated e-book combines synchronized audio with digital text, highlighting the currently spoken word or sentence during playback. This format supports early literacy and assists individuals with reading challenges, while also allowing general readers to seamlessly switch between reading and listening. With the emergence of natural-sounding neural Text-to-Speech (TTS) technology, several commercial services have been developed to leverage these technology for converting standard text e-books into high-quality narrated e-books. However, no open-source solutions currently exist to perform this task. In this paper, we present Calliope, an open-source framework designed to fill this gap. Our method leverages state-of-the-art open-source TTS to convert a text e-book into a narrated e-book in the EPUB 3 Media Overlay format. The method offers several innovative steps: audio timestamps are captured directly during TTS, ensuring exact synchronization between narration and text highlighting; the publisher‚Äôs original typography, styling, and embedded media are strictly preserved; and the entire pipeline operates offline. This offline capability eliminates recurring API costs, mitigates privacy concerns, and avoids copyright compliance issues associated with cloud-based services. The framework currently supports the state-of-the-art open-source TTS systems XTTS-v2 and Chatterbox. A potential alternative approach involves first generating narration via TTS and subsequently synchronizing it with the text using forced alignment. However, while our method ensures exact synchronization, our experiments show that forced alignment introduces drift between the audio and text highlighting significant enough to degrade the reading experience. Source code and usage instructions are available at https://github.com/hugohammer/TTS-Narrated-Ebook-Creator.git.\nüìÑ Download PDF\nMotivating REML via Prediction-Error Covariances in EM Updates for Linear Mixed Models Authors: Andrew T. Karl Venue: arXiv (2026)\nWe present a computational motivation for restricted maximum likelihood (REML) estimation in linear mixed models using an expectation‚Äìmaximization (EM) algorithm. At each iteration, maximum likelihood (ML) and REML solve the same mixed-model equations for the best linear unbiased estimator (BLUE) of the fixed effects and the best linear unbiased predictor (BLUP) of the random effects. They differ only in the trace adjustments used in the variance-component updates: ML uses conditional covariances of the random effects given the data, whereas REML uses prediction-error covariances from Henderson‚Äôs C-matrix, reflecting uncertainty from estimating the fixed effects. Short R code makes this switch explicit, exposes the key matrices for classroom inspection, and reproduces lme4 ML and REML fits.\nüìÑ Download PDF\nChaos and Parrondo‚Äôs paradox: An overview Authors: Marcelo A. Pires, Erveton P. Pinto, Jose S. C√°novas, Silvio M. Duarte Queir√≥s Venue: arXiv (2026)\nParrondo‚Äôs paradox (PP) is a fundamental principle in nonlinear science where the alternation of individually losing strategies leads to a winning outcome. In this topical review, we provide the first systematic panorama of the synergy between PP and chaos. We observe a bidirectional connection between the two areas. The first direction is the translation of PP into the interplay between Order and Chaos through either Chaos + Chaos $\\to$ Order (CCO) or Order + Order $\\to$ Chaos (OOC). In this vein, many quantifiers, such as Lyapunov Exponents, $Œª$, and entropic measures, are used. Second, we note that chaos can be used to engineer switching protocols that can lead to nontrivial effects in diverse PP cases. Our review clarifies the universality of PP and highlights its robust theoretical and practical applications across several areas of science and technology. Finally, we delineate key open questions, emphasizing the unresolved theoretical limits, the role of high-dimensional maps and continuous flows, and the critical need for more experimental verification of the dynamic PP in chaotic systems. For completeness, we also provide a full Python code that allows the reader to observe the many facets of the PP.\nüìÑ Download PDF\nScaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment Authors: Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone Venue: arXiv (2026)\nThe long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the ‚Äúintention-action gap.‚Äô‚Äô We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce ‚Äúboot-time compute‚Äù and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.\nüìÑ Download PDF\nUniT: Unified Multimodal Chain-of-Thought Test-time Scaling Authors: Leon Liangyu Chen, Haoyu Ma, Zhipeng Fan, Ziqi Huang, Animesh Sinha, Xiaoliang Dai, Jialiang Wang, Zecheng He, Jianwei Yang, Chunyuan Li, Junzhe Sun, Chu Wang, Serena Yeung-Levy, Felix Juefei-Xu Venue: arXiv (2026)\nUnified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.\nüìÑ Download PDF\nAttentionRetriever: Attention Layers are Secretly Long Document Retrievers Authors: David Jiahao Fu, Lam Thanh Do, Jiayu Li, Kevin Chen-Chuan Chang Venue: arXiv (2026)\nRetrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.\nüìÑ Download PDF\nAgentic Test-Time Scaling for WebAgents Authors: Nicholas Lee, Lutfi Eren Erdogan, Chris Joseph John, Surya Krishnapillai, Michael W. Mahoney, Kurt Keutzer, Amir Gholami Venue: arXiv (2026)\nTest-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent‚Äôs own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.\nüìÑ Download PDF\nOn-Policy Context Distillation for Language Models Authors: Tianzhu Ye, Li Dong, Xun Wu, Shaohan Huang, Furu Wei Venue: arXiv (2026)\nContext distillation enables language models to internalize in-context knowledge into their parameters. In our work, we propose On-Policy Context Distillation (OPCD), a framework that bridges on-policy distillation with context distillation by training a student model on its own generated trajectories while minimizing reverse Kullback-Leibler divergence against a context-conditioned teacher. We demonstrate the effectiveness of OPCD on two important applications: experiential knowledge distillation, where models extract and consolidate transferable knowledge from their historical solution traces, and system prompt distillation, where models internalize beneficial behaviors encoded in optimized prompts. Across mathematical reasoning, text-based games, and domain-specific tasks, OPCD consistently outperforms baseline methods, achieving higher task accuracy while better preserving out-of-distribution capabilities. We further show that OPCD enables effective cross-size distillation, where smaller student models can internalize experiential knowledge from larger teachers.\nüìÑ Download PDF\nT3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization Authors: Tunyu Zhang, Xinxi Zhang, Ligong Han, Haizhou Shi, Xiaoxiao He, Zhuowei Li, Hao Wang, Kai Xu, Akash Srivastava, Hao Wang, Vladimir Pavlovic, Dimitris N. Metaxas Venue: arXiv (2026)\nDiffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model‚Äôs own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D.\nüìÑ Download PDF\nLDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion Authors: Jiangran Lyu, Kai Liu, Xuheng Zhang, Haoran Liao, Yusen Feng, Wenxuan Zhu, Tingrui Shen, Jiayi Chen, Jiazhao Zhang, Yifei Dong, Wenbo Cui, Senmao Qi, Shuo Wang, Yixin Zheng, Mi Yan, Xuesong Shi, Haoran Li, Dongbin Zhao, Ming-Yu Liu, Zhizheng Zhang, Li Yi, Yizhou Wang, He Wang Venue: arXiv (2026)\nRecent robot foundation models largely rely on large-scale behavior cloning, which imitates expert actions but discards transferable dynamics knowledge embedded in heterogeneous embodied data. While the Unified World Model (UWM) formulation has the potential to leverage such diverse data, existing instantiations struggle to scale to foundation-level due to coarse data usage and fragmented datasets. We introduce LDA-1B, a robot foundation model that scales through universal embodied data ingestion by jointly learning dynamics, policy, and visual forecasting, assigning distinct roles to data of varying quality. To support this regime at scale, we assemble and standardize EI-30k, an embodied interaction dataset comprising over 30k hours of human and robot trajectories in a unified format. Scalable dynamics learning over such heterogeneous data is enabled by prediction in a structured DINO latent space, which avoids redundant pixel-space appearance modeling. Complementing this representation, LDA-1B employs a multi-modal diffusion transformer to handle asynchronous vision and action streams, enabling stable training at the 1B-parameter scale. Experiments in simulation and the real world show LDA-1B outperforms prior methods (e.g., $œÄ_{0.5}$) by up to 21%, 48%, and 23% on contact-rich, dexterous, and long-horizon tasks, respectively. Notably, LDA-1B enables data-efficient fine-tuning, gaining 10% by leveraging 30% low-quality trajectories typically harmful and discarded.\nüìÑ Download PDF\nLearning to Forget Attention: Memory Consolidation for Adaptive Compute Reduction Authors: Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma Venue: arXiv (2026)\nHybrid architectures combining state-space models with attention have achieved strong efficiency-quality tradeoffs, yet existing approaches either apply attention uniformly or learn static sparse patterns. This misses a key opportunity: \\emph{attention demand should decrease over time as recurring patterns become familiar}. We present a surprising finding from analyzing GPT-2 models: \\textbf{88%} of attention operations retrieve information already predictable from the model‚Äôs hidden state, and this redundancy does \\emph{not} decrease during training. Motivated by this observation, we introduce \\textbf{\\ours{}} (\\textbf{C}onsolidation-based \\textbf{R}outing for \\textbf{A}daptive \\textbf{M}emory), a biologically inspired memory consolidation mechanism that gradually distills episodic retrievals into parametric semantic memory. Unlike prior sparse attention methods, \\ours{} exhibits \\emph{decreasing attention utilization} over training, achieving a \\textbf{37.8$\\times$} reduction through a sharp phase transition at approximately 3K steps. We prove that this capability is \\emph{impossible} without consolidation: any static routing scheme requires $Œ©(f \\cdot n)$ attention for tasks with recurring patterns of frequency $f$. On our proposed SRCD benchmark, \\ours{} achieves \\textbf{100% retrieval accuracy} at 1.6% attention compute (vs.\\ 68% for baselines), and consolidated patterns transfer to unseen tasks with \\textbf{48‚Äì52%} attention reduction without retraining. Remarkably, the learned consolidation dynamics quantitatively match human episodic-to-semantic memory transition curves from cognitive psychology ($Œ≥= 0.43$ vs.\\ $Œ≥_{\\text{human}} \\approx 0.4$‚Äì$0.5$). Code and benchmarks are available at [anonymized].\nüìÑ Download PDF\nContinuous and Discrete-Time Filters: A Unified Operational Perspective Authors: Luca Giangrande Venue: arXiv (2026)\nThis paper presents a unified tutorial treatment of continuous-time and discrete-time linear time-invariant systems, emphasizing their shared dynamical structure and the physical constraints that differentiate their realizations. Rather than introducing new mathematical tools, the manuscript revisits foundational concepts-transfer functions, poles and zeros, impulse responses, and stability-from an operational perspective rooted in practical signal processing and circuit implementation. First-order systems are used as a minimal yet expressive framework to illustrate how integration, differentiation, filtering, and delay manifest across the Laplace and Z domains. Particular attention is given to causality, bandwidth limitations, sampling effects, and the approximation errors inherent in discrete-time representations. The goal is to bridge the gap between formal mathematical descriptions and the intuition required for reliable system design in mixed analog-digital environments.\nüìÑ Download PDF\nPedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation Authors: Bowei He, Yankai Chen, Xiaokun Zhang, Linghe Kong, Philip S. Yu, Xue Liu, Chen Ma Venue: arXiv (2026)\nKnowledge distillation from Large Language Models (LLMs) to smaller models has emerged as a critical technique for deploying efficient AI systems. However, current methods for distillation via synthetic data lack pedagogical awareness, treating knowledge transfer as a one-off data synthesis and training task rather than a systematic learning process. In this paper, we propose a novel pedagogically-inspired framework for LLM knowledge distillation that draws from fundamental educational principles. Our approach introduces a three-stage pipeline ‚Äì Knowledge Identifier, Organizer, and Adapter (IOA) ‚Äì that systematically identifies knowledge deficiencies in student models, organizes knowledge delivery through progressive curricula, and adapts representations to match the cognitive capacity of student models. We integrate Bloom‚Äôs Mastery Learning Principles and Vygotsky‚Äôs Zone of Proximal Development to create a dynamic distillation process where student models approach teacher model‚Äôs performance on prerequisite knowledge before advancing, and new knowledge is introduced with controlled, gradual difficulty increments. Extensive experiments using LLaMA-3.1/3.2 and Qwen2.5 as student models demonstrate that IOA achieves significant improvements over baseline distillation methods, with student models retaining 94.7% of teacher performance on DollyEval while using less than 1/10th of the parameters. Our framework particularly excels in complex reasoning tasks, showing 19.2% improvement on MATH and 22.3% on HumanEval compared with state-of-the-art baselines.\nüìÑ Download PDF\nAmortized Molecular Optimization via Group Relative Policy Optimization Authors: Muhammad bin Javaid, Hasham Hussain, Ashima Khanna, Berke Kisin, Jonathan Pirnay, Alexander Mitsos, Dominik G. Grimm, Martin Grohe Venue: arXiv (2026)\nMolecular design encompasses tasks ranging from de-novo design to structural alteration of given molecules or fragments. For the latter, state-of-the-art methods predominantly function as ‚ÄúInstance Optimizers‚Äô‚Äô, expending significant compute restarting the search for every input structure. While model-based approaches theoretically offer amortized efficiency by learning a policy transferable to unseen structures, existing methods struggle to generalize. We identify a key failure mode: the high variance arising from the heterogeneous difficulty of distinct starting structures. To address this, we introduce GRXForm, adapting a pre-trained Graph Transformer model that optimizes molecules via sequential atom-and-bond additions. We employ Group Relative Policy Optimization (GRPO) for goal-directed fine-tuning to mitigate variance by normalizing rewards relative to the starting structure. Empirically, GRXForm generalizes to out-of-distribution molecular scaffolds without inference-time oracle calls or refinement, achieving scores in multi-objective optimization competitive with leading instance optimizers.\nüìÑ Download PDF\nThe Wandering Supermassive Black Hole Powering the off-nuclear TDE AT2024tvd Authors: M. Guolo, A. Mummery, S. van Velzen, M. Nicholl, S. Gezari, Y. Yao, K. C. Chambers, T. de Boer, M. E. Huber, C. -C. Lin, T. B. Lowe, E. A. Magnier, G. Paek, R. Wainscoat Venue: arXiv (2026)\nWe present an analysis of the spectral energy distribution (SED) of the off-nuclear tidal disruption event (TDE) AT2024tvd during its late-time plateau phase, combining X-ray spectra and UV/optical photometry. Using a fully relativistic, compact accretion disk model with self-consistent inner-disk Comptonization, we reproduce the observed SED without significant residuals. The inferred black hole mass ${\\rm log}{10}(M{\\bullet}/M_\\odot) \\approx 6.0 \\pm 0.2$, and the inferred disk parameters place AT2024tvd within known TDE-disk scaling relations ($L_{\\rm bol}^{\\rm disk}/L_{\\rm Edd} \\propto T_{\\rm p}^4 \\propto M_{\\bullet}^{-1}$, $L_{\\rm plat} \\propto M_{\\bullet}^{2/3}$, $R_{\\rm out}/r_{\\rm g} \\propto M_{\\bullet}^{-2/3}$). Our results show that: (i) there is no \\textit{detected} star cluster or dwarf galaxy associated with the source, down to a mass limit of $\\log_{10}(M_{\\rm gal}/M_{\\odot}) \\leq 7.6$; (ii) the black hole is a wandering supermassive, rather than intermediate-mass, black hole; and (iii) the source represents an extreme case of black hole-to-host mass ratio, with $M_{\\bullet}/M_{\\rm gal} \u003e 3%$, consistent with a heavily tidally stripped nucleus. The latter aligns with cosmological simulations predicting that surviving host remnants of most wandering black holes should not retain a detectable stellar overdensity when located at small halo-centric distances. We discuss differences with previous analyses of this source and highlight why our modeling approach provides a more physically consistent solution with more reliable parameter inference.\nüìÑ Download PDF\nMonarchRT: Efficient Attention for Real-Time Video Generation Authors: Krish Agarwal, Zhuoming Chen, Cheng Luo, Yongqi Chen, Haizhong Zheng, Xun Huang, Atri Rudra, Beidi Chen Venue: arXiv (2026)\nReal-time video generation with Diffusion Transformers is bottlenecked by the quadratic cost of 3D self-attention, especially in real-time regimes that are both few-step and autoregressive, where errors compound across time and each denoising step must carry substantially more information. In this setting, we find that prior sparse-attention approximations break down, despite showing strong results for bidirectional, many-step diffusion. Specifically, we observe that video attention is not reliably sparse, but instead combines pronounced periodic structure driven by spatiotemporal position with dynamic, sparse semantic correspondences and dense mixing, exceeding the representational capacity of even oracle top-k attention. Building on this insight, we propose Monarch-RT, a structured attention parameterization for video diffusion models that factorizes attention using Monarch matrices. Through appropriately aligned block structure and our extended tiled Monarch parameterization, we achieve high expressivity while preserving computational efficiency. We further overcome the overhead of parameterization through finetuning, with custom Triton kernels. We first validate the high efficacy of Monarch-RT over existing sparse baselines designed only for bidirectional models. We further observe that Monarch-RT attains up to 95% attention sparsity with no loss in quality when applied to the state-of-the-art model Self-Forcing, making Monarch-RT a pioneering work on highly-capable sparse attention parameterization for real-time video generation. Our optimized implementation outperforms FlashAttention-2, FlashAttention-3, and FlashAttention-4 kernels on Nvidia RTX 5090, H100, and B200 GPUs respectively, providing kernel speedups in the range of 1.4-11.8X. This enables us, for the first time, to achieve true real-time video generation with Self-Forcing at 16 FPS on a single RTX 5090.\nüìÑ Download PDF\nExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction Authors: Nick Ferguson, Josh Pennington, Narek Beghian, Aravind Mohan, Douwe Kiela, Sheshansh Agrawal, Thien Hang Nguyen Venue: arXiv (2026)\nUnstructured documents like PDFs contain valuable structured information, but downstream systems require this data in reliable, standardized formats. LLMs are increasingly deployed to automate this extraction, making accuracy and reliability paramount. However, progress is bottlenecked by two gaps. First, no end-to-end benchmark evaluates PDF-to-JSON extraction under enterprise-scale schema breadth. Second, no principled methodology captures the semantics of nested extraction, where fields demand different notions of correctness (exact match for identifiers, tolerance for quantities, semantic equivalence for names), arrays require alignment, and omission must be distinguished from hallucination. We address both gaps with ExtractBench, an open-source benchmark and evaluation framework for PDF-to-JSON structured extraction. The benchmark pairs 35 PDF documents with JSON Schemas and human-annotated gold labels across economically valuable domains, yielding 12,867 evaluatable fields spanning schema complexities from tens to hundreds of fields. The evaluation framework treats the schema as an executable specification: each field declares its scoring metric. Baseline evaluations reveal that frontier models (GPT-5/5.2, Gemini-3 Flash/Pro, Claude 4.5 Opus/Sonnet) remain unreliable on realistic schemas. Performance degrades sharply with schema breadth, culminating in 0% valid output on a 369-field financial reporting schema across all tested models. We release ExtractBench at https://github.com/ContextualAI/extract-bench.\nüìÑ Download PDF\nStatus of the $S_8$ Tension: A 2026 Review of Probe Discrepancies Authors: Ioannis Pantos, Leandros Perivolaropoulos Venue: arXiv (2026)\nThe parameter $S_8 \\equiv œÉ_8 (Œ©_m/0.3)^{0.5}$ quantifies the amplitude of matter density fluctuations. A persistent discrepancy exists between early-universe CMB observations and late-universe probes. This review assesses the $S_8$ tension'' against a new 2026 baseline: a unified Combined CMB‚Äô‚Äô framework incorporating Planck, ACT DR6, and SPT-3G. This combined analysis yields $S_8 = 0.836^{+0.012}_{-0.013}$, providing a higher central value and reduced uncertainties compared to Planck alone. Compiling measurements from 2019‚Äì2026, we reveal a striking bifurcation: DES Year 6 results exhibit a statistically significant tension of $2.4œÉ$‚Äì$2.7œÉ$ \\citep{DESY6}, whereas KiDS Legacy results demonstrate statistical consistency at $\u003c1œÉ$ \\citep{Wright2025}. We examine systematic origins of this dichotomy, including photometric redshift calibration, intrinsic alignment modeling, and shear measurement pipelines. We further contextualize these findings with cluster counts (where eROSITA favors high values while SPT favors low), galaxy-galaxy lensing, and redshift-space distortions. The heterogeneous landscape suggests survey-specific systematic effects contribute substantially to observed discrepancies, though new physics beyond $Œõ$CDM cannot be excluded.\nüìÑ Download PDF\nAutomated Test Suite Enhancement Using Large Language Models with Few-shot Prompting Authors: Alex Chudic, G√ºl √áalƒ±klƒ± Venue: arXiv (2026)\nUnit testing is essential for verifying the functional correctness of code modules (e.g., classes, methods), but manually writing unit tests is often labor-intensive and time-consuming. Unit tests generated by tools that employ traditional approaches, such as search-based software testing (SBST), lack readability, naturalness, and practical usability. LLMs have recently provided promising results and become integral to developers‚Äô daily practices. Consequently, software repositories now include a mix of human-written tests, LLM-generated tests, and those from tools employing traditional approaches such as SBST. While LLMs‚Äô zero-shot capabilities have been widely studied, their few-shot learning potential for unit test generation remains underexplored. Few-shot prompting enables LLMs to learn from examples in the prompt, and automatically retrieving such examples could enhance test suites. This paper empirically investigates how few-shot prompting with different test artifact sources, comprising human, SBST, or LLM, affects the quality of LLM-generated unit tests as program comprehension artifacts and their contribution to improving existing test suites by evaluating not only correctness and coverage but also readability, cognitive complexity, and maintainability in hybrid human-AI codebases. We conducted experiments on HumanEval and ClassEval datasets using GPT-4o, which is integrated into GitHub Copilot and widely used among developers. We also assessed retrieval-based methods for selecting relevant examples. Our results show that LLMs can generate high-quality tests via few-shot prompting, with human-written examples producing the best coverage and correctness. Additionally, selecting examples based on the combined similarity of problem description and code consistently yields the most effective few-shot prompts.\nüìÑ Download PDF\nVision Transformer for Multi-Domain Phase Retrieval in Coherent Diffraction Imaging Authors: Jialun Liu, David Yang, Ian Robinson Venue: arXiv (2026)\nBragg coherent diffraction imaging (BCDI) phase retrieval becomes rapidly difficult in the strong-phase regime, where a crystal contains distortions beyond half a lattice spacing. An important special case is the phase domain problem, where blocks of a crystal are displaced with sharp jumps at domain walls. The strong-phase, here defined as beyond $\\pm œÄ/2$, generates split Bragg peaks and dense fringe structure for which classical iterative solvers often stagnate or return different solutions from different initialisations. Here, we introduce an unsupervised Fourier Vision Transformer (Fourier ViT) to solve this block-phase, multi-domain phase-retrieval problem directly from measured 2D Bragg diffraction intensities. Fourier ViT couples reciprocal-space information globally through multiscale Fourier token mixing, while shallow convolutional front and back-ends provide local filtering and reconstruction. We validate the approach on large-scale synthetic datasets of Voronoi multi-domain crystals with strong-phase contrast under realistic noise corruptions, and on experimental diffraction from a $\\mathrm{La}_{2-x}\\mathrm{Ca}_x\\mathrm{MnO}_4$ nanocrystal. Across the regimes considered, Fourier ViT achieves the lowest reciprocal-space mismatch ($œá^2$) among the compared methods and preserves domain-resolved phase reconstructions for increasing numbers of domains. On experimental data, with the same real-space support, Fourier ViT matches the iterative benchmark $œá^2$ while improving robustness to random initialisations, yielding a higher success rate of low-$œá^2$ reconstructions than the complex convolutional neural network baseline.\nüìÑ Download PDF\nDetecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation Authors: Julia Belikova, Danila Rozhevskii, Dennis Svirin, Konstantin Polev, Alexander Panchenko Venue: arXiv (2026)\nEfficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens. Yet, the limits of compressibility ‚Äì and when compression begins to erase task-relevant content ‚Äì remain underexplored. In this paper, we define \\emph{token overflow} as a regime in which compressed representations no longer contain sufficient information to answer a given query, and propose a methodology to characterize and detect it. In the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing a practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA, SQuADv2, and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling low-cost pre-LLM gating to mitigate compression-induced errors.\nüìÑ Download PDF\nQuery-focused and Memory-aware Reranker for Long Context Processing Authors: Yuqing Li, Jiangnan Li, Mo Yu, Guoxuan Ding, Zheng Lin, Weiping Wang, Jie Zhou Venue: arXiv (2026)\nBuilt upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance.\nüìÑ Download PDF\nüîç linguistics Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching Authors: Huai-Hsun Cheng, Siang-Ling Zhang, Yu-Lun Liu Venue: arXiv (2026)\nVisual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the ‚Äúdual-constraint‚Äù: initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a ‚Äúcommon structural subspace‚Äù valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/\nüìÑ Download PDF\nFunction-Space Decoupled Diffusion for Forward and Inverse Modeling in Carbon Capture and Storage Authors: Xin Ju, Jiachen Yao, Anima Anandkumar, Sally M. Benson, Gege Wen Venue: arXiv (2026)\nAccurate characterization of subsurface flow is critical for Carbon Capture and Storage (CCS) but remains challenged by the ill-posed nature of inverse problems with sparse observations. We present Fun-DDPS, a generative framework that combines function-space diffusion models with differentiable neural operator surrogates for both forward and inverse modeling. Our approach learns a prior distribution over geological parameters (geomodel) using a single-channel diffusion model, then leverages a Local Neural Operator (LNO) surrogate to provide physics-consistent guidance for cross-field conditioning on the dynamics field. This decoupling allows the diffusion prior to robustly recover missing information in parameter space, while the surrogate provides efficient gradient-based guidance for data assimilation. We demonstrate Fun-DDPS on synthetic CCS modeling datasets, achieving two key results: (1) For forward modeling with only 25% observations, Fun-DDPS achieves 7.7% relative error compared to 86.9% for standard surrogates (an 11x improvement), proving its capability to handle extreme data sparsity where deterministic methods fail. (2) We provide the first rigorous validation of diffusion-based inverse solvers against asymptotically exact Rejection Sampling (RS) posteriors. Both Fun-DDPS and the joint-state baseline (Fun-DPS) achieve Jensen-Shannon divergence less than 0.06 against the ground truth. Crucially, Fun-DDPS produces physically consistent realizations free from the high-frequency artifacts observed in joint-state baselines, achieving this with 4x improved sample efficiency compared to rejection sampling.\nüìÑ Download PDF\nCreative Ownership in the Age of AI Authors: Annie Liang, Jay Lu Venue: arXiv (2026)\nCopyright law focuses on whether a new work is ‚Äúsubstantially similar‚Äù to an existing one, but generative AI can closely imitate style without copying content, a capability now central to ongoing litigation. We argue that existing definitions of infringement are ill-suited to this setting and propose a new criterion: a generative AI output infringes on an existing work if it could not have been generated without that work in its training corpus. To operationalize this definition, we model generative systems as closure operators mapping a corpus of existing works to an output of new works. AI generated outputs are \\emph{permissible} if they do not infringe on any existing work according to our criterion. Our results characterize structural properties of permissible generation and reveal a sharp asymptotic dichotomy: when the process of organic creations is light-tailed, dependence on individual works eventually vanishes, so that regulation imposes no limits on AI generation; with heavy-tailed creations, regulation can be persistently constraining.\nüìÑ Download PDF\nCM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use Authors: Zhen Zhang, Kaiqiang Song, Xun Wang, Yebowen Hu, Weixiang Yan, Chenyang Zhao, Henry Peng Zou, Haoyun Deng, Sathish Reddy Indurthi, Shujian Liu, Simin Ma, Xiaoyang Wang, Xin Eric Wang, Song Wang Venue: arXiv (2026)\nAI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2, an RL framework that replaces verifiable outcome rewards with checklist rewards. CM2 decomposes each turn‚Äôs intended behavior into fine-grained binary criteria with explicit evidence grounding and structured metadata, turning open-ended judging into more stable classification-style decisions. To balance stability and informativeness, our method adopts a strategy of sparse reward assignment but dense evaluation criteria. Training is performed in a scalable LLM-simulated tool environment, avoiding heavy engineering for large tool sets. Experiments show that CM2 consistently improves over supervised fine-tuning. Starting from an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12 points on ToolSandbox. The results match or even outperform similarly sized open-source baselines, including the judging model. CM2 thus provides a scalable recipe for optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards. Code provided by the open-source community: https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.\nüìÑ Download PDF\nTransmit or Idle: Efficient AoI Optimal Transmission Policy for Gossiping Receivers Authors: Irtiza Hasan, Ahmed Arafa Venue: arXiv (2026)\nWe study the optimal transmission and scheduling policy for a transmitter (source) communicating with two gossiping receivers aiming at tracking the source‚Äôs status over time using the age of information (AoI) metric. Gossiping enables local information exchange in a decentralized manner without relying solely on the transmitter‚Äôs direct communication, which we assume incurs a transmission cost. On the other hand, gossiping may be communicating stale information, necessitating the transmitter‚Äôs intervention. With communication links having specific success probabilities, we formulate an average-cost Markov Decision Process (MDP) to jointly minimize the sum AoI and transmission cost for such a system in a time-slotted setting. We employ the Relative Value Iteration (RVI) algorithm to evaluate the optimal policy for the transmitter and then prove several structural properties showing that it has an age-difference threshold structure with minimum age activation in the case where gossiping is relatively more reliable. Specifically, direct transmission is optimal only if the minimum AoI of the receivers is large enough and their age difference is below a certain threshold. Otherwise, the transmitter idles to effectively take advantage of gossiping and reduce direct transmission costs. Numerical evaluations demonstrate the significance of our optimal policy compared to multiple baselines. Our result is a first step towards characterizing optimal freshness and transmission cost trade-offs in gossiping networks.\nüìÑ Download PDF\nThink like a Scientist: Physics-guided LLM Agent for Equation Discovery Authors: Jianke Yang, Ohm Venkatachalam, Mohammad Kianezhad, Sharvaree Vadgama, Rose Yu Venue: arXiv (2026)\nExplaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.\nüìÑ Download PDF\nOxygen left behind: Atmospheric Enrichment due to Fractionation in Sub-Neptunes using BOREAS Authors: Marilina Valatsou, Caroline Dorn, Pierlou Marty, James E. Owen Venue: arXiv (2026)\nThe evolution of exoplanetary atmospheres is strongly influenced by atmospheric escape, particularly for close-in planets. Fractionation during atmospheric loss can preferentially remove lighter elements such as hydrogen, while retaining heavier species like oxygen. In this study, we investigate how and under what conditions hydrodynamic escape and chemical fractionation jointly shape the mass and composition of exoplanet atmospheres, especially for mixed H2 + H2O atmospheres. We develop BOREAS, a self-consistent mass loss model coupling a 1D Parker wind formulation with a mass-dependent fractionation scheme, which we apply across a range of planet masses, radii, equilibrium temperatures, and incident XUV fluxes, allowing us to track hydrogen and oxygen escape rates at different snapshots in time. We find that oxygen is efficiently retained over most of the parameter space. Significant oxygen loss occurs under high incident XUV fluxes, while at intermediate fluxes oxygen loss is largely confined to low-gravity planets. Where oxygen is retained, irradiation is too weak to drive significant escape of hydrogen and thus limiting atmospheric enrichment. By contrast, our model predicts that sub-Neptunes undergo substantial atmospheric enrichment over approx. 200 Myr when hydrogen escape is efficient and accompanied by partial oxygen entrainment. Notably, our results imply that sub-Neptunes near the radius valley can evolve into water-rich planets, in agreement with GJ 9827 d. Present-day water-rich atmospheres may have originated from water-poor envelopes under some conditions, highlighting the need to include chemical fractionation in evolution models. BOREAS is publicly available.\nüìÑ Download PDF\nA Rule-based Computational Model for Gaidhlig Morphology Authors: Peter J Barclay Venue: arXiv (2026)\nLanguage models and software tools are essential to support the continuing vitality of lesser-used languages; however, currently popular neural models require considerable data for training, which normally is not available for such low-resource languages. This paper describes work-in-progress to construct a rule-based model of Gaidhlig morphology using data from Wiktionary, arguing that rule-based systems effectively leverage limited sample data, support greater interpretability, and provide insights useful in the design of teaching materials. The use of SQL for querying the occurrence of different lexical patterns is investigated, and a declarative rule-base is presented that allows Python utilities to derive inflected forms of Gaidhlig words. This functionality could be used to support educational tools that teach or explain language patterns, for example, or to support higher level tools such as rule-based dependency parsers. This approach adds value to the data already present in Wiktionary by adapting it to new use-cases.\nüìÑ Download PDF\nMeta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Meta-Learning Authors: Xubin Wang, Weijia Jia Venue: arXiv (2026)\nDemonstration selection is a practical bottleneck in in-context learning (ICL): under a tight prompt budget, accuracy can change substantially depending on which few-shot examples are included, yet selection must remain cheap enough to run per query over large candidate pools. We propose Meta-Sel, a lightweight supervised meta-learning approach for intent classification that learns a fast, interpretable scoring function for (candidate, query) pairs from labeled training data. Meta-Sel constructs a meta-dataset by sampling pairs from the training split and using class agreement as supervision, then trains a calibrated logistic regressor on two inexpensive meta-features: TF‚ÄìIDF cosine similarity and a length-compatibility ratio. At inference time, the selector performs a single vectorized scoring pass over the full candidate pool and returns the top-k demonstrations, requiring no model fine-tuning, no online exploration, and no additional LLM calls. This yields deterministic rankings and makes the selection mechanism straightforward to audit via interpretable feature weights. Beyond proposing Meta-Sel, we provide a broad empirical study of demonstration selection, benchmarking 12 methods ‚Äì spanning prompt engineering baselines, heuristic selection, reinforcement learning, and influence-based approaches ‚Äì across four intent datasets and five open-source LLMs. Across this benchmark, Meta-Sel consistently ranks among the top-performing methods, is particularly effective for smaller models where selection quality can partially compensate for limited model capacity, and maintains competitive selection-time overhead.\nüìÑ Download PDF\nTwo-point functions in boundary loop models Authors: Max Downing, Jesper Lykke Jacobsen, Rongvoram Nivesvivat, Hubert Saleur Venue: arXiv (2026)\nUsing techniques of conformal bootstrap, we propose analytical expressions for a large class of two-point functions of bulk fields in critical loop models defined on the upper-half plane. Our results include the two-point connectivities in the Fortuin‚ÄìKasteleyn random cluster model with both free and wired boundary conditions. We link the continuum expressions to lattice quantities by computing universal ratios of amplitudes for the two-point connectivities, and find excellent agreement with transfer-matrix numerics.\nüìÑ Download PDF\nMomentum LMS Theory beyond Stationarity: Stability, Tracking, and Regret Authors: Yifei Jin, Xin Zheng, Lei Guo Venue: arXiv (2026)\nIn large-scale data processing scenarios, data often arrive in sequential streams generated by complex systems that exhibit drifting distributions and time-varying system parameters. This nonstationarity challenges theoretical analysis, as it violates classical assumptions of i.i.d. (independent and identically distributed) samples, necessitating algorithms capable of real-time updates without expensive retraining. An effective approach should process each sample in a single pass, while maintaining computational and memory complexities independent of the data stream length. Motivated by these challenges, this paper investigates the Momentum Least Mean Squares (MLMS) algorithm as an adaptive identification tool, leveraging its computational simplicity and online processing capabilities. Theoretically, we derive tracking performance and regret bounds for the MLMS in time-varying stochastic linear systems under various practical conditions. Unlike classical LMS, whose stability can be characterized by first-order random vector difference equations, MLMS introduces an additional dynamical state due to momentum, leading to second-order time-varying random vector difference equations whose stability analysis hinges on more complicated products of random matrices, which poses a substantially challenging problem to resolve. Experiments on synthetic and real-world data streams demonstrate that MLMS achieves rapid adaptation and robust tracking, in agreement with our theoretical results especially in nonstationary settings, highlighting its promise for modern streaming and online learning applications.\nüìÑ Download PDF\nTuning Optical Properties of FTO via Carbonaceous Al2O3 Microdot Deposition by DC plasma sputtering Authors: Sarah Salah, Ahmed Atlam, Nagat Elkahwagy, Abdelhamid Elshaer, Mohammed Shihab Venue: arXiv (2026)\nFluorine-doped tin oxide (FTO) is a key transparent conductive oxide for photovoltaic and optoelectronic devices, yet its high reflectance limits light-trapping efficiency. This work demonstrates a simple DC plasma sputtering approach to deposit carbonaceous Al2O3 microdots on FTO under controlled Ar, O2, and Ar-O2 atmospheres. For plasma discharge in the normal mode, with plasma density 10^-9 cm^-3 and temperature of 2 eV, Volmer-Weber growth produced discrete microdots whose size and distribution were tuned by gas composition: dense, uniform dots in Ar (approximately 0.89 um radius), agglomerated structures in O2, and intermediate morphologies in mixed atmospheres. Structural analysis confirmed gamma-Al2O3 formation with carbon incorporation, while SEM revealed morphology-driven optical behavior. UV-Vis measurements showed that Ar-O2 coatings achieved the lowest reflectance across the visible range, outperforming bare FTO and other conditions. These findings establish a clear link between sputtering parameters, surface morphology, and optical performance, offering a scalable route to anti-reflective, light-trapping coatings for next-generation solar cells and optoelectronic devices.\nüìÑ Download PDF\nLearning to Control: The iUzawa-Net for Nonsmooth Optimal Control of Linear PDEs Authors: Yongcun Song, Xiaoming Yuan, Hangrui Yue, Tianyou Zeng Venue: arXiv (2026)\nWe propose an optimization-informed deep neural network approach, named iUzawa-Net, aiming for the first solver that enables real-time solutions for a class of nonsmooth optimal control problems of linear partial differential equations (PDEs). The iUzawa-Net unrolls an inexact Uzawa method for saddle point problems, replacing classical preconditioners and PDE solvers with specifically designed learnable neural networks. We prove universal approximation properties and establish the asymptotic $\\varepsilon$-optimality for the iUzawa-Net, and validate its promising numerical efficiency through nonsmooth elliptic and parabolic optimal control problems. Our techniques offer a versatile framework for designing and analyzing various optimization-informed deep learning approaches to optimal control and other PDE-constrained optimization problems. The proposed learning-to-control approach synergizes model-based optimization algorithms and data-driven deep learning techniques, inheriting the merits of both methodologies.\nüìÑ Download PDF\nKagome edge states under lattice termination, spin-orbit coupling, and magnetic order Authors: Sajid Sekh, Annica M. Black-Schaffer, Andrzej Ptok Venue: arXiv (2026)\nWe study the edge state properties of a two-dimensional kagome lattice using a tight-binding approach, focusing on the role of lattice termination, spin-orbit coupling, and magnetic order. In the pristine limit, we show that the existence of localized edge states is highly sensitive to boundary geometry, with certain terminations completely suppressing edge modes. Kane-Mele spin-orbit coupling opens a bulk gap and stabilizes topologically protected helical edge states, yielding a robust $\\mathbb{Z}_2$ insulating phase that is insensitive to termination details. In contrast, the combined effect of a Zeeman field and Rashba spin-orbit coupling drives the system into Chern insulating phases, with Chern numbers consistent with the number of chiral edge modes. We further demonstrate that non-coplanar magnetic textures generate multiple Chern phases through finite scalar spin chirality, with Kane-Mele coupling strongly tuning the topological gaps. Our results provide important insights into the tunability of edge states in the kagome lattice, which can be key to designing materials with novel electronic properties and topological phases.\nüìÑ Download PDF\nQuantum-Coherent Thermodynamics: Leaf Typicality via Minimum-Variance Foliation Authors: Maurizio Fagotti Venue: arXiv (2026)\nEquilibrium statistical ensembles commute with the Hamiltonian and thus carry no coherence in the energy eigenbasis. We develop a thermodynamic framework in which energy fluctuations can retain genuinely quantum-coherent contributions. We foliate state space into ‚Äúminimum-variance leaves,‚Äù defined by minimizing the average energy variance over all pure-state decompositions, with the minimum set by the quantum Fisher information. On each leaf we construct the least-biased state compatible with normalization and mean energy, defining a leaf-canonical ensemble. The Gibbs ensemble is recovered on the distinguished commuting leaf, while generic states are organized by their leaf label. This structure provides a natural setting to extend eigenstate thermalization beyond equilibrium via a ‚Äúleaf typicality‚Äù hypothesis. According to that hypothesis, under unitary time evolution local observables depend only on the leaf and energy and, at all times, are reproduced by evolving a representative (pure) state drawn from the optimal ensemble.\nüìÑ Download PDF\nBenchmarking Vision-Language Models for French PDF-to-Markdown Conversion Authors: Bruno Rigal, Victor Dupriez, Alexis Mignon, Ronan Le Hy, Nicolas Mery Venue: arXiv (2026)\nThis report evaluates PDF-to-Markdown conversion using recent Vision-Language Models (VLMs) on challenging French documents. Document parsing is a critical step for Retrieval-Augmented Generation (RAG) pipelines, where transcription and layout errors propagate to downstream retrieval and grounding. Existing benchmarks often emphasize English or Chinese and can over-penalize benign formatting and linearization choices (e.g., line breaks, list segmentation, alternative table renderings) that are largely irrelevant for downstream use. We introduce a French-focused benchmark of difficult pages selected via model-disagreement sampling from a corpus of 60{,}000 documents, covering handwritten forms, complex layouts, dense tables, and graphics-rich pages. Evaluation is performed with unit-test-style checks that target concrete failure modes (text presence, reading order, and local table constraints) combined with category-specific normalization designed to discount presentation-only variance. Across 15 models, we observe substantially higher robustness for the strongest proprietary models on handwriting and forms, while several open-weights systems remain competitive on standard printed layouts.\nüìÑ Download PDF\nThinking with Drafting: Optical Decompression via Logical Reconstruction Authors: Jingxuan Wei, Honghao He, Caijun Jia, Siyuan Li, Zheng Sun, Yuhang Xu, Yuanyuan Lin, Linzhuang Sun, Yuchen Wu, Bihui Yu, Xiangxiang Zhang, Cheng Tan Venue: arXiv (2026)\nExisting multimodal large language models have achieved high-fidelity visual perception and exploratory visual generation. However, a precision paradox persists in complex reasoning tasks: optical perception systems transcribe symbols without capturing logical topology, while pixel-based generative models produce visual artifacts lacking mathematical exactness. To bridge this gap, we propose that reasoning over visual inputs be reconceptualized as optical decompression-the process of reconstructing latent logical structures from compressed visual tokens. Guided by the axiom that Parsing is Reasoning, we introduce Thinking with Drafting (TwD), which utilizes a minimalist Domain-Specific Language (DSL) as a grounding intermediate representation. Unlike standard approaches that hallucinate answers directly, TwD forces the model to draft its mental model into executable code, rendering deterministic visual proofs for self-verification. To validate this, we present VisAlg, a visual algebra benchmark. Experiments demonstrate that TwD serve as a superior cognitive scaffold. Our work establishes a closed-loop system where visual generation acts not as a creative output but as a logical verifier, offering a generalizable path for visual reasoning.\nüìÑ Download PDF\nDo MLLMs Really Understand Space? A Mathematical Reasoning Evaluation Authors: Shuo Lu, Jianjie Cheng, Yinuo Xu, Yongcan Yu, Lijun Sheng, Peijie Wang, Siru Jiang, Yongguan Hu, Run Ling, Yihua Shao, Ao Ma, Wei Feng, Lingxiao He, Meng Wang, Qianlong Xie, Xingxing Wang, Ran He, Jian Liang Venue: arXiv (2026)\nMultimodal large language models (MLLMs) have achieved strong performance on perception-oriented tasks, yet their ability to perform mathematical spatial reasoning, defined as the capacity to parse and manipulate two- and three-dimensional relations, remains unclear. Humans easily solve textbook-style spatial reasoning problems with over 95% accuracy, but we find that most leading MLLMs fail to reach even 60% on the same tasks. This striking gap highlights spatial reasoning as a fundamental weakness of current models. To investigate this gap, we present MathSpatial, a unified framework for evaluating and improving spatial reasoning in MLLMs. MathSpatial includes three complementary components: (i) MathSpatial-Bench, a benchmark of 2K problems across three categories and eleven subtypes, designed to isolate reasoning difficulty from perceptual noise; (ii) MathSpatial-Corpus, a training dataset of 8K additional problems with verified solutions; and (iii) MathSpatial-SRT, which models reasoning as structured traces composed of three atomic operations‚ÄìCorrelate, Constrain, and Infer. Experiments show that fine-tuning Qwen2.5-VL-7B on MathSpatial achieves competitive accuracy while reducing tokens by 25%. MathSpatial provides the first large-scale resource that disentangles perception from reasoning, enabling precise measurement and comprehensive understanding of mathematical spatial reasoning in MLLMs.\nüìÑ Download PDF\nWhen Visibility Outpaces Verification: Delayed Verification and Narrative Lock-in in Agentic AI Discourse Authors: Hanjing Shi, Dominic DiFranzo Venue: arXiv (2026)\nAgentic AI systems-autonomous entities capable of independent planning and execution-reshape the landscape of human-AI trust. Long before direct system exposure, user expectations are mediated through high-stakes public discourse on social platforms. However, platform-mediated engagement signals (e.g., upvotes) may inadvertently function as a credibility proxy,'' potentially stifling critical evaluation. This paper investigates the interplay between social proof and verification timing in online discussions of agentic AI. Analyzing a longitudinal dataset from two distinct Reddit communities with contrasting interaction cultures-r/OpenClaw and r/Moltbook-we operationalize verification cues via reproducible lexical rules and model the time-to-first-verification‚Äô‚Äô using a right-censored survival analysis framework. Our findings reveal a systemic Popularity Paradox'': high-visibility discussions in both subreddits experience significantly delayed or entirely absent verification cues compared to low-visibility threads. This temporal lag creates a critical window for Narrative Lock-in,‚Äô‚Äô where early, unverified claims crystallize into collective cognitive biases before evidence-seeking behaviors emerge. We discuss the implications of this credibility-by-visibility'' effect for AI safety and propose epistemic friction‚Äô‚Äô as a design intervention to rebalance engagement-driven platforms.\nüìÑ Download PDF\nBizarre Love Triangle: Generative AI, Art, and Kitsch Authors: Dejan Grba Venue: arXiv (2026)\nGenerative artificial intelligence (GenAI) has engrossed the mainstream culture, expanded AI‚Äôs creative user base, and catalyzed economic, legal, and aesthetic issues that stir a lively public debate. Unsurprisingly, GenAI tools proliferate kitsch in the hands of amateurs and hobbyists, but various shortcomings also induce kitsch into a more ambitious, professional artists‚Äô production with GenAI. I explore them in this paper. Following the introductory outline of digital kitsch and AI art, I review GenAI artworks that manifest five interrelated types of kitsch-engendering expressive flaws: the superficial foregrounding or faulty circumvention of generative models‚Äô formal signatures, the feeble critique of AI, the mimetics, and the unacknowledged poetic similarities, all marked by an overreliance on AI as a cultural signifier. I discuss the normalization of these blunders through GenAI art‚Äôs good standing within the art world and keen relationship with the AI industry, which contributes to the adulteration of AI discourse and the possible corruption of artistic literacy. In conclusion, I emphasize that recognizing different facets of artists‚Äô uncritical embrace of techno-cultural trends, comprehending their functions, and anticipating their unintended effects is crucial for reaching relevance and responsibility in AI art.\nüìÑ Download PDF\nRepulsive Gravitational Force as a Witness of the Quantum Nature of Gravity Authors: Pablo L. Saldanha, Chiara Marletto, Vlatko Vedral Venue: arXiv (2026)\nWe show that a single spatially superposed ‚Äòsource‚Äô mass acting on a ‚Äòprobe‚Äô matter wavepacket can reveal the quantum nature of the gravitational field. For this we use a specific state preparation and measurement of the superposed source mass, including a postselection, which altogether results in a repulsive gravitational force on the probe particle. A classical gravitational field can never lead to repulsion, as the effect requires quantum interference of two distinct states of gravity. We also present a calculation in the Heisenberg picture under the formalism of weak values that illustrates how repulsion is achieved. Finally, we estimate the range of parameters (masses and the spatio-temporal extent of interference) for which the experiment is feasible.\nüìÑ Download PDF\nPost-measurement states are (very) useful for measurement discrimination Authors: Charbel Eid, Marco T√∫lio Quintino Venue: arXiv (2026)\nThe standard approach to quantum measurement discrimination is to perform the given unknown measurement on a probe state, possibly entangled with an auxiliary system, and make a decision based on the measurement outcome obtained. In this work, we go beyond the standard aforementioned scenarios by consider not only the classical measurement outcome of a measurement, but also its the post-measurement quantum state. More specifically, instead of considering only the positive-operator valued measure (POVM) operators, we consider their associated L√ºders‚Äô instrument associated with them. We prove that, when the post-measurement quantum states are available, the task of discriminating two qubit projective measurements is equivalent to discriminating two copies of quantum states associated to each projector pair, extending previous results known for the case where probe states are separable. Then, we proceed by showing that the advantage of considering post-measurement states in measurement discrimination can be large. We formalise this claim by presenting a family of pairs of measurements where the ratio between the discrimination bias of the measurement discrimination task with and without post-measurement states can be arbitrarily large. This shows that, while the post-measurement state was neglected in most of the previous literature, its use can significantly improve the performance of quantum measurement discrimination.\nüìÑ Download PDF\n‚ÄúSorry, I Didn‚Äôt Catch That‚Äù: How Speech Models Miss What Matters Most Authors: Kaitlyn Zhou, Martijn Bartelds, Federico Bianchi, James Zou Venue: arXiv (2026)\nDespite speech recognition systems achieving low word error rates on standard benchmarks, they often fail on short, high-stakes utterances in real-world deployments. Here, we study this failure mode in a high-stakes task: the transcription of U.S. street names as spoken by U.S. participants. We evaluate 15 models from OpenAI, Deepgram, Google, and Microsoft on recordings from linguistically diverse U.S. speakers and find an average transcription error rate of 44%. We quantify the downstream impact of failed transcriptions by geographic locations and show that mis-transcriptions systematically cause errors for all speakers, but that routing distance errors are twice as large for non-English primary speakers compared to English primary speakers. To mitigate this harm, we introduce a synthetic data generation approach that produces diverse pronunciations of named entities using open-source text-to-speech models. Fine-tuning with less than 1,000 synthetic samples improves street name transcription accuracy by nearly 60% (relative to base models) for non-English primary speakers. Our results highlight a critical gap between benchmark performance and real-world reliability in speech systems and demonstrate a simple, scalable path to reducing high-stakes transcription errors.\nüìÑ Download PDF\nThe Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics Authors: Christian Intern√≤, Jumpei Yamaguchi, Loren Amdahl-Culleton, Markus Olhofer, David Klindt, Barbara Hammer Venue: arXiv (2026)\nDetermining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-capacity probes), but such interventions can change the representations being measured and thus confound what was learned during self-supervised learning (SSL). We propose a non-invasive evaluation protocol, PhyIP. We test whether physical quantities are linearly decodable from frozen representations, motivated by the linear representation hypothesis. Across fluid dynamics and orbital mechanics, we find that when SSL achieves low error, latent structure becomes linearly accessible. PhyIP recovers internal energy and Newtonian inverse-square scaling on OOD tests (e.g., $œÅ\u003e 0.90$). In contrast, adaptation-based evaluations can collapse this structure ($œÅ\\approx 0.05$). These findings suggest that adaptation-based evaluation can obscure latent structures and that low-capacity probes offer a more accurate evaluation of physical world models.\nüìÑ Download PDF\nUbiquitous yet forgotten: broad absorptions in the optical spectra of low-mass X-ray binaries Authors: D. Mata Sanchez, T. Munoz-Darias, J. Casares, M. A. P. Torres, M. Armas Padilla Venue: arXiv (2026)\nOptical outburst spectra of low-mass X-ray binaries enable studies of extreme accretion and ejection phenomena. While some of their spectroscopic features have been analysed in detail, the appearance of broad absorptions in the optical regime has been traditionally neglected. In this work, we introduce the first population study dedicated to these features with the aim to understand their fundamental properties and discuss them in the context of their origin. We complement the study with a spectroscopic database of six low-mass X-ray binaries during outburst, in order to assess their evolution. We find that broad absorptions are ubiquitous, with the majority of black hole low-mass X-ray binaries exhibiting them in spite of a typically scarce outburst coverage. Their detection does not depend on the orbital inclination or the compact object nature, but they seem favoured in systems with orbital periods shorter than \u003c 11 h. They predominantly occur in the hydrogen Balmer series, being stronger at shorter wavelengths, and they are detected across all X-ray states. We find that the normalised depth of these broad absorptions is anti-correlated with the system luminosity, and that they show constant line ratios over the whole sample. Based on these properties, we favour a scenario where BAs arise from a stable, optically thick layer of the accretion disc, below the hotter chromosphere-like region producing the emission line components. Our study is consistent with the continuous presence of broad absorptions during the whole outburst, with their visibility being conditioned by the emission lines filling the broad absorption profile and veiling by the X-ray reprocessed continuum.\nüìÑ Download PDF\nIs Online Linear Optimization Sufficient for Strategic Robustness? Authors: Yang Cai, Haipeng Luo, Chen-Yu Wei, Weiqiang Zheng Venue: arXiv (2026)\nWe consider bidding in repeated Bayesian first-price auctions. Bidding algorithms that achieve optimal regret have been extensively studied, but their strategic robustness to the seller‚Äôs manipulation remains relatively underexplored. Bidding algorithms based on no-swap-regret algorithms achieve both desirable properties, but are suboptimal in terms of statistical and computational efficiency. In contrast, online gradient ascent is the only algorithm that achieves $O(\\sqrt{TK})$ regret and strategic robustness [KSS24], where $T$ denotes the number of auctions and $K$ the number of bids. In this paper, we explore whether simple online linear optimization (OLO) algorithms suffice for bidding algorithms with both desirable properties. Our main result shows that sublinear linearized regret is sufficient for strategic robustness. Specifically, we construct simple black-box reductions that convert any OLO algorithm into a strategically robust no-regret bidding algorithm, in both known and unknown value distribution settings. For the known value distribution case, our reduction yields a bidding algorithm that achieves $O(\\sqrt{T \\log K})$ regret and strategic robustness (with exponential improvement on the $K$-dependence compared to [KSS24]). For the unknown value distribution case, our reduction gives a bidding algorithm with high-probability $O(\\sqrt{T (\\log K+\\log(T/Œ¥)})$ regret and strategic robustness, while removing the bounded density assumption made in [KSS24].\nüìÑ Download PDF\nCommunity Concealment from Unsupervised Graph Learning-Based Clustering Authors: Dalyapraz Manatova, Pablo Moriano, L. Jean Camp Venue: arXiv (2026)\nGraph neural networks (GNNs) are designed to use attributed graphs to learn representations. Such representations are beneficial in the unsupervised learning of clusters and community detection. Nonetheless, such inference may reveal sensitive groups, clustered systems, or collective behaviors, raising concerns regarding group-level privacy. Community attribution in social and critical infrastructure networks, for example, can expose coordinated asset groups, operational hierarchies, and system dependencies that could be used for profiling or intelligence gathering. We study a defensive setting in which a data publisher (defender) seeks to conceal a community of interest while making limited, utility-aware changes in the network. Our analysis indicates that community concealment is strongly influenced by two quantifiable factors: connectivity at the community boundary and feature similarity between the protected community and adjacent communities. Informed by these findings, we present a perturbation strategy that rewires a set of selected edges and modifies node features to reduce the distinctiveness leveraged by GNN message passing. The proposed method outperforms DICE in our experiments on synthetic benchmarks and real network graphs under identical perturbation budgets. Overall, it achieves median relative concealment improvements of approximately 20-45% across the evaluated settings. These findings demonstrate a mitigation strategy against GNN-based community learning and highlight group-level privacy risks intrinsic to graph learning.\nüìÑ Download PDF\nHilbert‚Äôs Program and Infinity Authors: Richard Zach Venue: arXiv (2026)\nThe primary aim of Hilbert‚Äôs proof theory was to establish the consistency of classical mathematics using finitary means only. Hilbert‚Äôs strategy for doing this was to eliminate the infinite (in the form of unbounded quantifiers) from formalized proofs using the so-called epsilon substitution method. The result is a formal proof which does not mention or appeal to infinite objects or ‚Äúconcept-formations.‚Äù However, as later developments showed, the consistency proof itself lets the infinite back into proof theory, through a back door, so to speak. The paper outlines the epsilon substitution method as an example of how proof-theoretic constructions ‚Äúeliminate the infinite‚Äù from formal proofs, and how they aim to establish conservativity and consistency. The proof also requires an argument that this proof theoretic construction always works. This second argument, however, requires possibly infinitary reasoning at the meta-level, using induction on ordinal notations.\nüìÑ Download PDF\nUnravelling Abstract Cyclic Proofs into Proofs by Induction Authors: Lide Grotenhuis, Dani√´l Otten Venue: arXiv (2026)\nCyclic proof theory breaks tradition by allowing certain infinite proofs: those that can be represented by a finite graph, while satisfying a soundness condition. We reconcile cyclic proofs with traditional finite proofs: we extend abstract cyclic proof systems with a well-founded induction principle, and transform any cyclic proof into a finite proof in the extended system. Moreover, this transformation preserves the structure of the cyclic proof. Our results leverage an annotated representation of cyclic proofs, which allows us to extract induction hypotheses and to determine their introduction order. The representation is essentially a reset proof with one key modification: names must be covered in a uniform way before a reset. This innovation allows us to handle cyclic proofs where the underlying inductive sort is non-linear. Our framework is general enough to cover recursive functions satisfying the size-change termination principle, which are viewed as cyclic proofs under the Curry-Howard correspondence.\nüìÑ Download PDF\nLaCy: What Small Language Models Can and Should Learn is Not Just a Question of Loss Authors: Szilvia Ujv√°ry, Louis B√©thune, Pierre Ablin, Jo√£o Monteiro, Marco Cuturi, Michael Kirchhof Venue: arXiv (2026)\nLanguage models have consistently grown to compress more world knowledge into their parameters, but the knowledge that can be pretrained into them is upper-bounded by their parameter size. Especially the capacity of Small Language Models (SLMs) is limited, leading to factually incorrect generations. This problem is often mitigated by giving the SLM access to an outside source: the ability to query a larger model, documents, or a database. Under this setting, we study the fundamental question of \\emph{which tokens an SLM can and should learn} during pretraining, versus \\emph{which ones it should delegate} via a \\texttt{} token. We find that this is not simply a question of loss: although the loss is predictive of whether a predicted token mismatches the ground-truth, some tokens are \\emph{acceptable} in that they are truthful alternative continuations of a pretraining document, and should not trigger a \\texttt{} even if their loss is high. We find that a spaCy grammar parser can help augment the loss signal to decide which tokens the SLM should learn to delegate to prevent factual errors and which are safe to learn and predict even under high losses. We propose LaCy, a novel pretraining method based on this token selection philosophy. Our experiments demonstrate that LaCy models successfully learn which tokens to predict and where to delegate for help. This results in higher FactScores when generating in a cascade with a bigger model and outperforms Rho or LLM-judge trained SLMs, while being simpler and cheaper.\nüìÑ Download PDF\nLeveraging LLMs to support co-evolution between definitions and instances of textual DSLs: A Systematic Evaluation Authors: Weixing Zhang, Bowen Jiang, Yuhong Fu, Anne Koziolek, Regina Hebig, Daniel Str√ºber Venue: arXiv (2026)\nSoftware languages evolve over time for reasons such as feature additions. When grammars evolve, textual instances that originally conformed to them may become outdated. While model-driven engineering provides many techniques for co-evolving models with metamodel changes, these approaches are not designed for textual DSLs and may lose human-relevant information such as layout and comments. This study systematically evaluates the potential of large language models (LLMs) for co-evolving grammars and instances of textual DSLs. Using Claude Sonnet 4.5 and GPT-5.2 across ten case languages with ten runs each, we assess both correctness and preservation of human-oriented information. Results show strong performance on small-scale cases ($\\geq$94% precision and recall for instances requiring fewer than 20 modified lines), but performance degraded with scale: Claude maintains 85% recall at 40 lines, while GPT fails on the largest instances. Response time increases substantially with instance size, and grammar evolution complexity and deletion granularity affect performance more than change type. These findings clarify when LLM-based co-evolution is effective and where current limitations remain.\nüìÑ Download PDF\nLLM-based Triplet Extraction from Financial Reports Authors: Dante Wesslund, Ville Stenstr√∂m, Pontus Linde, Alexander Holmberg Venue: arXiv (2026)\nCorporate financial reports are a valuable source of structured knowledge for Knowledge Graph construction, but the lack of annotated ground truth in this domain makes evaluation difficult. We present a semi-automated pipeline for Subject-Predicate-Object triplet extraction that uses ontology-driven proxy metrics, specifically Ontology Conformance and Faithfulness, instead of ground-truth-based evaluation. We compare a static, manually engineered ontology against a fully automated, document-specific ontology induction approach across different LLMs and two corporate annual reports. The automatically induced ontology achieves 100% schema conformance in all configurations, eliminating the ontology drift observed with the manual approach. We also propose a hybrid verification strategy that combines regex matching with an LLM-as-a-judge check, reducing apparent subject hallucination rates from 65.2% to 1.6% by filtering false positives caused by coreference resolution. Finally, we identify a systematic asymmetry between subject and object hallucinations, which we attribute to passive constructions and omitted agents in financial prose.\nüìÑ Download PDF\nüîç psycholinguistics Half-plane non-coexistence without FKG Authors: Frederik Ravn Klausen, Noah Kravitz Venue: arXiv (2026)\nFor $Œº$ an edge percolation measure on the infinite square lattice, let $Œº_{\\textit{hp}}$ (respectively, $Œº^{hp}$) denote its marginal (respectively, the marginal of its planar dual process) on the upper half-plane. We show that if $Œº$ is translation-invariant and ergodic and almost surely has only finitely many infinite clusters, then either almost surely $Œº{hp}$ has no infinite cluster, or almost surely $Œº^_{hp}$ has no infinite cluster. By the classical Burton‚ÄìKeane argument, these hypotheses are satisfied if $Œº$ is translation-invariant and ergodic and has finite-energy. In contrast to previous ``non-coexistence‚Äô‚Äô theorems, our result does not impose a positive-correlation (FKG) hypothesis on $Œº$. Our arguments also apply to the random-cluster model (including the regime $q\u003c1$, which lacks FKG), the uniform spanning tree, and the uniform odd subgraph.\nüìÑ Download PDF\nOn the implicit regularization of Langevin dynamics with projected noise Authors: Govind Menon, Austin J. Stromme, Adrien Vacher Venue: arXiv (2026)\nWe study Langevin dynamics with noise projected onto the directions orthogonal to an isometric group action. This mathematical model is introduced to shed new light on the effects of symmetry on stochastic gradient descent for over-parametrized models. Our main result identifies a novel form of implicit regularization: when the initial and target density are both invariant under the group action, Langevin dynamics with projected noise is equivalent in law to Langevin dynamics with isotropic diffusion but with an additional drift term proportional to the negative log volume of the group orbit. We prove this result by constructing a coupling of the two processes via a third process on the group itself, and identify the additional drift as the mean curvature of the orbits.\nüìÑ Download PDF\nContention Resolution, With and Without a Global Clock Authors: Zixi Cai, Kuowen Chen, Shengquan Du, Tsvi Kopelowitz, Seth Pettie, Ben Plosk Venue: arXiv (2026)\nIn the Contention Resolution problem $n$ parties each wish to have exclusive use of a shared resource for one unit of time. The problem has been studied since the early 1970s, under a variety of assumptions on feedback given to the parties, how the parties wake up, knowledge of $n$, and so on. The most consistent assumption is that parties do not have access to a global clock, only their local time since wake-up. This is surprising because the assumption of a global clock is both technologically realistic and algorithmically interesting. It enriches the problem, and opens the door to entirely new techniques. Our primary results are: [1] We design a new Contention Resolution protocol that guarantees latency $$O\\left(\\left(n\\log\\log n\\log^{(3)} n\\log^{(4)} n\\cdots \\log^{(\\log^* n)} n\\right)\\cdot 2^{\\log^* n}\\right) \\le n(\\log\\log n)^{1+o(1)}$$ in expectation and with high probability. This already establishes at least a roughly $\\log n$ complexity gap between randomized protocols in GlobalClock and LocalClock. [2] Prior analyses of randomized ContentionResolution protocols in LocalClock guaranteed a certain latency with high probability, i.e., with probability $1-1/\\text{poly}(n)$. We observe that it is just as natural to measure expected latency, and prove a $\\log n$-factor complexity gap between the two objectives for memoryless protocols. The In-Expectation complexity is $Œò(n \\log n/\\log\\log n)$ whereas the With-High-Probability latency is $Œò(n\\log^2 n/\\log\\log n)$. Three of these four upper and lower bounds are new. [3] Given the complexity separation above, one would naturally want a ContentionResolution protocol that is optimal under both the In-Expectation and With-High-Probability metrics. This is impossible! It is even impossible to achieve In-Expectation latency $o(n\\log^2 n/(\\log\\log n)^2)$ and With-High-Probability latency $n\\log^{O(1)} n$ simultaneously.\nüìÑ Download PDF\nTRACER: Trajectory Risk Aggregation for Critical Episodes in Agentic Reasoning Authors: Sina Tayebati, Divake Kumar, Nastaran Darabi, Davide Ettori, Ranganath Krishnan, Amit Ranjan Trivedi Venue: arXiv (2026)\nEstimating uncertainty for AI agents in real-world multi-turn tool-using interaction with humans is difficult because failures are often triggered by sparse critical episodes (e.g., looping, incoherent tool use, or user-agent miscoordination) even when local generation appears confident. Existing uncertainty proxies focus on single-shot text generation and therefore miss these trajectory-level breakdown signals. We introduce TRACER, a trajectory-level uncertainty metric for dual-control Tool-Agent-User interaction. TRACER combines content-aware surprisal with situational-awareness signals, semantic and lexical repetition, and tool-grounded coherence gaps, and aggregates them using a tail-focused risk functional with a MAX-composite step risk to surface decisive anomalies. We evaluate TRACER on $œÑ^2$-bench by predicting task failure and selective task execution. To this end, TRACER improves AUROC by up to 37.1% and AUARC by up to 55% over baselines, enabling earlier and more accurate detection of uncertainty in complex conversational tool-use settings. Our code and benchmark are available at https://github.com/sinatayebati/agent-tracer.\nüìÑ Download PDF\nBlock Stacking, Airplane Refueling, and Robust Appointment Scheduling Authors: Simon Gmeiner, Andreas S. Schulz Venue: arXiv (2026)\nHow can a stack of identical blocks be arranged to extend beyond the edge of a table as far as possible? We consider a generalization of this classic puzzle to blocks that differ in width and mass. Despite the seemingly simple premise, we demonstrate that it is unlikely that one can efficiently determine a stack configuration of maximum overhang. Formally, we prove that the Block-Stacking Problem is NP-hard, partially answering an open question from the literature. Furthermore, we demonstrate that the restriction to stacks without counterweights has a surprising connection to the Airplane Refueling Problem, another famous puzzle, and to Robust Appointment Scheduling, a problem of practical relevance. In addition to revealing a remarkable relation to the real-world challenge of devising schedules under uncertainty, their equivalence unveils a polynomial-time approximation scheme, that is, a $(1+Œµ)$-approximation algorithm, for Block Stacking without counterbalancing and a $(2+Œµ)$-approximation algorithm for the general case.\nüìÑ Download PDF\nInterpolation and Prewar-Postwar Output Volatility and Shock-Persistence Debate: A Closer Look and New Results Authors: Hashem Dezhbakhsh, Daniel Levy Venue: arXiv (2026)\nIt is well established that the US prewar output was more volatile and less shock persistent than the postwar output. This is often attributed to the data interpolation employed to construct the prewar series. Our analytical results, however, indicate that commonly used linear interpolation has the opposite effect on shock persistence and volatility of a series - it increases shock persistence and reduces volatility. The surprising implication of this finding is that the actual differences between the volatility and shock persistence of the prewar and postwar output series are likely greater than the existing literature recognizes, and interpolation has dampened rather than magnified this difference. Consequently, the view that postwar output was more stable than prewar output because of the effectiveness of the postwar stabilization policies and institutional changes has considerable merit. Our results hold for parsimonious stationary and nonstationary time series commonly used to model macroeconomic time series\nüìÑ Download PDF\nReionization Bubbles from Real-Space Cross Correlations of Line Intensity Maps Authors: Emilie Th√©lie, Sarah Libanore, Yonatan Sklansky, Julian B. Mu√±oz, Ely D. Kovetz Venue: arXiv (2026)\nWe propose a new way to reconstruct the ionized-bubble size distribution during the Epoch of Reionization (EoR) through the real-space cross-correlation of 21-cm and star-forming line-intensity maps. Understanding the evolution and timing of the EoR is crucial for both astrophysics and cosmology, and a wealth of information on the first sources can be extracted from the study of ionized bubbles. Nevertheless, directly mapping bubbles is challenging due to the high redshifts involved, possible selection biases, and foregrounds in 21-cm maps. Here, we exploit the real-space cross-correlation $Œæ_{21,ŒΩ}$ between 21-cm and line-intensity mapping (LIM) signals to reconstruct the evolution of bubble sizes during reionization. For the first time, we show that $Œæ_{21,ŒΩ}(r)$ departs from a saturation level for each separation $r$ when bubbles of size $r$ begin to form, providing a handle for the onset of bubbles of each radius. Moreover, we demonstrate that $Œæ_{21,ŒΩ}$ evolves from positive to negative as the EoR progresses, reaching a minimum (i.e. maximum anti-correlation) when bubbles of radius $r$ reach peak abundance. We show that these results are robust to changes in the astrophysical model as well as the timing/topology of reionization. This real-space observable complements usual Fourier-space estimators by capturing the localized nature of bubbles, offering new insights into the sources driving cosmic reionization.\nüìÑ Download PDF\nMoonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications Authors: Manjunath Kudlur, Evan King, James Wang, Pete Warden Venue: arXiv (2026)\nLatency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this global dependency incurs quadratic complexity in sequence length, inducing an inherent ‚Äúencode-the-whole-utterance‚Äù latency profile. For streaming use cases, this causes TTFT to grow linearly with utterance length as the encoder must process the entire prefix before any decoder token can be emitted. To better meet the needs of on-device, streaming ASR use cases we introduce Moonshine v2, an ergodic streaming-encoder ASR model that employs sliding-window self-attention to achieve bounded, low-latency inference while preserving strong local context. Our models achieve state of the art word error rates across standard benchmarks, attaining accuracy on-par with models 6x their size while running significantly faster. These results demonstrate that carefully designed local attention is competitive with the accuracy of full attention at a fraction of the size and latency cost, opening new possibilities for interactive speech interfaces on edge devices.\nüìÑ Download PDF\nExtending the Cosmological Collider: New Scaling Regimes and Constraints from BOSS Authors: Daniel Green, Jiashu Han, Benjamin Wallisch Venue: arXiv (2026)\nPrimordial non-Gaussianity generated by additional fields during inflation offers a compelling observational target. Heavy fields imprint characteristic oscillatory signals in non-Gaussian correlation functions of the inflaton, a process sometimes referred to as cosmological-collider physics. These distinct signatures are compelling windows into ultra-high-energy physics, but are often suppressed, making standard equilateral non-Gaussianity the most promising discovery channel in many scenarios. In this paper, we show that direct couplings between the inflaton and additional fields can lead to a wide variety of novel, observationally relevant signals which open new parameter regimes that simultaneously exhibit the characteristics of light and heavy fields. We identify these primordial signatures in the late-time observables of the large-scale structure of the Universe, where they most significantly modify the scale-dependent bias of the galaxy power spectrum to include an oscillatory modulation around a non-trivial power law. We explore the full range of parameters that phenomenologically arise in these models and study the sensitivity of current and future galaxy surveys, finding that this new class of primordial non-Gaussianity is particularly accessible in near-term surveys due to its oscillatory feature. Finally, we perform an analysis of existing data from the final release of the Baryon Oscillation Spectroscopic Survey (BOSS DR12). While we find no evidence for a signal, we demonstrate significant improvements in sensitivity over respective non-oscillatory scenarios and place the first constraints on this extended parameter space of oscillatory non-Gaussianity.\nüìÑ Download PDF\nVIRENA: Virtual Arena for Research, Education, and Democratic Innovation Authors: Emma Hoes, K. Jonathan Klueser, Fabrizio Gilardi Venue: arXiv (2026)\nDigital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platform that enables controlled experimentation in realistic social media environments. Multiple participants interact simultaneously in realistic replicas of feed-based platforms (Instagram, Facebook, Reddit) and messaging apps (WhatsApp, Messenger). Large language model-powered AI agents participate alongside humans with configurable personas and realistic behavior. Researchers can manipulate content moderation approaches, pre-schedule stimulus content, and run experiments across conditions through a visual interface requiring no programming skills. VIRENA makes possible research designs that were previously impractical: studying human‚ÄìAI interaction in realistic social contexts, experimentally comparing moderation interventions, and observing group deliberation as it unfolds. Built on open-source technologies that ensure data remain under institutional control and comply with data protection requirements, VIRENA is currently in use at the University of Zurich and available for pilot collaborations. Designed for researchers, educators, and public organizations alike, VIRENA‚Äôs no-code interface makes controlled social media simulation accessible across disciplines and sectors. This paper documents its design, architecture, and capabilities.\nüìÑ Download PDF\nMaking the complete OpenAIRE citation graph easily accessible through compact data representation Authors: Joakim Skarding, Pavel Sanda Venue: arXiv (2026)\nThe OpenAIRE graph contains a large citation graph dataset, with over 200 million publications and over 2 billion citations. The current graph is available as a dump with metadata which uncompressed totals ~TB. This makes it hard to process on conventional computers. To make this network more available for the community we provide a processed OpenAIRE graph which is downscaled to 32GB, while preserving the full graph structure. Apart from this we offer the processed data in very simple format, which allows further straightforward manipulation. We also provide a python pipeline, which can be used to process the next releases of the OpenAIRE graph.\nüìÑ Download PDF\n6G Empowering Future Robotics: A Vision for Next-Generation Autonomous Systems Authors: Mona Ghassemian, Andr√©s Meseguer Valenzuela, Ana Garcia Armada, Dejan Vukobratovic, Periklis Chatzimisios, Kaspar Althoefer, Ranga Rao Venkatesha Prasad Venue: arXiv (2026)\nThe convergence of robotics and next-generation communication is a critical driver of technological advancement. As the world transitions from 5G to 6G, the foundational capabilities of wireless networks are evolving to support increasingly complex and autonomous robotic systems. This paper examines the transformative impact of 6G on enhancing key robotics functionalities. It provides a systematic mapping of IMT-2030 key performance indicators to robotic functional blocks including sensing, perception, cognition, actuation and self-learning. Building upon this mapping, we propose a high-level architectural framework integrating robotic, intelligent, and network service planes, underscoring the need for a holistic approach. As an example use case, we present a real-time, dynamic safety framework enabled by IMT-2030 capabilities for safe and efficient human-robot collaboration in shared spaces.\nüìÑ Download PDF\nGPT-4o Lacks Core Features of Theory of Mind Authors: John Muchovej, Amanda Royka, Shane Lee, Julian Jara-Ettinger Venue: arXiv (2026)\nDo Large Language Models (LLMs) possess a Theory of Mind (ToM)? Research into this question has focused on evaluating LLMs against benchmarks and found success across a range of social tasks. However, these evaluations do not test for the actual representations posited by ToM: namely, a causal model of mental states and behavior. Here, we use a cognitively-grounded definition of ToM to develop and test a new evaluation framework. Specifically, our approach probes whether LLMs have a coherent, domain-general, and consistent model of how mental states cause behavior ‚Äì regardless of whether that model matches a human-like ToM. We find that even though LLMs succeed in approximating human judgments in a simple ToM paradigm, they fail at a logically equivalent task and exhibit low consistency between their action predictions and corresponding mental state inferences. As such, these findings suggest that the social proficiency exhibited by LLMs is not the result of an domain-general or consistent ToM.\nüìÑ Download PDF\nüîç llm Certification of linear optical quantum state preparation Authors: Riko Schadow, Naomi Spier, Stefan N. van den Hoven, Malaquias Correa Anguita, Redlef B. G. Braamhaar, Sara Marzban, Jens Eisert, Jelmer J. Renema, Nathan Walk Venue: arXiv (2026)\nCertification is important to guarantee the correct functioning of quantum devices. A key certification task is verifying that a device has produced a desired output state. In this work, we study this task in the context of photonic platforms, where single photons are propagated through linear optical interferometers to create large, entangled resource states for metrology, communication, quantum advantage demonstrations and for so-called linear optical quantum computing (LOQC). This setting derives its computational power from the indistinguishability of the photons, i.e., their relative overlap. Therefore, standard fidelity witnesses developed for distinguishable particles (including qubits) do not apply directly, because they merely certify the closeness to some fixed target state. We introduce a measure of fidelity suitable for this setting and show several different ways to witness it, based on earlier proposals for measuring genuine multi-photon indistinguishability. We argue that a witness based upon the discrete Fourier transform is an optimal choice. We experimentally implement this witness and certify the fidelity of several multi-photon states.\nüìÑ Download PDF\nSelf-Supervised Learning via Flow-Guided Neural Operator on Time-Series Data Authors: Duy Nguyen, Jiachen Yao, Jiayun Wang, Julius Berner, Animashree Anandkumar Venue: arXiv (2026)\nSelf-supervised learning (SSL) is a powerful paradigm for learning from unlabeled time-series data. However, popular methods such as masked autoencoders (MAEs) rely on reconstructing inputs from a fixed, predetermined masking ratio. Instead of this static design, we propose treating the corruption level as a new degree of freedom for representation learning, enhancing flexibility and performance. To achieve this, we introduce the Flow-Guided Neural Operator (FGNO), a novel framework combining operator learning with flow matching for SSL training. FGNO learns mappings in functional spaces by using Short-Time Fourier Transform to unify different time resolutions. We extract a rich hierarchy of features by tapping into different network layers and flow times that apply varying strengths of noise to the input data. This enables the extraction of versatile representations, from low-level patterns to high-level global features, using a single model adaptable to specific tasks. Unlike prior generative SSL methods that use noisy inputs during inference, we propose using clean inputs for representation extraction while learning representations with noise; this eliminates randomness and boosts accuracy. We evaluate FGNO across three biomedical domains, where it consistently outperforms established baselines. Our method yields up to 35% AUROC gains in neural signal decoding (BrainTreeBank), 16% RMSE reductions in skin temperature prediction (DREAMT), and over 20% improvement in accuracy and macro-F1 on SleepEDF under low-data regimes. These results highlight FGNO‚Äôs robustness to data scarcity and its superior capacity to learn expressive representations for diverse time series.\nüìÑ Download PDF\nA Stochastic Cluster Expansion for Electronic Correlation in Large Systems Authors: Annabelle Canestraight, Anthony J. Dominic, Andres Montoya-Castillo, Libor Veis, Vojtech Vlcek Venue: arXiv (2026)\nAccurate many-body treatments of condensed-phase systems are challenging because correlated solvers such as full configuration interaction (FCI) and the density matrix renormalization group (DMRG) scale exponentially with system size. Downfolding and embedding approaches mitigate this cost but typically require prior selection of a correlated subspace, which can be difficult to determine in heterogeneous or extended systems. Here, we introduce a stochastic cluster expansion framework for efficiently recovering the total correlation energy of large systems with near-DMRG accuracy, without the need to select an active space a priori. By combining correlation contributions from randomly sampled environment orbitals with an exactly treated subspace of interest, the method reproduces total energies for non-reacting and reactive systems while drastically reducing computational cost. The approach also provides a quantitative diagnostic for molecule-solvent correlation, guiding principled embedding decisions. This framework enables systematically improvable many-body calculations in extended systems, opening the door to high-accuracy studies of chemical processes in condensed phase environments.\nüìÑ Download PDF\nIntrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces Authors: Anthony Kobanda, Waris Radji Venue: arXiv (2026)\nJoint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short article, we connect these viewpoints by restricting attention to a principled class of JEPA energy functions : intrinsic (least-action) energies, defined as infima of accumulated local effort over admissible trajectories between two states. Under mild closure and additivity assumptions, any intrinsic energy is a quasimetric. In goal-reaching control, optimal cost-to-go functions admit exactly this intrinsic form ; inversely, JEPAs trained to model intrinsic energies lie in the quasimetric value class targeted by QRL. Moreover, we observe why symmetric finite energies are structurally mismatched with one-way reachability, motivating asymmetric (quasimetric) energies when directionality matters.\nüìÑ Download PDF\nüîç neuroscience Coherent Perfect Tunneling at Exceptional Points via Directional Degeneracy Authors: Huayang Cai, Bishuang Chen Venue: arXiv (2026)\nCoherent perfect tunneling in the presence of loss and asymmetry remains a fundamental challenge in wave transport, a universal problem across optics, acoustics, and quantum mechanics. Here we demonstrate coherent perfect tunneling at an exceptional point in a passive one-dimensional waveguide cascade with three coupled interfaces. Using a waveguide-invariant scattering framework, we show that the suppression of a selected output channel originates from a directional scattering degeneracy rather than from resonance or absorption collapse. This exceptional-point condition emerges when interference between boundary-induced feedback loops promotes a simple zero of the scattering response to a second-order degeneracy. As a direct consequence, fixed coherent excitation produces a robust quartic leakage law within a transparency-dominated tunneling window. These results establish directional degeneracy as a general mechanism for loss-tolerant tunneling enabled by exceptional points across a broad class of wave systems.\nüìÑ Download PDF\nRealization of a cavity-coupled Rydberg array Authors: Jacopo De Santis, Bal√°zs Dura-Kov√°cs, Mehmet √ñnc√º, Adrien Bouscal, Dimitrios Vasileiadis, Johannes Zeiher Venue: arXiv (2026)\nScalable quantum computers and quantum networks require the combination of quantum processing nodes with efficient light-matter interfaces to distribute quantum information in local or long-distance quantum networks. Neutral-atom arrays have both been coupled to Rydberg states to enable high-fidelity quantum gates in universal processing architectures, and to optical cavities to realize interfaces to photons. However, combining these two capabilities and coupling atom arrays to highly excited Rydberg states in the mode of an optical cavity has been an outstanding challenge. Here we present a novel cavity-coupled Rydberg array that achieves this long-standing goal. We prepare, detect, and control individual atoms in a scalable optical tweezer array, couple them strongly to the optical mode of a high-finesse optical cavity and excite them in a controlled way to Rydberg states. We show that strong coupling to an optical cavity - demonstrated via the dispersive shift of the resonance of the cavity in presence of the atoms - and strong Rydberg interactions - demonstrated via the collective enhancement of Rydberg coupling in the atomic array - can be achieved in our setup at the same spatial location. Our presented experimental platform opens the path to several new directions, including the realization of quantum network nodes, quantum simulation of long-range interacting, open quantum systems and photonic-state engineering leveraging high-fidelity Rydberg control.\nüìÑ Download PDF\nChoose Your Agent: Tradeoffs in Adopting AI Advisors, Coaches, and Delegates in Multi-Party Negotiation Authors: Kehang Zhu, Lithium Thain, Vivian Tsai, James Wexler, Crystal Qian Venue: arXiv (2026)\nAs AI usage becomes more prevalent in social contexts, understanding agent-user interaction is critical to designing systems that improve both individual and group outcomes. We present an online behavioral experiment (N = 243) in which participants play three multi-turn bargaining games in groups of three. Each game, presented in randomized order, grants \\textit{access to} a single LLM assistance modality: proactive recommendations from an \\textit{Advisor}, reactive feedback from a \\textit{Coach}, or autonomous execution by a \\textit{Delegate}; all modalities are powered by an underlying LLM that achieves superhuman performance in an all-agent environment. On each turn, participants privately decide whether to act manually or use the AI modality available in that game. Despite preferring the \\textit{Advisor} modality, participants achieve the highest mean individual gains with the \\textit{Delegate}, demonstrating a preference-performance misalignment. Moreover, delegation generates positive externalities; even non-adopting users in \\textit{access-to-delegate} treatment groups benefit by receiving higher-quality offers. Mechanism analysis reveals that the \\textit{Delegate} agent acts as a market maker, injecting rational, Pareto-improving proposals that restructure the trading environment. Our research reveals a gap between agent capabilities and realized group welfare. While autonomous agents can exhibit super-human strategic performance, their impact on realized welfare gains can be constrained by interfaces, user perceptions, and adoption barriers. Assistance modalities should be designed as mechanisms with endogenous participation; adoption-compatible interaction rules are a prerequisite to improving human welfare with automated assistance.\nüìÑ Download PDF\nDIVER: A Robust Text-to-SQL System with Dynamic Interactive Value Linking and Evidence Reasoning Authors: Yafeng Nan, Haifeng Sun, Zirui Zhuang, Qi Qi, Guojun Chu, Jianxin Liao, Dan Pei, Jingyu Wang Venue: arXiv (2026)\nIn the era of large language models, Text-to-SQL, as a natural language interface for databases, is playing an increasingly important role. The sota Text-to-SQL models have achieved impressive accuracy, but their performance critically relies on expert-written evidence, which typically clarifies schema and value linking that existing models struggle to identify. Such limitations stem from the ambiguity of user queries and, more importantly, the complexity of comprehending large-scale and dynamic database values. Consequently, in real-world scenarios where expert assistance is unavailable, existing methods suffer a severe performance collapse, with execution accuracy dropping by over 10%. This underscores their lack of robustness. To address this, we propose DIVER, a robust system that automates evidence reasoning with dynamic interactive value linking. It leverages a compatible toolbox containing diverse tools to probe the database. Then, restricted by a structured workspace (CoTF, Chain of Thoughts and Facts), it reflects based on probe results and selects a new tool for next round of probing. Through this automatically iterative process, DIVER identifies schema and value linking missed by existing methods. Based on these accurate linkings, DIVER is able to infer correct usage of SQL functions and formulas and generate high-quality evidence, achieving robust Text-to-SQL without expert assistance. Extensive experiments demonstrate that: 1) The DIVER system significantly enhances the robustness of various Text-to-SQL models, improving performance by up to 10.82% in Execution Accuracy (EX) and 16.09% in Valid Efficiency Score (VES). 2) Our dynamic interactive value linking significantly improves the robustness of existing systems and the accuracy of schema and value linking, especially when confronted with challenges posed by large-scale, dynamic database values.\nüìÑ Download PDF\nüîç data_resources Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training Authors: Miaosen Zhang, Yishan Liu, Shuxia Lin, Xu Yang, Qi Dai, Chong Luo, Weihao Jiang, Peng Hou, Anxiang Zeng, Xin Geng, Baining Guo Venue: arXiv (2026)\nSupervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL‚Äôs use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \\textbf{\\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \\textbf{\\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \\textbf{\\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model‚Äôs distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT\nüìÑ Download PDF\nSAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engine Optimization Authors: Sunghwan Kim, Wooseok Jeong, Serin Kim, Sangam Lee, Dongha Lee Venue: arXiv (2026)\nSearch-Augmented Generative Engines (SAGE) have emerged as a new paradigm for information access, bridging web-scale retrieval with generative capabilities to deliver synthesized answers. This shift has fundamentally reshaped how web content gains exposure online, giving rise to Search-Augmented Generative Engine Optimization (SAGEO), the practice of optimizing web documents to improve their visibility in AI-generated responses. Despite growing interest, no evaluation environment currently supports comprehensive investigation of SAGEO. Specifically, existing benchmarks lack end-to-end visibility evaluation of optimization strategies, operating on pre-determined candidate documents that abstract away retrieval and reranking preceding generation. Moreover, existing benchmarks discard structural information (e.g., schema markup) present in real web documents, overlooking the rich signals that search systems actively leverage in practice. Motivated by these gaps, we introduce SAGEO Arena, a realistic and reproducible environment for stage-level SAGEO analysis. Our objective is to jointly target search-oriented optimization (SEO) and generation-centric optimization (GEO). To achieve this, we integrate a full generative search pipeline over a large-scale corpus of web documents with rich structural information. Our findings reveal that existing approaches remain largely impractical under realistic conditions and often degrade performance in retrieval and reranking. We also find that structural information helps mitigate these limitations, and that effective SAGEO requires tailoring optimization to each pipeline stage. Overall, our benchmark paves the way for realistic SAGEO evaluation and optimization beyond simplified settings.\nüìÑ Download PDF\nEmpirical Gaussian Processes Authors: Jihao Andreas Lin, Sebastian Ament, Louis C. Tiao, David Eriksson, Maximilian Balandat, Eytan Bakshy Venue: arXiv (2026)\nGaussian processes (GPs) are powerful and widely used probabilistic regression models, but their effectiveness in practice is often limited by the choice of kernel function. This kernel function is typically handcrafted from a small set of standard functions, a process that requires expert knowledge, results in limited adaptivity to data, and imposes strong assumptions on the hypothesis space. We study Empirical GPs, a principled framework for constructing flexible, data-driven GP priors that overcome these limitations. Rather than relying on standard parametric kernels, we estimate the mean and covariance functions empirically from a corpus of historical observations, enabling the prior to reflect rich, non-trivial covariance structures present in the data. Theoretically, we show that the resulting model converges to the GP that is closest (in KL-divergence sense) to the real data generating process. Practically, we formulate the problem of learning the GP prior from independent datasets as likelihood estimation and derive an Expectation-Maximization algorithm with closed-form updates, allowing the model handle heterogeneous observation locations across datasets. We demonstrate that Empirical GPs achieve competitive performance on learning curve extrapolation and time series forecasting benchmarks.\nüìÑ Download PDF\nSci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision Authors: Xiaohan He, Shiyang Feng, Songtao Huang, Lei Bai, Bin Wang, Bo Zhang Venue: arXiv (2026)\nLarge language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE.\nüìÑ Download PDF\nTargeted Syntactic Evaluation of Language Models on Georgian Case Alignment Authors: Daniel Gallagher, Gerhard Heyer Venue: arXiv (2026)\nThis paper evaluates the performance of transformer-based language models on split-ergative case alignment in Georgian, a particularly rare system for assigning grammatical cases to mark argument roles. We focus on subject and object marking determined through various permutations of nominative, ergative, and dative noun forms. A treebank-based approach for the generation of minimal pairs using the Grew query language is implemented. We create a dataset of 370 syntactic tests made up of seven tasks containing 50-70 samples each, where three noun forms are tested in any given sample. Five encoder- and two decoder-only models are evaluated with word- and/or sentence-level accuracy metrics. Regardless of the specific syntactic makeup, models performed worst in assigning the ergative case correctly and strongest in assigning the nominative case correctly. Performance correlated with the overall frequency distribution of the three forms (NOM \u003e DAT \u003e ERG). Though data scarcity is a known issue for low-resource languages, we show that the highly specific role of the ergative along with a lack of available training data likely contributes to poor performance on this case. The dataset is made publicly available and the methodology provides an interesting avenue for future syntactic evaluations of languages where benchmarks are limited.\nüìÑ Download PDF\nFederated Gaussian Process Learning via Pseudo-Representations for Large-Scale Multi-Robot Systems Authors: Sanket A. Salunkhe, George P. Kontoudis Venue: arXiv (2026)\nMulti-robot systems require scalable and federated methods to model complex environments under computational and communication constraints. Gaussian Processes (GPs) offer robust probabilistic modeling, but suffer from cubic computational complexity, limiting their applicability in large-scale deployments. To address this challenge, we introduce the pxpGP, a novel distributed GP framework tailored for both centralized and decentralized large-scale multi-robot networks. Our approach leverages sparse variational inference to generate a local compact pseudo-representation. We introduce a sparse variational optimization scheme that bounds local pseudo-datasets and formulate a global scaled proximal-inexact consensus alternating direction method of multipliers (ADMM) with adaptive parameter updates and warm-start initialization. Experiments on synthetic and real-world datasets demonstrate that pxpGP and its decentralized variant, dec-pxpGP, outperform existing distributed GP methods in hyperparameter estimation and prediction accuracy, particularly in large-scale networks.\nüìÑ Download PDF\nüîç emotion_language Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision Authors: Anika Tabassum Meem, Muntasir Hossain Nadid, Md Zesun Ahmed Mia Venue: arXiv (2026)\nNeuromorphic vision systems based on spiking neural networks (SNNs) offer ultra-low-power perception for event-based and frame-based cameras, yet catastrophic forgetting remains a critical barrier to deployment in continually evolving environments. Existing continual learning methods, developed primarily for artificial neural networks, seldom jointly optimize accuracy and energy efficiency, with particularly limited exploration on event-based datasets. We propose an energy-aware spike budgeting framework for continual SNN learning that integrates experience replay, learnable leaky integrate-and-fire neuron parameters, and an adaptive spike scheduler to enforce dataset-specific energy constraints during training. Our approach exhibits modality-dependent behavior: on frame-based datasets (MNIST, CIFAR-10), spike budgeting acts as a sparsity-inducing regularizer, improving accuracy while reducing spike rates by up to 47%; on event-based datasets (DVS-Gesture, N-MNIST, CIFAR-10-DVS), controlled budget relaxation enables accuracy gains up to 17.45 percentage points with minimal computational overhead. Across five benchmarks spanning both modalities, our method demonstrates consistent performance improvements while minimizing dynamic power consumption, advancing the practical viability of continual learning in neuromorphic vision systems.\nüìÑ Download PDF\nWaveFormer: Wavelet Embedding Transformer for Biomedical Signals Authors: Habib Irani, Bikram De, Vangelis Metsis Venue: arXiv (2026)\nBiomedical signal classification presents unique challenges due to long sequences, complex temporal dynamics, and multi-scale frequency patterns that are poorly captured by standard transformer architectures. We propose WaveFormer, a transformer architecture that integrates wavelet decomposition at two critical stages: embedding construction, where multi-channel Discrete Wavelet Transform (DWT) extracts frequency features to create tokens containing both time-domain and frequency-domain information, and positional encoding, where Dynamic Wavelet Positional Encoding (DyWPE) adapts position embeddings to signal-specific temporal structure through mono-channel DWT analysis. We evaluate WaveFormer on eight diverse datasets spanning human activity recognition and brain signal analysis, with sequence lengths ranging from 50 to 3000 timesteps and channel counts from 1 to 144. Experimental results demonstrate that WaveFormer achieves competitive performance through comprehensive frequency-aware processing. Our approach provides a principled framework for incorporating frequency-domain knowledge into transformer-based time series classification.\nüìÑ Download PDF\nUnknown Attack Detection in IoT Networks using Large Language Models: A Robust, Data-efficient Approach Authors: Shan Ali, Feifei Niu, Paria Shirani, Lionel C. Briand Venue: arXiv (2026)\nThe rapid evolution of cyberattacks continues to drive the emergence of unknown (zero-day) threats, posing significant challenges for network intrusion detection systems in Internet of Things (IoT) networks. Existing machine learning and deep learning approaches typically rely on large labeled datasets, payload inspection, or closed-set classification, limiting their effectiveness under data scarcity, encrypted traffic, and distribution shifts. Consequently, detecting unknown attacks in realistic IoT deployments remains difficult. To address these limitations, we propose SiamXBERT, a robust and data-efficient Siamese meta-learning framework empowered by a transformer-based language model for unknown attack detection. The proposed approach constructs a dual-modality feature representation by integrating flow-level and packet-level information, enabling richer behavioral modeling while remaining compatible with encrypted traffic. Through meta-learning, the model rapidly adapts to new attack types using only a small number of labeled samples and generalizes to previously unseen behaviors. Extensive experiments on representative IoT intrusion datasets demonstrate that SiamXBERT consistently outperforms state-of-the-art baselines under both within-dataset and cross-dataset settings while requiring significantly less training data, achieving up to \\num{78.8}% improvement in unknown F1-score. These results highlight the practicality of SiamXBERT for robust unknown attack detection in real-world IoT environments.\nüìÑ Download PDF\nLocal Integrable Symmetries of Diffieties Authors: Fran√ßois Ollivier, Yirmeyahu J. Kaminski Venue: arXiv (2026)\nIn the framework of diffieties, introduced by Vinogradov, we introduce integrable infinitesimal symmetries and show that they define a one parameter pseudogroup of local diffiety morphisms. We prove some preliminary results allowing to reduce the computation of integrable infinitesimal symmetries of a given order to solving a system of partial differential equations.We provide examples for which we can reduce to a linear system that can be solved by hand computation, and investigate some consequences for the local classification of diffiety, with a special interest for testing if a diffiety is flat.\nüìÑ Download PDF\nLegitimate Overrides in Decentralized Protocols Authors: Oghenekaro Elem, Nimrod Talmon Venue: arXiv (2026)\nDecentralized protocols claim immutable, rule-based execution, yet many embed emergency mechanisms such as chain-level freezes, protocol pauses, and account quarantines. These overrides are crucial for responding to exploits and systemic failures, but they expose a core tension: when does intervention preserve trust and when is it perceived as illegitimate discretion? With approximately $10$ billion in technical exploit losses potentially addressable by onchain intervention (2016‚Äì2026), the design of these mechanisms has high practical stakes, but current approaches remain ad hoc and ideologically charged. We address this gap by developing a Scope $\\times$ Authority taxonomy that maps the design space of emergency architectures along two dimensions: the precision of the intervention and the concentration of trigger authority. We formalize the resulting tradeoffs of a standing centralization cost versus containment speed and collateral disruption as a stochastic cost-minimization problem; and derive three testable predictions. Assessing these predictions against 705 documented exploit incidents, we find that containment time varies systematically by authority type; that losses follow a heavy-tailed distribution ($Œ±\\approx 1.33$) concentrating risk in rare catastrophic events; and that community sentiment measurably modulates the effective cost of maintaining intervention capability. The analysis yields concrete design principles that move emergency governance from ideological debate towards quantitative engineering.\nüìÑ Download PDF\nBayesian inference for the automultinomial model with an application to landcover data Authors: Maria Paula Duenas-Herrera, Stephen Berg, Murali Haran Venue: arXiv (2026)\nMulticategory lattice data arise in a wide variety of disciplines such as image analysis, biology, and forestry. We consider modeling such data with the automultinomial model, which can be viewed as a natural extension of the autologistic model to multicategory responses, or equivalently as an extension of the Potts model that incorporates covariate information into a pure-intercept model. The automultinomial model has the advantage of having a unique parameter that controls the spatial correlation. However, the model‚Äôs likelihood involves an intractable normalizing function of the model parameters that poses serious computational problems for likelihood-based inference. We address this difficulty by performing Bayesian inference through the Double-Metropolis Hastings algorithm, and implement diagnostics to assess the convergence to the target posterior distribution. Through simulation studies and an application to land cover data, we find that the automultinomial model is flexible across a wide range of spatial correlations while maintaining a relatively simple specification. For large data sets we find it also has advantages over spatial generalized linear mixed models. To make this model practical for scientists, we provide recommendations for its specification and computational implementation.\nüìÑ Download PDF\nEquivalent Circuit Modeling of Grid-Forming Inverters in (Sub)-Transient Time-Frame Authors: Ambuj Gupta, Balarko Chaudhuri, Mark O‚ÄôMalley Venue: arXiv (2026)\nThe widely accepted definition of grid-forming (GFM) inverter states that it should behave as a (nearly) constant voltage source behind an impedance by maintaining a (nearly) constant internal voltage phasor in the sub-transient to transient time frame. Some system operators further mandate permissible ranges for this effective impedance. However, these specifications do not clearly define the location of the internal voltage source, and no systematic method exists to quantify its effective impedance for a black-box GFM model. To address this, we first compare the transient responses of an ideal voltage source and a GFM to show that an idealistic GFM maintains a (nearly) constant voltage across the filter capacitor, rather than at the inverter switches. Then we propose a systematic method to quantify the effective impedance of a GFM from its black-box model using frequency-domain admittance plots. Using standard PSCAD GFM models developed by NREL, we demonstrate that the GFM‚Äôs equivalent impedance model captures the sub-transient response and static voltage stability limit reasonably accurately.\nüìÑ Download PDF\nHow Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics Authors: Yurong Chen, Yu He, Michael I. Jordan, Fan Yao Venue: arXiv (2026)\nStandard methods for aligning large language models with human preferences learn from pairwise comparisons among sampled candidate responses and regularize toward a reference policy. Despite their effectiveness, the effects of sampling and reference choices are poorly understood theoretically. We investigate these effects through Identity Preference Optimization, a widely used preference alignment framework, and show that proper instance-dependent sampling can yield stronger ranking guarantees, while skewed on-policy sampling can induce excessive concentration under structured preferences. We then analyze iterative alignment dynamics in which the learned policy feeds back into future sampling and reference policies, reflecting a common practice of model-generated preference data. We prove that these dynamics can exhibit persistent oscillations or entropy collapse for certain parameter choices, and characterize regimes that guarantee stability. Our theoretical insights extend to Direct Preference Optimization, indicating the phenomena we captured are common to a broader class of preference-alignment methods. Experiments on real-world preference data validate our findings.\nüìÑ Download PDF\nThe Dark Side of the Moon: Listening to Scalar-Induced Gravitational Waves Authors: D. Blas, J. W. Foster, Y. Gouttenoire, A. J. Iovino, I. Musco, S. Trifinopoulos, M. Vanvlasselaer Venue: arXiv (2026)\nThe collapse of large-amplitude primordial curvature perturbations into planetary-mass primordial black holes generates a scalar-induced gravitational wave background in the $Œº$Hz frequency range that may be detectable by future Lunar Laser Ranging and Satellite Laser Ranging data. We derive projected constraints on the primordial black hole population from a null detection of stochastic gravitational wave background by these experiments, including the impact of the electroweak phase transition on the abundance of planetary-mass primordial black holes. We also discuss the connection between the obtained projected constraints and the recent microlensing observations by the HSC collaboration of the Andromeda Galaxy.\nüìÑ Download PDF\nPhase Estimation from Amplitude Collapse in Correlated Matter-Wave Interference Authors: Daniel Derr, Dominik Pfeiffer, Ludwig Lind, Gerhard Birkl, Enno Giese Venue: arXiv (2026)\nOperating matter-wave interferometers as quantum detectors for fundamental physics or inertial sensors in real-world applications with unprecedented accuracies relies on noise rejection, often implemented by correlating two sensors. Such sensors can be spatially separated (gradiometry or gravitational-wave detection) or consist of different internal states (magnetometry or quantum clock interferometry), in which case a signal-amplitude modulation may serve as a signature of a differential phase. In this work, we introduce Phase Estimation from Amplitude Collapse (PEAC) by applying targeted fitting methods for different magnetically sensitive substates of an atom interferometer. We demonstrate that PEAC provides higher trueness (up to 80% bias reduction) than standard tools for perfectly correlated signals. At its working point near, but not exactly at phase settings resulting in vanishing amplitude, it achieves precision competitive with standard methods, contrasting prior claims of optimal operation at vanishing amplitude. PEAC presents a generally applicable complementary evaluation method for correlated interferometers without phase stability, increasing the overall accuracy and enabling applications beyond atom interferometry.\nüìÑ Download PDF\nAnonymous Contracts Authors: Johannes Brustle, Paul Duetting, Stefano Leonardi, Tomasz Ponitka, Matteo Russo Venue: arXiv (2026)\nWe study a multi-agent contracting problem where agents exert costly effort to achieve individually observable binary outcomes. While the principal can theoretically extract the full social welfare using a discriminatory contract that tailors payments to individual costs, such contracts may be perceived as unfair. In this work, we introduce and analyze anonymous contracts, where payments depend solely on the total number of successes, ensuring identical treatment of agents. We first establish that every anonymous contract admits a pure Nash equilibrium. However, because general anonymous contracts can suffer from multiple equilibria with unbounded gaps in principal utility, we identify uniform anonymous contracts as a desirable subclass. We prove that uniform anonymous contracts guarantee a unique equilibrium, thereby providing robust performance guarantees. In terms of efficiency, we prove that under limited liability, anonymous contracts cannot generally approximate the social welfare better than a factor logarithmic in the spread of agent success probabilities. We show that uniform contracts are sufficient to match this theoretical limit. Finally, we demonstrate that removing limited liability significantly boosts performance: anonymous contracts generally achieve an $O(\\log n)$ approximation to the social welfare and, surprisingly, can extract the full welfare whenever agents‚Äô success probabilities are distinct. This reveals a structural reversal: widely spread probabilities are the hardest case under limited liability, whereas identical probabilities become the hardest case when limited liability is removed.\nüìÑ Download PDF\nSystematic Operator Construction for Non-relativistic Effective Field Theories: Hilbert Series versus Young Tensor Authors: Yong-Kang Li, Yi-Ning Wang, Jiang-Hao Yu Venue: arXiv (2026)\nThis work establishes a systematic framework for operator construction in the non-relativistic effective field theory, incorporating both the three dimensional Euclidean symmetry and the internal symmetries. By employing double cover of the rotation group, we extend the Hilbert series to the non-relativistic systems, and eliminates redundancies introduced by the spin operator. We also generalize the Young tensor method to the non-relativistic cases through the $SU(2)$ semi-standard Young tableaux, which allows for the construction of operator bases with repeated fields at any given mass dimension. Utilizing the Young tensor technique and Hibert series as cross-check, we obtain the complete operator bases for the following cases: heavy particle (and also heavy quark) effective theory operators up to mass dimension 9; pion-less effective theory operators, including nucleon-nucleon contact interactions up to $\\mathcal{O}(Q^4)$ and three-nucleon interactions at $\\mathcal{O}(Q^2)$; and finally the spin-1/2 dark matter-nucleon operators up to $\\mathcal{O}(v^4)$.\nüìÑ Download PDF\n","wordCount":"22053","inLanguage":"en","datePublished":"2026-02-15T15:30:24.51342Z","dateModified":"2026-02-15T15:30:24.51342Z","author":{"@type":"Person","name":"Gary"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://garyforreal.me/en/posts/paper/paper-2026-02-15-weekly/"},"publisher":{"@type":"Organization","name":"Gary's House","logo":{"@type":"ImageObject","url":"https://garyforreal.me/img/Q.jpg"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><script src=https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://garyforreal.me/en/ accesskey=h title="Gary's Blog (Alt + H)"><img src=https://garyforreal.me/img/me.jpg alt aria-label=logo height=35>Gary's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://garyforreal.me/zh/ title=‰∏≠Êñá aria-label=‰∏≠Êñá>‰∏≠Êñá</a></li></ul></div></div><ul id=menu><li><a href=https://garyforreal.me/en/search title="üîçSearch (Alt + /)" accesskey=/><span>üîçSearch</span></a></li><li><a href=https://garyforreal.me/en/ title=üè†Homepage><span>üè†Homepage</span></a></li><li><a href=https://garyforreal.me/en/posts title=üìöArticle><span>üìöArticle</span></a></li><li><a href=https://garyforreal.me/en/archives/ title=‚è±Archives><span>‚è±Archives</span></a></li><li><a href=https://garyforreal.me/en/music/ title=üéµmusic><span>üéµmusic</span></a></li><li><a href=https://garyforreal.me/en/about title=üôãüèª‚Äç‚ôÇÔ∏èAbout><span>üôãüèª‚Äç‚ôÇÔ∏èAbout</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://garyforreal.me/en/>Home</a>&nbsp;¬ª&nbsp;<a href=https://garyforreal.me/en/posts/>Posts</a>&nbsp;¬ª&nbsp;<a href=https://garyforreal.me/en/posts/paper/>Paper</a></div><h1 class="post-title entry-hint-parent">Weekly Paper Notes - 2026-02-15</h1><div class=post-meta><span title='2026-02-15 15:30:24.51342 +0000 UTC'>2026-02-15</span>&nbsp;¬∑&nbsp;104 min&nbsp;¬∑&nbsp;104 min&nbsp;¬∑&nbsp;Gary&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://garyforreal.me/zh/posts/paper/paper-2026-02-15-weekly/>‰∏≠Êñá</a></li></ul><div class=meta-item>&nbsp¬∑&nbsp
        <span id=busuanzi_container_page_pv>Êú¨ÊñáÈòÖËØªÈáè<span id=busuanzi_value_page_pv></span>Ê¨°</span></div></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#weekly-paper-notes aria-label="Weekly Paper Notes">Weekly Paper Notes</a><ul><li><a href=#-multilingual aria-label="üîç multilingual">üîç multilingual</a><ul><li><a href=#a-technical-curriculum-on-language-oriented-artificial-intelligence-in-translation-and-specialised-communicationhttpsarxivorgabs260212251v1 aria-label="A technical curriculum on language-oriented artificial intelligence in translation and specialised communication"><a href=https://arxiv.org/abs/2602.12251v1>A technical curriculum on language-oriented artificial intelligence in translation and specialised communication</a></a></li><li><a href=#statistical-parsing-for-logical-information-retrievalhttpsarxivorgabs260212170v1 aria-label="Statistical Parsing for Logical Information Retrieval"><a href=https://arxiv.org/abs/2602.12170v1>Statistical Parsing for Logical Information Retrieval</a></a></li><li><a href=#citilink-minutes-a-multilayer-annotated-dataset-of-municipal-meeting-minuteshttpsarxivorgabs260212137v1 aria-label="CitiLink-Minutes: A Multilayer Annotated Dataset of Municipal Meeting Minutes"><a href=https://arxiv.org/abs/2602.12137v1>CitiLink-Minutes: A Multilayer Annotated Dataset of Municipal Meeting Minutes</a></a></li><li><a href=#dhplt-large-scale-multilingual-diachronic-corpora-and-word-representations-for-semantic-change-modellinghttpsarxivorgabs260211968v1 aria-label="DHPLT: large-scale multilingual diachronic corpora and word representations for semantic change modelling"><a href=https://arxiv.org/abs/2602.11968v1>DHPLT: large-scale multilingual diachronic corpora and word representations for semantic change modelling</a></a></li><li><a href=#scaling-model-and-data-for-multilingual-machine-translation-with-open-large-language-modelshttpsarxivorgabs260211961v1 aria-label="Scaling Model and Data for Multilingual Machine Translation with Open Large Language Models"><a href=https://arxiv.org/abs/2602.11961v1>Scaling Model and Data for Multilingual Machine Translation with Open Large Language Models</a></a></li><li><a href=#a-subword-embedding-approach-for-variation-detection-in-luxembourgish-user-commentshttpsarxivorgabs260211795v1 aria-label="A Subword Embedding Approach for Variation Detection in Luxembourgish User Comments"><a href=https://arxiv.org/abs/2602.11795v1>A Subword Embedding Approach for Variation Detection in Luxembourgish User Comments</a></a></li><li><a href=#towards-reliable-machine-translation-scaling-llms-for-critical-error-detection-and-safetyhttpsarxivorgabs260211444v1 aria-label="Towards Reliable Machine Translation: Scaling LLMs for Critical Error Detection and Safety"><a href=https://arxiv.org/abs/2602.11444v1>Towards Reliable Machine Translation: Scaling LLMs for Critical Error Detection and Safety</a></a></li><li><a href=#on-the-robustness-of-knowledge-editing-for-detoxificationhttpsarxivorgabs260210504v1 aria-label="On the Robustness of Knowledge Editing for Detoxification"><a href=https://arxiv.org/abs/2602.10504v1>On the Robustness of Knowledge Editing for Detoxification</a></a></li><li><a href=#from-fusha-to-folk-exploring-cross-lingual-transfer-in-arabic-language-modelshttpsarxivorgabs260209826v1 aria-label="From FusHa to Folk: Exploring Cross-Lingual Transfer in Arabic Language Models"><a href=https://arxiv.org/abs/2602.09826v1>From FusHa to Folk: Exploring Cross-Lingual Transfer in Arabic Language Models</a></a></li><li><a href=#lemur-a-corpus-for-robust-fine-tuning-of-multilingual-law-embedding-models-for-retrievalhttpsarxivorgabs260209570v1 aria-label="LEMUR: A Corpus for Robust Fine-Tuning of Multilingual Law Embedding Models for Retrieval"><a href=https://arxiv.org/abs/2602.09570v1>LEMUR: A Corpus for Robust Fine-Tuning of Multilingual Law Embedding Models for Retrieval</a></a></li><li><a href=#unsupervised-cross-lingual-part-of-speech-tagging-with-monolingual-corpora-onlyhttpsarxivorgabs260209366v1 aria-label="Unsupervised Cross-Lingual Part-of-Speech Tagging with Monolingual Corpora Only"><a href=https://arxiv.org/abs/2602.09366v1>Unsupervised Cross-Lingual Part-of-Speech Tagging with Monolingual Corpora Only</a></a></li><li><a href=#sinfos-a-parallel-dataset-for-translating-sinhala-figures-of-speechhttpsarxivorgabs260209866v1 aria-label="SinFoS: A Parallel Dataset for Translating Sinhala Figures of Speech"><a href=https://arxiv.org/abs/2602.09866v1>SinFoS: A Parallel Dataset for Translating Sinhala Figures of Speech</a></a></li><li><a href=#do-not-treat-code-as-natural-language-implications-for-repository-level-code-generation-and-beyondhttpsarxivorgabs260211671v1 aria-label="Do Not Treat Code as Natural Language: Implications for Repository-Level Code Generation and Beyond"><a href=https://arxiv.org/abs/2602.11671v1>Do Not Treat Code as Natural Language: Implications for Repository-Level Code Generation and Beyond</a></a></li><li><a href=#a-human-centric-framework-for-data-attribution-in-large-language-modelshttpsarxivorgabs260210995v1 aria-label="A Human-Centric Framework for Data Attribution in Large Language Models"><a href=https://arxiv.org/abs/2602.10995v1>A Human-Centric Framework for Data Attribution in Large Language Models</a></a></li><li><a href=#i-can-tell-whether-you-are-a-native-hawl%c3%aari-speaker-how-ann-cnn-and-rnn-perform-in-nli-native-language-identificationhttpsarxivorgabs260210832v1 aria-label="I can tell whether you are a Native Hawl√™ri Speaker! How ANN, CNN, and RNN perform in NLI-Native Language Identification"><a href=https://arxiv.org/abs/2602.10832v1>I can tell whether you are a Native Hawl√™ri Speaker! How ANN, CNN, and RNN perform in NLI-Native Language Identification</a></a></li><li><a href=#autonomous-continual-learning-of-computer-use-agents-for-environment-adaptationhttpsarxivorgabs260210356v1 aria-label="Autonomous Continual Learning of Computer-Use Agents for Environment Adaptation"><a href=https://arxiv.org/abs/2602.10356v1>Autonomous Continual Learning of Computer-Use Agents for Environment Adaptation</a></a></li><li><a href=#calliope-a-tts-based-narrated-e-book-creator-ensuring-exact-synchronization-privacy-and-layout-fidelityhttpsarxivorgabs260210735v1 aria-label="Calliope: A TTS-based Narrated E-book Creator Ensuring Exact Synchronization, Privacy, and Layout Fidelity"><a href=https://arxiv.org/abs/2602.10735v1>Calliope: A TTS-based Narrated E-book Creator Ensuring Exact Synchronization, Privacy, and Layout Fidelity</a></a></li><li><a href=#motivating-reml-via-prediction-error-covariances-in-em-updates-for-linear-mixed-modelshttpsarxivorgabs260209247v1 aria-label="Motivating REML via Prediction-Error Covariances in EM Updates for Linear Mixed Models"><a href=https://arxiv.org/abs/2602.09247v1>Motivating REML via Prediction-Error Covariances in EM Updates for Linear Mixed Models</a></a></li><li><a href=#chaos-and-parrondos-paradox-an-overviewhttpsarxivorgabs260208135v1 aria-label="Chaos and Parrondo&rsquo;s paradox: An overview"><a href=https://arxiv.org/abs/2602.08135v1>Chaos and Parrondo&rsquo;s paradox: An overview</a></a></li><li><a href=#scaling-verification-can-be-more-effective-than-scaling-policy-learning-for-vision-language-action-alignmenthttpsarxivorgabs260212281v1 aria-label="Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment"><a href=https://arxiv.org/abs/2602.12281v1>Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment</a></a></li><li><a href=#unit-unified-multimodal-chain-of-thought-test-time-scalinghttpsarxivorgabs260212279v1 aria-label="UniT: Unified Multimodal Chain-of-Thought Test-time Scaling"><a href=https://arxiv.org/abs/2602.12279v1>UniT: Unified Multimodal Chain-of-Thought Test-time Scaling</a></a></li><li><a href=#attentionretriever-attention-layers-are-secretly-long-document-retrievershttpsarxivorgabs260212278v1 aria-label="AttentionRetriever: Attention Layers are Secretly Long Document Retrievers"><a href=https://arxiv.org/abs/2602.12278v1>AttentionRetriever: Attention Layers are Secretly Long Document Retrievers</a></a></li><li><a href=#agentic-test-time-scaling-for-webagentshttpsarxivorgabs260212276v1 aria-label="Agentic Test-Time Scaling for WebAgents"><a href=https://arxiv.org/abs/2602.12276v1>Agentic Test-Time Scaling for WebAgents</a></a></li><li><a href=#on-policy-context-distillation-for-language-modelshttpsarxivorgabs260212275v1 aria-label="On-Policy Context Distillation for Language Models"><a href=https://arxiv.org/abs/2602.12275v1>On-Policy Context Distillation for Language Models</a></a></li><li><a href=#t3d-few-step-diffusion-language-models-via-trajectory-self-distillation-with-direct-discriminative-optimizationhttpsarxivorgabs260212262v1 aria-label="T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization"><a href=https://arxiv.org/abs/2602.12262v1>T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization</a></a></li><li><a href=#lda-1b-scaling-latent-dynamics-action-model-via-universal-embodied-data-ingestionhttpsarxivorgabs260212215v1 aria-label="LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion"><a href=https://arxiv.org/abs/2602.12215v1>LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion</a></a></li><li><a href=#learning-to-forget-attention-memory-consolidation-for-adaptive-compute-reductionhttpsarxivorgabs260212204v1 aria-label="Learning to Forget Attention: Memory Consolidation for Adaptive Compute Reduction"><a href=https://arxiv.org/abs/2602.12204v1>Learning to Forget Attention: Memory Consolidation for Adaptive Compute Reduction</a></a></li><li><a href=#continuous-and-discrete-time-filters-a-unified-operational-perspectivehttpsarxivorgabs260212198v1 aria-label="Continuous and Discrete-Time Filters: A Unified Operational Perspective"><a href=https://arxiv.org/abs/2602.12198v1>Continuous and Discrete-Time Filters: A Unified Operational Perspective</a></a></li><li><a href=#pedagogically-inspired-data-synthesis-for-language-model-knowledge-distillationhttpsarxivorgabs260212172v1 aria-label="Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation"><a href=https://arxiv.org/abs/2602.12172v1>Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation</a></a></li><li><a href=#amortized-molecular-optimization-via-group-relative-policy-optimizationhttpsarxivorgabs260212162v1 aria-label="Amortized Molecular Optimization via Group Relative Policy Optimization"><a href=https://arxiv.org/abs/2602.12162v1>Amortized Molecular Optimization via Group Relative Policy Optimization</a></a></li><li><a href=#the-wandering-supermassive-black-hole-powering-the-off-nuclear-tde-at2024tvdhttpsarxivorgabs260212272v1 aria-label="The Wandering Supermassive Black Hole Powering the off-nuclear TDE AT2024tvd"><a href=https://arxiv.org/abs/2602.12272v1>The Wandering Supermassive Black Hole Powering the off-nuclear TDE AT2024tvd</a></a></li><li><a href=#monarchrt-efficient-attention-for-real-time-video-generationhttpsarxivorgabs260212271v1 aria-label="MonarchRT: Efficient Attention for Real-Time Video Generation"><a href=https://arxiv.org/abs/2602.12271v1>MonarchRT: Efficient Attention for Real-Time Video Generation</a></a></li><li><a href=#extractbench-a-benchmark-and-evaluation-methodology-for-complex-structured-extractionhttpsarxivorgabs260212247v1 aria-label="ExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction"><a href=https://arxiv.org/abs/2602.12247v1>ExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction</a></a></li><li><a href=#status-of-the-s_8-tension-a-2026-review-of-probe-discrepancieshttpsarxivorgabs260212238v1 aria-label="Status of the $S_8$ Tension: A 2026 Review of Probe Discrepancies"><a href=https://arxiv.org/abs/2602.12238v1>Status of the $S_8$ Tension: A 2026 Review of Probe Discrepancies</a></a></li><li><a href=#automated-test-suite-enhancement-using-large-language-models-with-few-shot-promptinghttpsarxivorgabs260212256v1 aria-label="Automated Test Suite Enhancement Using Large Language Models with Few-shot Prompting"><a href=https://arxiv.org/abs/2602.12256v1>Automated Test Suite Enhancement Using Large Language Models with Few-shot Prompting</a></a></li><li><a href=#vision-transformer-for-multi-domain-phase-retrieval-in-coherent-diffraction-imaginghttpsarxivorgabs260212255v1 aria-label="Vision Transformer for Multi-Domain Phase Retrieval in Coherent Diffraction Imaging"><a href=https://arxiv.org/abs/2602.12255v1>Vision Transformer for Multi-Domain Phase Retrieval in Coherent Diffraction Imaging</a></a></li><li><a href=#detecting-overflow-in-compressed-token-representations-for-retrieval-augmented-generationhttpsarxivorgabs260212235v1 aria-label="Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation"><a href=https://arxiv.org/abs/2602.12235v1>Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation</a></a></li><li><a href=#query-focused-and-memory-aware-reranker-for-long-context-processinghttpsarxivorgabs260212192v1 aria-label="Query-focused and Memory-aware Reranker for Long Context Processing"><a href=https://arxiv.org/abs/2602.12192v1>Query-focused and Memory-aware Reranker for Long Context Processing</a></a></li></ul></li><li><a href=#-linguistics aria-label="üîç linguistics">üîç linguistics</a><ul><li><a href=#stroke-of-surprise-progressive-semantic-illusions-in-vector-sketchinghttpsarxivorgabs260212280v1 aria-label="Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching"><a href=https://arxiv.org/abs/2602.12280v1>Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching</a></a></li><li><a href=#function-space-decoupled-diffusion-for-forward-and-inverse-modeling-in-carbon-capture-and-storagehttpsarxivorgabs260212274v1 aria-label="Function-Space Decoupled Diffusion for Forward and Inverse Modeling in Carbon Capture and Storage"><a href=https://arxiv.org/abs/2602.12274v1>Function-Space Decoupled Diffusion for Forward and Inverse Modeling in Carbon Capture and Storage</a></a></li><li><a href=#creative-ownership-in-the-age-of-aihttpsarxivorgabs260212270v1 aria-label="Creative Ownership in the Age of AI"><a href=https://arxiv.org/abs/2602.12270v1>Creative Ownership in the Age of AI</a></a></li><li><a href=#cm2-reinforcement-learning-with-checklist-rewards-for-multi-turn-and-multi-step-agentic-tool-usehttpsarxivorgabs260212268v1 aria-label="CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use"><a href=https://arxiv.org/abs/2602.12268v1>CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use</a></a></li><li><a href=#transmit-or-idle-efficient-aoi-optimal-transmission-policy-for-gossiping-receivershttpsarxivorgabs260212264v1 aria-label="Transmit or Idle: Efficient AoI Optimal Transmission Policy for Gossiping Receivers"><a href=https://arxiv.org/abs/2602.12264v1>Transmit or Idle: Efficient AoI Optimal Transmission Policy for Gossiping Receivers</a></a></li><li><a href=#think-like-a-scientist-physics-guided-llm-agent-for-equation-discoveryhttpsarxivorgabs260212259v1 aria-label="Think like a Scientist: Physics-guided LLM Agent for Equation Discovery"><a href=https://arxiv.org/abs/2602.12259v1>Think like a Scientist: Physics-guided LLM Agent for Equation Discovery</a></a></li><li><a href=#oxygen-left-behind-atmospheric-enrichment-due-to-fractionation-in-sub-neptunes-using-boreashttpsarxivorgabs260212201v1 aria-label="Oxygen left behind: Atmospheric Enrichment due to Fractionation in Sub-Neptunes using BOREAS"><a href=https://arxiv.org/abs/2602.12201v1>Oxygen left behind: Atmospheric Enrichment due to Fractionation in Sub-Neptunes using BOREAS</a></a></li><li><a href=#a-rule-based-computational-model-for-gaidhlig-morphologyhttpsarxivorgabs260212132v1 aria-label="A Rule-based Computational Model for Gaidhlig Morphology"><a href=https://arxiv.org/abs/2602.12132v1>A Rule-based Computational Model for Gaidhlig Morphology</a></a></li><li><a href=#meta-sel-efficient-demonstration-selection-for-in-context-learning-via-supervised-meta-learninghttpsarxivorgabs260212123v1 aria-label="Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Meta-Learning"><a href=https://arxiv.org/abs/2602.12123v1>Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Meta-Learning</a></a></li><li><a href=#two-point-functions-in-boundary-loop-modelshttpsarxivorgabs260212000v1 aria-label="Two-point functions in boundary loop models"><a href=https://arxiv.org/abs/2602.12000v1>Two-point functions in boundary loop models</a></a></li><li><a href=#momentum-lms-theory-beyond-stationarity-stability-tracking-and-regrethttpsarxivorgabs260211995v1 aria-label="Momentum LMS Theory beyond Stationarity: Stability, Tracking, and Regret"><a href=https://arxiv.org/abs/2602.11995v1>Momentum LMS Theory beyond Stationarity: Stability, Tracking, and Regret</a></a></li><li><a href=#tuning-optical-properties-of-fto-via-carbonaceous-al2o3-microdot-deposition-by-dc-plasma-sputteringhttpsarxivorgabs260211970v1 aria-label="Tuning Optical Properties of FTO via Carbonaceous Al2O3 Microdot Deposition by DC plasma sputtering"><a href=https://arxiv.org/abs/2602.11970v1>Tuning Optical Properties of FTO via Carbonaceous Al2O3 Microdot Deposition by DC plasma sputtering</a></a></li><li><a href=#learning-to-control-the-iuzawa-net-for-nonsmooth-optimal-control-of-linear-pdeshttpsarxivorgabs260212273v1 aria-label="Learning to Control: The iUzawa-Net for Nonsmooth Optimal Control of Linear PDEs"><a href=https://arxiv.org/abs/2602.12273v1>Learning to Control: The iUzawa-Net for Nonsmooth Optimal Control of Linear PDEs</a></a></li><li><a href=#kagome-edge-states-under-lattice-termination-spin-orbit-coupling-and-magnetic-orderhttpsarxivorgabs260212223v1 aria-label="Kagome edge states under lattice termination, spin-orbit coupling, and magnetic order"><a href=https://arxiv.org/abs/2602.12223v1>Kagome edge states under lattice termination, spin-orbit coupling, and magnetic order</a></a></li><li><a href=#quantum-coherent-thermodynamics-leaf-typicality-via-minimum-variance-foliationhttpsarxivorgabs260212212v1 aria-label="Quantum-Coherent Thermodynamics: Leaf Typicality via Minimum-Variance Foliation"><a href=https://arxiv.org/abs/2602.12212v1>Quantum-Coherent Thermodynamics: Leaf Typicality via Minimum-Variance Foliation</a></a></li><li><a href=#benchmarking-vision-language-models-for-french-pdf-to-markdown-conversionhttpsarxivorgabs260211960v1 aria-label="Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion"><a href=https://arxiv.org/abs/2602.11960v1>Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion</a></a></li><li><a href=#thinking-with-drafting-optical-decompression-via-logical-reconstructionhttpsarxivorgabs260211731v1 aria-label="Thinking with Drafting: Optical Decompression via Logical Reconstruction"><a href=https://arxiv.org/abs/2602.11731v1>Thinking with Drafting: Optical Decompression via Logical Reconstruction</a></a></li><li><a href=#do-mllms-really-understand-space-a-mathematical-reasoning-evaluationhttpsarxivorgabs260211635v1 aria-label="Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation"><a href=https://arxiv.org/abs/2602.11635v1>Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation</a></a></li><li><a href=#when-visibility-outpaces-verification-delayed-verification-and-narrative-lock-in-in-agentic-ai-discoursehttpsarxivorgabs260211412v1 aria-label="When Visibility Outpaces Verification: Delayed Verification and Narrative Lock-in in Agentic AI Discourse"><a href=https://arxiv.org/abs/2602.11412v1>When Visibility Outpaces Verification: Delayed Verification and Narrative Lock-in in Agentic AI Discourse</a></a></li><li><a href=#bizarre-love-triangle-generative-ai-art-and-kitschhttpsarxivorgabs260211353v1 aria-label="Bizarre Love Triangle: Generative AI, Art, and Kitsch"><a href=https://arxiv.org/abs/2602.11353v1>Bizarre Love Triangle: Generative AI, Art, and Kitsch</a></a></li><li><a href=#repulsive-gravitational-force-as-a-witness-of-the-quantum-nature-of-gravityhttpsarxivorgabs260212266v1 aria-label="Repulsive Gravitational Force as a Witness of the Quantum Nature of Gravity"><a href=https://arxiv.org/abs/2602.12266v1>Repulsive Gravitational Force as a Witness of the Quantum Nature of Gravity</a></a></li><li><a href=#post-measurement-states-are-very-useful-for-measurement-discriminationhttpsarxivorgabs260212258v1 aria-label="Post-measurement states are (very) useful for measurement discrimination"><a href=https://arxiv.org/abs/2602.12258v1>Post-measurement states are (very) useful for measurement discrimination</a></a></li><li><a href=#sorry-i-didnt-catch-that-how-speech-models-miss-what-matters-mosthttpsarxivorgabs260212249v1 aria-label="&ldquo;Sorry, I Didn&rsquo;t Catch That&rdquo;: How Speech Models Miss What Matters Most"><a href=https://arxiv.org/abs/2602.12249v1>&ldquo;Sorry, I Didn&rsquo;t Catch That&rdquo;: How Speech Models Miss What Matters Most</a></a></li><li><a href=#the-observer-effect-in-world-models-invasive-adaptation-corrupts-latent-physicshttpsarxivorgabs260212218v1 aria-label="The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics"><a href=https://arxiv.org/abs/2602.12218v1>The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics</a></a></li><li><a href=#ubiquitous-yet-forgotten-broad-absorptions-in-the-optical-spectra-of-low-mass-x-ray-binarieshttpsarxivorgabs260212282v1 aria-label="Ubiquitous yet forgotten: broad absorptions in the optical spectra of low-mass X-ray binaries"><a href=https://arxiv.org/abs/2602.12282v1>Ubiquitous yet forgotten: broad absorptions in the optical spectra of low-mass X-ray binaries</a></a></li><li><a href=#is-online-linear-optimization-sufficient-for-strategic-robustnesshttpsarxivorgabs260212253v1 aria-label="Is Online Linear Optimization Sufficient for Strategic Robustness?"><a href=https://arxiv.org/abs/2602.12253v1>Is Online Linear Optimization Sufficient for Strategic Robustness?</a></a></li><li><a href=#community-concealment-from-unsupervised-graph-learning-based-clusteringhttpsarxivorgabs260212250v1 aria-label="Community Concealment from Unsupervised Graph Learning-Based Clustering"><a href=https://arxiv.org/abs/2602.12250v1>Community Concealment from Unsupervised Graph Learning-Based Clustering</a></a></li><li><a href=#hilberts-program-and-infinityhttpsarxivorgabs260212131v1 aria-label="Hilbert&rsquo;s Program and Infinity"><a href=https://arxiv.org/abs/2602.12131v1>Hilbert&rsquo;s Program and Infinity</a></a></li><li><a href=#unravelling-abstract-cyclic-proofs-into-proofs-by-inductionhttpsarxivorgabs260212054v1 aria-label="Unravelling Abstract Cyclic Proofs into Proofs by Induction"><a href=https://arxiv.org/abs/2602.12054v1>Unravelling Abstract Cyclic Proofs into Proofs by Induction</a></a></li><li><a href=#lacy-what-small-language-models-can-and-should-learn-is-not-just-a-question-of-losshttpsarxivorgabs260212005v1 aria-label="LaCy: What Small Language Models Can and Should Learn is Not Just a Question of Loss"><a href=https://arxiv.org/abs/2602.12005v1>LaCy: What Small Language Models Can and Should Learn is Not Just a Question of Loss</a></a></li><li><a href=#leveraging-llms-to-support-co-evolution-between-definitions-and-instances-of-textual-dsls-a-systematic-evaluationhttpsarxivorgabs260211904v1 aria-label="Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs: A Systematic Evaluation"><a href=https://arxiv.org/abs/2602.11904v1>Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs: A Systematic Evaluation</a></a></li><li><a href=#llm-based-triplet-extraction-from-financial-reportshttpsarxivorgabs260211886v1 aria-label="LLM-based Triplet Extraction from Financial Reports"><a href=https://arxiv.org/abs/2602.11886v1>LLM-based Triplet Extraction from Financial Reports</a></a></li></ul></li><li><a href=#-psycholinguistics aria-label="üîç psycholinguistics">üîç psycholinguistics</a><ul><li><a href=#half-plane-non-coexistence-without-fkghttpsarxivorgabs260212261v1 aria-label="Half-plane non-coexistence without FKG"><a href=https://arxiv.org/abs/2602.12261v1>Half-plane non-coexistence without FKG</a></a></li><li><a href=#on-the-implicit-regularization-of-langevin-dynamics-with-projected-noisehttpsarxivorgabs260212257v1 aria-label="On the implicit regularization of Langevin dynamics with projected noise"><a href=https://arxiv.org/abs/2602.12257v1>On the implicit regularization of Langevin dynamics with projected noise</a></a></li><li><a href=#contention-resolution-with-and-without-a-global-clockhttpsarxivorgabs260212070v1 aria-label="Contention Resolution, With and Without a Global Clock"><a href=https://arxiv.org/abs/2602.12070v1>Contention Resolution, With and Without a Global Clock</a></a></li><li><a href=#tracer-trajectory-risk-aggregation-for-critical-episodes-in-agentic-reasoninghttpsarxivorgabs260211409v1 aria-label="TRACER: Trajectory Risk Aggregation for Critical Episodes in Agentic Reasoning"><a href=https://arxiv.org/abs/2602.11409v1>TRACER: Trajectory Risk Aggregation for Critical Episodes in Agentic Reasoning</a></a></li><li><a href=#block-stacking-airplane-refueling-and-robust-appointment-schedulinghttpsarxivorgabs260211366v1 aria-label="Block Stacking, Airplane Refueling, and Robust Appointment Scheduling"><a href=https://arxiv.org/abs/2602.11366v1>Block Stacking, Airplane Refueling, and Robust Appointment Scheduling</a></a></li><li><a href=#interpolation-and-prewar-postwar-output-volatility-and-shock-persistence-debate-a-closer-look-and-new-resultshttpsarxivorgabs260211334v1 aria-label="Interpolation and Prewar-Postwar Output Volatility and Shock-Persistence Debate: A Closer Look and New Results"><a href=https://arxiv.org/abs/2602.11334v1>Interpolation and Prewar-Postwar Output Volatility and Shock-Persistence Debate: A Closer Look and New Results</a></a></li><li><a href=#reionization-bubbles-from-real-space-cross-correlations-of-line-intensity-mapshttpsarxivorgabs260212277v1 aria-label="Reionization Bubbles from Real-Space Cross Correlations of Line Intensity Maps"><a href=https://arxiv.org/abs/2602.12277v1>Reionization Bubbles from Real-Space Cross Correlations of Line Intensity Maps</a></a></li><li><a href=#moonshine-v2-ergodic-streaming-encoder-asr-for-latency-critical-speech-applicationshttpsarxivorgabs260212241v1 aria-label="Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications"><a href=https://arxiv.org/abs/2602.12241v1>Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications</a></a></li><li><a href=#extending-the-cosmological-collider-new-scaling-regimes-and-constraints-from-bosshttpsarxivorgabs260212232v1 aria-label="Extending the Cosmological Collider: New Scaling Regimes and Constraints from BOSS"><a href=https://arxiv.org/abs/2602.12232v1>Extending the Cosmological Collider: New Scaling Regimes and Constraints from BOSS</a></a></li><li><a href=#virena-virtual-arena-for-research-education-and-democratic-innovationhttpsarxivorgabs260212207v1 aria-label="VIRENA: Virtual Arena for Research, Education, and Democratic Innovation"><a href=https://arxiv.org/abs/2602.12207v1>VIRENA: Virtual Arena for Research, Education, and Democratic Innovation</a></a></li><li><a href=#making-the-complete-openaire-citation-graph-easily-accessible-through-compact-data-representationhttpsarxivorgabs260212206v1 aria-label="Making the complete OpenAIRE citation graph easily accessible through compact data representation"><a href=https://arxiv.org/abs/2602.12206v1>Making the complete OpenAIRE citation graph easily accessible through compact data representation</a></a></li><li><a href=#6g-empowering-future-robotics-a-vision-for-next-generation-autonomous-systemshttpsarxivorgabs260212246v1 aria-label="6G Empowering Future Robotics: A Vision for Next-Generation Autonomous Systems"><a href=https://arxiv.org/abs/2602.12246v1>6G Empowering Future Robotics: A Vision for Next-Generation Autonomous Systems</a></a></li><li><a href=#gpt-4o-lacks-core-features-of-theory-of-mindhttpsarxivorgabs260212150v1 aria-label="GPT-4o Lacks Core Features of Theory of Mind"><a href=https://arxiv.org/abs/2602.12150v1>GPT-4o Lacks Core Features of Theory of Mind</a></a></li></ul></li><li><a href=#-llm aria-label="üîç llm">üîç llm</a><ul><li><a href=#certification-of-linear-optical-quantum-state-preparationhttpsarxivorgabs260212269v1 aria-label="Certification of linear optical quantum state preparation"><a href=https://arxiv.org/abs/2602.12269v1>Certification of linear optical quantum state preparation</a></a></li><li><a href=#self-supervised-learning-via-flow-guided-neural-operator-on-time-series-datahttpsarxivorgabs260212267v1 aria-label="Self-Supervised Learning via Flow-Guided Neural Operator on Time-Series Data"><a href=https://arxiv.org/abs/2602.12267v1>Self-Supervised Learning via Flow-Guided Neural Operator on Time-Series Data</a></a></li><li><a href=#a-stochastic-cluster-expansion-for-electronic-correlation-in-large-systemshttpsarxivorgabs260212254v1 aria-label="A Stochastic Cluster Expansion for Electronic Correlation in Large Systems"><a href=https://arxiv.org/abs/2602.12254v1>A Stochastic Cluster Expansion for Electronic Correlation in Large Systems</a></a></li><li><a href=#intrinsic-energy-joint-embedding-predictive-architectures-induce-quasimetric-spaceshttpsarxivorgabs260212245v1 aria-label="Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces"><a href=https://arxiv.org/abs/2602.12245v1>Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces</a></a></li></ul></li><li><a href=#-neuroscience aria-label="üîç neuroscience">üîç neuroscience</a><ul><li><a href=#coherent-perfect-tunneling-at-exceptional-points-via-directional-degeneracyhttpsarxivorgabs260212161v1 aria-label="Coherent Perfect Tunneling at Exceptional Points via Directional Degeneracy"><a href=https://arxiv.org/abs/2602.12161v1>Coherent Perfect Tunneling at Exceptional Points via Directional Degeneracy</a></a></li><li><a href=#realization-of-a-cavity-coupled-rydberg-arrayhttpsarxivorgabs260212152v1 aria-label="Realization of a cavity-coupled Rydberg array"><a href=https://arxiv.org/abs/2602.12152v1>Realization of a cavity-coupled Rydberg array</a></a></li><li><a href=#choose-your-agent-tradeoffs-in-adopting-ai-advisors-coaches-and-delegates-in-multi-party-negotiationhttpsarxivorgabs260212089v1 aria-label="Choose Your Agent: Tradeoffs in Adopting AI Advisors, Coaches, and Delegates in Multi-Party Negotiation"><a href=https://arxiv.org/abs/2602.12089v1>Choose Your Agent: Tradeoffs in Adopting AI Advisors, Coaches, and Delegates in Multi-Party Negotiation</a></a></li><li><a href=#diver-a-robust-text-to-sql-system-with-dynamic-interactive-value-linking-and-evidence-reasoninghttpsarxivorgabs260212064v1 aria-label="DIVER: A Robust Text-to-SQL System with Dynamic Interactive Value Linking and Evidence Reasoning"><a href=https://arxiv.org/abs/2602.12064v1>DIVER: A Robust Text-to-SQL System with Dynamic Interactive Value Linking and Evidence Reasoning</a></a></li></ul></li><li><a href=#-data_resources aria-label="üîç data_resources">üîç data_resources</a><ul><li><a href=#towards-on-policy-sft-distribution-discriminant-theory-and-its-applications-in-llm-traininghttpsarxivorgabs260212222v1 aria-label="Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training"><a href=https://arxiv.org/abs/2602.12222v1>Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training</a></a></li><li><a href=#sageo-arena-a-realistic-environment-for-evaluating-search-augmented-generative-engine-optimizationhttpsarxivorgabs260212187v1 aria-label="SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engine Optimization"><a href=https://arxiv.org/abs/2602.12187v1>SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engine Optimization</a></a></li><li><a href=#empirical-gaussian-processeshttpsarxivorgabs260212082v1 aria-label="Empirical Gaussian Processes"><a href=https://arxiv.org/abs/2602.12082v1>Empirical Gaussian Processes</a></a></li><li><a href=#sci-coe-co-evolving-scientific-reasoning-llms-via-geometric-consensus-with-sparse-supervisionhttpsarxivorgabs260212164v1 aria-label="Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision"><a href=https://arxiv.org/abs/2602.12164v1>Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision</a></a></li><li><a href=#targeted-syntactic-evaluation-of-language-models-on-georgian-case-alignmenthttpsarxivorgabs260210661v1 aria-label="Targeted Syntactic Evaluation of Language Models on Georgian Case Alignment"><a href=https://arxiv.org/abs/2602.10661v1>Targeted Syntactic Evaluation of Language Models on Georgian Case Alignment</a></a></li><li><a href=#federated-gaussian-process-learning-via-pseudo-representations-for-large-scale-multi-robot-systemshttpsarxivorgabs260212243v1 aria-label="Federated Gaussian Process Learning via Pseudo-Representations for Large-Scale Multi-Robot Systems"><a href=https://arxiv.org/abs/2602.12243v1>Federated Gaussian Process Learning via Pseudo-Representations for Large-Scale Multi-Robot Systems</a></a></li></ul></li><li><a href=#-emotion_language aria-label="üîç emotion_language">üîç emotion_language</a><ul><li><a href=#energy-aware-spike-budgeting-for-continual-learning-in-spiking-neural-networks-for-neuromorphic-visionhttpsarxivorgabs260212236v1 aria-label="Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision"><a href=https://arxiv.org/abs/2602.12236v1>Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision</a></a></li><li><a href=#waveformer-wavelet-embedding-transformer-for-biomedical-signalshttpsarxivorgabs260212189v1 aria-label="WaveFormer: Wavelet Embedding Transformer for Biomedical Signals"><a href=https://arxiv.org/abs/2602.12189v1>WaveFormer: Wavelet Embedding Transformer for Biomedical Signals</a></a></li><li><a href=#unknown-attack-detection-in-iot-networks-using-large-language-models-a-robust-data-efficient-approachhttpsarxivorgabs260212183v1 aria-label="Unknown Attack Detection in IoT Networks using Large Language Models: A Robust, Data-efficient Approach"><a href=https://arxiv.org/abs/2602.12183v1>Unknown Attack Detection in IoT Networks using Large Language Models: A Robust, Data-efficient Approach</a></a></li><li><a href=#local-integrable-symmetries-of-diffietieshttpsarxivorgabs260212103v1 aria-label="Local Integrable Symmetries of Diffieties"><a href=https://arxiv.org/abs/2602.12103v1>Local Integrable Symmetries of Diffieties</a></a></li><li><a href=#legitimate-overrides-in-decentralized-protocolshttpsarxivorgabs260212260v1 aria-label="Legitimate Overrides in Decentralized Protocols"><a href=https://arxiv.org/abs/2602.12260v1>Legitimate Overrides in Decentralized Protocols</a></a></li><li><a href=#bayesian-inference-for-the-automultinomial-model-with-an-application-to-landcover-datahttpsarxivorgabs260212216v1 aria-label="Bayesian inference for the automultinomial model with an application to landcover data"><a href=https://arxiv.org/abs/2602.12216v1>Bayesian inference for the automultinomial model with an application to landcover data</a></a></li><li><a href=#equivalent-circuit-modeling-of-grid-forming-inverters-in-sub-transient-time-framehttpsarxivorgabs260212202v1 aria-label="Equivalent Circuit Modeling of Grid-Forming Inverters in (Sub)-Transient Time-Frame"><a href=https://arxiv.org/abs/2602.12202v1>Equivalent Circuit Modeling of Grid-Forming Inverters in (Sub)-Transient Time-Frame</a></a></li><li><a href=#how-sampling-shapes-llm-alignment-from-one-shot-optima-to-iterative-dynamicshttpsarxivorgabs260212180v1 aria-label="How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics"><a href=https://arxiv.org/abs/2602.12180v1>How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics</a></a></li><li><a href=#the-dark-side-of-the-moon-listening-to-scalar-induced-gravitational-waveshttpsarxivorgabs260212252v1 aria-label="The Dark Side of the Moon: Listening to Scalar-Induced Gravitational Waves"><a href=https://arxiv.org/abs/2602.12252v1>The Dark Side of the Moon: Listening to Scalar-Induced Gravitational Waves</a></a></li><li><a href=#phase-estimation-from-amplitude-collapse-in-correlated-matter-wave-interferencehttpsarxivorgabs260212227v1 aria-label="Phase Estimation from Amplitude Collapse in Correlated Matter-Wave Interference"><a href=https://arxiv.org/abs/2602.12227v1>Phase Estimation from Amplitude Collapse in Correlated Matter-Wave Interference</a></a></li><li><a href=#anonymous-contractshttpsarxivorgabs260212118v1 aria-label="Anonymous Contracts"><a href=https://arxiv.org/abs/2602.12118v1>Anonymous Contracts</a></a></li><li><a href=#systematic-operator-construction-for-non-relativistic-effective-field-theories-hilbert-series-versus-young-tensorhttpsarxivorgabs260212263v1 aria-label="Systematic Operator Construction for Non-relativistic Effective Field Theories: Hilbert Series versus Young Tensor"><a href=https://arxiv.org/abs/2602.12263v1>Systematic Operator Construction for Non-relativistic Effective Field Theories: Hilbert Series versus Young Tensor</a></a></li></ul></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h1 id=weekly-paper-notes>Weekly Paper Notes<a hidden class=anchor aria-hidden=true href=#weekly-paper-notes>#</a></h1><h2 id=-multilingual>üîç multilingual<a hidden class=anchor aria-hidden=true href=#-multilingual>#</a></h2><h3 id=a-technical-curriculum-on-language-oriented-artificial-intelligence-in-translation-and-specialised-communicationhttpsarxivorgabs260212251v1><a href=https://arxiv.org/abs/2602.12251v1>A technical curriculum on language-oriented artificial intelligence in translation and specialised communication</a><a hidden class=anchor aria-hidden=true href=#a-technical-curriculum-on-language-oriented-artificial-intelligence-in-translation-and-specialised-communicationhttpsarxivorgabs260212251v1>#</a></h3><p><strong>Authors:</strong> Ralph Kr√ºger
<strong>Venue:</strong> arXiv (2026)</p><p>This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&amp;T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12251v1">üìÑ Download PDF</a></p><hr><h3 id=statistical-parsing-for-logical-information-retrievalhttpsarxivorgabs260212170v1><a href=https://arxiv.org/abs/2602.12170v1>Statistical Parsing for Logical Information Retrieval</a><a hidden class=anchor aria-hidden=true href=#statistical-parsing-for-logical-information-retrievalhttpsarxivorgabs260212170v1>#</a></h3><p><strong>Authors:</strong> Greg Coppola
<strong>Venue:</strong> arXiv (2026)</p><p>In previous work (Coppola, 2024) we introduced the Quantified Boolean Bayesian Network (QBBN), a logical graphical model that implements the forward fragment of natural deduction (Prawitz, 1965) as a probabilistic factor graph. That work left two gaps: no negation/backward reasoning, and no parser for natural language.
This paper addresses both gaps across inference, semantics, and syntax. For inference, we extend the QBBN with NEG factors enforcing P(x) + P(neg x) = 1, enabling contrapositive reasoning (modus tollens) via backward lambda messages, completing Prawitz&rsquo;s simple elimination rules. The engine handles 44/44 test cases spanning 22 reasoning patterns. For semantics, we present a typed logical language with role-labeled predicates, modal quantifiers, and three tiers of expressiveness following Prawitz: first-order quantification, propositions as arguments, and predicate quantification via lambda abstraction. For syntax, we present a typed slot grammar that deterministically compiles sentences to logical form (33/33 correct, zero ambiguity). LLMs handle disambiguation (95% PP attachment accuracy) but cannot produce structured parses directly (12.4% UAS), confirming grammars are necessary. The architecture: LLM preprocesses, grammar parses, LLM reranks, QBBN infers.
We argue this reconciles formal semantics with Sutton&rsquo;s &ldquo;bitter lesson&rdquo; (2019): LLMs eliminate the annotation bottleneck that killed formal NLP, serving as annotator while the QBBN serves as verifier. Code: <a href=https://github.com/gregorycoppola/world>https://github.com/gregorycoppola/world</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12170v1">üìÑ Download PDF</a></p><hr><h3 id=citilink-minutes-a-multilayer-annotated-dataset-of-municipal-meeting-minuteshttpsarxivorgabs260212137v1><a href=https://arxiv.org/abs/2602.12137v1>CitiLink-Minutes: A Multilayer Annotated Dataset of Municipal Meeting Minutes</a><a hidden class=anchor aria-hidden=true href=#citilink-minutes-a-multilayer-annotated-dataset-of-municipal-meeting-minuteshttpsarxivorgabs260212137v1>#</a></h3><p><strong>Authors:</strong> Ricardo Campos, Ana Filipa Pacheco, Ana Lu√≠sa Fernandes, In√™s Cantante, Rute Rebou√ßas, Lu√≠s Filipe Cunha, Jos√© Miguel Isidro, Jos√© Pedro Evans, Miguel Marques, Rodrigo Batista, Evelin Amorim, Al√≠pio Jorge, Nuno Guimar√£es, S√©rgio Nunes, Ant√≥nio Leal, Purifica√ß√£o Silvano
<strong>Venue:</strong> arXiv (2026)</p><p>City councils play a crucial role in local governance, directly influencing citizens&rsquo; daily lives through decisions made during municipal meetings. These deliberations are formally documented in meeting minutes, which serve as official records of discussions, decisions, and voting outcomes. Despite their importance, municipal meeting records have received little attention in Information Retrieval (IR) and Natural Language Processing (NLP), largely due to the lack of annotated datasets, which ultimately limit the development of computational models. To address this gap, we introduce CitiLink-Minutes, a multilayer dataset of 120 European Portuguese municipal meeting minutes from six municipalities. Unlike prior annotated datasets of parliamentary or video records, CitiLink-Minutes provides multilayer annotations and structured linkage of official written minutes. The dataset contains over one million tokens, with all personal identifiers de-identified. Each minute was manually annotated by two trained annotators and curated by an experienced linguist across three complementary dimensions: (1) metadata, (2) subjects of discussion, and (3) voting outcomes, totaling over 38,000 individual annotations. Released under FAIR principles and accompanied by baseline results on metadata extraction, topic classification, and vote labeling, CitiLink-Minutes demonstrates its potential for downstream NLP and IR tasks, while promoting transparent access to municipal decisions.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12137v1">üìÑ Download PDF</a></p><hr><h3 id=dhplt-large-scale-multilingual-diachronic-corpora-and-word-representations-for-semantic-change-modellinghttpsarxivorgabs260211968v1><a href=https://arxiv.org/abs/2602.11968v1>DHPLT: large-scale multilingual diachronic corpora and word representations for semantic change modelling</a><a hidden class=anchor aria-hidden=true href=#dhplt-large-scale-multilingual-diachronic-corpora-and-word-representations-for-semantic-change-modellinghttpsarxivorgabs260211968v1>#</a></h3><p><strong>Authors:</strong> Mariia Fedorova, Andrey Kutuzov, Khonzoda Umarova
<strong>Venue:</strong> arXiv (2026)</p><p>In this resource paper, we present DHPLT, an open collection of diachronic corpora in 41 diverse languages. DHPLT is based on the web-crawled HPLT datasets; we use web crawl timestamps as the approximate signal of document creation time. The collection covers three time periods: 2011-2015, 2020-2021 and 2024-present (1 million documents per time period for each language). We additionally provide pre-computed word type and token embeddings and lexical substitutions for our chosen target words, while at the same time leaving it open for the other researchers to come up with their own target words using the same datasets. DHPLT aims at filling in the current lack of multilingual diachronic corpora for semantic change modelling (beyond a dozen of high-resource languages). It opens the way for a variety of new experimental setups in this field. All the resources described in this paper are available at <a href=https://data.hplt-project.org/three/diachronic/>https://data.hplt-project.org/three/diachronic/</a>, sorted by language.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.11968v1">üìÑ Download PDF</a></p><hr><h3 id=scaling-model-and-data-for-multilingual-machine-translation-with-open-large-language-modelshttpsarxivorgabs260211961v1><a href=https://arxiv.org/abs/2602.11961v1>Scaling Model and Data for Multilingual Machine Translation with Open Large Language Models</a><a hidden class=anchor aria-hidden=true href=#scaling-model-and-data-for-multilingual-machine-translation-with-open-large-language-modelshttpsarxivorgabs260211961v1>#</a></h3><p><strong>Authors:</strong> Yuzhe Shang, Pengzhi Gao, Wei Liu, Jian Luan, Jinsong Su
<strong>Venue:</strong> arXiv (2026)</p><p>Open large language models (LLMs) have demonstrated improving multilingual capabilities in recent years. In this paper, we present a study of open LLMs for multilingual machine translation (MT) across a range of languages, and investigate the effects of model scaling and data scaling when adapting open LLMs to multilingual MT through continual pretraining and instruction finetuning. Based on the Gemma3 model family, we develop MiLMMT-46, which achieves top-tier multilingual translation performance across 46 languages. Extensive experiments show that MiLMMT-46 consistently outperforms recent state-of-the-art (SOTA) models, including Seed-X, HY-MT-1.5, and TranslateGemma, and achieves competitive performance with strong proprietary systems such as Google Translate and Gemini 3 Pro.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.11961v1">üìÑ Download PDF</a></p><hr><h3 id=a-subword-embedding-approach-for-variation-detection-in-luxembourgish-user-commentshttpsarxivorgabs260211795v1><a href=https://arxiv.org/abs/2602.11795v1>A Subword Embedding Approach for Variation Detection in Luxembourgish User Comments</a><a hidden class=anchor aria-hidden=true href=#a-subword-embedding-approach-for-variation-detection-in-luxembourgish-user-commentshttpsarxivorgabs260211795v1>#</a></h3><p><strong>Authors:</strong> Anne-Marie Lutgen, Alistair Plum, Christoph Purschke
<strong>Venue:</strong> arXiv (2026)</p><p>This paper presents an embedding-based approach to detecting variation without relying on prior normalisation or predefined variant lists. The method trains subword embeddings on raw text and groups related forms through combined cosine and n-gram similarity. This allows spelling and morphological diversity to be examined and analysed as linguistic structure rather than treated as noise. Using a large corpus of Luxembourgish user comments, the approach uncovers extensive lexical and orthographic variation that aligns with patterns described in dialectal and sociolinguistic research. The induced families capture systematic correspondences and highlight areas of regional and stylistic differentiation. The procedure does not strictly require manual annotation, but does produce transparent clusters that support both quantitative and qualitative analysis. The results demonstrate that distributional modelling can reveal meaningful patterns of variation even in &lsquo;&rsquo;noisy&rsquo;&rsquo; or low-resource settings, offering a reproducible methodological framework for studying language variety in multilingual and small-language contexts.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.11795v1">üìÑ Download PDF</a></p><hr><h3 id=towards-reliable-machine-translation-scaling-llms-for-critical-error-detection-and-safetyhttpsarxivorgabs260211444v1><a href=https://arxiv.org/abs/2602.11444v1>Towards Reliable Machine Translation: Scaling LLMs for Critical Error Detection and Safety</a><a hidden class=anchor aria-hidden=true href=#towards-reliable-machine-translation-scaling-llms-for-critical-error-detection-and-safetyhttpsarxivorgabs260211444v1>#</a></h3><p><strong>Authors:</strong> Muskaan Chopra, Lorenz Sparrenberg, Rafet Sifa
<strong>Venue:</strong> arXiv (2026)</p><p>Machine Translation (MT) plays a pivotal role in cross-lingual information access, public policy communication, and equitable knowledge dissemination. However, critical meaning errors, such as factual distortions, intent reversals, or biased translations, can undermine the reliability, fairness, and safety of multilingual systems. In this work, we explore the capacity of instruction-tuned Large Language Models (LLMs) to detect such critical errors, evaluating models across a range of parameters using the publicly accessible data sets. Our findings show that model scaling and adaptation strategies (zero-shot, few-shot, fine-tuning) yield consistent improvements, outperforming encoder-only baselines like XLM-R and ModernBERT. We argue that improving critical error detection in MT contributes to safer, more trustworthy, and socially accountable information systems by reducing the risk of disinformation, miscommunication, and linguistic harm, especially in high-stakes or underrepresented contexts. This work positions error detection not merely as a technical challenge, but as a necessary safeguard in the pursuit of just and responsible multilingual AI. The code will be made available at GitHub.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.11444v1">üìÑ Download PDF</a></p><hr><h3 id=on-the-robustness-of-knowledge-editing-for-detoxificationhttpsarxivorgabs260210504v1><a href=https://arxiv.org/abs/2602.10504v1>On the Robustness of Knowledge Editing for Detoxification</a><a hidden class=anchor aria-hidden=true href=#on-the-robustness-of-knowledge-editing-for-detoxificationhttpsarxivorgabs260210504v1>#</a></h3><p><strong>Authors:</strong> Ming Dong, Shiyi Tang, Ziyan Peng, Guanyi Chen, Tingting He
<strong>Venue:</strong> arXiv (2026)</p><p>Knowledge-Editing-based (KE-based) detoxification has emerged as a promising approach for mitigating harmful behaviours in Large Language Models. Existing evaluations, however, largely rely on automatic toxicity classifiers, implicitly assuming that reduced toxicity scores reflect genuine behavioural suppression. In this work, we propose a robustness-oriented evaluation framework for KE-based detoxification that examines its reliability beyond standard classifier-based metrics along three dimensions: optimisation robustness, compositional robustness, and cross-lingual robustness. We identify pseudo-detoxification as a common failure mode, where apparent toxicity reductions arise from degenerate generation behaviours rather than meaningful suppression of unsafe content. We further show that detoxification effectiveness degrades when multiple unsafe behaviours are edited jointly, and that both monolingual and cross-lingual detoxification remain effective only under specific model-method combinations. Overall, our results indicate that KE-based detoxification is robust only for certain models, limited numbers of detoxification objectives, and a subset of languages.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.10504v1">üìÑ Download PDF</a></p><hr><h3 id=from-fusha-to-folk-exploring-cross-lingual-transfer-in-arabic-language-modelshttpsarxivorgabs260209826v1><a href=https://arxiv.org/abs/2602.09826v1>From FusHa to Folk: Exploring Cross-Lingual Transfer in Arabic Language Models</a><a hidden class=anchor aria-hidden=true href=#from-fusha-to-folk-exploring-cross-lingual-transfer-in-arabic-language-modelshttpsarxivorgabs260209826v1>#</a></h3><p><strong>Authors:</strong> Abdulmuizz Khalak, Abderrahmane Issam, Gerasimos Spanakis
<strong>Venue:</strong> arXiv (2026)</p><p>Arabic Language Models (LMs) are pretrained predominately on Modern Standard Arabic (MSA) and are expected to transfer to its dialects. While MSA as the standard written variety is commonly used in formal settings, people speak and write online in various dialects that are spread across the Arab region. This poses limitations for Arabic LMs, since its dialects vary in their similarity to MSA. In this work we study cross-lingual transfer of Arabic models using probing on 3 Natural Language Processing (NLP) Tasks, and representational similarity. Our results indicate that transfer is possible but disproportionate across dialects, which we find to be partially explained by their geographic proximity. Furthermore, we find evidence for negative interference in models trained to support all Arabic dialects. This questions their degree of similarity, and raises concerns for cross-lingual transfer in Arabic models.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.09826v1">üìÑ Download PDF</a></p><hr><h3 id=lemur-a-corpus-for-robust-fine-tuning-of-multilingual-law-embedding-models-for-retrievalhttpsarxivorgabs260209570v1><a href=https://arxiv.org/abs/2602.09570v1>LEMUR: A Corpus for Robust Fine-Tuning of Multilingual Law Embedding Models for Retrieval</a><a hidden class=anchor aria-hidden=true href=#lemur-a-corpus-for-robust-fine-tuning-of-multilingual-law-embedding-models-for-retrievalhttpsarxivorgabs260209570v1>#</a></h3><p><strong>Authors:</strong> Narges Baba Ahmadi, Jan Strich, Martin Semmann, Chris Biemann
<strong>Venue:</strong> arXiv (2026)</p><p>Large language models (LLMs) are increasingly used to access legal information. Yet, their deployment in multilingual legal settings is constrained by unreliable retrieval and the lack of domain-adapted, open-embedding models. In particular, existing multilingual legal corpora are not designed for semantic retrieval, and PDF-based legislative sources introduce substantial noise due to imperfect text extraction. To address these challenges, we introduce LEMUR, a large-scale multilingual corpus of EU environmental legislation constructed from 24,953 official EUR-Lex PDF documents covering 25 languages. We quantify the fidelity of PDF-to-text conversion by measuring lexical consistency against authoritative HTML versions using the Lexical Content Score (LCS). Building on LEMUR, we fine-tune three state-of-the-art multilingual embedding models using contrastive objectives in both monolingual and bilingual settings, reflecting realistic legal-retrieval scenarios. Experiments across low- and high-resource languages demonstrate that legal-domain fine-tuning consistently improves Top-k retrieval accuracy relative to strong baselines, with particularly pronounced gains for low-resource languages. Cross-lingual evaluations show that these improvements transfer to unseen languages, indicating that fine-tuning primarily enhances language-independent, content-level legal representations rather than language-specific cues. We publish code\footnote{\href{https://github.com/nargesbh/eur_lex}{GitHub Repository}} and data\footnote{\href{https://huggingface.co/datasets/G4KMU/LEMUR}{Hugging Face Dataset}}.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.09570v1">üìÑ Download PDF</a></p><hr><h3 id=unsupervised-cross-lingual-part-of-speech-tagging-with-monolingual-corpora-onlyhttpsarxivorgabs260209366v1><a href=https://arxiv.org/abs/2602.09366v1>Unsupervised Cross-Lingual Part-of-Speech Tagging with Monolingual Corpora Only</a><a hidden class=anchor aria-hidden=true href=#unsupervised-cross-lingual-part-of-speech-tagging-with-monolingual-corpora-onlyhttpsarxivorgabs260209366v1>#</a></h3><p><strong>Authors:</strong> Jianyu Zheng
<strong>Venue:</strong> arXiv (2026)</p><p>Due to the scarcity of part-of-speech annotated data, existing studies on low-resource languages typically adopt unsupervised approaches for POS tagging. Among these, POS tag projection with word alignment method transfers POS tags from a high-resource source language to a low-resource target language based on parallel corpora, making it particularly suitable for low-resource language settings. However, this approach relies heavily on parallel corpora, which are often unavailable for many low-resource languages. To overcome this limitation, we propose a fully unsupervised cross-lingual part-of-speech(POS) tagging framework that relies solely on monolingual corpora by leveraging unsupervised neural machine translation(UNMT) system. This UNMT system first translates sentences from a high-resource language into a low-resource one, thereby constructing pseudo-parallel sentence pairs. Then, we train a POS tagger for the target language following the standard projection procedure based on word alignments. Moreover, we propose a multi-source projection technique to calibrate the projected POS tags on the target side, enhancing to train a more effective POS tagger. We evaluate our framework on 28 language pairs, covering four source languages (English, German, Spanish and French) and seven target languages (Afrikaans, Basque, Finnis, Indonesian, Lithuanian, Portuguese and Turkish). Experimental results show that our method can achieve performance comparable to the baseline cross-lingual POS tagger with parallel sentence pairs, and even exceeds it for certain target languages. Furthermore, our proposed multi-source projection technique further boosts performance, yielding an average improvement of 1.3% over previous methods.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.09366v1">üìÑ Download PDF</a></p><hr><h3 id=sinfos-a-parallel-dataset-for-translating-sinhala-figures-of-speechhttpsarxivorgabs260209866v1><a href=https://arxiv.org/abs/2602.09866v1>SinFoS: A Parallel Dataset for Translating Sinhala Figures of Speech</a><a hidden class=anchor aria-hidden=true href=#sinfos-a-parallel-dataset-for-translating-sinhala-figures-of-speechhttpsarxivorgabs260209866v1>#</a></h3><p><strong>Authors:</strong> Johan Sofalas, Dilushri Pavithra, Nevidu Jayatilleke, Ruvan Weerasinghe
<strong>Venue:</strong> arXiv (2026)</p><p>Figures of Speech (FoS) consist of multi-word phrases that are deeply intertwined with culture. While Neural Machine Translation (NMT) performs relatively well with the figurative expressions of high-resource languages, it often faces challenges when dealing with low-resource languages like Sinhala due to limited available data. To address this limitation, we introduce a corpus of 2,344 Sinhala figures of speech with cultural and cross-lingual annotations. We examine this dataset to classify the cultural origins of the figures of speech and to identify their cross-lingual equivalents. Additionally, we have developed a binary classifier to differentiate between two types of FOS in the dataset, achieving an accuracy rate of approximately 92%. We also evaluate the performance of existing LLMs on this dataset. Our findings reveal significant shortcomings in the current capabilities of LLMs, as these models often struggle to accurately convey idiomatic meanings. By making this dataset publicly available, we offer a crucial benchmark for future research in low-resource NLP and culturally aware machine translation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.09866v1">üìÑ Download PDF</a></p><hr><h3 id=do-not-treat-code-as-natural-language-implications-for-repository-level-code-generation-and-beyondhttpsarxivorgabs260211671v1><a href=https://arxiv.org/abs/2602.11671v1>Do Not Treat Code as Natural Language: Implications for Repository-Level Code Generation and Beyond</a><a hidden class=anchor aria-hidden=true href=#do-not-treat-code-as-natural-language-implications-for-repository-level-code-generation-and-beyondhttpsarxivorgabs260211671v1>#</a></h3><p><strong>Authors:</strong> Minh Le-Anh, Huyen Nguyen, Khanh An Tran, Nam Le Hai, Linh Ngo Van, Nghi D. Q. Bui, Bach Le
<strong>Venue:</strong> arXiv (2026)</p><p>Large language models for code (CodeLLMs) have demonstrated remarkable success in standalone code completion and generation, sometimes even surpassing human performance, yet their effectiveness diminishes in repository-level settings where cross-file dependencies and structural context are essential. Existing Retrieval-Augmented Generation (RAG) approaches often borrow strategies from NLP, relying on chunking-based indexing and similarity-based retrieval. Chunking results in the loss of coherence between code units and overlooks structural relationships, while similarity-driven methods frequently miss functionally relevant dependencies such as helper functions, classes, or global variables. To address these limitations, we present Hydra, a repository-level code generation framework that treats code as structured code rather than natural language. Our approach introduces (i) a structure-aware indexing strategy that represents repositories as hierarchical trees of functions, classes, and variables, preserving code structure and dependencies, (ii) a lightweight dependency-aware retriever (DAR) that explicitly identifies and retrieves the true dependencies required by a target function, and (iii) a hybrid retrieval mechanism that combines DAR with similarity-based retrieval to provide both essential building blocks and practical usage examples. Extensive experiments on the challenging DevEval and RepoExec benchmarks, both requiring function implementation from real-world repositories with complex large repository context, show that Hydra achieves state-of-the-art performance across open- and closed-source CodeLLMs. Notably, our method establishes a new state of the art in repository-level code generation, surpassing strongest baseline by over 5% in Pass@1 and even enabling smaller models to match or exceed the performance of much larger ones that rely on existing retrievers.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.11671v1">üìÑ Download PDF</a></p><hr><h3 id=a-human-centric-framework-for-data-attribution-in-large-language-modelshttpsarxivorgabs260210995v1><a href=https://arxiv.org/abs/2602.10995v1>A Human-Centric Framework for Data Attribution in Large Language Models</a><a hidden class=anchor aria-hidden=true href=#a-human-centric-framework-for-data-attribution-in-large-language-modelshttpsarxivorgabs260210995v1>#</a></h3><p><strong>Authors:</strong> Amelie W√ºhrl, Mattes Ruckdeschel, Kyle Lo, Anna Rogers
<strong>Venue:</strong> arXiv (2026)</p><p>In the current Large Language Model (LLM) ecosystem, creators have little agency over how their data is used, and LLM users may find themselves unknowingly plagiarizing existing sources. Attribution of LLM-generated text to LLM input data could help with these challenges, but so far we have more questions than answers: what elements of LLM outputs require attribution, what goals should it serve, how should it be implemented?
We contribute a human-centric data attribution framework, which situates the attribution problem within the broader data economy. Specific use cases for attribution, such as creative writing assistance or fact-checking, can be specified via a set of parameters (including stakeholder objectives and implementation criteria). These criteria are up for negotiation by the relevant stakeholder groups: creators, LLM users, and their intermediaries (publishers, platforms, AI companies). The outcome of domain-specific negotiations can be implemented and tested for whether the stakeholder goals are achieved. The proposed approach provides a bridge between methodological NLP work on data attribution, governance work on policy interventions, and economic analysis of creator incentives for a sustainable equilibrium in the data economy.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.10995v1">üìÑ Download PDF</a></p><hr><h3 id=i-can-tell-whether-you-are-a-native-hawl√™ri-speaker-how-ann-cnn-and-rnn-perform-in-nli-native-language-identificationhttpsarxivorgabs260210832v1><a href=https://arxiv.org/abs/2602.10832v1>I can tell whether you are a Native Hawl√™ri Speaker! How ANN, CNN, and RNN perform in NLI-Native Language Identification</a><a hidden class=anchor aria-hidden=true href=#i-can-tell-whether-you-are-a-native-hawl√™ri-speaker-how-ann-cnn-and-rnn-perform-in-nli-native-language-identificationhttpsarxivorgabs260210832v1>#</a></h3><p><strong>Authors:</strong> Hardi Garari, Hossein Hassani
<strong>Venue:</strong> arXiv (2026)</p><p>Native Language Identification (NLI) is a task in Natural Language Processing (NLP) that typically determines the native language of an author through their writing or a speaker through their speaking. It has various applications in different areas, such as forensic linguistics and general linguistics studies. Although considerable research has been conducted on NLI regarding two different languages, such as English and German, the literature indicates a significant gap regarding NLI for dialects and subdialects. The gap becomes wider in less-resourced languages such as Kurdish. This research focuses on NLI within the context of a subdialect of Sorani (Central) Kurdish. It aims to investigate the NLI for Hewl√™ri, a subdialect spoken in Hewl√™r (Erbil), the Capital of the Kurdistan Region of Iraq. We collected about 24 hours of speech by recording interviews with 40 native or non-native Hewl√™ri speakers, 17 female and 23 male. We created three Neural Network-based models: Artificial Neural Network (ANN), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), which were evaluated through 66 experiments, covering various time-frames from 1 to 60 seconds, undersampling, oversampling, and cross-validation. The RNN model showed the highest accuracy of 95.92% for 5-second audio segmentation, using an 80:10:10 data splitting scheme. The created dataset is the first speech dataset for NLI on the Hewl√™ri subdialect in the Sorani Kurdish dialect, which can be of benefit to various research areas.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.10832v1">üìÑ Download PDF</a></p><hr><h3 id=autonomous-continual-learning-of-computer-use-agents-for-environment-adaptationhttpsarxivorgabs260210356v1><a href=https://arxiv.org/abs/2602.10356v1>Autonomous Continual Learning of Computer-Use Agents for Environment Adaptation</a><a hidden class=anchor aria-hidden=true href=#autonomous-continual-learning-of-computer-use-agents-for-environment-adaptationhttpsarxivorgabs260210356v1>#</a></h3><p><strong>Authors:</strong> Tianci Xue, Zeyi Liao, Tianneng Shi, Zilu Wang, Kai Zhang, Dawn Song, Yu Su, Huan Sun
<strong>Venue:</strong> arXiv (2026)</p><p>Real-world digital environments are highly diverse and dynamic. These characteristics cause agents to frequently encounter unseen scenarios and distribution shifts, making continual learning in specific environments essential for computer-use agents (CUAs). However, a key challenge lies in obtaining high-quality and environment-grounded agent data without relying on costly human annotation. In this work, we introduce ACuRL, an Autonomous Curriculum Reinforcement Learning framework that continually adapts agents to specific environments with zero human data. The agent first explores target environments to acquire initial experiences. During subsequent iterative training, a curriculum task generator leverages these experiences together with feedback from the previous iteration to synthesize new tasks tailored for the agent&rsquo;s current capabilities. To provide reliable reward signals, we introduce CUAJudge, a robust automatic evaluator for CUAs that achieves 93% agreement with human judgments. Empirically, our method effectively enables both intra-environment and cross-environment continual learning, yielding 4-22% performance gains without catastrophic forgetting on existing environments. Further analyses show highly sparse updates (e.g., 20% parameters), which helps explain the effective and robust adaptation. Our data and code are available at <a href=https://github.com/OSU-NLP-Group/ACuRL>https://github.com/OSU-NLP-Group/ACuRL</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.10356v1">üìÑ Download PDF</a></p><hr><h3 id=calliope-a-tts-based-narrated-e-book-creator-ensuring-exact-synchronization-privacy-and-layout-fidelityhttpsarxivorgabs260210735v1><a href=https://arxiv.org/abs/2602.10735v1>Calliope: A TTS-based Narrated E-book Creator Ensuring Exact Synchronization, Privacy, and Layout Fidelity</a><a hidden class=anchor aria-hidden=true href=#calliope-a-tts-based-narrated-e-book-creator-ensuring-exact-synchronization-privacy-and-layout-fidelityhttpsarxivorgabs260210735v1>#</a></h3><p><strong>Authors:</strong> Hugo L. Hammer, Vajira Thambawita, P√•l Halvorsen
<strong>Venue:</strong> arXiv (2026)</p><p>A narrated e-book combines synchronized audio with digital text, highlighting the currently spoken word or sentence during playback. This format supports early literacy and assists individuals with reading challenges, while also allowing general readers to seamlessly switch between reading and listening. With the emergence of natural-sounding neural Text-to-Speech (TTS) technology, several commercial services have been developed to leverage these technology for converting standard text e-books into high-quality narrated e-books. However, no open-source solutions currently exist to perform this task. In this paper, we present Calliope, an open-source framework designed to fill this gap. Our method leverages state-of-the-art open-source TTS to convert a text e-book into a narrated e-book in the EPUB 3 Media Overlay format. The method offers several innovative steps: audio timestamps are captured directly during TTS, ensuring exact synchronization between narration and text highlighting; the publisher&rsquo;s original typography, styling, and embedded media are strictly preserved; and the entire pipeline operates offline. This offline capability eliminates recurring API costs, mitigates privacy concerns, and avoids copyright compliance issues associated with cloud-based services. The framework currently supports the state-of-the-art open-source TTS systems XTTS-v2 and Chatterbox. A potential alternative approach involves first generating narration via TTS and subsequently synchronizing it with the text using forced alignment. However, while our method ensures exact synchronization, our experiments show that forced alignment introduces drift between the audio and text highlighting significant enough to degrade the reading experience. Source code and usage instructions are available at <a href=https://github.com/hugohammer/TTS-Narrated-Ebook-Creator.git>https://github.com/hugohammer/TTS-Narrated-Ebook-Creator.git</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.10735v1">üìÑ Download PDF</a></p><hr><h3 id=motivating-reml-via-prediction-error-covariances-in-em-updates-for-linear-mixed-modelshttpsarxivorgabs260209247v1><a href=https://arxiv.org/abs/2602.09247v1>Motivating REML via Prediction-Error Covariances in EM Updates for Linear Mixed Models</a><a hidden class=anchor aria-hidden=true href=#motivating-reml-via-prediction-error-covariances-in-em-updates-for-linear-mixed-modelshttpsarxivorgabs260209247v1>#</a></h3><p><strong>Authors:</strong> Andrew T. Karl
<strong>Venue:</strong> arXiv (2026)</p><p>We present a computational motivation for restricted maximum likelihood (REML) estimation in linear mixed models using an expectation&ndash;maximization (EM) algorithm. At each iteration, maximum likelihood (ML) and REML solve the same mixed-model equations for the best linear unbiased estimator (BLUE) of the fixed effects and the best linear unbiased predictor (BLUP) of the random effects. They differ only in the trace adjustments used in the variance-component updates: ML uses conditional covariances of the random effects given the data, whereas REML uses prediction-error covariances from Henderson&rsquo;s C-matrix, reflecting uncertainty from estimating the fixed effects. Short R code makes this switch explicit, exposes the key matrices for classroom inspection, and reproduces lme4 ML and REML fits.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.09247v1">üìÑ Download PDF</a></p><hr><h3 id=chaos-and-parrondos-paradox-an-overviewhttpsarxivorgabs260208135v1><a href=https://arxiv.org/abs/2602.08135v1>Chaos and Parrondo&rsquo;s paradox: An overview</a><a hidden class=anchor aria-hidden=true href=#chaos-and-parrondos-paradox-an-overviewhttpsarxivorgabs260208135v1>#</a></h3><p><strong>Authors:</strong> Marcelo A. Pires, Erveton P. Pinto, Jose S. C√°novas, Silvio M. Duarte Queir√≥s
<strong>Venue:</strong> arXiv (2026)</p><p>Parrondo&rsquo;s paradox (PP) is a fundamental principle in nonlinear science where the alternation of individually losing strategies leads to a winning outcome. In this topical review, we provide the first systematic panorama of the synergy between PP and chaos. We observe a bidirectional connection between the two areas. The first direction is the translation of PP into the interplay between Order and Chaos through either Chaos + Chaos $\to$ Order (CCO) or Order + Order $\to$ Chaos (OOC). In this vein, many quantifiers, such as Lyapunov Exponents, $Œª$, and entropic measures, are used. Second, we note that chaos can be used to engineer switching protocols that can lead to nontrivial effects in diverse PP cases. Our review clarifies the universality of PP and highlights its robust theoretical and practical applications across several areas of science and technology. Finally, we delineate key open questions, emphasizing the unresolved theoretical limits, the role of high-dimensional maps and continuous flows, and the critical need for more experimental verification of the dynamic PP in chaotic systems. For completeness, we also provide a full Python code that allows the reader to observe the many facets of the PP.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.08135v1">üìÑ Download PDF</a></p><hr><h3 id=scaling-verification-can-be-more-effective-than-scaling-policy-learning-for-vision-language-action-alignmenthttpsarxivorgabs260212281v1><a href=https://arxiv.org/abs/2602.12281v1>Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment</a><a hidden class=anchor aria-hidden=true href=#scaling-verification-can-be-more-effective-than-scaling-policy-learning-for-vision-language-action-alignmenthttpsarxivorgabs260212281v1>#</a></h3><p><strong>Authors:</strong> Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone
<strong>Venue:</strong> arXiv (2026)</p><p>The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the &ldquo;intention-action gap.&rsquo;&rsquo; We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce &ldquo;boot-time compute&rdquo; and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12281v1">üìÑ Download PDF</a></p><hr><h3 id=unit-unified-multimodal-chain-of-thought-test-time-scalinghttpsarxivorgabs260212279v1><a href=https://arxiv.org/abs/2602.12279v1>UniT: Unified Multimodal Chain-of-Thought Test-time Scaling</a><a hidden class=anchor aria-hidden=true href=#unit-unified-multimodal-chain-of-thought-test-time-scalinghttpsarxivorgabs260212279v1>#</a></h3><p><strong>Authors:</strong> Leon Liangyu Chen, Haoyu Ma, Zhipeng Fan, Ziqi Huang, Animesh Sinha, Xiaoliang Dai, Jialiang Wang, Zecheng He, Jianwei Yang, Chunyuan Li, Junzhe Sun, Chu Wang, Serena Yeung-Levy, Felix Juefei-Xu
<strong>Venue:</strong> arXiv (2026)</p><p>Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12279v1">üìÑ Download PDF</a></p><hr><h3 id=attentionretriever-attention-layers-are-secretly-long-document-retrievershttpsarxivorgabs260212278v1><a href=https://arxiv.org/abs/2602.12278v1>AttentionRetriever: Attention Layers are Secretly Long Document Retrievers</a><a hidden class=anchor aria-hidden=true href=#attentionretriever-attention-layers-are-secretly-long-document-retrievershttpsarxivorgabs260212278v1>#</a></h3><p><strong>Authors:</strong> David Jiahao Fu, Lam Thanh Do, Jiayu Li, Kevin Chen-Chuan Chang
<strong>Venue:</strong> arXiv (2026)</p><p>Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12278v1">üìÑ Download PDF</a></p><hr><h3 id=agentic-test-time-scaling-for-webagentshttpsarxivorgabs260212276v1><a href=https://arxiv.org/abs/2602.12276v1>Agentic Test-Time Scaling for WebAgents</a><a hidden class=anchor aria-hidden=true href=#agentic-test-time-scaling-for-webagentshttpsarxivorgabs260212276v1>#</a></h3><p><strong>Authors:</strong> Nicholas Lee, Lutfi Eren Erdogan, Chris Joseph John, Surya Krishnapillai, Michael W. Mahoney, Kurt Keutzer, Amir Gholami
<strong>Venue:</strong> arXiv (2026)</p><p>Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent&rsquo;s own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12276v1">üìÑ Download PDF</a></p><hr><h3 id=on-policy-context-distillation-for-language-modelshttpsarxivorgabs260212275v1><a href=https://arxiv.org/abs/2602.12275v1>On-Policy Context Distillation for Language Models</a><a hidden class=anchor aria-hidden=true href=#on-policy-context-distillation-for-language-modelshttpsarxivorgabs260212275v1>#</a></h3><p><strong>Authors:</strong> Tianzhu Ye, Li Dong, Xun Wu, Shaohan Huang, Furu Wei
<strong>Venue:</strong> arXiv (2026)</p><p>Context distillation enables language models to internalize in-context knowledge into their parameters. In our work, we propose On-Policy Context Distillation (OPCD), a framework that bridges on-policy distillation with context distillation by training a student model on its own generated trajectories while minimizing reverse Kullback-Leibler divergence against a context-conditioned teacher. We demonstrate the effectiveness of OPCD on two important applications: experiential knowledge distillation, where models extract and consolidate transferable knowledge from their historical solution traces, and system prompt distillation, where models internalize beneficial behaviors encoded in optimized prompts. Across mathematical reasoning, text-based games, and domain-specific tasks, OPCD consistently outperforms baseline methods, achieving higher task accuracy while better preserving out-of-distribution capabilities. We further show that OPCD enables effective cross-size distillation, where smaller student models can internalize experiential knowledge from larger teachers.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12275v1">üìÑ Download PDF</a></p><hr><h3 id=t3d-few-step-diffusion-language-models-via-trajectory-self-distillation-with-direct-discriminative-optimizationhttpsarxivorgabs260212262v1><a href=https://arxiv.org/abs/2602.12262v1>T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization</a><a hidden class=anchor aria-hidden=true href=#t3d-few-step-diffusion-language-models-via-trajectory-self-distillation-with-direct-discriminative-optimizationhttpsarxivorgabs260212262v1>#</a></h3><p><strong>Authors:</strong> Tunyu Zhang, Xinxi Zhang, Ligong Han, Haizhou Shi, Xiaoxiao He, Zhuowei Li, Hao Wang, Kai Xu, Akash Srivastava, Hao Wang, Vladimir Pavlovic, Dimitris N. Metaxas
<strong>Venue:</strong> arXiv (2026)</p><p>Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model&rsquo;s own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at <a href=https://github.com/Tyrion58/T3D>https://github.com/Tyrion58/T3D</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12262v1">üìÑ Download PDF</a></p><hr><h3 id=lda-1b-scaling-latent-dynamics-action-model-via-universal-embodied-data-ingestionhttpsarxivorgabs260212215v1><a href=https://arxiv.org/abs/2602.12215v1>LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion</a><a hidden class=anchor aria-hidden=true href=#lda-1b-scaling-latent-dynamics-action-model-via-universal-embodied-data-ingestionhttpsarxivorgabs260212215v1>#</a></h3><p><strong>Authors:</strong> Jiangran Lyu, Kai Liu, Xuheng Zhang, Haoran Liao, Yusen Feng, Wenxuan Zhu, Tingrui Shen, Jiayi Chen, Jiazhao Zhang, Yifei Dong, Wenbo Cui, Senmao Qi, Shuo Wang, Yixin Zheng, Mi Yan, Xuesong Shi, Haoran Li, Dongbin Zhao, Ming-Yu Liu, Zhizheng Zhang, Li Yi, Yizhou Wang, He Wang
<strong>Venue:</strong> arXiv (2026)</p><p>Recent robot foundation models largely rely on large-scale behavior cloning, which imitates expert actions but discards transferable dynamics knowledge embedded in heterogeneous embodied data. While the Unified World Model (UWM) formulation has the potential to leverage such diverse data, existing instantiations struggle to scale to foundation-level due to coarse data usage and fragmented datasets. We introduce LDA-1B, a robot foundation model that scales through universal embodied data ingestion by jointly learning dynamics, policy, and visual forecasting, assigning distinct roles to data of varying quality. To support this regime at scale, we assemble and standardize EI-30k, an embodied interaction dataset comprising over 30k hours of human and robot trajectories in a unified format. Scalable dynamics learning over such heterogeneous data is enabled by prediction in a structured DINO latent space, which avoids redundant pixel-space appearance modeling. Complementing this representation, LDA-1B employs a multi-modal diffusion transformer to handle asynchronous vision and action streams, enabling stable training at the 1B-parameter scale. Experiments in simulation and the real world show LDA-1B outperforms prior methods (e.g., $œÄ_{0.5}$) by up to 21%, 48%, and 23% on contact-rich, dexterous, and long-horizon tasks, respectively. Notably, LDA-1B enables data-efficient fine-tuning, gaining 10% by leveraging 30% low-quality trajectories typically harmful and discarded.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12215v1">üìÑ Download PDF</a></p><hr><h3 id=learning-to-forget-attention-memory-consolidation-for-adaptive-compute-reductionhttpsarxivorgabs260212204v1><a href=https://arxiv.org/abs/2602.12204v1>Learning to Forget Attention: Memory Consolidation for Adaptive Compute Reduction</a><a hidden class=anchor aria-hidden=true href=#learning-to-forget-attention-memory-consolidation-for-adaptive-compute-reductionhttpsarxivorgabs260212204v1>#</a></h3><p><strong>Authors:</strong> Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma
<strong>Venue:</strong> arXiv (2026)</p><p>Hybrid architectures combining state-space models with attention have achieved strong efficiency-quality tradeoffs, yet existing approaches either apply attention uniformly or learn static sparse patterns. This misses a key opportunity: \emph{attention demand should decrease over time as recurring patterns become familiar}. We present a surprising finding from analyzing GPT-2 models: \textbf{88%} of attention operations retrieve information already predictable from the model&rsquo;s hidden state, and this redundancy does \emph{not} decrease during training. Motivated by this observation, we introduce \textbf{\ours{}} (\textbf{C}onsolidation-based \textbf{R}outing for \textbf{A}daptive \textbf{M}emory), a biologically inspired memory consolidation mechanism that gradually distills episodic retrievals into parametric semantic memory. Unlike prior sparse attention methods, \ours{} exhibits \emph{decreasing attention utilization} over training, achieving a \textbf{37.8$\times$} reduction through a sharp phase transition at approximately 3K steps. We prove that this capability is \emph{impossible} without consolidation: any static routing scheme requires $Œ©(f \cdot n)$ attention for tasks with recurring patterns of frequency $f$. On our proposed SRCD benchmark, \ours{} achieves \textbf{100% retrieval accuracy} at 1.6% attention compute (vs.\ 68% for baselines), and consolidated patterns transfer to unseen tasks with \textbf{48&ndash;52%} attention reduction without retraining. Remarkably, the learned consolidation dynamics quantitatively match human episodic-to-semantic memory transition curves from cognitive psychology ($Œ≥= 0.43$ vs.\ $Œ≥_{\text{human}} \approx 0.4$&ndash;$0.5$). Code and benchmarks are available at [anonymized].</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12204v1">üìÑ Download PDF</a></p><hr><h3 id=continuous-and-discrete-time-filters-a-unified-operational-perspectivehttpsarxivorgabs260212198v1><a href=https://arxiv.org/abs/2602.12198v1>Continuous and Discrete-Time Filters: A Unified Operational Perspective</a><a hidden class=anchor aria-hidden=true href=#continuous-and-discrete-time-filters-a-unified-operational-perspectivehttpsarxivorgabs260212198v1>#</a></h3><p><strong>Authors:</strong> Luca Giangrande
<strong>Venue:</strong> arXiv (2026)</p><p>This paper presents a unified tutorial treatment of continuous-time and discrete-time linear time-invariant systems, emphasizing their shared dynamical structure and the physical constraints that differentiate their realizations. Rather than introducing new mathematical tools, the manuscript revisits foundational concepts-transfer functions, poles and zeros, impulse responses, and stability-from an operational perspective rooted in practical signal processing and circuit implementation. First-order systems are used as a minimal yet expressive framework to illustrate how integration, differentiation, filtering, and delay manifest across the Laplace and Z domains. Particular attention is given to causality, bandwidth limitations, sampling effects, and the approximation errors inherent in discrete-time representations. The goal is to bridge the gap between formal mathematical descriptions and the intuition required for reliable system design in mixed analog-digital environments.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12198v1">üìÑ Download PDF</a></p><hr><h3 id=pedagogically-inspired-data-synthesis-for-language-model-knowledge-distillationhttpsarxivorgabs260212172v1><a href=https://arxiv.org/abs/2602.12172v1>Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation</a><a hidden class=anchor aria-hidden=true href=#pedagogically-inspired-data-synthesis-for-language-model-knowledge-distillationhttpsarxivorgabs260212172v1>#</a></h3><p><strong>Authors:</strong> Bowei He, Yankai Chen, Xiaokun Zhang, Linghe Kong, Philip S. Yu, Xue Liu, Chen Ma
<strong>Venue:</strong> arXiv (2026)</p><p>Knowledge distillation from Large Language Models (LLMs) to smaller models has emerged as a critical technique for deploying efficient AI systems. However, current methods for distillation via synthetic data lack pedagogical awareness, treating knowledge transfer as a one-off data synthesis and training task rather than a systematic learning process. In this paper, we propose a novel pedagogically-inspired framework for LLM knowledge distillation that draws from fundamental educational principles. Our approach introduces a three-stage pipeline &ndash; Knowledge Identifier, Organizer, and Adapter (IOA) &ndash; that systematically identifies knowledge deficiencies in student models, organizes knowledge delivery through progressive curricula, and adapts representations to match the cognitive capacity of student models. We integrate Bloom&rsquo;s Mastery Learning Principles and Vygotsky&rsquo;s Zone of Proximal Development to create a dynamic distillation process where student models approach teacher model&rsquo;s performance on prerequisite knowledge before advancing, and new knowledge is introduced with controlled, gradual difficulty increments. Extensive experiments using LLaMA-3.1/3.2 and Qwen2.5 as student models demonstrate that IOA achieves significant improvements over baseline distillation methods, with student models retaining 94.7% of teacher performance on DollyEval while using less than 1/10th of the parameters. Our framework particularly excels in complex reasoning tasks, showing 19.2% improvement on MATH and 22.3% on HumanEval compared with state-of-the-art baselines.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12172v1">üìÑ Download PDF</a></p><hr><h3 id=amortized-molecular-optimization-via-group-relative-policy-optimizationhttpsarxivorgabs260212162v1><a href=https://arxiv.org/abs/2602.12162v1>Amortized Molecular Optimization via Group Relative Policy Optimization</a><a hidden class=anchor aria-hidden=true href=#amortized-molecular-optimization-via-group-relative-policy-optimizationhttpsarxivorgabs260212162v1>#</a></h3><p><strong>Authors:</strong> Muhammad bin Javaid, Hasham Hussain, Ashima Khanna, Berke Kisin, Jonathan Pirnay, Alexander Mitsos, Dominik G. Grimm, Martin Grohe
<strong>Venue:</strong> arXiv (2026)</p><p>Molecular design encompasses tasks ranging from de-novo design to structural alteration of given molecules or fragments. For the latter, state-of-the-art methods predominantly function as &ldquo;Instance Optimizers&rsquo;&rsquo;, expending significant compute restarting the search for every input structure. While model-based approaches theoretically offer amortized efficiency by learning a policy transferable to unseen structures, existing methods struggle to generalize. We identify a key failure mode: the high variance arising from the heterogeneous difficulty of distinct starting structures. To address this, we introduce GRXForm, adapting a pre-trained Graph Transformer model that optimizes molecules via sequential atom-and-bond additions. We employ Group Relative Policy Optimization (GRPO) for goal-directed fine-tuning to mitigate variance by normalizing rewards relative to the starting structure. Empirically, GRXForm generalizes to out-of-distribution molecular scaffolds without inference-time oracle calls or refinement, achieving scores in multi-objective optimization competitive with leading instance optimizers.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12162v1">üìÑ Download PDF</a></p><hr><h3 id=the-wandering-supermassive-black-hole-powering-the-off-nuclear-tde-at2024tvdhttpsarxivorgabs260212272v1><a href=https://arxiv.org/abs/2602.12272v1>The Wandering Supermassive Black Hole Powering the off-nuclear TDE AT2024tvd</a><a hidden class=anchor aria-hidden=true href=#the-wandering-supermassive-black-hole-powering-the-off-nuclear-tde-at2024tvdhttpsarxivorgabs260212272v1>#</a></h3><p><strong>Authors:</strong> M. Guolo, A. Mummery, S. van Velzen, M. Nicholl, S. Gezari, Y. Yao, K. C. Chambers, T. de Boer, M. E. Huber, C. -C. Lin, T. B. Lowe, E. A. Magnier, G. Paek, R. Wainscoat
<strong>Venue:</strong> arXiv (2026)</p><p>We present an analysis of the spectral energy distribution (SED) of the off-nuclear tidal disruption event (TDE) AT2024tvd during its late-time plateau phase, combining X-ray spectra and UV/optical photometry. Using a fully relativistic, compact accretion disk model with self-consistent inner-disk Comptonization, we reproduce the observed SED without significant residuals. The inferred black hole mass ${\rm log}{10}(M{\bullet}/M_\odot) \approx 6.0 \pm 0.2$, and the inferred disk parameters place AT2024tvd within known TDE-disk scaling relations ($L_{\rm bol}^{\rm disk}/L_{\rm Edd} \propto T_{\rm p}^4 \propto M_{\bullet}^{-1}$, $L_{\rm plat} \propto M_{\bullet}^{2/3}$, $R_{\rm out}/r_{\rm g} \propto M_{\bullet}^{-2/3}$). Our results show that: (i) there is no \textit{detected} star cluster or dwarf galaxy associated with the source, down to a mass limit of $\log_{10}(M_{\rm gal}/M_{\odot}) \leq 7.6$; (ii) the black hole is a wandering supermassive, rather than intermediate-mass, black hole; and (iii) the source represents an extreme case of black hole-to-host mass ratio, with $M_{\bullet}/M_{\rm gal} > 3%$, consistent with a heavily tidally stripped nucleus. The latter aligns with cosmological simulations predicting that surviving host remnants of most wandering black holes should not retain a detectable stellar overdensity when located at small halo-centric distances. We discuss differences with previous analyses of this source and highlight why our modeling approach provides a more physically consistent solution with more reliable parameter inference.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12272v1">üìÑ Download PDF</a></p><hr><h3 id=monarchrt-efficient-attention-for-real-time-video-generationhttpsarxivorgabs260212271v1><a href=https://arxiv.org/abs/2602.12271v1>MonarchRT: Efficient Attention for Real-Time Video Generation</a><a hidden class=anchor aria-hidden=true href=#monarchrt-efficient-attention-for-real-time-video-generationhttpsarxivorgabs260212271v1>#</a></h3><p><strong>Authors:</strong> Krish Agarwal, Zhuoming Chen, Cheng Luo, Yongqi Chen, Haizhong Zheng, Xun Huang, Atri Rudra, Beidi Chen
<strong>Venue:</strong> arXiv (2026)</p><p>Real-time video generation with Diffusion Transformers is bottlenecked by the quadratic cost of 3D self-attention, especially in real-time regimes that are both few-step and autoregressive, where errors compound across time and each denoising step must carry substantially more information. In this setting, we find that prior sparse-attention approximations break down, despite showing strong results for bidirectional, many-step diffusion. Specifically, we observe that video attention is not reliably sparse, but instead combines pronounced periodic structure driven by spatiotemporal position with dynamic, sparse semantic correspondences and dense mixing, exceeding the representational capacity of even oracle top-k attention. Building on this insight, we propose Monarch-RT, a structured attention parameterization for video diffusion models that factorizes attention using Monarch matrices. Through appropriately aligned block structure and our extended tiled Monarch parameterization, we achieve high expressivity while preserving computational efficiency. We further overcome the overhead of parameterization through finetuning, with custom Triton kernels. We first validate the high efficacy of Monarch-RT over existing sparse baselines designed only for bidirectional models. We further observe that Monarch-RT attains up to 95% attention sparsity with no loss in quality when applied to the state-of-the-art model Self-Forcing, making Monarch-RT a pioneering work on highly-capable sparse attention parameterization for real-time video generation. Our optimized implementation outperforms FlashAttention-2, FlashAttention-3, and FlashAttention-4 kernels on Nvidia RTX 5090, H100, and B200 GPUs respectively, providing kernel speedups in the range of 1.4-11.8X. This enables us, for the first time, to achieve true real-time video generation with Self-Forcing at 16 FPS on a single RTX 5090.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12271v1">üìÑ Download PDF</a></p><hr><h3 id=extractbench-a-benchmark-and-evaluation-methodology-for-complex-structured-extractionhttpsarxivorgabs260212247v1><a href=https://arxiv.org/abs/2602.12247v1>ExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction</a><a hidden class=anchor aria-hidden=true href=#extractbench-a-benchmark-and-evaluation-methodology-for-complex-structured-extractionhttpsarxivorgabs260212247v1>#</a></h3><p><strong>Authors:</strong> Nick Ferguson, Josh Pennington, Narek Beghian, Aravind Mohan, Douwe Kiela, Sheshansh Agrawal, Thien Hang Nguyen
<strong>Venue:</strong> arXiv (2026)</p><p>Unstructured documents like PDFs contain valuable structured information, but downstream systems require this data in reliable, standardized formats. LLMs are increasingly deployed to automate this extraction, making accuracy and reliability paramount. However, progress is bottlenecked by two gaps. First, no end-to-end benchmark evaluates PDF-to-JSON extraction under enterprise-scale schema breadth. Second, no principled methodology captures the semantics of nested extraction, where fields demand different notions of correctness (exact match for identifiers, tolerance for quantities, semantic equivalence for names), arrays require alignment, and omission must be distinguished from hallucination. We address both gaps with ExtractBench, an open-source benchmark and evaluation framework for PDF-to-JSON structured extraction. The benchmark pairs 35 PDF documents with JSON Schemas and human-annotated gold labels across economically valuable domains, yielding 12,867 evaluatable fields spanning schema complexities from tens to hundreds of fields. The evaluation framework treats the schema as an executable specification: each field declares its scoring metric. Baseline evaluations reveal that frontier models (GPT-5/5.2, Gemini-3 Flash/Pro, Claude 4.5 Opus/Sonnet) remain unreliable on realistic schemas. Performance degrades sharply with schema breadth, culminating in 0% valid output on a 369-field financial reporting schema across all tested models. We release ExtractBench at <a href=https://github.com/ContextualAI/extract-bench>https://github.com/ContextualAI/extract-bench</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12247v1">üìÑ Download PDF</a></p><hr><h3 id=status-of-the-s_8-tension-a-2026-review-of-probe-discrepancieshttpsarxivorgabs260212238v1><a href=https://arxiv.org/abs/2602.12238v1>Status of the $S_8$ Tension: A 2026 Review of Probe Discrepancies</a><a hidden class=anchor aria-hidden=true href=#status-of-the-s_8-tension-a-2026-review-of-probe-discrepancieshttpsarxivorgabs260212238v1>#</a></h3><p><strong>Authors:</strong> Ioannis Pantos, Leandros Perivolaropoulos
<strong>Venue:</strong> arXiv (2026)</p><p>The parameter $S_8 \equiv œÉ_8 (Œ©_m/0.3)^{0.5}$ quantifies the amplitude of matter density fluctuations. A persistent discrepancy exists between early-universe CMB observations and late-universe probes. This review assesses the <code>$S_8$ tension'' against a new 2026 baseline: a unified </code>Combined CMB&rsquo;&rsquo; framework incorporating Planck, ACT DR6, and SPT-3G. This combined analysis yields $S_8 = 0.836^{+0.012}_{-0.013}$, providing a higher central value and reduced uncertainties compared to Planck alone. Compiling measurements from 2019&ndash;2026, we reveal a striking bifurcation: DES Year 6 results exhibit a statistically significant tension of $2.4œÉ$&ndash;$2.7œÉ$ \citep{DESY6}, whereas KiDS Legacy results demonstrate statistical consistency at $&lt;1œÉ$ \citep{Wright2025}. We examine systematic origins of this dichotomy, including photometric redshift calibration, intrinsic alignment modeling, and shear measurement pipelines. We further contextualize these findings with cluster counts (where eROSITA favors high values while SPT favors low), galaxy-galaxy lensing, and redshift-space distortions. The heterogeneous landscape suggests survey-specific systematic effects contribute substantially to observed discrepancies, though new physics beyond $Œõ$CDM cannot be excluded.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12238v1">üìÑ Download PDF</a></p><hr><h3 id=automated-test-suite-enhancement-using-large-language-models-with-few-shot-promptinghttpsarxivorgabs260212256v1><a href=https://arxiv.org/abs/2602.12256v1>Automated Test Suite Enhancement Using Large Language Models with Few-shot Prompting</a><a hidden class=anchor aria-hidden=true href=#automated-test-suite-enhancement-using-large-language-models-with-few-shot-promptinghttpsarxivorgabs260212256v1>#</a></h3><p><strong>Authors:</strong> Alex Chudic, G√ºl √áalƒ±klƒ±
<strong>Venue:</strong> arXiv (2026)</p><p>Unit testing is essential for verifying the functional correctness of code modules (e.g., classes, methods), but manually writing unit tests is often labor-intensive and time-consuming. Unit tests generated by tools that employ traditional approaches, such as search-based software testing (SBST), lack readability, naturalness, and practical usability. LLMs have recently provided promising results and become integral to developers&rsquo; daily practices. Consequently, software repositories now include a mix of human-written tests, LLM-generated tests, and those from tools employing traditional approaches such as SBST. While LLMs&rsquo; zero-shot capabilities have been widely studied, their few-shot learning potential for unit test generation remains underexplored. Few-shot prompting enables LLMs to learn from examples in the prompt, and automatically retrieving such examples could enhance test suites. This paper empirically investigates how few-shot prompting with different test artifact sources, comprising human, SBST, or LLM, affects the quality of LLM-generated unit tests as program comprehension artifacts and their contribution to improving existing test suites by evaluating not only correctness and coverage but also readability, cognitive complexity, and maintainability in hybrid human-AI codebases. We conducted experiments on HumanEval and ClassEval datasets using GPT-4o, which is integrated into GitHub Copilot and widely used among developers. We also assessed retrieval-based methods for selecting relevant examples. Our results show that LLMs can generate high-quality tests via few-shot prompting, with human-written examples producing the best coverage and correctness. Additionally, selecting examples based on the combined similarity of problem description and code consistently yields the most effective few-shot prompts.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12256v1">üìÑ Download PDF</a></p><hr><h3 id=vision-transformer-for-multi-domain-phase-retrieval-in-coherent-diffraction-imaginghttpsarxivorgabs260212255v1><a href=https://arxiv.org/abs/2602.12255v1>Vision Transformer for Multi-Domain Phase Retrieval in Coherent Diffraction Imaging</a><a hidden class=anchor aria-hidden=true href=#vision-transformer-for-multi-domain-phase-retrieval-in-coherent-diffraction-imaginghttpsarxivorgabs260212255v1>#</a></h3><p><strong>Authors:</strong> Jialun Liu, David Yang, Ian Robinson
<strong>Venue:</strong> arXiv (2026)</p><p>Bragg coherent diffraction imaging (BCDI) phase retrieval becomes rapidly difficult in the strong-phase regime, where a crystal contains distortions beyond half a lattice spacing. An important special case is the phase domain problem, where blocks of a crystal are displaced with sharp jumps at domain walls. The strong-phase, here defined as beyond $\pm œÄ/2$, generates split Bragg peaks and dense fringe structure for which classical iterative solvers often stagnate or return different solutions from different initialisations. Here, we introduce an unsupervised Fourier Vision Transformer (Fourier ViT) to solve this block-phase, multi-domain phase-retrieval problem directly from measured 2D Bragg diffraction intensities. Fourier ViT couples reciprocal-space information globally through multiscale Fourier token mixing, while shallow convolutional front and back-ends provide local filtering and reconstruction. We validate the approach on large-scale synthetic datasets of Voronoi multi-domain crystals with strong-phase contrast under realistic noise corruptions, and on experimental diffraction from a $\mathrm{La}_{2-x}\mathrm{Ca}_x\mathrm{MnO}_4$ nanocrystal. Across the regimes considered, Fourier ViT achieves the lowest reciprocal-space mismatch ($œá^2$) among the compared methods and preserves domain-resolved phase reconstructions for increasing numbers of domains. On experimental data, with the same real-space support, Fourier ViT matches the iterative benchmark $œá^2$ while improving robustness to random initialisations, yielding a higher success rate of low-$œá^2$ reconstructions than the complex convolutional neural network baseline.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12255v1">üìÑ Download PDF</a></p><hr><h3 id=detecting-overflow-in-compressed-token-representations-for-retrieval-augmented-generationhttpsarxivorgabs260212235v1><a href=https://arxiv.org/abs/2602.12235v1>Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation</a><a hidden class=anchor aria-hidden=true href=#detecting-overflow-in-compressed-token-representations-for-retrieval-augmented-generationhttpsarxivorgabs260212235v1>#</a></h3><p><strong>Authors:</strong> Julia Belikova, Danila Rozhevskii, Dennis Svirin, Konstantin Polev, Alexander Panchenko
<strong>Venue:</strong> arXiv (2026)</p><p>Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens. Yet, the limits of compressibility &ndash; and when compression begins to erase task-relevant content &ndash; remain underexplored. In this paper, we define \emph{token overflow} as a regime in which compressed representations no longer contain sufficient information to answer a given query, and propose a methodology to characterize and detect it. In the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing a practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA, SQuADv2, and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling low-cost pre-LLM gating to mitigate compression-induced errors.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12235v1">üìÑ Download PDF</a></p><hr><h3 id=query-focused-and-memory-aware-reranker-for-long-context-processinghttpsarxivorgabs260212192v1><a href=https://arxiv.org/abs/2602.12192v1>Query-focused and Memory-aware Reranker for Long Context Processing</a><a hidden class=anchor aria-hidden=true href=#query-focused-and-memory-aware-reranker-for-long-context-processinghttpsarxivorgabs260212192v1>#</a></h3><p><strong>Authors:</strong> Yuqing Li, Jiangnan Li, Mo Yu, Guoxuan Ding, Zheng Lin, Weiping Wang, Jie Zhou
<strong>Venue:</strong> arXiv (2026)</p><p>Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12192v1">üìÑ Download PDF</a></p><hr><h2 id=-linguistics>üîç linguistics<a hidden class=anchor aria-hidden=true href=#-linguistics>#</a></h2><h3 id=stroke-of-surprise-progressive-semantic-illusions-in-vector-sketchinghttpsarxivorgabs260212280v1><a href=https://arxiv.org/abs/2602.12280v1>Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching</a><a hidden class=anchor aria-hidden=true href=#stroke-of-surprise-progressive-semantic-illusions-in-vector-sketchinghttpsarxivorgabs260212280v1>#</a></h3><p><strong>Authors:</strong> Huai-Hsun Cheng, Siang-Ling Zhang, Yu-Lun Liu
<strong>Venue:</strong> arXiv (2026)</p><p>Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the &ldquo;dual-constraint&rdquo;: initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a &ldquo;common structural subspace&rdquo; valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: <a href=https://stroke-of-surprise.github.io/>https://stroke-of-surprise.github.io/</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12280v1">üìÑ Download PDF</a></p><hr><h3 id=function-space-decoupled-diffusion-for-forward-and-inverse-modeling-in-carbon-capture-and-storagehttpsarxivorgabs260212274v1><a href=https://arxiv.org/abs/2602.12274v1>Function-Space Decoupled Diffusion for Forward and Inverse Modeling in Carbon Capture and Storage</a><a hidden class=anchor aria-hidden=true href=#function-space-decoupled-diffusion-for-forward-and-inverse-modeling-in-carbon-capture-and-storagehttpsarxivorgabs260212274v1>#</a></h3><p><strong>Authors:</strong> Xin Ju, Jiachen Yao, Anima Anandkumar, Sally M. Benson, Gege Wen
<strong>Venue:</strong> arXiv (2026)</p><p>Accurate characterization of subsurface flow is critical for Carbon Capture and Storage (CCS) but remains challenged by the ill-posed nature of inverse problems with sparse observations. We present Fun-DDPS, a generative framework that combines function-space diffusion models with differentiable neural operator surrogates for both forward and inverse modeling. Our approach learns a prior distribution over geological parameters (geomodel) using a single-channel diffusion model, then leverages a Local Neural Operator (LNO) surrogate to provide physics-consistent guidance for cross-field conditioning on the dynamics field. This decoupling allows the diffusion prior to robustly recover missing information in parameter space, while the surrogate provides efficient gradient-based guidance for data assimilation. We demonstrate Fun-DDPS on synthetic CCS modeling datasets, achieving two key results: (1) For forward modeling with only 25% observations, Fun-DDPS achieves 7.7% relative error compared to 86.9% for standard surrogates (an 11x improvement), proving its capability to handle extreme data sparsity where deterministic methods fail. (2) We provide the first rigorous validation of diffusion-based inverse solvers against asymptotically exact Rejection Sampling (RS) posteriors. Both Fun-DDPS and the joint-state baseline (Fun-DPS) achieve Jensen-Shannon divergence less than 0.06 against the ground truth. Crucially, Fun-DDPS produces physically consistent realizations free from the high-frequency artifacts observed in joint-state baselines, achieving this with 4x improved sample efficiency compared to rejection sampling.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12274v1">üìÑ Download PDF</a></p><hr><h3 id=creative-ownership-in-the-age-of-aihttpsarxivorgabs260212270v1><a href=https://arxiv.org/abs/2602.12270v1>Creative Ownership in the Age of AI</a><a hidden class=anchor aria-hidden=true href=#creative-ownership-in-the-age-of-aihttpsarxivorgabs260212270v1>#</a></h3><p><strong>Authors:</strong> Annie Liang, Jay Lu
<strong>Venue:</strong> arXiv (2026)</p><p>Copyright law focuses on whether a new work is &ldquo;substantially similar&rdquo; to an existing one, but generative AI can closely imitate style without copying content, a capability now central to ongoing litigation. We argue that existing definitions of infringement are ill-suited to this setting and propose a new criterion: a generative AI output infringes on an existing work if it could not have been generated without that work in its training corpus. To operationalize this definition, we model generative systems as closure operators mapping a corpus of existing works to an output of new works. AI generated outputs are \emph{permissible} if they do not infringe on any existing work according to our criterion. Our results characterize structural properties of permissible generation and reveal a sharp asymptotic dichotomy: when the process of organic creations is light-tailed, dependence on individual works eventually vanishes, so that regulation imposes no limits on AI generation; with heavy-tailed creations, regulation can be persistently constraining.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12270v1">üìÑ Download PDF</a></p><hr><h3 id=cm2-reinforcement-learning-with-checklist-rewards-for-multi-turn-and-multi-step-agentic-tool-usehttpsarxivorgabs260212268v1><a href=https://arxiv.org/abs/2602.12268v1>CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use</a><a hidden class=anchor aria-hidden=true href=#cm2-reinforcement-learning-with-checklist-rewards-for-multi-turn-and-multi-step-agentic-tool-usehttpsarxivorgabs260212268v1>#</a></h3><p><strong>Authors:</strong> Zhen Zhang, Kaiqiang Song, Xun Wang, Yebowen Hu, Weixiang Yan, Chenyang Zhao, Henry Peng Zou, Haoyun Deng, Sathish Reddy Indurthi, Shujian Liu, Simin Ma, Xiaoyang Wang, Xin Eric Wang, Song Wang
<strong>Venue:</strong> arXiv (2026)</p><p>AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2, an RL framework that replaces verifiable outcome rewards with checklist rewards. CM2 decomposes each turn&rsquo;s intended behavior into fine-grained binary criteria with explicit evidence grounding and structured metadata, turning open-ended judging into more stable classification-style decisions. To balance stability and informativeness, our method adopts a strategy of sparse reward assignment but dense evaluation criteria. Training is performed in a scalable LLM-simulated tool environment, avoiding heavy engineering for large tool sets. Experiments show that CM2 consistently improves over supervised fine-tuning. Starting from an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12 points on ToolSandbox. The results match or even outperform similarly sized open-source baselines, including the judging model. CM2 thus provides a scalable recipe for optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards. Code provided by the open-source community: <a href=https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent>https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12268v1">üìÑ Download PDF</a></p><hr><h3 id=transmit-or-idle-efficient-aoi-optimal-transmission-policy-for-gossiping-receivershttpsarxivorgabs260212264v1><a href=https://arxiv.org/abs/2602.12264v1>Transmit or Idle: Efficient AoI Optimal Transmission Policy for Gossiping Receivers</a><a hidden class=anchor aria-hidden=true href=#transmit-or-idle-efficient-aoi-optimal-transmission-policy-for-gossiping-receivershttpsarxivorgabs260212264v1>#</a></h3><p><strong>Authors:</strong> Irtiza Hasan, Ahmed Arafa
<strong>Venue:</strong> arXiv (2026)</p><p>We study the optimal transmission and scheduling policy for a transmitter (source) communicating with two gossiping receivers aiming at tracking the source&rsquo;s status over time using the age of information (AoI) metric. Gossiping enables local information exchange in a decentralized manner without relying solely on the transmitter&rsquo;s direct communication, which we assume incurs a transmission cost. On the other hand, gossiping may be communicating stale information, necessitating the transmitter&rsquo;s intervention. With communication links having specific success probabilities, we formulate an average-cost Markov Decision Process (MDP) to jointly minimize the sum AoI and transmission cost for such a system in a time-slotted setting. We employ the Relative Value Iteration (RVI) algorithm to evaluate the optimal policy for the transmitter and then prove several structural properties showing that it has an age-difference threshold structure with minimum age activation in the case where gossiping is relatively more reliable. Specifically, direct transmission is optimal only if the minimum AoI of the receivers is large enough and their age difference is below a certain threshold. Otherwise, the transmitter idles to effectively take advantage of gossiping and reduce direct transmission costs. Numerical evaluations demonstrate the significance of our optimal policy compared to multiple baselines. Our result is a first step towards characterizing optimal freshness and transmission cost trade-offs in gossiping networks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12264v1">üìÑ Download PDF</a></p><hr><h3 id=think-like-a-scientist-physics-guided-llm-agent-for-equation-discoveryhttpsarxivorgabs260212259v1><a href=https://arxiv.org/abs/2602.12259v1>Think like a Scientist: Physics-guided LLM Agent for Equation Discovery</a><a hidden class=anchor aria-hidden=true href=#think-like-a-scientist-physics-guided-llm-agent-for-equation-discoveryhttpsarxivorgabs260212259v1>#</a></h3><p><strong>Authors:</strong> Jianke Yang, Ohm Venkatachalam, Mohammad Kianezhad, Sharvaree Vadgama, Rose Yu
<strong>Venue:</strong> arXiv (2026)</p><p>Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12259v1">üìÑ Download PDF</a></p><hr><h3 id=oxygen-left-behind-atmospheric-enrichment-due-to-fractionation-in-sub-neptunes-using-boreashttpsarxivorgabs260212201v1><a href=https://arxiv.org/abs/2602.12201v1>Oxygen left behind: Atmospheric Enrichment due to Fractionation in Sub-Neptunes using BOREAS</a><a hidden class=anchor aria-hidden=true href=#oxygen-left-behind-atmospheric-enrichment-due-to-fractionation-in-sub-neptunes-using-boreashttpsarxivorgabs260212201v1>#</a></h3><p><strong>Authors:</strong> Marilina Valatsou, Caroline Dorn, Pierlou Marty, James E. Owen
<strong>Venue:</strong> arXiv (2026)</p><p>The evolution of exoplanetary atmospheres is strongly influenced by atmospheric escape, particularly for close-in planets. Fractionation during atmospheric loss can preferentially remove lighter elements such as hydrogen, while retaining heavier species like oxygen. In this study, we investigate how and under what conditions hydrodynamic escape and chemical fractionation jointly shape the mass and composition of exoplanet atmospheres, especially for mixed H2 + H2O atmospheres. We develop BOREAS, a self-consistent mass loss model coupling a 1D Parker wind formulation with a mass-dependent fractionation scheme, which we apply across a range of planet masses, radii, equilibrium temperatures, and incident XUV fluxes, allowing us to track hydrogen and oxygen escape rates at different snapshots in time. We find that oxygen is efficiently retained over most of the parameter space. Significant oxygen loss occurs under high incident XUV fluxes, while at intermediate fluxes oxygen loss is largely confined to low-gravity planets. Where oxygen is retained, irradiation is too weak to drive significant escape of hydrogen and thus limiting atmospheric enrichment. By contrast, our model predicts that sub-Neptunes undergo substantial atmospheric enrichment over approx. 200 Myr when hydrogen escape is efficient and accompanied by partial oxygen entrainment. Notably, our results imply that sub-Neptunes near the radius valley can evolve into water-rich planets, in agreement with GJ 9827 d. Present-day water-rich atmospheres may have originated from water-poor envelopes under some conditions, highlighting the need to include chemical fractionation in evolution models. BOREAS is publicly available.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12201v1">üìÑ Download PDF</a></p><hr><h3 id=a-rule-based-computational-model-for-gaidhlig-morphologyhttpsarxivorgabs260212132v1><a href=https://arxiv.org/abs/2602.12132v1>A Rule-based Computational Model for Gaidhlig Morphology</a><a hidden class=anchor aria-hidden=true href=#a-rule-based-computational-model-for-gaidhlig-morphologyhttpsarxivorgabs260212132v1>#</a></h3><p><strong>Authors:</strong> Peter J Barclay
<strong>Venue:</strong> arXiv (2026)</p><p>Language models and software tools are essential to support the continuing vitality of lesser-used languages; however, currently popular neural models require considerable data for training, which normally is not available for such low-resource languages. This paper describes work-in-progress to construct a rule-based model of Gaidhlig morphology using data from Wiktionary, arguing that rule-based systems effectively leverage limited sample data, support greater interpretability, and provide insights useful in the design of teaching materials. The use of SQL for querying the occurrence of different lexical patterns is investigated, and a declarative rule-base is presented that allows Python utilities to derive inflected forms of Gaidhlig words. This functionality could be used to support educational tools that teach or explain language patterns, for example, or to support higher level tools such as rule-based dependency parsers. This approach adds value to the data already present in Wiktionary by adapting it to new use-cases.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12132v1">üìÑ Download PDF</a></p><hr><h3 id=meta-sel-efficient-demonstration-selection-for-in-context-learning-via-supervised-meta-learninghttpsarxivorgabs260212123v1><a href=https://arxiv.org/abs/2602.12123v1>Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Meta-Learning</a><a hidden class=anchor aria-hidden=true href=#meta-sel-efficient-demonstration-selection-for-in-context-learning-via-supervised-meta-learninghttpsarxivorgabs260212123v1>#</a></h3><p><strong>Authors:</strong> Xubin Wang, Weijia Jia
<strong>Venue:</strong> arXiv (2026)</p><p>Demonstration selection is a practical bottleneck in in-context learning (ICL): under a tight prompt budget, accuracy can change substantially depending on which few-shot examples are included, yet selection must remain cheap enough to run per query over large candidate pools. We propose Meta-Sel, a lightweight supervised meta-learning approach for intent classification that learns a fast, interpretable scoring function for (candidate, query) pairs from labeled training data.
Meta-Sel constructs a meta-dataset by sampling pairs from the training split and using class agreement as supervision, then trains a calibrated logistic regressor on two inexpensive meta-features: TF&ndash;IDF cosine similarity and a length-compatibility ratio. At inference time, the selector performs a single vectorized scoring pass over the full candidate pool and returns the top-k demonstrations, requiring no model fine-tuning, no online exploration, and no additional LLM calls. This yields deterministic rankings and makes the selection mechanism straightforward to audit via interpretable feature weights.
Beyond proposing Meta-Sel, we provide a broad empirical study of demonstration selection, benchmarking 12 methods &ndash; spanning prompt engineering baselines, heuristic selection, reinforcement learning, and influence-based approaches &ndash; across four intent datasets and five open-source LLMs. Across this benchmark, Meta-Sel consistently ranks among the top-performing methods, is particularly effective for smaller models where selection quality can partially compensate for limited model capacity, and maintains competitive selection-time overhead.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12123v1">üìÑ Download PDF</a></p><hr><h3 id=two-point-functions-in-boundary-loop-modelshttpsarxivorgabs260212000v1><a href=https://arxiv.org/abs/2602.12000v1>Two-point functions in boundary loop models</a><a hidden class=anchor aria-hidden=true href=#two-point-functions-in-boundary-loop-modelshttpsarxivorgabs260212000v1>#</a></h3><p><strong>Authors:</strong> Max Downing, Jesper Lykke Jacobsen, Rongvoram Nivesvivat, Hubert Saleur
<strong>Venue:</strong> arXiv (2026)</p><p>Using techniques of conformal bootstrap, we propose analytical expressions for a large class of two-point functions of bulk fields in critical loop models defined on the upper-half plane. Our results include the two-point connectivities in the Fortuin&ndash;Kasteleyn random cluster model with both free and wired boundary conditions. We link the continuum expressions to lattice quantities by computing universal ratios of amplitudes for the two-point connectivities, and find excellent agreement with transfer-matrix numerics.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12000v1">üìÑ Download PDF</a></p><hr><h3 id=momentum-lms-theory-beyond-stationarity-stability-tracking-and-regrethttpsarxivorgabs260211995v1><a href=https://arxiv.org/abs/2602.11995v1>Momentum LMS Theory beyond Stationarity: Stability, Tracking, and Regret</a><a hidden class=anchor aria-hidden=true href=#momentum-lms-theory-beyond-stationarity-stability-tracking-and-regrethttpsarxivorgabs260211995v1>#</a></h3><p><strong>Authors:</strong> Yifei Jin, Xin Zheng, Lei Guo
<strong>Venue:</strong> arXiv (2026)</p><p>In large-scale data processing scenarios, data often arrive in sequential streams generated by complex systems that exhibit drifting distributions and time-varying system parameters. This nonstationarity challenges theoretical analysis, as it violates classical assumptions of i.i.d. (independent and identically distributed) samples, necessitating algorithms capable of real-time updates without expensive retraining. An effective approach should process each sample in a single pass, while maintaining computational and memory complexities independent of the data stream length. Motivated by these challenges, this paper investigates the Momentum Least Mean Squares (MLMS) algorithm as an adaptive identification tool, leveraging its computational simplicity and online processing capabilities. Theoretically, we derive tracking performance and regret bounds for the MLMS in time-varying stochastic linear systems under various practical conditions. Unlike classical LMS, whose stability can be characterized by first-order random vector difference equations, MLMS introduces an additional dynamical state due to momentum, leading to second-order time-varying random vector difference equations whose stability analysis hinges on more complicated products of random matrices, which poses a substantially challenging problem to resolve. Experiments on synthetic and real-world data streams demonstrate that MLMS achieves rapid adaptation and robust tracking, in agreement with our theoretical results especially in nonstationary settings, highlighting its promise for modern streaming and online learning applications.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.11995v1">üìÑ Download PDF</a></p><hr><h3 id=tuning-optical-properties-of-fto-via-carbonaceous-al2o3-microdot-deposition-by-dc-plasma-sputteringhttpsarxivorgabs260211970v1><a href=https://arxiv.org/abs/2602.11970v1>Tuning Optical Properties of FTO via Carbonaceous Al2O3 Microdot Deposition by DC plasma sputtering</a><a hidden class=anchor aria-hidden=true href=#tuning-optical-properties-of-fto-via-carbonaceous-al2o3-microdot-deposition-by-dc-plasma-sputteringhttpsarxivorgabs260211970v1>#</a></h3><p><strong>Authors:</strong> Sarah Salah, Ahmed Atlam, Nagat Elkahwagy, Abdelhamid Elshaer, Mohammed Shihab
<strong>Venue:</strong> arXiv (2026)</p><p>Fluorine-doped tin oxide (FTO) is a key transparent conductive oxide for photovoltaic and optoelectronic devices, yet its high reflectance limits light-trapping efficiency. This work demonstrates a simple DC plasma sputtering approach to deposit carbonaceous Al2O3 microdots on FTO under controlled Ar, O2, and Ar-O2 atmospheres. For plasma discharge in the normal mode, with plasma density 10^-9 cm^-3 and temperature of 2 eV, Volmer-Weber growth produced discrete microdots whose size and distribution were tuned by gas composition: dense, uniform dots in Ar (approximately 0.89 um radius), agglomerated structures in O2, and intermediate morphologies in mixed atmospheres. Structural analysis confirmed gamma-Al2O3 formation with carbon incorporation, while SEM revealed morphology-driven optical behavior. UV-Vis measurements showed that Ar-O2 coatings achieved the lowest reflectance across the visible range, outperforming bare FTO and other conditions. These findings establish a clear link between sputtering parameters, surface morphology, and optical performance, offering a scalable route to anti-reflective, light-trapping coatings for next-generation solar cells and optoelectronic devices.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.11970v1">üìÑ Download PDF</a></p><hr><h3 id=learning-to-control-the-iuzawa-net-for-nonsmooth-optimal-control-of-linear-pdeshttpsarxivorgabs260212273v1><a href=https://arxiv.org/abs/2602.12273v1>Learning to Control: The iUzawa-Net for Nonsmooth Optimal Control of Linear PDEs</a><a hidden class=anchor aria-hidden=true href=#learning-to-control-the-iuzawa-net-for-nonsmooth-optimal-control-of-linear-pdeshttpsarxivorgabs260212273v1>#</a></h3><p><strong>Authors:</strong> Yongcun Song, Xiaoming Yuan, Hangrui Yue, Tianyou Zeng
<strong>Venue:</strong> arXiv (2026)</p><p>We propose an optimization-informed deep neural network approach, named iUzawa-Net, aiming for the first solver that enables real-time solutions for a class of nonsmooth optimal control problems of linear partial differential equations (PDEs). The iUzawa-Net unrolls an inexact Uzawa method for saddle point problems, replacing classical preconditioners and PDE solvers with specifically designed learnable neural networks. We prove universal approximation properties and establish the asymptotic $\varepsilon$-optimality for the iUzawa-Net, and validate its promising numerical efficiency through nonsmooth elliptic and parabolic optimal control problems. Our techniques offer a versatile framework for designing and analyzing various optimization-informed deep learning approaches to optimal control and other PDE-constrained optimization problems. The proposed learning-to-control approach synergizes model-based optimization algorithms and data-driven deep learning techniques, inheriting the merits of both methodologies.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12273v1">üìÑ Download PDF</a></p><hr><h3 id=kagome-edge-states-under-lattice-termination-spin-orbit-coupling-and-magnetic-orderhttpsarxivorgabs260212223v1><a href=https://arxiv.org/abs/2602.12223v1>Kagome edge states under lattice termination, spin-orbit coupling, and magnetic order</a><a hidden class=anchor aria-hidden=true href=#kagome-edge-states-under-lattice-termination-spin-orbit-coupling-and-magnetic-orderhttpsarxivorgabs260212223v1>#</a></h3><p><strong>Authors:</strong> Sajid Sekh, Annica M. Black-Schaffer, Andrzej Ptok
<strong>Venue:</strong> arXiv (2026)</p><p>We study the edge state properties of a two-dimensional kagome lattice using a tight-binding approach, focusing on the role of lattice termination, spin-orbit coupling, and magnetic order. In the pristine limit, we show that the existence of localized edge states is highly sensitive to boundary geometry, with certain terminations completely suppressing edge modes. Kane-Mele spin-orbit coupling opens a bulk gap and stabilizes topologically protected helical edge states, yielding a robust $\mathbb{Z}_2$ insulating phase that is insensitive to termination details. In contrast, the combined effect of a Zeeman field and Rashba spin-orbit coupling drives the system into Chern insulating phases, with Chern numbers consistent with the number of chiral edge modes. We further demonstrate that non-coplanar magnetic textures generate multiple Chern phases through finite scalar spin chirality, with Kane-Mele coupling strongly tuning the topological gaps. Our results provide important insights into the tunability of edge states in the kagome lattice, which can be key to designing materials with novel electronic properties and topological phases.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12223v1">üìÑ Download PDF</a></p><hr><h3 id=quantum-coherent-thermodynamics-leaf-typicality-via-minimum-variance-foliationhttpsarxivorgabs260212212v1><a href=https://arxiv.org/abs/2602.12212v1>Quantum-Coherent Thermodynamics: Leaf Typicality via Minimum-Variance Foliation</a><a hidden class=anchor aria-hidden=true href=#quantum-coherent-thermodynamics-leaf-typicality-via-minimum-variance-foliationhttpsarxivorgabs260212212v1>#</a></h3><p><strong>Authors:</strong> Maurizio Fagotti
<strong>Venue:</strong> arXiv (2026)</p><p>Equilibrium statistical ensembles commute with the Hamiltonian and thus carry no coherence in the energy eigenbasis. We develop a thermodynamic framework in which energy fluctuations can retain genuinely quantum-coherent contributions. We foliate state space into &ldquo;minimum-variance leaves,&rdquo; defined by minimizing the average energy variance over all pure-state decompositions, with the minimum set by the quantum Fisher information. On each leaf we construct the least-biased state compatible with normalization and mean energy, defining a leaf-canonical ensemble. The Gibbs ensemble is recovered on the distinguished commuting leaf, while generic states are organized by their leaf label. This structure provides a natural setting to extend eigenstate thermalization beyond equilibrium via a &ldquo;leaf typicality&rdquo; hypothesis. According to that hypothesis, under unitary time evolution local observables depend only on the leaf and energy and, at all times, are reproduced by evolving a representative (pure) state drawn from the optimal ensemble.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12212v1">üìÑ Download PDF</a></p><hr><h3 id=benchmarking-vision-language-models-for-french-pdf-to-markdown-conversionhttpsarxivorgabs260211960v1><a href=https://arxiv.org/abs/2602.11960v1>Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion</a><a hidden class=anchor aria-hidden=true href=#benchmarking-vision-language-models-for-french-pdf-to-markdown-conversionhttpsarxivorgabs260211960v1>#</a></h3><p><strong>Authors:</strong> Bruno Rigal, Victor Dupriez, Alexis Mignon, Ronan Le Hy, Nicolas Mery
<strong>Venue:</strong> arXiv (2026)</p><p>This report evaluates PDF-to-Markdown conversion using recent Vision-Language Models (VLMs) on challenging French documents. Document parsing is a critical step for Retrieval-Augmented Generation (RAG) pipelines, where transcription and layout errors propagate to downstream retrieval and grounding. Existing benchmarks often emphasize English or Chinese and can over-penalize benign formatting and linearization choices (e.g., line breaks, list segmentation, alternative table renderings) that are largely irrelevant for downstream use.
We introduce a French-focused benchmark of difficult pages selected via model-disagreement sampling from a corpus of 60{,}000 documents, covering handwritten forms, complex layouts, dense tables, and graphics-rich pages. Evaluation is performed with unit-test-style checks that target concrete failure modes (text presence, reading order, and local table constraints) combined with category-specific normalization designed to discount presentation-only variance. Across 15 models, we observe substantially higher robustness for the strongest proprietary models on handwriting and forms, while several open-weights systems remain competitive on standard printed layouts.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.11960v1">üìÑ Download PDF</a></p><hr><h3 id=thinking-with-drafting-optical-decompression-via-logical-reconstructionhttpsarxivorgabs260211731v1><a href=https://arxiv.org/abs/2602.11731v1>Thinking with Drafting: Optical Decompression via Logical Reconstruction</a><a hidden class=anchor aria-hidden=true href=#thinking-with-drafting-optical-decompression-via-logical-reconstructionhttpsarxivorgabs260211731v1>#</a></h3><p><strong>Authors:</strong> Jingxuan Wei, Honghao He, Caijun Jia, Siyuan Li, Zheng Sun, Yuhang Xu, Yuanyuan Lin, Linzhuang Sun, Yuchen Wu, Bihui Yu, Xiangxiang Zhang, Cheng Tan
<strong>Venue:</strong> arXiv (2026)</p><p>Existing multimodal large language models have achieved high-fidelity visual perception and exploratory visual generation. However, a precision paradox persists in complex reasoning tasks: optical perception systems transcribe symbols without capturing logical topology, while pixel-based generative models produce visual artifacts lacking mathematical exactness. To bridge this gap, we propose that reasoning over visual inputs be reconceptualized as optical decompression-the process of reconstructing latent logical structures from compressed visual tokens. Guided by the axiom that Parsing is Reasoning, we introduce Thinking with Drafting (TwD), which utilizes a minimalist Domain-Specific Language (DSL) as a grounding intermediate representation. Unlike standard approaches that hallucinate answers directly, TwD forces the model to draft its mental model into executable code, rendering deterministic visual proofs for self-verification. To validate this, we present VisAlg, a visual algebra benchmark. Experiments demonstrate that TwD serve as a superior cognitive scaffold. Our work establishes a closed-loop system where visual generation acts not as a creative output but as a logical verifier, offering a generalizable path for visual reasoning.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.11731v1">üìÑ Download PDF</a></p><hr><h3 id=do-mllms-really-understand-space-a-mathematical-reasoning-evaluationhttpsarxivorgabs260211635v1><a href=https://arxiv.org/abs/2602.11635v1>Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation</a><a hidden class=anchor aria-hidden=true href=#do-mllms-really-understand-space-a-mathematical-reasoning-evaluationhttpsarxivorgabs260211635v1>#</a></h3><p><strong>Authors:</strong> Shuo Lu, Jianjie Cheng, Yinuo Xu, Yongcan Yu, Lijun Sheng, Peijie Wang, Siru Jiang, Yongguan Hu, Run Ling, Yihua Shao, Ao Ma, Wei Feng, Lingxiao He, Meng Wang, Qianlong Xie, Xingxing Wang, Ran He, Jian Liang
<strong>Venue:</strong> arXiv (2026)</p><p>Multimodal large language models (MLLMs) have achieved strong performance on perception-oriented tasks, yet their ability to perform mathematical spatial reasoning, defined as the capacity to parse and manipulate two- and three-dimensional relations, remains unclear. Humans easily solve textbook-style spatial reasoning problems with over 95% accuracy, but we find that most leading MLLMs fail to reach even 60% on the same tasks. This striking gap highlights spatial reasoning as a fundamental weakness of current models. To investigate this gap, we present MathSpatial, a unified framework for evaluating and improving spatial reasoning in MLLMs. MathSpatial includes three complementary components: (i) MathSpatial-Bench, a benchmark of 2K problems across three categories and eleven subtypes, designed to isolate reasoning difficulty from perceptual noise; (ii) MathSpatial-Corpus, a training dataset of 8K additional problems with verified solutions; and (iii) MathSpatial-SRT, which models reasoning as structured traces composed of three atomic operations&ndash;Correlate, Constrain, and Infer. Experiments show that fine-tuning Qwen2.5-VL-7B on MathSpatial achieves competitive accuracy while reducing tokens by 25%. MathSpatial provides the first large-scale resource that disentangles perception from reasoning, enabling precise measurement and comprehensive understanding of mathematical spatial reasoning in MLLMs.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.11635v1">üìÑ Download PDF</a></p><hr><h3 id=when-visibility-outpaces-verification-delayed-verification-and-narrative-lock-in-in-agentic-ai-discoursehttpsarxivorgabs260211412v1><a href=https://arxiv.org/abs/2602.11412v1>When Visibility Outpaces Verification: Delayed Verification and Narrative Lock-in in Agentic AI Discourse</a><a hidden class=anchor aria-hidden=true href=#when-visibility-outpaces-verification-delayed-verification-and-narrative-lock-in-in-agentic-ai-discoursehttpsarxivorgabs260211412v1>#</a></h3><p><strong>Authors:</strong> Hanjing Shi, Dominic DiFranzo
<strong>Venue:</strong> arXiv (2026)</p><p>Agentic AI systems-autonomous entities capable of independent planning and execution-reshape the landscape of human-AI trust. Long before direct system exposure, user expectations are mediated through high-stakes public discourse on social platforms. However, platform-mediated engagement signals (e.g., upvotes) may inadvertently function as a <code>credibility proxy,'' potentially stifling critical evaluation. This paper investigates the interplay between social proof and verification timing in online discussions of agentic AI. Analyzing a longitudinal dataset from two distinct Reddit communities with contrasting interaction cultures-r/OpenClaw and r/Moltbook-we operationalize verification cues via reproducible lexical rules and model the </code>time-to-first-verification&rsquo;&rsquo; using a right-censored survival analysis framework.
Our findings reveal a systemic <code>Popularity Paradox'': high-visibility discussions in both subreddits experience significantly delayed or entirely absent verification cues compared to low-visibility threads. This temporal lag creates a critical window for </code>Narrative Lock-in,&rsquo;&rsquo; where early, unverified claims crystallize into collective cognitive biases before evidence-seeking behaviors emerge. We discuss the implications of this <code>credibility-by-visibility'' effect for AI safety and propose </code>epistemic friction&rsquo;&rsquo; as a design intervention to rebalance engagement-driven platforms.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.11412v1">üìÑ Download PDF</a></p><hr><h3 id=bizarre-love-triangle-generative-ai-art-and-kitschhttpsarxivorgabs260211353v1><a href=https://arxiv.org/abs/2602.11353v1>Bizarre Love Triangle: Generative AI, Art, and Kitsch</a><a hidden class=anchor aria-hidden=true href=#bizarre-love-triangle-generative-ai-art-and-kitschhttpsarxivorgabs260211353v1>#</a></h3><p><strong>Authors:</strong> Dejan Grba
<strong>Venue:</strong> arXiv (2026)</p><p>Generative artificial intelligence (GenAI) has engrossed the mainstream culture, expanded AI&rsquo;s creative user base, and catalyzed economic, legal, and aesthetic issues that stir a lively public debate. Unsurprisingly, GenAI tools proliferate kitsch in the hands of amateurs and hobbyists, but various shortcomings also induce kitsch into a more ambitious, professional artists&rsquo; production with GenAI. I explore them in this paper. Following the introductory outline of digital kitsch and AI art, I review GenAI artworks that manifest five interrelated types of kitsch-engendering expressive flaws: the superficial foregrounding or faulty circumvention of generative models&rsquo; formal signatures, the feeble critique of AI, the mimetics, and the unacknowledged poetic similarities, all marked by an overreliance on AI as a cultural signifier. I discuss the normalization of these blunders through GenAI art&rsquo;s good standing within the art world and keen relationship with the AI industry, which contributes to the adulteration of AI discourse and the possible corruption of artistic literacy. In conclusion, I emphasize that recognizing different facets of artists&rsquo; uncritical embrace of techno-cultural trends, comprehending their functions, and anticipating their unintended effects is crucial for reaching relevance and responsibility in AI art.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.11353v1">üìÑ Download PDF</a></p><hr><h3 id=repulsive-gravitational-force-as-a-witness-of-the-quantum-nature-of-gravityhttpsarxivorgabs260212266v1><a href=https://arxiv.org/abs/2602.12266v1>Repulsive Gravitational Force as a Witness of the Quantum Nature of Gravity</a><a hidden class=anchor aria-hidden=true href=#repulsive-gravitational-force-as-a-witness-of-the-quantum-nature-of-gravityhttpsarxivorgabs260212266v1>#</a></h3><p><strong>Authors:</strong> Pablo L. Saldanha, Chiara Marletto, Vlatko Vedral
<strong>Venue:</strong> arXiv (2026)</p><p>We show that a single spatially superposed &lsquo;source&rsquo; mass acting on a &lsquo;probe&rsquo; matter wavepacket can reveal the quantum nature of the gravitational field. For this we use a specific state preparation and measurement of the superposed source mass, including a postselection, which altogether results in a repulsive gravitational force on the probe particle. A classical gravitational field can never lead to repulsion, as the effect requires quantum interference of two distinct states of gravity. We also present a calculation in the Heisenberg picture under the formalism of weak values that illustrates how repulsion is achieved. Finally, we estimate the range of parameters (masses and the spatio-temporal extent of interference) for which the experiment is feasible.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12266v1">üìÑ Download PDF</a></p><hr><h3 id=post-measurement-states-are-very-useful-for-measurement-discriminationhttpsarxivorgabs260212258v1><a href=https://arxiv.org/abs/2602.12258v1>Post-measurement states are (very) useful for measurement discrimination</a><a hidden class=anchor aria-hidden=true href=#post-measurement-states-are-very-useful-for-measurement-discriminationhttpsarxivorgabs260212258v1>#</a></h3><p><strong>Authors:</strong> Charbel Eid, Marco T√∫lio Quintino
<strong>Venue:</strong> arXiv (2026)</p><p>The standard approach to quantum measurement discrimination is to perform the given unknown measurement on a probe state, possibly entangled with an auxiliary system, and make a decision based on the measurement outcome obtained. In this work, we go beyond the standard aforementioned scenarios by consider not only the classical measurement outcome of a measurement, but also its the post-measurement quantum state. More specifically, instead of considering only the positive-operator valued measure (POVM) operators, we consider their associated L√ºders&rsquo; instrument associated with them. We prove that, when the post-measurement quantum states are available, the task of discriminating two qubit projective measurements is equivalent to discriminating two copies of quantum states associated to each projector pair, extending previous results known for the case where probe states are separable. Then, we proceed by showing that the advantage of considering post-measurement states in measurement discrimination can be large. We formalise this claim by presenting a family of pairs of measurements where the ratio between the discrimination bias of the measurement discrimination task with and without post-measurement states can be arbitrarily large. This shows that, while the post-measurement state was neglected in most of the previous literature, its use can significantly improve the performance of quantum measurement discrimination.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12258v1">üìÑ Download PDF</a></p><hr><h3 id=sorry-i-didnt-catch-that-how-speech-models-miss-what-matters-mosthttpsarxivorgabs260212249v1><a href=https://arxiv.org/abs/2602.12249v1>&ldquo;Sorry, I Didn&rsquo;t Catch That&rdquo;: How Speech Models Miss What Matters Most</a><a hidden class=anchor aria-hidden=true href=#sorry-i-didnt-catch-that-how-speech-models-miss-what-matters-mosthttpsarxivorgabs260212249v1>#</a></h3><p><strong>Authors:</strong> Kaitlyn Zhou, Martijn Bartelds, Federico Bianchi, James Zou
<strong>Venue:</strong> arXiv (2026)</p><p>Despite speech recognition systems achieving low word error rates on standard benchmarks, they often fail on short, high-stakes utterances in real-world deployments. Here, we study this failure mode in a high-stakes task: the transcription of U.S. street names as spoken by U.S. participants. We evaluate 15 models from OpenAI, Deepgram, Google, and Microsoft on recordings from linguistically diverse U.S. speakers and find an average transcription error rate of 44%. We quantify the downstream impact of failed transcriptions by geographic locations and show that mis-transcriptions systematically cause errors for all speakers, but that routing distance errors are twice as large for non-English primary speakers compared to English primary speakers. To mitigate this harm, we introduce a synthetic data generation approach that produces diverse pronunciations of named entities using open-source text-to-speech models. Fine-tuning with less than 1,000 synthetic samples improves street name transcription accuracy by nearly 60% (relative to base models) for non-English primary speakers. Our results highlight a critical gap between benchmark performance and real-world reliability in speech systems and demonstrate a simple, scalable path to reducing high-stakes transcription errors.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12249v1">üìÑ Download PDF</a></p><hr><h3 id=the-observer-effect-in-world-models-invasive-adaptation-corrupts-latent-physicshttpsarxivorgabs260212218v1><a href=https://arxiv.org/abs/2602.12218v1>The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics</a><a hidden class=anchor aria-hidden=true href=#the-observer-effect-in-world-models-invasive-adaptation-corrupts-latent-physicshttpsarxivorgabs260212218v1>#</a></h3><p><strong>Authors:</strong> Christian Intern√≤, Jumpei Yamaguchi, Loren Amdahl-Culleton, Markus Olhofer, David Klindt, Barbara Hammer
<strong>Venue:</strong> arXiv (2026)</p><p>Determining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-capacity probes), but such interventions can change the representations being measured and thus confound what was learned during self-supervised learning (SSL). We propose a non-invasive evaluation protocol, PhyIP. We test whether physical quantities are linearly decodable from frozen representations, motivated by the linear representation hypothesis. Across fluid dynamics and orbital mechanics, we find that when SSL achieves low error, latent structure becomes linearly accessible. PhyIP recovers internal energy and Newtonian inverse-square scaling on OOD tests (e.g., $œÅ> 0.90$). In contrast, adaptation-based evaluations can collapse this structure ($œÅ\approx 0.05$). These findings suggest that adaptation-based evaluation can obscure latent structures and that low-capacity probes offer a more accurate evaluation of physical world models.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12218v1">üìÑ Download PDF</a></p><hr><h3 id=ubiquitous-yet-forgotten-broad-absorptions-in-the-optical-spectra-of-low-mass-x-ray-binarieshttpsarxivorgabs260212282v1><a href=https://arxiv.org/abs/2602.12282v1>Ubiquitous yet forgotten: broad absorptions in the optical spectra of low-mass X-ray binaries</a><a hidden class=anchor aria-hidden=true href=#ubiquitous-yet-forgotten-broad-absorptions-in-the-optical-spectra-of-low-mass-x-ray-binarieshttpsarxivorgabs260212282v1>#</a></h3><p><strong>Authors:</strong> D. Mata Sanchez, T. Munoz-Darias, J. Casares, M. A. P. Torres, M. Armas Padilla
<strong>Venue:</strong> arXiv (2026)</p><p>Optical outburst spectra of low-mass X-ray binaries enable studies of extreme accretion and ejection phenomena. While some of their spectroscopic features have been analysed in detail, the appearance of broad absorptions in the optical regime has been traditionally neglected. In this work, we introduce the first population study dedicated to these features with the aim to understand their fundamental properties and discuss them in the context of their origin. We complement the study with a spectroscopic database of six low-mass X-ray binaries during outburst, in order to assess their evolution. We find that broad absorptions are ubiquitous, with the majority of black hole low-mass X-ray binaries exhibiting them in spite of a typically scarce outburst coverage. Their detection does not depend on the orbital inclination or the compact object nature, but they seem favoured in systems with orbital periods shorter than &lt; 11 h. They predominantly occur in the hydrogen Balmer series, being stronger at shorter wavelengths, and they are detected across all X-ray states. We find that the normalised depth of these broad absorptions is anti-correlated with the system luminosity, and that they show constant line ratios over the whole sample. Based on these properties, we favour a scenario where BAs arise from a stable, optically thick layer of the accretion disc, below the hotter chromosphere-like region producing the emission line components. Our study is consistent with the continuous presence of broad absorptions during the whole outburst, with their visibility being conditioned by the emission lines filling the broad absorption profile and veiling by the X-ray reprocessed continuum.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12282v1">üìÑ Download PDF</a></p><hr><h3 id=is-online-linear-optimization-sufficient-for-strategic-robustnesshttpsarxivorgabs260212253v1><a href=https://arxiv.org/abs/2602.12253v1>Is Online Linear Optimization Sufficient for Strategic Robustness?</a><a hidden class=anchor aria-hidden=true href=#is-online-linear-optimization-sufficient-for-strategic-robustnesshttpsarxivorgabs260212253v1>#</a></h3><p><strong>Authors:</strong> Yang Cai, Haipeng Luo, Chen-Yu Wei, Weiqiang Zheng
<strong>Venue:</strong> arXiv (2026)</p><p>We consider bidding in repeated Bayesian first-price auctions. Bidding algorithms that achieve optimal regret have been extensively studied, but their strategic robustness to the seller&rsquo;s manipulation remains relatively underexplored. Bidding algorithms based on no-swap-regret algorithms achieve both desirable properties, but are suboptimal in terms of statistical and computational efficiency. In contrast, online gradient ascent is the only algorithm that achieves $O(\sqrt{TK})$ regret and strategic robustness [KSS24], where $T$ denotes the number of auctions and $K$ the number of bids.
In this paper, we explore whether simple online linear optimization (OLO) algorithms suffice for bidding algorithms with both desirable properties. Our main result shows that sublinear linearized regret is sufficient for strategic robustness. Specifically, we construct simple black-box reductions that convert any OLO algorithm into a strategically robust no-regret bidding algorithm, in both known and unknown value distribution settings. For the known value distribution case, our reduction yields a bidding algorithm that achieves $O(\sqrt{T \log K})$ regret and strategic robustness (with exponential improvement on the $K$-dependence compared to [KSS24]). For the unknown value distribution case, our reduction gives a bidding algorithm with high-probability $O(\sqrt{T (\log K+\log(T/Œ¥)})$ regret and strategic robustness, while removing the bounded density assumption made in [KSS24].</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12253v1">üìÑ Download PDF</a></p><hr><h3 id=community-concealment-from-unsupervised-graph-learning-based-clusteringhttpsarxivorgabs260212250v1><a href=https://arxiv.org/abs/2602.12250v1>Community Concealment from Unsupervised Graph Learning-Based Clustering</a><a hidden class=anchor aria-hidden=true href=#community-concealment-from-unsupervised-graph-learning-based-clusteringhttpsarxivorgabs260212250v1>#</a></h3><p><strong>Authors:</strong> Dalyapraz Manatova, Pablo Moriano, L. Jean Camp
<strong>Venue:</strong> arXiv (2026)</p><p>Graph neural networks (GNNs) are designed to use attributed graphs to learn representations. Such representations are beneficial in the unsupervised learning of clusters and community detection. Nonetheless, such inference may reveal sensitive groups, clustered systems, or collective behaviors, raising concerns regarding group-level privacy. Community attribution in social and critical infrastructure networks, for example, can expose coordinated asset groups, operational hierarchies, and system dependencies that could be used for profiling or intelligence gathering. We study a defensive setting in which a data publisher (defender) seeks to conceal a community of interest while making limited, utility-aware changes in the network. Our analysis indicates that community concealment is strongly influenced by two quantifiable factors: connectivity at the community boundary and feature similarity between the protected community and adjacent communities. Informed by these findings, we present a perturbation strategy that rewires a set of selected edges and modifies node features to reduce the distinctiveness leveraged by GNN message passing. The proposed method outperforms DICE in our experiments on synthetic benchmarks and real network graphs under identical perturbation budgets. Overall, it achieves median relative concealment improvements of approximately 20-45% across the evaluated settings. These findings demonstrate a mitigation strategy against GNN-based community learning and highlight group-level privacy risks intrinsic to graph learning.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12250v1">üìÑ Download PDF</a></p><hr><h3 id=hilberts-program-and-infinityhttpsarxivorgabs260212131v1><a href=https://arxiv.org/abs/2602.12131v1>Hilbert&rsquo;s Program and Infinity</a><a hidden class=anchor aria-hidden=true href=#hilberts-program-and-infinityhttpsarxivorgabs260212131v1>#</a></h3><p><strong>Authors:</strong> Richard Zach
<strong>Venue:</strong> arXiv (2026)</p><p>The primary aim of Hilbert&rsquo;s proof theory was to establish the consistency of classical mathematics using finitary means only. Hilbert&rsquo;s strategy for doing this was to eliminate the infinite (in the form of unbounded quantifiers) from formalized proofs using the so-called epsilon substitution method. The result is a formal proof which does not mention or appeal to infinite objects or &ldquo;concept-formations.&rdquo; However, as later developments showed, the consistency proof itself lets the infinite back into proof theory, through a back door, so to speak. The paper outlines the epsilon substitution method as an example of how proof-theoretic constructions &ldquo;eliminate the infinite&rdquo; from formal proofs, and how they aim to establish conservativity and consistency. The proof also requires an argument that this proof theoretic construction always works. This second argument, however, requires possibly infinitary reasoning at the meta-level, using induction on ordinal notations.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12131v1">üìÑ Download PDF</a></p><hr><h3 id=unravelling-abstract-cyclic-proofs-into-proofs-by-inductionhttpsarxivorgabs260212054v1><a href=https://arxiv.org/abs/2602.12054v1>Unravelling Abstract Cyclic Proofs into Proofs by Induction</a><a hidden class=anchor aria-hidden=true href=#unravelling-abstract-cyclic-proofs-into-proofs-by-inductionhttpsarxivorgabs260212054v1>#</a></h3><p><strong>Authors:</strong> Lide Grotenhuis, Dani√´l Otten
<strong>Venue:</strong> arXiv (2026)</p><p>Cyclic proof theory breaks tradition by allowing certain infinite proofs: those that can be represented by a finite graph, while satisfying a soundness condition. We reconcile cyclic proofs with traditional finite proofs: we extend abstract cyclic proof systems with a well-founded induction principle, and transform any cyclic proof into a finite proof in the extended system. Moreover, this transformation preserves the structure of the cyclic proof.
Our results leverage an annotated representation of cyclic proofs, which allows us to extract induction hypotheses and to determine their introduction order. The representation is essentially a reset proof with one key modification: names must be covered in a uniform way before a reset. This innovation allows us to handle cyclic proofs where the underlying inductive sort is non-linear.
Our framework is general enough to cover recursive functions satisfying the size-change termination principle, which are viewed as cyclic proofs under the Curry-Howard correspondence.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12054v1">üìÑ Download PDF</a></p><hr><h3 id=lacy-what-small-language-models-can-and-should-learn-is-not-just-a-question-of-losshttpsarxivorgabs260212005v1><a href=https://arxiv.org/abs/2602.12005v1>LaCy: What Small Language Models Can and Should Learn is Not Just a Question of Loss</a><a hidden class=anchor aria-hidden=true href=#lacy-what-small-language-models-can-and-should-learn-is-not-just-a-question-of-losshttpsarxivorgabs260212005v1>#</a></h3><p><strong>Authors:</strong> Szilvia Ujv√°ry, Louis B√©thune, Pierre Ablin, Jo√£o Monteiro, Marco Cuturi, Michael Kirchhof
<strong>Venue:</strong> arXiv (2026)</p><p>Language models have consistently grown to compress more world knowledge into their parameters, but the knowledge that can be pretrained into them is upper-bounded by their parameter size. Especially the capacity of Small Language Models (SLMs) is limited, leading to factually incorrect generations. This problem is often mitigated by giving the SLM access to an outside source: the ability to query a larger model, documents, or a database. Under this setting, we study the fundamental question of \emph{which tokens an SLM can and should learn} during pretraining, versus \emph{which ones it should delegate} via a \texttt{<call>} token. We find that this is not simply a question of loss: although the loss is predictive of whether a predicted token mismatches the ground-truth, some tokens are \emph{acceptable} in that they are truthful alternative continuations of a pretraining document, and should not trigger a \texttt{<call>} even if their loss is high. We find that a spaCy grammar parser can help augment the loss signal to decide which tokens the SLM should learn to delegate to prevent factual errors and which are safe to learn and predict even under high losses. We propose LaCy, a novel pretraining method based on this token selection philosophy. Our experiments demonstrate that LaCy models successfully learn which tokens to predict and where to delegate for help. This results in higher FactScores when generating in a cascade with a bigger model and outperforms Rho or LLM-judge trained SLMs, while being simpler and cheaper.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12005v1">üìÑ Download PDF</a></p><hr><h3 id=leveraging-llms-to-support-co-evolution-between-definitions-and-instances-of-textual-dsls-a-systematic-evaluationhttpsarxivorgabs260211904v1><a href=https://arxiv.org/abs/2602.11904v1>Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs: A Systematic Evaluation</a><a hidden class=anchor aria-hidden=true href=#leveraging-llms-to-support-co-evolution-between-definitions-and-instances-of-textual-dsls-a-systematic-evaluationhttpsarxivorgabs260211904v1>#</a></h3><p><strong>Authors:</strong> Weixing Zhang, Bowen Jiang, Yuhong Fu, Anne Koziolek, Regina Hebig, Daniel Str√ºber
<strong>Venue:</strong> arXiv (2026)</p><p>Software languages evolve over time for reasons such as feature additions. When grammars evolve, textual instances that originally conformed to them may become outdated. While model-driven engineering provides many techniques for co-evolving models with metamodel changes, these approaches are not designed for textual DSLs and may lose human-relevant information such as layout and comments. This study systematically evaluates the potential of large language models (LLMs) for co-evolving grammars and instances of textual DSLs. Using Claude Sonnet 4.5 and GPT-5.2 across ten case languages with ten runs each, we assess both correctness and preservation of human-oriented information. Results show strong performance on small-scale cases ($\geq$94% precision and recall for instances requiring fewer than 20 modified lines), but performance degraded with scale: Claude maintains 85% recall at 40 lines, while GPT fails on the largest instances. Response time increases substantially with instance size, and grammar evolution complexity and deletion granularity affect performance more than change type. These findings clarify when LLM-based co-evolution is effective and where current limitations remain.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.11904v1">üìÑ Download PDF</a></p><hr><h3 id=llm-based-triplet-extraction-from-financial-reportshttpsarxivorgabs260211886v1><a href=https://arxiv.org/abs/2602.11886v1>LLM-based Triplet Extraction from Financial Reports</a><a hidden class=anchor aria-hidden=true href=#llm-based-triplet-extraction-from-financial-reportshttpsarxivorgabs260211886v1>#</a></h3><p><strong>Authors:</strong> Dante Wesslund, Ville Stenstr√∂m, Pontus Linde, Alexander Holmberg
<strong>Venue:</strong> arXiv (2026)</p><p>Corporate financial reports are a valuable source of structured knowledge for Knowledge Graph construction, but the lack of annotated ground truth in this domain makes evaluation difficult. We present a semi-automated pipeline for Subject-Predicate-Object triplet extraction that uses ontology-driven proxy metrics, specifically Ontology Conformance and Faithfulness, instead of ground-truth-based evaluation. We compare a static, manually engineered ontology against a fully automated, document-specific ontology induction approach across different LLMs and two corporate annual reports. The automatically induced ontology achieves 100% schema conformance in all configurations, eliminating the ontology drift observed with the manual approach. We also propose a hybrid verification strategy that combines regex matching with an LLM-as-a-judge check, reducing apparent subject hallucination rates from 65.2% to 1.6% by filtering false positives caused by coreference resolution. Finally, we identify a systematic asymmetry between subject and object hallucinations, which we attribute to passive constructions and omitted agents in financial prose.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.11886v1">üìÑ Download PDF</a></p><hr><h2 id=-psycholinguistics>üîç psycholinguistics<a hidden class=anchor aria-hidden=true href=#-psycholinguistics>#</a></h2><h3 id=half-plane-non-coexistence-without-fkghttpsarxivorgabs260212261v1><a href=https://arxiv.org/abs/2602.12261v1>Half-plane non-coexistence without FKG</a><a hidden class=anchor aria-hidden=true href=#half-plane-non-coexistence-without-fkghttpsarxivorgabs260212261v1>#</a></h3><p><strong>Authors:</strong> Frederik Ravn Klausen, Noah Kravitz
<strong>Venue:</strong> arXiv (2026)</p><p>For $Œº$ an edge percolation measure on the infinite square lattice, let $Œº_{\textit{hp}}$ (respectively, $Œº^<em><em>{hp}$) denote its marginal (respectively, the marginal of its planar dual process) on the upper half-plane. We show that if $Œº$ is translation-invariant and ergodic and almost surely has only finitely many infinite clusters, then either almost surely $Œº</em>{hp}$ has no infinite cluster, or almost surely $Œº^</em>_{hp}$ has no infinite cluster. By the classical Burton&ndash;Keane argument, these hypotheses are satisfied if $Œº$ is translation-invariant and ergodic and has finite-energy. In contrast to previous ``non-coexistence&rsquo;&rsquo; theorems, our result does not impose a positive-correlation (FKG) hypothesis on $Œº$. Our arguments also apply to the random-cluster model (including the regime $q&lt;1$, which lacks FKG), the uniform spanning tree, and the uniform odd subgraph.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12261v1">üìÑ Download PDF</a></p><hr><h3 id=on-the-implicit-regularization-of-langevin-dynamics-with-projected-noisehttpsarxivorgabs260212257v1><a href=https://arxiv.org/abs/2602.12257v1>On the implicit regularization of Langevin dynamics with projected noise</a><a hidden class=anchor aria-hidden=true href=#on-the-implicit-regularization-of-langevin-dynamics-with-projected-noisehttpsarxivorgabs260212257v1>#</a></h3><p><strong>Authors:</strong> Govind Menon, Austin J. Stromme, Adrien Vacher
<strong>Venue:</strong> arXiv (2026)</p><p>We study Langevin dynamics with noise projected onto the directions orthogonal to an isometric group action. This mathematical model is introduced to shed new light on the effects of symmetry on stochastic gradient descent for over-parametrized models. Our main result identifies a novel form of implicit regularization: when the initial and target density are both invariant under the group action, Langevin dynamics with projected noise is equivalent in law to Langevin dynamics with isotropic diffusion but with an additional drift term proportional to the negative log volume of the group orbit. We prove this result by constructing a coupling of the two processes via a third process on the group itself, and identify the additional drift as the mean curvature of the orbits.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12257v1">üìÑ Download PDF</a></p><hr><h3 id=contention-resolution-with-and-without-a-global-clockhttpsarxivorgabs260212070v1><a href=https://arxiv.org/abs/2602.12070v1>Contention Resolution, With and Without a Global Clock</a><a hidden class=anchor aria-hidden=true href=#contention-resolution-with-and-without-a-global-clockhttpsarxivorgabs260212070v1>#</a></h3><p><strong>Authors:</strong> Zixi Cai, Kuowen Chen, Shengquan Du, Tsvi Kopelowitz, Seth Pettie, Ben Plosk
<strong>Venue:</strong> arXiv (2026)</p><p>In the Contention Resolution problem $n$ parties each wish to have exclusive use of a shared resource for one unit of time. The problem has been studied since the early 1970s, under a variety of assumptions on feedback given to the parties, how the parties wake up, knowledge of $n$, and so on. The most consistent assumption is that parties do not have access to a global clock, only their local time since wake-up. This is surprising because the assumption of a global clock is both technologically realistic and algorithmically interesting. It enriches the problem, and opens the door to entirely new techniques. Our primary results are: [1] We design a new Contention Resolution protocol that guarantees latency $$O\left(\left(n\log\log n\log^{(3)} n\log^{(4)} n\cdots \log^{(\log^* n)} n\right)\cdot 2^{\log^* n}\right) \le n(\log\log n)^{1+o(1)}$$ in expectation and with high probability. This already establishes at least a roughly $\log n$ complexity gap between randomized protocols in GlobalClock and LocalClock. [2] Prior analyses of randomized ContentionResolution protocols in LocalClock guaranteed a certain latency with high probability, i.e., with probability $1-1/\text{poly}(n)$. We observe that it is just as natural to measure expected latency, and prove a $\log n$-factor complexity gap between the two objectives for memoryless protocols. The In-Expectation complexity is $Œò(n \log n/\log\log n)$ whereas the With-High-Probability latency is $Œò(n\log^2 n/\log\log n)$. Three of these four upper and lower bounds are new. [3] Given the complexity separation above, one would naturally want a ContentionResolution protocol that is optimal under both the In-Expectation and With-High-Probability metrics. This is impossible! It is even impossible to achieve In-Expectation latency $o(n\log^2 n/(\log\log n)^2)$ and With-High-Probability latency $n\log^{O(1)} n$ simultaneously.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12070v1">üìÑ Download PDF</a></p><hr><h3 id=tracer-trajectory-risk-aggregation-for-critical-episodes-in-agentic-reasoninghttpsarxivorgabs260211409v1><a href=https://arxiv.org/abs/2602.11409v1>TRACER: Trajectory Risk Aggregation for Critical Episodes in Agentic Reasoning</a><a hidden class=anchor aria-hidden=true href=#tracer-trajectory-risk-aggregation-for-critical-episodes-in-agentic-reasoninghttpsarxivorgabs260211409v1>#</a></h3><p><strong>Authors:</strong> Sina Tayebati, Divake Kumar, Nastaran Darabi, Davide Ettori, Ranganath Krishnan, Amit Ranjan Trivedi
<strong>Venue:</strong> arXiv (2026)</p><p>Estimating uncertainty for AI agents in real-world multi-turn tool-using interaction with humans is difficult because failures are often triggered by sparse critical episodes (e.g., looping, incoherent tool use, or user-agent miscoordination) even when local generation appears confident. Existing uncertainty proxies focus on single-shot text generation and therefore miss these trajectory-level breakdown signals. We introduce TRACER, a trajectory-level uncertainty metric for dual-control Tool-Agent-User interaction. TRACER combines content-aware surprisal with situational-awareness signals, semantic and lexical repetition, and tool-grounded coherence gaps, and aggregates them using a tail-focused risk functional with a MAX-composite step risk to surface decisive anomalies. We evaluate TRACER on $œÑ^2$-bench by predicting task failure and selective task execution. To this end, TRACER improves AUROC by up to 37.1% and AUARC by up to 55% over baselines, enabling earlier and more accurate detection of uncertainty in complex conversational tool-use settings. Our code and benchmark are available at <a href=https://github.com/sinatayebati/agent-tracer>https://github.com/sinatayebati/agent-tracer</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.11409v1">üìÑ Download PDF</a></p><hr><h3 id=block-stacking-airplane-refueling-and-robust-appointment-schedulinghttpsarxivorgabs260211366v1><a href=https://arxiv.org/abs/2602.11366v1>Block Stacking, Airplane Refueling, and Robust Appointment Scheduling</a><a hidden class=anchor aria-hidden=true href=#block-stacking-airplane-refueling-and-robust-appointment-schedulinghttpsarxivorgabs260211366v1>#</a></h3><p><strong>Authors:</strong> Simon Gmeiner, Andreas S. Schulz
<strong>Venue:</strong> arXiv (2026)</p><p>How can a stack of identical blocks be arranged to extend beyond the edge of a table as far as possible? We consider a generalization of this classic puzzle to blocks that differ in width and mass. Despite the seemingly simple premise, we demonstrate that it is unlikely that one can efficiently determine a stack configuration of maximum overhang. Formally, we prove that the Block-Stacking Problem is NP-hard, partially answering an open question from the literature. Furthermore, we demonstrate that the restriction to stacks without counterweights has a surprising connection to the Airplane Refueling Problem, another famous puzzle, and to Robust Appointment Scheduling, a problem of practical relevance. In addition to revealing a remarkable relation to the real-world challenge of devising schedules under uncertainty, their equivalence unveils a polynomial-time approximation scheme, that is, a $(1+Œµ)$-approximation algorithm, for Block Stacking without counterbalancing and a $(2+Œµ)$-approximation algorithm for the general case.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.11366v1">üìÑ Download PDF</a></p><hr><h3 id=interpolation-and-prewar-postwar-output-volatility-and-shock-persistence-debate-a-closer-look-and-new-resultshttpsarxivorgabs260211334v1><a href=https://arxiv.org/abs/2602.11334v1>Interpolation and Prewar-Postwar Output Volatility and Shock-Persistence Debate: A Closer Look and New Results</a><a hidden class=anchor aria-hidden=true href=#interpolation-and-prewar-postwar-output-volatility-and-shock-persistence-debate-a-closer-look-and-new-resultshttpsarxivorgabs260211334v1>#</a></h3><p><strong>Authors:</strong> Hashem Dezhbakhsh, Daniel Levy
<strong>Venue:</strong> arXiv (2026)</p><p>It is well established that the US prewar output was more volatile and less shock persistent than the postwar output. This is often attributed to the data interpolation employed to construct the prewar series. Our analytical results, however, indicate that commonly used linear interpolation has the opposite effect on shock persistence and volatility of a series - it increases shock persistence and reduces volatility. The surprising implication of this finding is that the actual differences between the volatility and shock persistence of the prewar and postwar output series are likely greater than the existing literature recognizes, and interpolation has dampened rather than magnified this difference. Consequently, the view that postwar output was more stable than prewar output because of the effectiveness of the postwar stabilization policies and institutional changes has considerable merit. Our results hold for parsimonious stationary and nonstationary time series commonly used to model macroeconomic time series</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.11334v1">üìÑ Download PDF</a></p><hr><h3 id=reionization-bubbles-from-real-space-cross-correlations-of-line-intensity-mapshttpsarxivorgabs260212277v1><a href=https://arxiv.org/abs/2602.12277v1>Reionization Bubbles from Real-Space Cross Correlations of Line Intensity Maps</a><a hidden class=anchor aria-hidden=true href=#reionization-bubbles-from-real-space-cross-correlations-of-line-intensity-mapshttpsarxivorgabs260212277v1>#</a></h3><p><strong>Authors:</strong> Emilie Th√©lie, Sarah Libanore, Yonatan Sklansky, Julian B. Mu√±oz, Ely D. Kovetz
<strong>Venue:</strong> arXiv (2026)</p><p>We propose a new way to reconstruct the ionized-bubble size distribution during the Epoch of Reionization (EoR) through the real-space cross-correlation of 21-cm and star-forming line-intensity maps. Understanding the evolution and timing of the EoR is crucial for both astrophysics and cosmology, and a wealth of information on the first sources can be extracted from the study of ionized bubbles. Nevertheless, directly mapping bubbles is challenging due to the high redshifts involved, possible selection biases, and foregrounds in 21-cm maps. Here, we exploit the real-space cross-correlation $Œæ_{21,ŒΩ}$ between 21-cm and line-intensity mapping (LIM) signals to reconstruct the evolution of bubble sizes during reionization. For the first time, we show that $Œæ_{21,ŒΩ}(r)$ departs from a saturation level for each separation $r$ when bubbles of size $r$ begin to form, providing a handle for the onset of bubbles of each radius. Moreover, we demonstrate that $Œæ_{21,ŒΩ}$ evolves from positive to negative as the EoR progresses, reaching a minimum (i.e. maximum anti-correlation) when bubbles of radius $r$ reach peak abundance. We show that these results are robust to changes in the astrophysical model as well as the timing/topology of reionization. This real-space observable complements usual Fourier-space estimators by capturing the localized nature of bubbles, offering new insights into the sources driving cosmic reionization.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12277v1">üìÑ Download PDF</a></p><hr><h3 id=moonshine-v2-ergodic-streaming-encoder-asr-for-latency-critical-speech-applicationshttpsarxivorgabs260212241v1><a href=https://arxiv.org/abs/2602.12241v1>Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications</a><a hidden class=anchor aria-hidden=true href=#moonshine-v2-ergodic-streaming-encoder-asr-for-latency-critical-speech-applicationshttpsarxivorgabs260212241v1>#</a></h3><p><strong>Authors:</strong> Manjunath Kudlur, Evan King, James Wang, Pete Warden
<strong>Venue:</strong> arXiv (2026)</p><p>Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this global dependency incurs quadratic complexity in sequence length, inducing an inherent &ldquo;encode-the-whole-utterance&rdquo; latency profile. For streaming use cases, this causes TTFT to grow linearly with utterance length as the encoder must process the entire prefix before any decoder token can be emitted. To better meet the needs of on-device, streaming ASR use cases we introduce Moonshine v2, an ergodic streaming-encoder ASR model that employs sliding-window self-attention to achieve bounded, low-latency inference while preserving strong local context. Our models achieve state of the art word error rates across standard benchmarks, attaining accuracy on-par with models 6x their size while running significantly faster. These results demonstrate that carefully designed local attention is competitive with the accuracy of full attention at a fraction of the size and latency cost, opening new possibilities for interactive speech interfaces on edge devices.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12241v1">üìÑ Download PDF</a></p><hr><h3 id=extending-the-cosmological-collider-new-scaling-regimes-and-constraints-from-bosshttpsarxivorgabs260212232v1><a href=https://arxiv.org/abs/2602.12232v1>Extending the Cosmological Collider: New Scaling Regimes and Constraints from BOSS</a><a hidden class=anchor aria-hidden=true href=#extending-the-cosmological-collider-new-scaling-regimes-and-constraints-from-bosshttpsarxivorgabs260212232v1>#</a></h3><p><strong>Authors:</strong> Daniel Green, Jiashu Han, Benjamin Wallisch
<strong>Venue:</strong> arXiv (2026)</p><p>Primordial non-Gaussianity generated by additional fields during inflation offers a compelling observational target. Heavy fields imprint characteristic oscillatory signals in non-Gaussian correlation functions of the inflaton, a process sometimes referred to as cosmological-collider physics. These distinct signatures are compelling windows into ultra-high-energy physics, but are often suppressed, making standard equilateral non-Gaussianity the most promising discovery channel in many scenarios. In this paper, we show that direct couplings between the inflaton and additional fields can lead to a wide variety of novel, observationally relevant signals which open new parameter regimes that simultaneously exhibit the characteristics of light and heavy fields. We identify these primordial signatures in the late-time observables of the large-scale structure of the Universe, where they most significantly modify the scale-dependent bias of the galaxy power spectrum to include an oscillatory modulation around a non-trivial power law. We explore the full range of parameters that phenomenologically arise in these models and study the sensitivity of current and future galaxy surveys, finding that this new class of primordial non-Gaussianity is particularly accessible in near-term surveys due to its oscillatory feature. Finally, we perform an analysis of existing data from the final release of the Baryon Oscillation Spectroscopic Survey (BOSS DR12). While we find no evidence for a signal, we demonstrate significant improvements in sensitivity over respective non-oscillatory scenarios and place the first constraints on this extended parameter space of oscillatory non-Gaussianity.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12232v1">üìÑ Download PDF</a></p><hr><h3 id=virena-virtual-arena-for-research-education-and-democratic-innovationhttpsarxivorgabs260212207v1><a href=https://arxiv.org/abs/2602.12207v1>VIRENA: Virtual Arena for Research, Education, and Democratic Innovation</a><a hidden class=anchor aria-hidden=true href=#virena-virtual-arena-for-research-education-and-democratic-innovationhttpsarxivorgabs260212207v1>#</a></h3><p><strong>Authors:</strong> Emma Hoes, K. Jonathan Klueser, Fabrizio Gilardi
<strong>Venue:</strong> arXiv (2026)</p><p>Digital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platform that enables controlled experimentation in realistic social media environments. Multiple participants interact simultaneously in realistic replicas of feed-based platforms (Instagram, Facebook, Reddit) and messaging apps (WhatsApp, Messenger). Large language model-powered AI agents participate alongside humans with configurable personas and realistic behavior. Researchers can manipulate content moderation approaches, pre-schedule stimulus content, and run experiments across conditions through a visual interface requiring no programming skills. VIRENA makes possible research designs that were previously impractical: studying human&ndash;AI interaction in realistic social contexts, experimentally comparing moderation interventions, and observing group deliberation as it unfolds. Built on open-source technologies that ensure data remain under institutional control and comply with data protection requirements, VIRENA is currently in use at the University of Zurich and available for pilot collaborations. Designed for researchers, educators, and public organizations alike, VIRENA&rsquo;s no-code interface makes controlled social media simulation accessible across disciplines and sectors. This paper documents its design, architecture, and capabilities.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12207v1">üìÑ Download PDF</a></p><hr><h3 id=making-the-complete-openaire-citation-graph-easily-accessible-through-compact-data-representationhttpsarxivorgabs260212206v1><a href=https://arxiv.org/abs/2602.12206v1>Making the complete OpenAIRE citation graph easily accessible through compact data representation</a><a hidden class=anchor aria-hidden=true href=#making-the-complete-openaire-citation-graph-easily-accessible-through-compact-data-representationhttpsarxivorgabs260212206v1>#</a></h3><p><strong>Authors:</strong> Joakim Skarding, Pavel Sanda
<strong>Venue:</strong> arXiv (2026)</p><p>The OpenAIRE graph contains a large citation graph dataset, with over 200 million publications and over 2 billion citations. The current graph is available as a dump with metadata which uncompressed totals ~TB. This makes it hard to process on conventional computers. To make this network more available for the community we provide a processed OpenAIRE graph which is downscaled to 32GB, while preserving the full graph structure. Apart from this we offer the processed data in very simple format, which allows further straightforward manipulation. We also provide a python pipeline, which can be used to process the next releases of the OpenAIRE graph.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12206v1">üìÑ Download PDF</a></p><hr><h3 id=6g-empowering-future-robotics-a-vision-for-next-generation-autonomous-systemshttpsarxivorgabs260212246v1><a href=https://arxiv.org/abs/2602.12246v1>6G Empowering Future Robotics: A Vision for Next-Generation Autonomous Systems</a><a hidden class=anchor aria-hidden=true href=#6g-empowering-future-robotics-a-vision-for-next-generation-autonomous-systemshttpsarxivorgabs260212246v1>#</a></h3><p><strong>Authors:</strong> Mona Ghassemian, Andr√©s Meseguer Valenzuela, Ana Garcia Armada, Dejan Vukobratovic, Periklis Chatzimisios, Kaspar Althoefer, Ranga Rao Venkatesha Prasad
<strong>Venue:</strong> arXiv (2026)</p><p>The convergence of robotics and next-generation communication is a critical driver of technological advancement. As the world transitions from 5G to 6G, the foundational capabilities of wireless networks are evolving to support increasingly complex and autonomous robotic systems. This paper examines the transformative impact of 6G on enhancing key robotics functionalities. It provides a systematic mapping of IMT-2030 key performance indicators to robotic functional blocks including sensing, perception, cognition, actuation and self-learning. Building upon this mapping, we propose a high-level architectural framework integrating robotic, intelligent, and network service planes, underscoring the need for a holistic approach. As an example use case, we present a real-time, dynamic safety framework enabled by IMT-2030 capabilities for safe and efficient human-robot collaboration in shared spaces.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12246v1">üìÑ Download PDF</a></p><hr><h3 id=gpt-4o-lacks-core-features-of-theory-of-mindhttpsarxivorgabs260212150v1><a href=https://arxiv.org/abs/2602.12150v1>GPT-4o Lacks Core Features of Theory of Mind</a><a hidden class=anchor aria-hidden=true href=#gpt-4o-lacks-core-features-of-theory-of-mindhttpsarxivorgabs260212150v1>#</a></h3><p><strong>Authors:</strong> John Muchovej, Amanda Royka, Shane Lee, Julian Jara-Ettinger
<strong>Venue:</strong> arXiv (2026)</p><p>Do Large Language Models (LLMs) possess a Theory of Mind (ToM)? Research into this question has focused on evaluating LLMs against benchmarks and found success across a range of social tasks. However, these evaluations do not test for the actual representations posited by ToM: namely, a causal model of mental states and behavior. Here, we use a cognitively-grounded definition of ToM to develop and test a new evaluation framework. Specifically, our approach probes whether LLMs have a coherent, domain-general, and consistent model of how mental states cause behavior &ndash; regardless of whether that model matches a human-like ToM. We find that even though LLMs succeed in approximating human judgments in a simple ToM paradigm, they fail at a logically equivalent task and exhibit low consistency between their action predictions and corresponding mental state inferences. As such, these findings suggest that the social proficiency exhibited by LLMs is not the result of an domain-general or consistent ToM.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12150v1">üìÑ Download PDF</a></p><hr><h2 id=-llm>üîç llm<a hidden class=anchor aria-hidden=true href=#-llm>#</a></h2><h3 id=certification-of-linear-optical-quantum-state-preparationhttpsarxivorgabs260212269v1><a href=https://arxiv.org/abs/2602.12269v1>Certification of linear optical quantum state preparation</a><a hidden class=anchor aria-hidden=true href=#certification-of-linear-optical-quantum-state-preparationhttpsarxivorgabs260212269v1>#</a></h3><p><strong>Authors:</strong> Riko Schadow, Naomi Spier, Stefan N. van den Hoven, Malaquias Correa Anguita, Redlef B. G. Braamhaar, Sara Marzban, Jens Eisert, Jelmer J. Renema, Nathan Walk
<strong>Venue:</strong> arXiv (2026)</p><p>Certification is important to guarantee the correct functioning of quantum devices. A key certification task is verifying that a device has produced a desired output state. In this work, we study this task in the context of photonic platforms, where single photons are propagated through linear optical interferometers to create large, entangled resource states for metrology, communication, quantum advantage demonstrations and for so-called linear optical quantum computing (LOQC). This setting derives its computational power from the indistinguishability of the photons, i.e., their relative overlap. Therefore, standard fidelity witnesses developed for distinguishable particles (including qubits) do not apply directly, because they merely certify the closeness to some fixed target state. We introduce a measure of fidelity suitable for this setting and show several different ways to witness it, based on earlier proposals for measuring genuine multi-photon indistinguishability. We argue that a witness based upon the discrete Fourier transform is an optimal choice. We experimentally implement this witness and certify the fidelity of several multi-photon states.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12269v1">üìÑ Download PDF</a></p><hr><h3 id=self-supervised-learning-via-flow-guided-neural-operator-on-time-series-datahttpsarxivorgabs260212267v1><a href=https://arxiv.org/abs/2602.12267v1>Self-Supervised Learning via Flow-Guided Neural Operator on Time-Series Data</a><a hidden class=anchor aria-hidden=true href=#self-supervised-learning-via-flow-guided-neural-operator-on-time-series-datahttpsarxivorgabs260212267v1>#</a></h3><p><strong>Authors:</strong> Duy Nguyen, Jiachen Yao, Jiayun Wang, Julius Berner, Animashree Anandkumar
<strong>Venue:</strong> arXiv (2026)</p><p>Self-supervised learning (SSL) is a powerful paradigm for learning from unlabeled time-series data. However, popular methods such as masked autoencoders (MAEs) rely on reconstructing inputs from a fixed, predetermined masking ratio. Instead of this static design, we propose treating the corruption level as a new degree of freedom for representation learning, enhancing flexibility and performance. To achieve this, we introduce the Flow-Guided Neural Operator (FGNO), a novel framework combining operator learning with flow matching for SSL training. FGNO learns mappings in functional spaces by using Short-Time Fourier Transform to unify different time resolutions. We extract a rich hierarchy of features by tapping into different network layers and flow times that apply varying strengths of noise to the input data. This enables the extraction of versatile representations, from low-level patterns to high-level global features, using a single model adaptable to specific tasks. Unlike prior generative SSL methods that use noisy inputs during inference, we propose using clean inputs for representation extraction while learning representations with noise; this eliminates randomness and boosts accuracy. We evaluate FGNO across three biomedical domains, where it consistently outperforms established baselines. Our method yields up to 35% AUROC gains in neural signal decoding (BrainTreeBank), 16% RMSE reductions in skin temperature prediction (DREAMT), and over 20% improvement in accuracy and macro-F1 on SleepEDF under low-data regimes. These results highlight FGNO&rsquo;s robustness to data scarcity and its superior capacity to learn expressive representations for diverse time series.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12267v1">üìÑ Download PDF</a></p><hr><h3 id=a-stochastic-cluster-expansion-for-electronic-correlation-in-large-systemshttpsarxivorgabs260212254v1><a href=https://arxiv.org/abs/2602.12254v1>A Stochastic Cluster Expansion for Electronic Correlation in Large Systems</a><a hidden class=anchor aria-hidden=true href=#a-stochastic-cluster-expansion-for-electronic-correlation-in-large-systemshttpsarxivorgabs260212254v1>#</a></h3><p><strong>Authors:</strong> Annabelle Canestraight, Anthony J. Dominic, Andres Montoya-Castillo, Libor Veis, Vojtech Vlcek
<strong>Venue:</strong> arXiv (2026)</p><p>Accurate many-body treatments of condensed-phase systems are challenging because correlated solvers such as full configuration interaction (FCI) and the density matrix renormalization group (DMRG) scale exponentially with system size. Downfolding and embedding approaches mitigate this cost but typically require prior selection of a correlated subspace, which can be difficult to determine in heterogeneous or extended systems. Here, we introduce a stochastic cluster expansion framework for efficiently recovering the total correlation energy of large systems with near-DMRG accuracy, without the need to select an active space a priori. By combining correlation contributions from randomly sampled environment orbitals with an exactly treated subspace of interest, the method reproduces total energies for non-reacting and reactive systems while drastically reducing computational cost. The approach also provides a quantitative diagnostic for molecule-solvent correlation, guiding principled embedding decisions. This framework enables systematically improvable many-body calculations in extended systems, opening the door to high-accuracy studies of chemical processes in condensed phase environments.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12254v1">üìÑ Download PDF</a></p><hr><h3 id=intrinsic-energy-joint-embedding-predictive-architectures-induce-quasimetric-spaceshttpsarxivorgabs260212245v1><a href=https://arxiv.org/abs/2602.12245v1>Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces</a><a hidden class=anchor aria-hidden=true href=#intrinsic-energy-joint-embedding-predictive-architectures-induce-quasimetric-spaceshttpsarxivorgabs260212245v1>#</a></h3><p><strong>Authors:</strong> Anthony Kobanda, Waris Radji
<strong>Venue:</strong> arXiv (2026)</p><p>Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short article, we connect these viewpoints by restricting attention to a principled class of JEPA energy functions : intrinsic (least-action) energies, defined as infima of accumulated local effort over admissible trajectories between two states. Under mild closure and additivity assumptions, any intrinsic energy is a quasimetric. In goal-reaching control, optimal cost-to-go functions admit exactly this intrinsic form ; inversely, JEPAs trained to model intrinsic energies lie in the quasimetric value class targeted by QRL. Moreover, we observe why symmetric finite energies are structurally mismatched with one-way reachability, motivating asymmetric (quasimetric) energies when directionality matters.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12245v1">üìÑ Download PDF</a></p><hr><h2 id=-neuroscience>üîç neuroscience<a hidden class=anchor aria-hidden=true href=#-neuroscience>#</a></h2><h3 id=coherent-perfect-tunneling-at-exceptional-points-via-directional-degeneracyhttpsarxivorgabs260212161v1><a href=https://arxiv.org/abs/2602.12161v1>Coherent Perfect Tunneling at Exceptional Points via Directional Degeneracy</a><a hidden class=anchor aria-hidden=true href=#coherent-perfect-tunneling-at-exceptional-points-via-directional-degeneracyhttpsarxivorgabs260212161v1>#</a></h3><p><strong>Authors:</strong> Huayang Cai, Bishuang Chen
<strong>Venue:</strong> arXiv (2026)</p><p>Coherent perfect tunneling in the presence of loss and asymmetry remains a fundamental challenge in wave transport, a universal problem across optics, acoustics, and quantum mechanics. Here we demonstrate coherent perfect tunneling at an exceptional point in a passive one-dimensional waveguide cascade with three coupled interfaces. Using a waveguide-invariant scattering framework, we show that the suppression of a selected output channel originates from a directional scattering degeneracy rather than from resonance or absorption collapse. This exceptional-point condition emerges when interference between boundary-induced feedback loops promotes a simple zero of the scattering response to a second-order degeneracy. As a direct consequence, fixed coherent excitation produces a robust quartic leakage law within a transparency-dominated tunneling window. These results establish directional degeneracy as a general mechanism for loss-tolerant tunneling enabled by exceptional points across a broad class of wave systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12161v1">üìÑ Download PDF</a></p><hr><h3 id=realization-of-a-cavity-coupled-rydberg-arrayhttpsarxivorgabs260212152v1><a href=https://arxiv.org/abs/2602.12152v1>Realization of a cavity-coupled Rydberg array</a><a hidden class=anchor aria-hidden=true href=#realization-of-a-cavity-coupled-rydberg-arrayhttpsarxivorgabs260212152v1>#</a></h3><p><strong>Authors:</strong> Jacopo De Santis, Bal√°zs Dura-Kov√°cs, Mehmet √ñnc√º, Adrien Bouscal, Dimitrios Vasileiadis, Johannes Zeiher
<strong>Venue:</strong> arXiv (2026)</p><p>Scalable quantum computers and quantum networks require the combination of quantum processing nodes with efficient light-matter interfaces to distribute quantum information in local or long-distance quantum networks. Neutral-atom arrays have both been coupled to Rydberg states to enable high-fidelity quantum gates in universal processing architectures, and to optical cavities to realize interfaces to photons. However, combining these two capabilities and coupling atom arrays to highly excited Rydberg states in the mode of an optical cavity has been an outstanding challenge. Here we present a novel cavity-coupled Rydberg array that achieves this long-standing goal. We prepare, detect, and control individual atoms in a scalable optical tweezer array, couple them strongly to the optical mode of a high-finesse optical cavity and excite them in a controlled way to Rydberg states. We show that strong coupling to an optical cavity - demonstrated via the dispersive shift of the resonance of the cavity in presence of the atoms - and strong Rydberg interactions - demonstrated via the collective enhancement of Rydberg coupling in the atomic array - can be achieved in our setup at the same spatial location. Our presented experimental platform opens the path to several new directions, including the realization of quantum network nodes, quantum simulation of long-range interacting, open quantum systems and photonic-state engineering leveraging high-fidelity Rydberg control.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12152v1">üìÑ Download PDF</a></p><hr><h3 id=choose-your-agent-tradeoffs-in-adopting-ai-advisors-coaches-and-delegates-in-multi-party-negotiationhttpsarxivorgabs260212089v1><a href=https://arxiv.org/abs/2602.12089v1>Choose Your Agent: Tradeoffs in Adopting AI Advisors, Coaches, and Delegates in Multi-Party Negotiation</a><a hidden class=anchor aria-hidden=true href=#choose-your-agent-tradeoffs-in-adopting-ai-advisors-coaches-and-delegates-in-multi-party-negotiationhttpsarxivorgabs260212089v1>#</a></h3><p><strong>Authors:</strong> Kehang Zhu, Lithium Thain, Vivian Tsai, James Wexler, Crystal Qian
<strong>Venue:</strong> arXiv (2026)</p><p>As AI usage becomes more prevalent in social contexts, understanding agent-user interaction is critical to designing systems that improve both individual and group outcomes. We present an online behavioral experiment (N = 243) in which participants play three multi-turn bargaining games in groups of three. Each game, presented in randomized order, grants \textit{access to} a single LLM assistance modality: proactive recommendations from an \textit{Advisor}, reactive feedback from a \textit{Coach}, or autonomous execution by a \textit{Delegate}; all modalities are powered by an underlying LLM that achieves superhuman performance in an all-agent environment. On each turn, participants privately decide whether to act manually or use the AI modality available in that game. Despite preferring the \textit{Advisor} modality, participants achieve the highest mean individual gains with the \textit{Delegate}, demonstrating a preference-performance misalignment. Moreover, delegation generates positive externalities; even non-adopting users in \textit{access-to-delegate} treatment groups benefit by receiving higher-quality offers. Mechanism analysis reveals that the \textit{Delegate} agent acts as a market maker, injecting rational, Pareto-improving proposals that restructure the trading environment. Our research reveals a gap between agent capabilities and realized group welfare. While autonomous agents can exhibit super-human strategic performance, their impact on realized welfare gains can be constrained by interfaces, user perceptions, and adoption barriers. Assistance modalities should be designed as mechanisms with endogenous participation; adoption-compatible interaction rules are a prerequisite to improving human welfare with automated assistance.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12089v1">üìÑ Download PDF</a></p><hr><h3 id=diver-a-robust-text-to-sql-system-with-dynamic-interactive-value-linking-and-evidence-reasoninghttpsarxivorgabs260212064v1><a href=https://arxiv.org/abs/2602.12064v1>DIVER: A Robust Text-to-SQL System with Dynamic Interactive Value Linking and Evidence Reasoning</a><a hidden class=anchor aria-hidden=true href=#diver-a-robust-text-to-sql-system-with-dynamic-interactive-value-linking-and-evidence-reasoninghttpsarxivorgabs260212064v1>#</a></h3><p><strong>Authors:</strong> Yafeng Nan, Haifeng Sun, Zirui Zhuang, Qi Qi, Guojun Chu, Jianxin Liao, Dan Pei, Jingyu Wang
<strong>Venue:</strong> arXiv (2026)</p><p>In the era of large language models, Text-to-SQL, as a natural language interface for databases, is playing an increasingly important role. The sota Text-to-SQL models have achieved impressive accuracy, but their performance critically relies on expert-written evidence, which typically clarifies schema and value linking that existing models struggle to identify. Such limitations stem from the ambiguity of user queries and, more importantly, the complexity of comprehending large-scale and dynamic database values. Consequently, in real-world scenarios where expert assistance is unavailable, existing methods suffer a severe performance collapse, with execution accuracy dropping by over 10%. This underscores their lack of robustness. To address this, we propose DIVER, a robust system that automates evidence reasoning with dynamic interactive value linking. It leverages a compatible toolbox containing diverse tools to probe the database. Then, restricted by a structured workspace (CoTF, Chain of Thoughts and Facts), it reflects based on probe results and selects a new tool for next round of probing. Through this automatically iterative process, DIVER identifies schema and value linking missed by existing methods. Based on these accurate linkings, DIVER is able to infer correct usage of SQL functions and formulas and generate high-quality evidence, achieving robust Text-to-SQL without expert assistance. Extensive experiments demonstrate that: 1) The DIVER system significantly enhances the robustness of various Text-to-SQL models, improving performance by up to 10.82% in Execution Accuracy (EX) and 16.09% in Valid Efficiency Score (VES). 2) Our dynamic interactive value linking significantly improves the robustness of existing systems and the accuracy of schema and value linking, especially when confronted with challenges posed by large-scale, dynamic database values.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12064v1">üìÑ Download PDF</a></p><hr><h2 id=-data_resources>üîç data_resources<a hidden class=anchor aria-hidden=true href=#-data_resources>#</a></h2><h3 id=towards-on-policy-sft-distribution-discriminant-theory-and-its-applications-in-llm-traininghttpsarxivorgabs260212222v1><a href=https://arxiv.org/abs/2602.12222v1>Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training</a><a hidden class=anchor aria-hidden=true href=#towards-on-policy-sft-distribution-discriminant-theory-and-its-applications-in-llm-traininghttpsarxivorgabs260212222v1>#</a></h3><p><strong>Authors:</strong> Miaosen Zhang, Yishan Liu, Shuxia Lin, Xu Yang, Qi Dai, Chong Luo, Weihao Jiang, Peng Hou, Anxiang Zeng, Xin Geng, Baining Guo
<strong>Venue:</strong> arXiv (2026)</p><p>Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL&rsquo;s use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \textbf{\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \textbf{\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \textbf{\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model&rsquo;s distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: <a href=https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT>https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12222v1">üìÑ Download PDF</a></p><hr><h3 id=sageo-arena-a-realistic-environment-for-evaluating-search-augmented-generative-engine-optimizationhttpsarxivorgabs260212187v1><a href=https://arxiv.org/abs/2602.12187v1>SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engine Optimization</a><a hidden class=anchor aria-hidden=true href=#sageo-arena-a-realistic-environment-for-evaluating-search-augmented-generative-engine-optimizationhttpsarxivorgabs260212187v1>#</a></h3><p><strong>Authors:</strong> Sunghwan Kim, Wooseok Jeong, Serin Kim, Sangam Lee, Dongha Lee
<strong>Venue:</strong> arXiv (2026)</p><p>Search-Augmented Generative Engines (SAGE) have emerged as a new paradigm for information access, bridging web-scale retrieval with generative capabilities to deliver synthesized answers. This shift has fundamentally reshaped how web content gains exposure online, giving rise to Search-Augmented Generative Engine Optimization (SAGEO), the practice of optimizing web documents to improve their visibility in AI-generated responses. Despite growing interest, no evaluation environment currently supports comprehensive investigation of SAGEO. Specifically, existing benchmarks lack end-to-end visibility evaluation of optimization strategies, operating on pre-determined candidate documents that abstract away retrieval and reranking preceding generation. Moreover, existing benchmarks discard structural information (e.g., schema markup) present in real web documents, overlooking the rich signals that search systems actively leverage in practice. Motivated by these gaps, we introduce SAGEO Arena, a realistic and reproducible environment for stage-level SAGEO analysis. Our objective is to jointly target search-oriented optimization (SEO) and generation-centric optimization (GEO). To achieve this, we integrate a full generative search pipeline over a large-scale corpus of web documents with rich structural information. Our findings reveal that existing approaches remain largely impractical under realistic conditions and often degrade performance in retrieval and reranking. We also find that structural information helps mitigate these limitations, and that effective SAGEO requires tailoring optimization to each pipeline stage. Overall, our benchmark paves the way for realistic SAGEO evaluation and optimization beyond simplified settings.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12187v1">üìÑ Download PDF</a></p><hr><h3 id=empirical-gaussian-processeshttpsarxivorgabs260212082v1><a href=https://arxiv.org/abs/2602.12082v1>Empirical Gaussian Processes</a><a hidden class=anchor aria-hidden=true href=#empirical-gaussian-processeshttpsarxivorgabs260212082v1>#</a></h3><p><strong>Authors:</strong> Jihao Andreas Lin, Sebastian Ament, Louis C. Tiao, David Eriksson, Maximilian Balandat, Eytan Bakshy
<strong>Venue:</strong> arXiv (2026)</p><p>Gaussian processes (GPs) are powerful and widely used probabilistic regression models, but their effectiveness in practice is often limited by the choice of kernel function. This kernel function is typically handcrafted from a small set of standard functions, a process that requires expert knowledge, results in limited adaptivity to data, and imposes strong assumptions on the hypothesis space. We study Empirical GPs, a principled framework for constructing flexible, data-driven GP priors that overcome these limitations. Rather than relying on standard parametric kernels, we estimate the mean and covariance functions empirically from a corpus of historical observations, enabling the prior to reflect rich, non-trivial covariance structures present in the data. Theoretically, we show that the resulting model converges to the GP that is closest (in KL-divergence sense) to the real data generating process. Practically, we formulate the problem of learning the GP prior from independent datasets as likelihood estimation and derive an Expectation-Maximization algorithm with closed-form updates, allowing the model handle heterogeneous observation locations across datasets. We demonstrate that Empirical GPs achieve competitive performance on learning curve extrapolation and time series forecasting benchmarks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12082v1">üìÑ Download PDF</a></p><hr><h3 id=sci-coe-co-evolving-scientific-reasoning-llms-via-geometric-consensus-with-sparse-supervisionhttpsarxivorgabs260212164v1><a href=https://arxiv.org/abs/2602.12164v1>Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision</a><a hidden class=anchor aria-hidden=true href=#sci-coe-co-evolving-scientific-reasoning-llms-via-geometric-consensus-with-sparse-supervisionhttpsarxivorgabs260212164v1>#</a></h3><p><strong>Authors:</strong> Xiaohan He, Shiyang Feng, Songtao Huang, Lei Bai, Bin Wang, Bo Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at <a href=https://github.com/InternScience/Sci-CoE>https://github.com/InternScience/Sci-CoE</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12164v1">üìÑ Download PDF</a></p><hr><h3 id=targeted-syntactic-evaluation-of-language-models-on-georgian-case-alignmenthttpsarxivorgabs260210661v1><a href=https://arxiv.org/abs/2602.10661v1>Targeted Syntactic Evaluation of Language Models on Georgian Case Alignment</a><a hidden class=anchor aria-hidden=true href=#targeted-syntactic-evaluation-of-language-models-on-georgian-case-alignmenthttpsarxivorgabs260210661v1>#</a></h3><p><strong>Authors:</strong> Daniel Gallagher, Gerhard Heyer
<strong>Venue:</strong> arXiv (2026)</p><p>This paper evaluates the performance of transformer-based language models on split-ergative case alignment in Georgian, a particularly rare system for assigning grammatical cases to mark argument roles. We focus on subject and object marking determined through various permutations of nominative, ergative, and dative noun forms. A treebank-based approach for the generation of minimal pairs using the Grew query language is implemented. We create a dataset of 370 syntactic tests made up of seven tasks containing 50-70 samples each, where three noun forms are tested in any given sample. Five encoder- and two decoder-only models are evaluated with word- and/or sentence-level accuracy metrics. Regardless of the specific syntactic makeup, models performed worst in assigning the ergative case correctly and strongest in assigning the nominative case correctly. Performance correlated with the overall frequency distribution of the three forms (NOM > DAT > ERG). Though data scarcity is a known issue for low-resource languages, we show that the highly specific role of the ergative along with a lack of available training data likely contributes to poor performance on this case. The dataset is made publicly available and the methodology provides an interesting avenue for future syntactic evaluations of languages where benchmarks are limited.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.10661v1">üìÑ Download PDF</a></p><hr><h3 id=federated-gaussian-process-learning-via-pseudo-representations-for-large-scale-multi-robot-systemshttpsarxivorgabs260212243v1><a href=https://arxiv.org/abs/2602.12243v1>Federated Gaussian Process Learning via Pseudo-Representations for Large-Scale Multi-Robot Systems</a><a hidden class=anchor aria-hidden=true href=#federated-gaussian-process-learning-via-pseudo-representations-for-large-scale-multi-robot-systemshttpsarxivorgabs260212243v1>#</a></h3><p><strong>Authors:</strong> Sanket A. Salunkhe, George P. Kontoudis
<strong>Venue:</strong> arXiv (2026)</p><p>Multi-robot systems require scalable and federated methods to model complex environments under computational and communication constraints. Gaussian Processes (GPs) offer robust probabilistic modeling, but suffer from cubic computational complexity, limiting their applicability in large-scale deployments. To address this challenge, we introduce the pxpGP, a novel distributed GP framework tailored for both centralized and decentralized large-scale multi-robot networks. Our approach leverages sparse variational inference to generate a local compact pseudo-representation. We introduce a sparse variational optimization scheme that bounds local pseudo-datasets and formulate a global scaled proximal-inexact consensus alternating direction method of multipliers (ADMM) with adaptive parameter updates and warm-start initialization. Experiments on synthetic and real-world datasets demonstrate that pxpGP and its decentralized variant, dec-pxpGP, outperform existing distributed GP methods in hyperparameter estimation and prediction accuracy, particularly in large-scale networks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12243v1">üìÑ Download PDF</a></p><hr><h2 id=-emotion_language>üîç emotion_language<a hidden class=anchor aria-hidden=true href=#-emotion_language>#</a></h2><h3 id=energy-aware-spike-budgeting-for-continual-learning-in-spiking-neural-networks-for-neuromorphic-visionhttpsarxivorgabs260212236v1><a href=https://arxiv.org/abs/2602.12236v1>Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision</a><a hidden class=anchor aria-hidden=true href=#energy-aware-spike-budgeting-for-continual-learning-in-spiking-neural-networks-for-neuromorphic-visionhttpsarxivorgabs260212236v1>#</a></h3><p><strong>Authors:</strong> Anika Tabassum Meem, Muntasir Hossain Nadid, Md Zesun Ahmed Mia
<strong>Venue:</strong> arXiv (2026)</p><p>Neuromorphic vision systems based on spiking neural networks (SNNs) offer ultra-low-power perception for event-based and frame-based cameras, yet catastrophic forgetting remains a critical barrier to deployment in continually evolving environments. Existing continual learning methods, developed primarily for artificial neural networks, seldom jointly optimize accuracy and energy efficiency, with particularly limited exploration on event-based datasets. We propose an energy-aware spike budgeting framework for continual SNN learning that integrates experience replay, learnable leaky integrate-and-fire neuron parameters, and an adaptive spike scheduler to enforce dataset-specific energy constraints during training. Our approach exhibits modality-dependent behavior: on frame-based datasets (MNIST, CIFAR-10), spike budgeting acts as a sparsity-inducing regularizer, improving accuracy while reducing spike rates by up to 47%; on event-based datasets (DVS-Gesture, N-MNIST, CIFAR-10-DVS), controlled budget relaxation enables accuracy gains up to 17.45 percentage points with minimal computational overhead. Across five benchmarks spanning both modalities, our method demonstrates consistent performance improvements while minimizing dynamic power consumption, advancing the practical viability of continual learning in neuromorphic vision systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12236v1">üìÑ Download PDF</a></p><hr><h3 id=waveformer-wavelet-embedding-transformer-for-biomedical-signalshttpsarxivorgabs260212189v1><a href=https://arxiv.org/abs/2602.12189v1>WaveFormer: Wavelet Embedding Transformer for Biomedical Signals</a><a hidden class=anchor aria-hidden=true href=#waveformer-wavelet-embedding-transformer-for-biomedical-signalshttpsarxivorgabs260212189v1>#</a></h3><p><strong>Authors:</strong> Habib Irani, Bikram De, Vangelis Metsis
<strong>Venue:</strong> arXiv (2026)</p><p>Biomedical signal classification presents unique challenges due to long sequences, complex temporal dynamics, and multi-scale frequency patterns that are poorly captured by standard transformer architectures. We propose WaveFormer, a transformer architecture that integrates wavelet decomposition at two critical stages: embedding construction, where multi-channel Discrete Wavelet Transform (DWT) extracts frequency features to create tokens containing both time-domain and frequency-domain information, and positional encoding, where Dynamic Wavelet Positional Encoding (DyWPE) adapts position embeddings to signal-specific temporal structure through mono-channel DWT analysis. We evaluate WaveFormer on eight diverse datasets spanning human activity recognition and brain signal analysis, with sequence lengths ranging from 50 to 3000 timesteps and channel counts from 1 to 144. Experimental results demonstrate that WaveFormer achieves competitive performance through comprehensive frequency-aware processing. Our approach provides a principled framework for incorporating frequency-domain knowledge into transformer-based time series classification.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12189v1">üìÑ Download PDF</a></p><hr><h3 id=unknown-attack-detection-in-iot-networks-using-large-language-models-a-robust-data-efficient-approachhttpsarxivorgabs260212183v1><a href=https://arxiv.org/abs/2602.12183v1>Unknown Attack Detection in IoT Networks using Large Language Models: A Robust, Data-efficient Approach</a><a hidden class=anchor aria-hidden=true href=#unknown-attack-detection-in-iot-networks-using-large-language-models-a-robust-data-efficient-approachhttpsarxivorgabs260212183v1>#</a></h3><p><strong>Authors:</strong> Shan Ali, Feifei Niu, Paria Shirani, Lionel C. Briand
<strong>Venue:</strong> arXiv (2026)</p><p>The rapid evolution of cyberattacks continues to drive the emergence of unknown (zero-day) threats, posing significant challenges for network intrusion detection systems in Internet of Things (IoT) networks. Existing machine learning and deep learning approaches typically rely on large labeled datasets, payload inspection, or closed-set classification, limiting their effectiveness under data scarcity, encrypted traffic, and distribution shifts. Consequently, detecting unknown attacks in realistic IoT deployments remains difficult. To address these limitations, we propose SiamXBERT, a robust and data-efficient Siamese meta-learning framework empowered by a transformer-based language model for unknown attack detection. The proposed approach constructs a dual-modality feature representation by integrating flow-level and packet-level information, enabling richer behavioral modeling while remaining compatible with encrypted traffic. Through meta-learning, the model rapidly adapts to new attack types using only a small number of labeled samples and generalizes to previously unseen behaviors. Extensive experiments on representative IoT intrusion datasets demonstrate that SiamXBERT consistently outperforms state-of-the-art baselines under both within-dataset and cross-dataset settings while requiring significantly less training data, achieving up to \num{78.8}% improvement in unknown F1-score. These results highlight the practicality of SiamXBERT for robust unknown attack detection in real-world IoT environments.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12183v1">üìÑ Download PDF</a></p><hr><h3 id=local-integrable-symmetries-of-diffietieshttpsarxivorgabs260212103v1><a href=https://arxiv.org/abs/2602.12103v1>Local Integrable Symmetries of Diffieties</a><a hidden class=anchor aria-hidden=true href=#local-integrable-symmetries-of-diffietieshttpsarxivorgabs260212103v1>#</a></h3><p><strong>Authors:</strong> Fran√ßois Ollivier, Yirmeyahu J. Kaminski
<strong>Venue:</strong> arXiv (2026)</p><p>In the framework of diffieties, introduced by Vinogradov, we introduce integrable infinitesimal symmetries and show that they define a one parameter pseudogroup of local diffiety morphisms. We prove some preliminary results allowing to reduce the computation of integrable infinitesimal symmetries of a given order to solving a system of partial differential equations.We provide examples for which we can reduce to a linear system that can be solved by hand computation, and investigate some consequences for the local classification of diffiety, with a special interest for testing if a diffiety is flat.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12103v1">üìÑ Download PDF</a></p><hr><h3 id=legitimate-overrides-in-decentralized-protocolshttpsarxivorgabs260212260v1><a href=https://arxiv.org/abs/2602.12260v1>Legitimate Overrides in Decentralized Protocols</a><a hidden class=anchor aria-hidden=true href=#legitimate-overrides-in-decentralized-protocolshttpsarxivorgabs260212260v1>#</a></h3><p><strong>Authors:</strong> Oghenekaro Elem, Nimrod Talmon
<strong>Venue:</strong> arXiv (2026)</p><p>Decentralized protocols claim immutable, rule-based execution, yet many embed emergency mechanisms such as chain-level freezes, protocol pauses, and account quarantines. These overrides are crucial for responding to exploits and systemic failures, but they expose a core tension: when does intervention preserve trust and when is it perceived as illegitimate discretion? With approximately $10$ billion in technical exploit losses potentially addressable by onchain intervention (2016&ndash;2026), the design of these mechanisms has high practical stakes, but current approaches remain ad hoc and ideologically charged. We address this gap by developing a Scope $\times$ Authority taxonomy that maps the design space of emergency architectures along two dimensions: the precision of the intervention and the concentration of trigger authority. We formalize the resulting tradeoffs of a standing centralization cost versus containment speed and collateral disruption as a stochastic cost-minimization problem; and derive three testable predictions. Assessing these predictions against 705 documented exploit incidents, we find that containment time varies systematically by authority type; that losses follow a heavy-tailed distribution ($Œ±\approx 1.33$) concentrating risk in rare catastrophic events; and that community sentiment measurably modulates the effective cost of maintaining intervention capability. The analysis yields concrete design principles that move emergency governance from ideological debate towards quantitative engineering.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12260v1">üìÑ Download PDF</a></p><hr><h3 id=bayesian-inference-for-the-automultinomial-model-with-an-application-to-landcover-datahttpsarxivorgabs260212216v1><a href=https://arxiv.org/abs/2602.12216v1>Bayesian inference for the automultinomial model with an application to landcover data</a><a hidden class=anchor aria-hidden=true href=#bayesian-inference-for-the-automultinomial-model-with-an-application-to-landcover-datahttpsarxivorgabs260212216v1>#</a></h3><p><strong>Authors:</strong> Maria Paula Duenas-Herrera, Stephen Berg, Murali Haran
<strong>Venue:</strong> arXiv (2026)</p><p>Multicategory lattice data arise in a wide variety of disciplines such as image analysis, biology, and forestry. We consider modeling such data with the automultinomial model, which can be viewed as a natural extension of the autologistic model to multicategory responses, or equivalently as an extension of the Potts model that incorporates covariate information into a pure-intercept model. The automultinomial model has the advantage of having a unique parameter that controls the spatial correlation. However, the model&rsquo;s likelihood involves an intractable normalizing function of the model parameters that poses serious computational problems for likelihood-based inference. We address this difficulty by performing Bayesian inference through the Double-Metropolis Hastings algorithm, and implement diagnostics to assess the convergence to the target posterior distribution. Through simulation studies and an application to land cover data, we find that the automultinomial model is flexible across a wide range of spatial correlations while maintaining a relatively simple specification. For large data sets we find it also has advantages over spatial generalized linear mixed models. To make this model practical for scientists, we provide recommendations for its specification and computational implementation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12216v1">üìÑ Download PDF</a></p><hr><h3 id=equivalent-circuit-modeling-of-grid-forming-inverters-in-sub-transient-time-framehttpsarxivorgabs260212202v1><a href=https://arxiv.org/abs/2602.12202v1>Equivalent Circuit Modeling of Grid-Forming Inverters in (Sub)-Transient Time-Frame</a><a hidden class=anchor aria-hidden=true href=#equivalent-circuit-modeling-of-grid-forming-inverters-in-sub-transient-time-framehttpsarxivorgabs260212202v1>#</a></h3><p><strong>Authors:</strong> Ambuj Gupta, Balarko Chaudhuri, Mark O&rsquo;Malley
<strong>Venue:</strong> arXiv (2026)</p><p>The widely accepted definition of grid-forming (GFM) inverter states that it should behave as a (nearly) constant voltage source behind an impedance by maintaining a (nearly) constant internal voltage phasor in the sub-transient to transient time frame. Some system operators further mandate permissible ranges for this effective impedance. However, these specifications do not clearly define the location of the internal voltage source, and no systematic method exists to quantify its effective impedance for a black-box GFM model. To address this, we first compare the transient responses of an ideal voltage source and a GFM to show that an idealistic GFM maintains a (nearly) constant voltage across the filter capacitor, rather than at the inverter switches. Then we propose a systematic method to quantify the effective impedance of a GFM from its black-box model using frequency-domain admittance plots. Using standard PSCAD GFM models developed by NREL, we demonstrate that the GFM&rsquo;s equivalent impedance model captures the sub-transient response and static voltage stability limit reasonably accurately.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12202v1">üìÑ Download PDF</a></p><hr><h3 id=how-sampling-shapes-llm-alignment-from-one-shot-optima-to-iterative-dynamicshttpsarxivorgabs260212180v1><a href=https://arxiv.org/abs/2602.12180v1>How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics</a><a hidden class=anchor aria-hidden=true href=#how-sampling-shapes-llm-alignment-from-one-shot-optima-to-iterative-dynamicshttpsarxivorgabs260212180v1>#</a></h3><p><strong>Authors:</strong> Yurong Chen, Yu He, Michael I. Jordan, Fan Yao
<strong>Venue:</strong> arXiv (2026)</p><p>Standard methods for aligning large language models with human preferences learn from pairwise comparisons among sampled candidate responses and regularize toward a reference policy. Despite their effectiveness, the effects of sampling and reference choices are poorly understood theoretically. We investigate these effects through Identity Preference Optimization, a widely used preference alignment framework, and show that proper instance-dependent sampling can yield stronger ranking guarantees, while skewed on-policy sampling can induce excessive concentration under structured preferences. We then analyze iterative alignment dynamics in which the learned policy feeds back into future sampling and reference policies, reflecting a common practice of model-generated preference data. We prove that these dynamics can exhibit persistent oscillations or entropy collapse for certain parameter choices, and characterize regimes that guarantee stability. Our theoretical insights extend to Direct Preference Optimization, indicating the phenomena we captured are common to a broader class of preference-alignment methods. Experiments on real-world preference data validate our findings.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12180v1">üìÑ Download PDF</a></p><hr><h3 id=the-dark-side-of-the-moon-listening-to-scalar-induced-gravitational-waveshttpsarxivorgabs260212252v1><a href=https://arxiv.org/abs/2602.12252v1>The Dark Side of the Moon: Listening to Scalar-Induced Gravitational Waves</a><a hidden class=anchor aria-hidden=true href=#the-dark-side-of-the-moon-listening-to-scalar-induced-gravitational-waveshttpsarxivorgabs260212252v1>#</a></h3><p><strong>Authors:</strong> D. Blas, J. W. Foster, Y. Gouttenoire, A. J. Iovino, I. Musco, S. Trifinopoulos, M. Vanvlasselaer
<strong>Venue:</strong> arXiv (2026)</p><p>The collapse of large-amplitude primordial curvature perturbations into planetary-mass primordial black holes generates a scalar-induced gravitational wave background in the $Œº$Hz frequency range that may be detectable by future Lunar Laser Ranging and Satellite Laser Ranging data. We derive projected constraints on the primordial black hole population from a null detection of stochastic gravitational wave background by these experiments, including the impact of the electroweak phase transition on the abundance of planetary-mass primordial black holes. We also discuss the connection between the obtained projected constraints and the recent microlensing observations by the HSC collaboration of the Andromeda Galaxy.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12252v1">üìÑ Download PDF</a></p><hr><h3 id=phase-estimation-from-amplitude-collapse-in-correlated-matter-wave-interferencehttpsarxivorgabs260212227v1><a href=https://arxiv.org/abs/2602.12227v1>Phase Estimation from Amplitude Collapse in Correlated Matter-Wave Interference</a><a hidden class=anchor aria-hidden=true href=#phase-estimation-from-amplitude-collapse-in-correlated-matter-wave-interferencehttpsarxivorgabs260212227v1>#</a></h3><p><strong>Authors:</strong> Daniel Derr, Dominik Pfeiffer, Ludwig Lind, Gerhard Birkl, Enno Giese
<strong>Venue:</strong> arXiv (2026)</p><p>Operating matter-wave interferometers as quantum detectors for fundamental physics or inertial sensors in real-world applications with unprecedented accuracies relies on noise rejection, often implemented by correlating two sensors. Such sensors can be spatially separated (gradiometry or gravitational-wave detection) or consist of different internal states (magnetometry or quantum clock interferometry), in which case a signal-amplitude modulation may serve as a signature of a differential phase. In this work, we introduce Phase Estimation from Amplitude Collapse (PEAC) by applying targeted fitting methods for different magnetically sensitive substates of an atom interferometer. We demonstrate that PEAC provides higher trueness (up to 80% bias reduction) than standard tools for perfectly correlated signals. At its working point near, but not exactly at phase settings resulting in vanishing amplitude, it achieves precision competitive with standard methods, contrasting prior claims of optimal operation at vanishing amplitude. PEAC presents a generally applicable complementary evaluation method for correlated interferometers without phase stability, increasing the overall accuracy and enabling applications beyond atom interferometry.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12227v1">üìÑ Download PDF</a></p><hr><h3 id=anonymous-contractshttpsarxivorgabs260212118v1><a href=https://arxiv.org/abs/2602.12118v1>Anonymous Contracts</a><a hidden class=anchor aria-hidden=true href=#anonymous-contractshttpsarxivorgabs260212118v1>#</a></h3><p><strong>Authors:</strong> Johannes Brustle, Paul Duetting, Stefano Leonardi, Tomasz Ponitka, Matteo Russo
<strong>Venue:</strong> arXiv (2026)</p><p>We study a multi-agent contracting problem where agents exert costly effort to achieve individually observable binary outcomes. While the principal can theoretically extract the full social welfare using a discriminatory contract that tailors payments to individual costs, such contracts may be perceived as unfair. In this work, we introduce and analyze anonymous contracts, where payments depend solely on the total number of successes, ensuring identical treatment of agents.
We first establish that every anonymous contract admits a pure Nash equilibrium. However, because general anonymous contracts can suffer from multiple equilibria with unbounded gaps in principal utility, we identify uniform anonymous contracts as a desirable subclass. We prove that uniform anonymous contracts guarantee a unique equilibrium, thereby providing robust performance guarantees.
In terms of efficiency, we prove that under limited liability, anonymous contracts cannot generally approximate the social welfare better than a factor logarithmic in the spread of agent success probabilities. We show that uniform contracts are sufficient to match this theoretical limit. Finally, we demonstrate that removing limited liability significantly boosts performance: anonymous contracts generally achieve an $O(\log n)$ approximation to the social welfare and, surprisingly, can extract the full welfare whenever agents&rsquo; success probabilities are distinct. This reveals a structural reversal: widely spread probabilities are the hardest case under limited liability, whereas identical probabilities become the hardest case when limited liability is removed.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12118v1">üìÑ Download PDF</a></p><hr><h3 id=systematic-operator-construction-for-non-relativistic-effective-field-theories-hilbert-series-versus-young-tensorhttpsarxivorgabs260212263v1><a href=https://arxiv.org/abs/2602.12263v1>Systematic Operator Construction for Non-relativistic Effective Field Theories: Hilbert Series versus Young Tensor</a><a hidden class=anchor aria-hidden=true href=#systematic-operator-construction-for-non-relativistic-effective-field-theories-hilbert-series-versus-young-tensorhttpsarxivorgabs260212263v1>#</a></h3><p><strong>Authors:</strong> Yong-Kang Li, Yi-Ning Wang, Jiang-Hao Yu
<strong>Venue:</strong> arXiv (2026)</p><p>This work establishes a systematic framework for operator construction in the non-relativistic effective field theory, incorporating both the three dimensional Euclidean symmetry and the internal symmetries. By employing double cover of the rotation group, we extend the Hilbert series to the non-relativistic systems, and eliminates redundancies introduced by the spin operator. We also generalize the Young tensor method to the non-relativistic cases through the $SU(2)$ semi-standard Young tableaux, which allows for the construction of operator bases with repeated fields at any given mass dimension. Utilizing the Young tensor technique and Hibert series as cross-check, we obtain the complete operator bases for the following cases: heavy particle (and also heavy quark) effective theory operators up to mass dimension 9; pion-less effective theory operators, including nucleon-nucleon contact interactions up to $\mathcal{O}(Q^4)$ and three-nucleon interactions at $\mathcal{O}(Q^2)$; and finally the spin-1/2 dark matter-nucleon operators up to $\mathcal{O}(v^4)$.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.12263v1">üìÑ Download PDF</a></p><hr></div><footer class=post-footer><ul class=post-tags></ul></footer><div><div class=pagination__title><span class=pagination__title-h style=font-size:20px>Comments</span><hr></div><div id=tcomment></div><script src=https://cdn.staticfile.org/twikoo/1.6.39/twikoo.all.min.js></script><script>twikoo.init({envId:"https://mangodb-theta.vercel.app/",el:"#tcomment",lang:"en-US",path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://garyforreal.me/en/>Gary's House</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>Views: <span id=busuanzi_value_site_pv></span>
</span><span id=busuanzi_container_site_uv>Visitors: <span id=busuanzi_value_site_uv></span></span></div></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>