<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Paper | Gary's House</title>
<meta name=keywords content><meta name=description content="Paper - Gary's House"><meta name=author content="Gary"><link rel=canonical href=https://garyforreal.me/en/posts/paper/><meta name=google-site-verification content="XYZabc"><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><link crossorigin=anonymous href=/assets/css/stylesheet.7e5251d8716d933fafcf3df70d7ecd02729661d6d232fc5dd0b8a85ef75e3409.css integrity="sha256-flJR2HFtkz+vzz33DX7NAnKWYdbSMvxd0LioXvdeNAk=" rel="preload stylesheet" as=style><link rel=icon href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=16x16 href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=32x32 href=https://garyforreal.me/img/Q.jpg><link rel=apple-touch-icon href=https://garyforreal.me/Q.jpg><link rel=mask-icon href=https://garyforreal.me/Q.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://garyforreal.me/en/posts/paper/index.xml><link rel=alternate hreflang=en href=https://garyforreal.me/en/posts/paper/><link rel=alternate hreflang=zh href=https://garyforreal.me/zh/posts/paper/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@200..900&display=swap" rel=stylesheet><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous referrerpolicy=no-referrer></script><meta property="og:title" content="Paper"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://garyforreal.me/en/posts/paper/"><meta name=twitter:card content="summary"><meta name=twitter:title content="Paper"><meta name=twitter:description content><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://garyforreal.me/en/posts/"},{"@type":"ListItem","position":2,"name":"Paper","item":"https://garyforreal.me/en/posts/paper/"}]}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><script src=https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js></script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://garyforreal.me/en/ accesskey=h title="Gary's Blog (Alt + H)"><img src=https://garyforreal.me/img/me.jpg alt aria-label=logo height=35>Gary's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://garyforreal.me/zh/ title=‰∏≠Êñá aria-label=‰∏≠Êñá>‰∏≠Êñá</a></li></ul></div></div><ul id=menu><li><a href=https://garyforreal.me/en/search title="üîçSearch (Alt + /)" accesskey=/><span>üîçSearch</span></a></li><li><a href=https://garyforreal.me/en/ title=üè†Homepage><span>üè†Homepage</span></a></li><li><a href=https://garyforreal.me/en/posts title=üìöArticle><span>üìöArticle</span></a></li><li><a href=https://garyforreal.me/en/archives/ title=‚è±Archives><span>‚è±Archives</span></a></li><li><a href=https://garyforreal.me/en/music/ title=üéµmusic><span>üéµmusic</span></a></li><li><a href=https://garyforreal.me/en/about title=üôãüèª‚Äç‚ôÇÔ∏èAbout><span>üôãüèª‚Äç‚ôÇÔ∏èAbout</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://garyforreal.me/en/>Home</a>&nbsp;¬ª&nbsp;<a href=https://garyforreal.me/en/posts/>Posts</a></div><h1>Paper</h1></header><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Weekly Paper Notes - 2025-12-28</h2></header><div class=entry-content><p>Weekly Paper Notes üîç multilingual Model Merging via Multi-Teacher Knowledge Distillation Authors: Seyed Arshan Dalili, Mehrdad Mahdavi Venue: arXiv (2025)
Model merging has emerged as a lightweight alternative to joint multi-task learning (MTL), yet the generalization properties of merged models remain largely unexplored. Establishing such theoretical guarantees is non-trivial, as the merging process typically forbids access to the original training data and involves combining fine-tuned models trained on fundamentally heterogeneous data distributions. Without a principled understanding of these dynamics, current methods often rely on heuristics to approximate the optimal combination of parameters. This dependence is most critical in coefficient scaling, the weighting factors that modulate the magnitude of each fine-tuned model‚Äôs contribution to the shared parameter. However, without a principled objective to guide their selection, these methods lead to brittle performance and are highly sensitive to scaling initialization. We address this gap by (i) establishing a novel flatness-aware PAC-Bayes generalization bound specifically for the model merging setting. This analysis introduces a ‚Äúcross-task heterogeneity‚Äù term that formally captures the mismatch between diverse fine-tuned model priors and the target multi-task distributions. Guided by this theoretical insight, (ii) we frame model merging as multi-teacher knowledge distillation on scarce, unlabeled data. We formally demonstrate that minimizing the student-teacher Kullback-Leibler divergence directly tightens the upper bound on the merged model‚Äôs excess risk. Guided by the flatness-aware bound derived, (iii) we operationalize this objective via SAMerging, a method that employs Sharpness-Aware Minimization (SAM) to find flat minima. Empirically, SAMerging establishes a new state of the art across vision and NLP benchmarks, achieving remarkable performance. The code is available at https://github.com/arshandalili/SAMerging.
...</p></div><footer class=entry-footer><span title='2025-12-28 15:24:24.510017 +0000 UTC'>2025-12-28</span>&nbsp;¬∑&nbsp;120 min&nbsp;¬∑&nbsp;Gary</footer><a class=entry-link aria-label="post link to Weekly Paper Notes - 2025-12-28" href=https://garyforreal.me/en/posts/paper/paper-2025-12-28-weekly/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Weekly Paper Notes - 2025-12-21</h2></header><div class=entry-content><p>Weekly Paper Notes üîç multilingual From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection Authors: Hao Li, Yubing Ren, Yanan Cao, Yingjie Li, Fang Fang, Xuebin Wang Venue: arXiv (2025)
Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.
...</p></div><footer class=entry-footer><span title='2025-12-21 15:23:23.007415 +0000 UTC'>2025-12-21</span>&nbsp;¬∑&nbsp;120 min&nbsp;¬∑&nbsp;Gary</footer><a class=entry-link aria-label="post link to Weekly Paper Notes - 2025-12-21" href=https://garyforreal.me/en/posts/paper/paper-2025-12-21-weekly/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Weekly Paper Notes - 2025-12-14</h2></header><div class=entry-content><p>Weekly Paper Notes üîç multilingual Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation Authors: Kevin Glocker, K√§triin Kukk, Romina Oji, Marcel Bollmann, Marco Kuhlmann, Jenny Kunz Venue: arXiv (2025)
Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model‚Äôs capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.
...</p></div><footer class=entry-footer><span title='2025-12-14 15:23:14.907278 +0000 UTC'>2025-12-14</span>&nbsp;¬∑&nbsp;123 min&nbsp;¬∑&nbsp;Gary</footer><a class=entry-link aria-label="post link to Weekly Paper Notes - 2025-12-14" href=https://garyforreal.me/en/posts/paper/paper-2025-12-14-weekly/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Weekly Paper Notes - 2025-12-09</h2></header><div class=entry-content><p>Weekly Paper Notes üîç multilingual M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG Authors: David Anugraha, Patrick Amadeus Irawan, Anshul Singh, En-Shiun Annie Lee, Genta Indra Winata Venue: arXiv (2025)
Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.
...</p></div><footer class=entry-footer><span title='2025-12-09 00:13:56.72846 +0000 UTC'>2025-12-09</span>&nbsp;¬∑&nbsp;44 min&nbsp;¬∑&nbsp;Gary</footer><a class=entry-link aria-label="post link to Weekly Paper Notes - 2025-12-09" href=https://garyforreal.me/en/posts/paper/paper-2025-12-09-weekly/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Weekly Paper Notes - 2025-12-08</h2></header><div class=entry-content><p>Weekly Paper Notes üîç multilingual M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG Authors: David Anugraha, Patrick Amadeus Irawan, Anshul Singh, En-Shiun Annie Lee, Genta Indra Winata Venue: arXiv (2025)
Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.
...</p></div><footer class=entry-footer><span title='2025-12-08 04:07:26.185792 +0000 UTC'>2025-12-08</span>&nbsp;¬∑&nbsp;44 min&nbsp;¬∑&nbsp;Gary</footer><a class=entry-link aria-label="post link to Weekly Paper Notes - 2025-12-08" href=https://garyforreal.me/en/posts/paper/paper-2025-12-08-weekly/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Weekly Paper Notes - 2025-12-07</h2></header><div class=entry-content><p>Weekly Paper Notes üîç cross-lingual KDA: Knowledge Distillation Adapter for Cross-Lingual Transfer Authors: Venue: (Tue,)
Ta-Bao Nguyen, Nguyen-Phuong Phan, Tung Le and Huy Tien Nguyen in Proceedings of the 18th International Natural Language Generation Conference
üì• Save to Zotero üìÑ Download PDF
Are Knowledge and Reference in Multilingual Language Models Cross-Lingually Consistent? Authors: Venue: (Fri,)
Xi Ai, Mahardika Krisna Ihsani and Min-Yen Kan in Findings of the Association for Computational Linguistics: EMNLP 2025
...</p></div><footer class=entry-footer><span title='2025-12-07 07:18:42.705067 +0000 UTC'>2025-12-07</span>&nbsp;¬∑&nbsp;10 min&nbsp;¬∑&nbsp;Gary</footer><a class=entry-link aria-label="post link to Weekly Paper Notes - 2025-12-07" href=https://garyforreal.me/en/posts/paper/paper-2025-12-07-weekly/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Weekly Paper Notes - 2025-12-04</h2></header><div class=entry-content><p>Weekly Paper Notes üîç cross-lingual KDA: Knowledge Distillation Adapter for Cross-Lingual Transfer Authors: Venue: (Tue,)
Ta-Bao Nguyen, Nguyen-Phuong Phan, Tung Le and Huy Tien Nguyen in Proceedings of the 18th International Natural Language Generation Conference
üîç code-switching Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case Study on English-Korean Code-Switching Authors: Venue: (Fri,)
Seoyeon Kim, Huiseo Kim, Chanjun Park, Jinyoung Yeo and Dongha Lee in Findings of the Association for Computational Linguistics: EMNLP 2025
...</p></div><footer class=entry-footer><span title='2025-12-04 08:53:15.015296 +0000 UTC'>2025-12-04</span>&nbsp;¬∑&nbsp;3 min&nbsp;¬∑&nbsp;Gary</footer><a class=entry-link aria-label="post link to Weekly Paper Notes - 2025-12-04" href=https://garyforreal.me/en/posts/paper/paper-2025-12-04-weekly/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Research_lab_works</h2></header><div class=entry-content><p>CREPE: Open-Domain Question Answering with False Presuppositions üéØ Target The authors introduced CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums.
üì¶ Dataset CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums.
Data Source: Reddit, the ELI5 subreddit.
Data Numbers: 8,400 Reddit questions with:
Labels (whether there are any false presuppositions). The false presuppositions and their corrections, if there are any false presuppositions in questions. ...</p></div><footer class=entry-footer><span title='2025-03-21 15:15:43 +0900 JST'>2025-03-21</span>&nbsp;¬∑&nbsp;14 min&nbsp;¬∑&nbsp;Gary</footer><a class=entry-link aria-label="post link to Research_lab_works" href=https://garyforreal.me/en/posts/paper/research_lab_works/></a></article></main><footer class=footer><span>&copy; 2026 <a href=https://garyforreal.me/en/>Gary's House</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>Views: <span id=busuanzi_value_site_pv></span>
</span><span id=busuanzi_container_site_uv>Visitors: <span id=busuanzi_value_site_uv></span></span></div></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>