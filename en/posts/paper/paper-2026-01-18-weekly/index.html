<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Weekly Paper Notes - 2026-01-18 | Gary's House</title>
<meta name=keywords content><meta name=description content="Weekly Paper Notes
üîç multilingual
CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning
Authors: Darshan Singh, Arsha Nagrani, Kawshik Manikantan, Harman Singh, Dinesh Tewari, Tobias Weyand, Cordelia Schmid, Anelia Angelova, Shachi Dave
Venue: arXiv (2026)
Recent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE&rsquo;s reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file#minerva-cultural"><meta name=author content="Gary"><link rel=canonical href=https://garyforreal.me/en/posts/paper/paper-2026-01-18-weekly/><meta name=google-site-verification content="XYZabc"><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><link crossorigin=anonymous href=/assets/css/stylesheet.7e5251d8716d933fafcf3df70d7ecd02729661d6d232fc5dd0b8a85ef75e3409.css integrity="sha256-flJR2HFtkz+vzz33DX7NAnKWYdbSMvxd0LioXvdeNAk=" rel="preload stylesheet" as=style><link rel=icon href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=16x16 href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=32x32 href=https://garyforreal.me/img/Q.jpg><link rel=apple-touch-icon href=https://garyforreal.me/Q.jpg><link rel=mask-icon href=https://garyforreal.me/Q.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://garyforreal.me/zh/posts/paper/paper-2026-01-18-weekly/><link rel=alternate hreflang=en href=https://garyforreal.me/en/posts/paper/paper-2026-01-18-weekly/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@200..900&display=swap" rel=stylesheet><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous referrerpolicy=no-referrer></script><meta property="og:title" content="Weekly Paper Notes - 2026-01-18"><meta property="og:description" content="Weekly Paper Notes
üîç multilingual
CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning
Authors: Darshan Singh, Arsha Nagrani, Kawshik Manikantan, Harman Singh, Dinesh Tewari, Tobias Weyand, Cordelia Schmid, Anelia Angelova, Shachi Dave
Venue: arXiv (2026)
Recent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE&rsquo;s reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file#minerva-cultural"><meta property="og:type" content="article"><meta property="og:url" content="https://garyforreal.me/en/posts/paper/paper-2026-01-18-weekly/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-18T15:24:18+00:00"><meta property="article:modified_time" content="2026-01-18T15:24:18+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Weekly Paper Notes - 2026-01-18"><meta name=twitter:description content="Weekly Paper Notes
üîç multilingual
CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning
Authors: Darshan Singh, Arsha Nagrani, Kawshik Manikantan, Harman Singh, Dinesh Tewari, Tobias Weyand, Cordelia Schmid, Anelia Angelova, Shachi Dave
Venue: arXiv (2026)
Recent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE&rsquo;s reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file#minerva-cultural"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://garyforreal.me/en/posts/"},{"@type":"ListItem","position":2,"name":"Paper","item":"https://garyforreal.me/en/posts/paper/"},{"@type":"ListItem","position":3,"name":"Weekly Paper Notes - 2026-01-18","item":"https://garyforreal.me/en/posts/paper/paper-2026-01-18-weekly/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Weekly Paper Notes - 2026-01-18","name":"Weekly Paper Notes - 2026-01-18","description":"Weekly Paper Notes üîç multilingual CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning Authors: Darshan Singh, Arsha Nagrani, Kawshik Manikantan, Harman Singh, Dinesh Tewari, Tobias Weyand, Cordelia Schmid, Anelia Angelova, Shachi Dave Venue: arXiv (2026)\nRecent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE\u0026rsquo;s reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file#minerva-cultural\n","keywords":[],"articleBody":"Weekly Paper Notes üîç multilingual CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning Authors: Darshan Singh, Arsha Nagrani, Kawshik Manikantan, Harman Singh, Dinesh Tewari, Tobias Weyand, Cordelia Schmid, Anelia Angelova, Shachi Dave Venue: arXiv (2026)\nRecent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE‚Äôs reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file#minerva-cultural\nüìÑ Download PDF\nForm and Meaning in Intrinsic Multilingual Evaluations Authors: Wessel Poelman, Miryam de Lhoneux Venue: arXiv (2026)\nIntrinsic evaluation metrics for conditional language models, such as perplexity or bits-per-character, are widely used in both mono- and multilingual settings. These metrics are rather straightforward to use and compare in monolingual setups, but rest on a number of assumptions in multilingual setups. One such assumption is that comparing the perplexity of CLMs on parallel sentences is indicative of their quality since the information content (here understood as the semantic meaning) is the same. However, the metrics are inherently measuring information content in the information-theoretic sense. We make this and other such assumptions explicit and discuss their implications. We perform experiments with six metrics on two multi-parallel corpora both with mono- and multilingual models. Ultimately, we find that current metrics are not universally comparable. We look at the form-meaning debate to provide some explanation for this.\nüìÑ Download PDF\nA Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5 Authors: Xingjun Ma, Yixu Wang, Hengyuan Xu, Yutao Wu, Yifan Ding, Yunhan Zhao, Zilong Wang, Jiabin Hua, Ming Wen, Jianan Liu, Ranjie Duan, Yifeng Gao, Yingshui Tan, Yunhao Chen, Hui Xue, Xin Wang, Wei Cheng, Jingjing Chen, Zuxuan Wu, Bo Li, Yu-Gang Jiang Venue: arXiv (2026)\nThe rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional‚Äìshaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.\nüìÑ Download PDF\nINDIC DIALECT: A Multi Task Benchmark to Evaluate and Translate in Indian Language Dialects Authors: Tarun Sharma, Manikandan Ravikiran, Sourava Kumar Behera, Pramit Bhattacharya, Arnab Bhattacharya, Rohit Saluja Venue: arXiv (2026)\nRecent NLP advances focus primarily on standardized languages, leaving most low-resource dialects under-served especially in Indian scenarios. In India, the issue is particularly important: despite Hindi being the third most spoken language globally (over 600 million speakers), its numerous dialects remain underrepresented. The situation is similar for Odia, which has around 45 million speakers. While some datasets exist which contain standard Hindi and Odia languages, their regional dialects have almost no web presence. We introduce INDIC-DIALECT, a human-curated parallel corpus of 13k sentence pairs spanning 11 dialects and 2 languages: Hindi and Odia. Using this corpus, we construct a multi-task benchmark with three tasks: dialect classification, multiple-choice question (MCQ) answering, and machine translation (MT). Our experiments show that LLMs like GPT-4o and Gemini 2.5 perform poorly on the classification task. While fine-tuned transformer based models pretrained on Indian languages substantially improve performance e.g., improving F1 from 19.6% to 89.8% on dialect classification. For dialect to language translation, we find that hybrid AI model achieves highest BLEU score of 61.32 compared to the baseline score of 23.36. Interestingly, due to complexity in generating dialect sentences, we observe that for language to dialect translation the ``rule-based followed by AI\" approach achieves best BLEU score of 48.44 compared to the baseline score of 27.59. INDIC-DIALECT thus is a new benchmark for dialect-aware Indic NLP, and we plan to release it as open source to support further work on low-resource Indian dialects.\nüìÑ Download PDF\nAn Efficient Long-Context Ranking Architecture With Calibrated LLM Distillation: Application to Person-Job Fit Authors: Warren Jouanneau, Emma Jouffroy, Marc Palyart Venue: arXiv (2026)\nFinding the most relevant person for a job proposal in real time is challenging, especially when resumes are long, structured, and multilingual. In this paper, we propose a re-ranking model based on a new generation of late cross-attention architecture, that decomposes both resumes and project briefs to efficiently handle long-context inputs with minimal computational overhead. To mitigate historical data biases, we use a generative large language model (LLM) as a teacher, generating fine-grained, semantically grounded supervision. This signal is distilled into our student model via an enriched distillation loss function. The resulting model produces skill-fit scores that enable consistent and interpretable person-job matching. Experiments on relevance, ranking, and calibration metrics demonstrate that our approach outperforms state-of-the-art baselines.\nüìÑ Download PDF\nMultilinguality as Sense Adaptation Authors: Jan Christian Blaise Cruz, David Ifeoluwa Adelani, Alham Fikri Aji Venue: arXiv (2026)\nWe approach multilinguality as sense adaptation: aligning latent meaning representations across languages rather than relying solely on shared parameters and scale. In this paper, we introduce SENse-based Symmetric Interlingual Alignment (SENSIA), which adapts a Backpack language model from one language to another by explicitly aligning sense-level mixtures and contextual representations on parallel data, while jointly training a target-language language modeling loss to preserve fluency. Across benchmarks on four typologically diverse languages, SENSIA generally outperforms comparable multilingual alignment methods and achieves competitive accuracy against monolingual from-scratch baselines while using 2-4x less target-language data. Analyses of learned sense geometry indicate that local sense topology and global structure relative to English are largely preserved, and ablations show that the method is robust in terms of design and scale.\nüìÑ Download PDF\nThe Straight and Narrow: Do LLMs Possess an Internal Moral Path? Authors: Luoming Hu, Jingjie Zeng, Liang Yang, Hongfei Lin Venue: arXiv (2026)\nEnhancing the moral alignment of Large Language Models (LLMs) is a critical challenge in AI safety. Current alignment techniques often act as superficial guardrails, leaving the intrinsic moral representations of LLMs largely untouched. In this paper, we bridge this gap by leveraging Moral Foundations Theory (MFT) to map and manipulate the fine-grained moral landscape of LLMs. Through cross-lingual linear probing, we validate the shared nature of moral representations in middle layers and uncover a shared yet different moral subspace between English and Chinese. Building upon this, we extract steerable Moral Vectors and successfully validate their efficacy at both internal and behavioral levels. Leveraging the high generalizability of morality, we propose Adaptive Moral Fusion (AMF), a dynamic inference-time intervention that synergizes probe detection with vector injection to tackle the safety-helpfulness trade-off. Empirical results confirm that our approach acts as a targeted intrinsic defense, effectively reducing incorrect refusals on benign queries while minimizing jailbreak success rates compared to standard baselines.\nüìÑ Download PDF\nUntangling Input Language from Reasoning Language: A Diagnostic Framework for Cross-Lingual Moral Alignment in LLMs Authors: Nan Li, Bo Kang, Tijl De Bie Venue: arXiv (2026)\nWhen LLMs judge moral dilemmas, do they reach different conclusions in different languages, and if so, why? Two factors could drive such differences: the language of the dilemma itself, or the language in which the model reasons. Standard evaluation conflates these by testing only matched conditions (e.g., English dilemma with English reasoning). We introduce a methodology that separately manipulates each factor, covering also mismatched conditions (e.g., English dilemma with Chinese reasoning), enabling decomposition of their contributions. To study \\emph{what} changes, we propose an approach to interpret the moral judgments in terms of Moral Foundations Theory. As a side result, we identify evidence for splitting the Authority dimension into a family-related and an institutional dimension. Applying this methodology to English-Chinese moral judgment with 13 LLMs, we demonstrate its diagnostic power: (1) the framework isolates reasoning-language effects as contributing twice the variance of input-language effects; (2) it detects context-dependency in nearly half of models that standard evaluation misses; and (3) a diagnostic taxonomy translates these patterns into deployment guidance. We release our code and datasets at https://anonymous.4open.science/r/CrossCulturalMoralJudgement.\nüìÑ Download PDF\nOne Instruction Does Not Fit All: How Well Do Embeddings Align Personas and Instructions in Low-Resource Indian Languages? Authors: Arya Shah, Himanshu beniwal, Mayank Singh Venue: arXiv (2026)\nAligning multilingual assistants with culturally grounded user preferences is essential for serving India‚Äôs linguistically diverse population of over one billion speakers across multiple scripts. However, existing benchmarks either focus on a single language or conflate retrieval with generation, leaving open the question of whether current embedding models can encode persona-instruction compatibility without relying on response synthesis. We present a unified benchmark spanning 12 Indian languages and four evaluation tasks: monolingual and cross-lingual persona-to-instruction retrieval, reverse retrieval from instruction to persona, and binary compatibility classification. Eight multilingual embedding models are evaluated in a frozen-encoder setting with a thin logistic regression head for classification. E5-Large-Instruct achieves the highest Recall@1 of 27.4% on monolingual retrieval and 20.7% on cross-lingual transfer, while BGE-M3 leads reverse retrieval at 32.1% Recall@1. For classification, LaBSE attains 75.3% AUROC with strong calibration. These findings offer practical guidance for model selection in Indic multilingual retrieval and establish reproducible baselines for future work\\footnote{Code, datasets, and models are publicly available at https://github.com/aryashah2k/PI-Indic-Align.\nüìÑ Download PDF\nHOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning Authors: Ziang Cui, Mengran Yu, Tianjiao Li, Chenyu Shi, Yingxuan Shi, Lusheng Zhang, Hongwei Lin Venue: arXiv (2026)\nLarge Language Models (LLMs) have achieved remarkable strides in multilingual translation but are hindered by a systemic cross-lingual verbosity bias, rendering them unsuitable for strict time-constrained tasks like subtitling and dubbing. Current prompt-engineering approaches struggle to resolve this conflict between semantic fidelity and rigid temporal feasibility. To bridge this gap, we first introduce Sand-Glass, a benchmark specifically designed to evaluate translation under syllable-level duration constraints. Furthermore, we propose HOMURA, a reinforcement learning framework that explicitly optimizes the trade-off between semantic preservation and temporal compliance. By employing a KL-regularized objective with a novel dynamic syllable-ratio reward, HOMURA effectively ‚Äútames‚Äù the output length. Experimental results demonstrate that our method significantly outperforms strong LLM baselines, achieving precise length control that respects linguistic density hierarchies without compromising semantic adequacy.\nüìÑ Download PDF\nMultilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text Authors: Piyush Singh Pasi Venue: arXiv (2026)\nMultimodal models excel in English, supported by abundant image-text and audio-text data, but performance drops sharply for other languages due to limited multilingual multimodal resources. Existing solutions rely heavily on machine translation, while advances in multilingual text modeling remain underutilized. We introduce METAL, a lightweight alignment method that learns only a few linear layers using English text alone to map multilingual text embeddings into a multimodal space. Despite its simplicity, METAL matches baseline performance in English (94.9 percent Recall at 10) and achieves strong zero-shot transfer (89.5 percent Recall at 10 averaged across 11 languages, 10 unseen) on XTD text-to-image retrieval. Qualitative t-SNE visualizations show that multilingual embeddings align tightly with multimodal representations, while weight analysis reveals that the transformation reshapes embedding geometry rather than performing trivial rotations. Beyond image-text retrieval, METAL generalizes to audio-text retrieval and cross-lingual text-to-image generation. We release code and checkpoints at https://github.com/m2m-codebase/M2M , as well as multilingual evaluation datasets including MSCOCO Multilingual 30K (https://huggingface.co/datasets/piyushsinghpasi/mscoco-multilingual-30k ), AudioCaps Multilingual (https://huggingface.co/datasets/piyushsinghpasi/audiocaps-multilingual ), and Clotho Multilingual (https://huggingface.co/datasets/piyushsinghpasi/clotho-multilingual ), to facilitate further research.\nüìÑ Download PDF\nCreating a Hybrid Rule and Neural Network Based Semantic Tagger using Silver Standard Data: the PyMUSAS framework for Multilingual Semantic Annotation Authors: Andrew Moore, Paul Rayson, Dawn Archer, Tim Czerniak, Dawn Knight, Daisy Lal, Gear√≥id √ì Donnchadha, M√≠che√°l √ì Meachair, Scott Piao, Elaine U√≠ Dhonnchadha, Johanna Vuorinen, Yan Yabo, Xiaobin Yang Venue: arXiv (2026)\nWord Sense Disambiguation (WSD) has been widely evaluated using the semantic frameworks of WordNet, BabelNet, and the Oxford Dictionary of English. However, for the UCREL Semantic Analysis System (USAS) framework, no open extensive evaluation has been performed beyond lexical coverage or single language evaluation. In this work, we perform the largest semantic tagging evaluation of the rule based system that uses the lexical resources in the USAS framework covering five different languages using four existing datasets and one novel Chinese dataset. We create a new silver labelled English dataset, to overcome the lack of manually tagged training data, that we train and evaluate various mono and multilingual neural models in both mono and cross-lingual evaluation setups with comparisons to their rule based counterparts, and show how a rule based system can be enhanced with a neural network model. The resulting neural network models, including the data they were trained on, the Chinese evaluation dataset, and all of the code have been released as open resources.\nüìÑ Download PDF\nGeoSteer: Faithful Chain-of-Thought Steering via Latent Manifold Gradients Authors: Kentaro Kazama, Daiki Shirafuji, Tatsuhiko Saito Venue: arXiv (2026)\nRecent advances in Large Language Models (LLMs) have improved multi-step reasoning. Most approaches rely on Chain-of-Thought (CoT) rationales. Previous studies have shown that LLMs often generate logically inconsistent reasoning steps even when their final answers are correct. These inconsistencies reduce the reliability of step-level reasoning. We propose GeoSteer, a manifold-based framework that improves the quality of intermediate reasoning. The method consists of: (1) constructing a CoT dataset with segment-level scores, (2) training a Variational Autoencoder (VAE) model and a quality estimation model to learn a low-dimensional manifold of high-quality CoT trajectories, and (3) steering hidden states of target LLMs toward higher-quality regions in the latent space. This update in a latent space behaves like a natural-gradient adjustment in the original hidden-state space. It ensures geometrically coherent steering. We evaluate GeoSteer on the GSM8k dataset using the Qwen3 series. We measure via answer accuracy and overall reasoning performance. GeoSteer improved the exact match accuracy by up to 2.6 points. It also enhanced the pairwise win rate by 5.3 points. These results indicate that GeoSteer provides an effective and controllable mechanism for improving the quality of intermediate reasoning in LLMs.\nüìÑ Download PDF\nAWED-FiNER: Agents, Web applications, and Expert Detectors for Fine-grained Named Entity Recognition across 36 Languages for 6.6 Billion Speakers Authors: Prachuryya Kaushik, Ashish Anand Venue: arXiv (2026)\nWe introduce AWED-FiNER, an open-source ecosystem designed to bridge the gap in Fine-grained Named Entity Recognition (FgNER) for 36 global languages spoken by more than 6.6 billion people. While Large Language Models (LLMs) dominate general Natural Language Processing (NLP) tasks, they often struggle with low-resource languages and fine-grained NLP tasks. AWED-FiNER provides a collection of agentic toolkits, web applications, and several state-of-the-art expert models that provides FgNER solutions across 36 languages. The agentic tools enable to route multilingual text to specialized expert models and fetch FgNER annotations within seconds. The web-based platforms provide ready-to-use FgNER annotation service for non-technical users. Moreover, the collection of language specific extremely small sized open-source state-of-the-art expert models facilitate offline deployment in resource contraint scenerios including edge devices. AWED-FiNER covers languages spoken by over 6.6 billion people, including a specific focus on vulnerable languages such as Bodo, Manipuri, Bishnupriya, and Mizo. The resources can be accessed here: Agentic Tool (https://github.com/PrachuryyaKaushik/AWED-FiNER), Web Application (https://hf.co/spaces/prachuryyaIITG/AWED-FiNER), and 49 Expert Detector Models (https://hf.co/collections/prachuryyaIITG/awed-finer).\nüìÑ Download PDF\nRole-Playing Agents Driven by Large Language Models: Current Status, Challenges, and Future Trends Authors: Ye Wang, Jiaxing Chen, Hongjiang Xiao Venue: arXiv (2026)\nIn recent years, with the rapid advancement of large language models (LLMs), role-playing language agents (RPLAs) have emerged as a prominent research focus at the intersection of natural language processing (NLP) and human-computer interaction. This paper systematically reviews the current development and key technologies of RPLAs, delineating the technological evolution from early rule-based template paradigms, through the language style imitation stage, to the cognitive simulation stage centered on personality modeling and memory mechanisms. It summarizes the critical technical pathways supporting high-quality role-playing, including psychological scale-driven character modeling, memory-augmented prompting mechanisms, and motivation-situation-based behavioral decision control. At the data level, the paper further analyzes the methods and challenges of constructing role-specific corpora, focusing on data sources, copyright constraints, and structured annotation processes. In terms of evaluation, it collates multi-dimensional assessment frameworks and benchmark datasets covering role knowledge, personality fidelity, value alignment, and interactive hallucination, while commenting on the advantages and disadvantages of methods such as human evaluation, reward models, and LLM-based scoring. Finally, the paper outlines future development directions of role-playing agents, including personality evolution modeling, multi-agent collaborative narrative, multimodal immersive interaction, and integration with cognitive neuroscience, aiming to provide a systematic perspective and methodological insights for subsequent research.\nüìÑ Download PDF\nEmpathy Applicability Modeling for General Health Queries Authors: Shan Randhawa, Agha Ali Raza, Kentaro Toyama, Julie Hui, Mustafa Naseem Venue: arXiv (2026)\nLLMs are increasingly being integrated into clinical workflows, yet they often lack clinical empathy, an essential aspect of effective doctor-patient communication. Existing NLP frameworks focus on reactively labeling empathy in doctors‚Äô responses but offer limited support for anticipatory modeling of empathy needs, especially in general health queries. We introduce the Empathy Applicability Framework (EAF), a theory-driven approach that classifies patient queries in terms of the applicability of emotional reactions and interpretations, based on clinical, contextual, and linguistic cues. We release a benchmark of real patient queries, dual-annotated by Humans and GPT-4o. In the subset with human consensus, we also observe substantial human-GPT alignment. To validate EAF, we train classifiers on human-labeled and GPT-only annotations to predict empathy applicability, achieving strong performance and outperforming the heuristic and zero-shot LLM baselines. Error analysis highlights persistent challenges: implicit distress, clinical-severity ambiguity, and contextual hardship, underscoring the need for multi-annotator modeling, clinician-in-the-loop calibration, and culturally diverse annotation. EAF provides a framework for identifying empathy needs before response generation, establishes a benchmark for anticipatory empathy modeling, and enables supporting empathetic communication in asynchronous healthcare.\nüìÑ Download PDF\nLLMs Got Rhythm? Hybrid Phonological Filtering for Greek Poetry Rhyme Detection and Generation Authors: Stergios Chatzikyriakidis Venue: arXiv (2026)\nLarge Language Models (LLMs), despite their remarkable capabilities across NLP tasks, struggle with phonologically-grounded phenomena like rhyme detection and generation. This is even more evident in lower-resource languages such as Modern Greek. In this paper, we present a hybrid system that combines LLMs with deterministic phonological algorithms to achieve accurate rhyme identification/analysis and generation. Our approach implements a comprehensive taxonomy of Greek rhyme types, including Pure, Rich, Imperfect, Mosaic, and Identical Pre-rhyme Vowel (IDV) patterns, and employs an agentic generation pipeline with phonological verification. We evaluate multiple prompting strategies (zero-shot, few-shot, Chain-of-Thought, and RAG-augmented) across several LLMs including Claude 3.7 and 4.5, GPT-4o, Gemini 2.0 and open-weight models like Llama 3.1 8B and 70B and Mistral Large. Results reveal a significant ‚ÄúReasoning Gap‚Äù: while native-like models (Claude 3.7) perform intuitively (40% accuracy in identification), reasoning-heavy models (Claude 4.5) achieve state-of-the-art performance (54%) only when prompted with Chain-of-Thought. Most critically, pure LLM generation fails catastrophically (under 4% valid poems), while our hybrid verification loop restores performance to 73.1%. We release our system and a crucial, rigorously cleaned corpus of 40,000+ rhymes, derived from the Anemoskala and Interwar Poetry corpora, to support future research.\nüìÑ Download PDF\nEvolving with AI: A Longitudinal Analysis of Developer Logs Authors: Agnia Sergeyuk, Eric Huang, Dariia Karaeva, Anastasiia Serova, Yaroslav Golubev, Iftekhar Ahmed Venue: arXiv (2026)\nAI-powered coding assistants are rapidly becoming fixtures in professional IDEs, yet their sustained influence on everyday development remains poorly understood. Prior research has focused on short-term use or self-reported perceptions, leaving open questions about how sustained AI use reshapes actual daily coding practices in the long term. We address this gap with a mixed-method study of AI adoption in IDEs, combining longitudinal two-year fine-grained telemetry from 800 developers with a survey of 62 professionals. We analyze five dimensions of workflow change: productivity, code quality, code editing, code reuse, and context switching. Telemetry reveals that AI users produce substantially more code but also delete significantly more. Meanwhile, survey respondents report productivity gains and perceive minimal changes in other dimensions. Our results offer empirical insights into the silent restructuring of software workflows and provide implications for designing future AI-augmented tooling.\nüìÑ Download PDF\nGeometry- and Topology-Informed Quantum Computing: From States to Real-Time Control with FPGA Prototypes Authors: Gunhee Cho Venue: arXiv (2026)\nThis book gives a geometry-first, hardware-aware route through quantum-information workflows, with one goal: connect states, circuits, and measurement to deterministic classical pipelines that make hybrid quantum systems run. Part 1 develops the backbone (essential linear algebra, the Bloch-sphere viewpoint, differential-geometric intuition, and quantum Fisher information geometry) so evolution can be read as motion on curved spaces and measurement as statistics. Part 2 reframes circuits as dataflow graphs: measurement outcomes are parsed, aggregated, and reduced to small linear-algebra updates that schedule the next pulses, highlighting why low-latency, low-jitter streaming matters. Part 3 treats multi-qubit structure and entanglement as geometry and computation, including teleportation, superdense coding, entanglement detection, and Shor‚Äôs algorithm via quantum phase estimation. Part 4 focuses on topological error correction and real-time decoding (Track A): stabilizer codes, surface-code decoding as ‚Äútopology -\u003e graph -\u003e algorithm‚Äù, and Union-Find decoders down to microarchitectural/RTL constraints, with verification, fault injection, and host/control-stack integration under product metrics (bounded latency, p99 tails, fail-closed policies, observability). Optional Track C covers quantum cryptography and streaming post-processing (BB84/E91, QBER/abort rules, privacy amplification, and zero-knowledge/post-quantum themes), emphasizing FSMs, counters, and hash pipelines. Appendices provide visualization-driven iCEstick labs (switch-to-bit conditioning, fixed-point phase arithmetic, FSM sequencing, minimal control ISAs), bridging principles to implementable systems.\nüìÑ Download PDF\nA free-fall-based switching criterion for P^3 T N-body methods in collisional stellar systems Authors: Long Wang, David M. Hernandez, Zepeng Zheng, Wanhao Huang Venue: arXiv (2026)\nThe P$^3$T scheme is a hybrid method for simulating gravitational $N$-body systems. It combines a fast particle-tree (PT) algorithm for long-range forces with a high-accuracy particle-particle (PP, direct $N$-body) solver for short-range interactions. Preserving both PT efficiency and PP accuracy requires a robust PT-PP switching criterion. We introduce a simple free-fall-based switching criterion for general stellar systems, alongside the commonly used velocity-dispersion-based ($œÉ$-based) criterion. Using the \\textsc{petar} code with the P$^3$T scheme and slow-down algorithmic regularization for binaries and higher-order multiples, we perform extensive simulations of star clusters to evaluate how each criterion affects energy conservation and binary evolution. For systems in virial equilibrium, we find that the free-fall-based criterion is generally more accurate for low-$œÉ$ or loose clusters containing binaries, whereas the $œÉ$-based criterion is better suited for high-$œÉ$ systems. Under subvirial or fractal initial conditions, both criteria struggle to maintain high energy conservation; however, the free-fall-based criterion improves as the tree timestep is reduced, whereas the $œÉ$-based degrades due to its low-accuracy treatment of two-body encounters.\nüìÑ Download PDF\nLearning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation Authors: Jiao Xu, Xin Chen, Lihe Zhang Venue: arXiv (2026)\nIn this paper, we present a new dynamic collaborative network for semi-supervised 3D vessel segmentation, termed DiCo. Conventional mean teacher (MT) methods typically employ a static approach, where the roles of the teacher and student models are fixed. However, due to the complexity of 3D vessel data, the teacher model may not always outperform the student model, leading to cognitive biases that can limit performance. To address this issue, we propose a dynamic collaborative network that allows the two models to dynamically switch their teacher-student roles. Additionally, we introduce a multi-view integration module to capture various perspectives of the inputs, mirroring the way doctors conduct medical analysis. We also incorporate adversarial supervision to constrain the shape of the segmented vessels in unlabeled data. In this process, the 3D volume is projected into 2D views to mitigate the impact of label inconsistencies. Experiments demonstrate that our DiCo method sets new state-of-the-art performance on three 3D vessel segmentation benchmarks. The code repository address is https://github.com/xujiaommcome/DiCo\nüìÑ Download PDF\nCan Large Language Models Understand, Reason About, and Generate Code-Switched Text? Authors: Genta Indra Winata, David Anugraha, Patrick Amadeus Irawan, Anirban Das, Haneul Yoo, Paresh Dashore, Shreyas Kulkarni, Ruochen Zhang, Haruki Sakajo, Frederikus Hudi, Anaelia Ovalle, Syrielle Montariol, Felix Gaschi, Michael Anugraha, Rutuj Ravindra Puranik, Zawad Hayat Ahmed, Adril Putra Merin, Emmanuele Chersoni Venue: arXiv (2026)\nCode-switching is a pervasive phenomenon in multilingual communication, yet the robustness of large language models (LLMs) in mixed-language settings remains insufficiently understood. In this work, we present a comprehensive evaluation of LLM capabilities in understanding, reasoning over, and generating code-switched text. We introduce CodeMixQA a novel benchmark with high-quality human annotations, comprising 16 diverse parallel code-switched language-pair variants that span multiple geographic regions and code-switching patterns, and include both original scripts and their transliterated forms. Using this benchmark, we analyze the reasoning behavior of LLMs on code-switched question-answering tasks, shedding light on how models process and reason over mixed-language inputs. We further conduct a systematic evaluation of LLM-generated synthetic code-switched text, focusing on both naturalness and semantic fidelity, and uncover key limitations in current generation capabilities. Our findings reveal persistent challenges in both reasoning and generation under code-switching conditions and provide actionable insights for building more robust multilingual LLMs. We release the dataset and code as open source.\nüìÑ Download PDF\nTransforming Crises into Opportunities: From Chaos to Urban Antifragility Authors: Joseph Uguet, Nicola Tollin, Jordi Morato Venue: arXiv (2026)\nUrban crises - floods, pandemics, economic shocks, and conflicts - function as accelerators of urban change, exposing structural vulnerabilities while creating windows for reinvention. Building on a prior theoretical contribution that identified fifteen principles of urban antifragility, this paper tests and operationalizes the framework through an empirical assessment of 26 cities selected for their post-crisis adaptation trajectories. Using a tailored diagnostic methodology, we benchmark cities‚Äô Stress Response Strategies (SRS) and then evaluate Urban Development Trajectories (UDT) across four weighted dimensions, positioning each case along a fragility-robustness-resilience-antifragility continuum and applying a balanced-threshold rule to confirm antifragile status. Results show that ‚Äúresilience enhanced by innovation and technology‚Äù is the most effective response typology (86.9/100), and that six cities meet the antifragile trajectory criteria. By mapping best practices to activated principles and analysing co-activations, the study identifies a robust ‚Äúhard core‚Äù of principles - Sustainable Resilience (O), Strategic Diversity (F), Proactive Innovation (I), and Active Prevention (N) - supplemented by operational enablers (e.g., anticipation, mobilization, shock absorption). The paper concludes by proposing an evidence-based, SDG-aligned operational model that links high-impact principle pairings to measurable indicators, offering a practical roadmap for cities seeking to convert crises into sustained transformation. Keywords: Post-crisis strategies, Urban antifragility, Sustainable cities and communities, Disaster resilience and urban regeneration, Risk governance and Black Swan adaptation.\nüìÑ Download PDF\nAlterbute: Editing Intrinsic Attributes of Objects in Images Authors: Tal Reiss, Daniel Winter, Matan Cohen, Alex Rav-Acha, Yael Pritch, Ariel Shamir, Yedid Hoshen Venue: arXiv (2026)\nWe introduce Alterbute, a diffusion-based method for editing an object‚Äôs intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ‚Äò‚ÄòPorsche 911 Carrera‚Äô‚Äô) that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.\nüìÑ Download PDF\nMatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching Authors: Changle Qu, Sunhao Dai, Hengyi Cai, Jun Xu, Shuaiqiang Wang, Dawei Yin Venue: arXiv (2026)\nTool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.\nüìÑ Download PDF\nFrom One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion Authors: Cheng Chen, Yuyu Guo, Pengpeng Zeng, Jingkuan Song, Peng Di, Hang Yu, Lianli Gao Venue: arXiv (2026)\nVision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.\nüìÑ Download PDF\nGrounding Agent Memory in Contextual Intent Authors: Ruozhen Yang, Yucheng Jiang, Yueqi Jiang, Priyanka Kargupta, Yunyi Zhang, Jiawei Han Venue: arXiv (2026)\nDeploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step‚Äôs intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history. For evaluation, we introduce CAME-Bench, a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.\nüìÑ Download PDF\nLIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals Authors: Gilat Toker, Nitay Calderon, Ohad Amosy, Roi Reichart Venue: arXiv (2026)\nConcept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.\nüìÑ Download PDF\nOn the origin of neural scaling laws: from random graphs to natural language Authors: Maissam Barkeshli, Alberto Alfarano, Andrey Gromov Venue: arXiv (2026)\nScaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scaling laws even in the absence of power law structure in the data correlations. We further consider dialing down the complexity of natural language systematically, by training on sequences sampled from increasingly simplified generative language models, from 4,2,1-layer transformer language models down to language bigrams, revealing a monotonic evolution of the scaling exponents. Our results also include scaling laws obtained from training on random walks on random graphs drawn from Erd√∂s-Renyi and scale-free Barab√°si-Albert ensembles. Finally, we revisit conventional scaling laws for language modeling, demonstrating that several essential results can be reproduced using 2 layer transformers with context length of 50, provide a critical analysis of various fits used in prior literature, demonstrate an alternative method for obtaining compute optimal curves as compared with current practice in published literature, and provide preliminary evidence that maximal update parameterization may be more parameter efficient than standard parameterization.\nüìÑ Download PDF\nSee Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection Authors: Amir Mallak, Erfan Aasi, Shiva Sreeram, Tsun-Hsuan Wang, Daniela Rus, Alaa Maalouf Venue: arXiv (2026)\nRecent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$% of variance is captured by $17/64$ principal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness. We present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient. For every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches. Thus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive. Extensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a $6.2$% average improvement and up to $20.4$% in closed-loop simulations, while being $2.4\\times$ faster. We conduct ablations over masking rates and patch-feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA. Finally, we show that the same learned policy transfers to a physical, real-world car without any tuning.\nüìÑ Download PDF\nImplementation of Oblivious Transfer over Binary-Input AWGN Channels by Polar Codes Authors: Pin-Hsun Lin, Hadi Aghaee, Christian Deppe, Eduard A. Jorswieck, Holger Boche Venue: arXiv (2026)\nWe develop a one-out-of-two-oblivious transfer protocol over the binary-input additive white Gaussian noise channel using polar codes. The scheme uses two decoder views linked by automorphisms of the polar transform and publicly draws the encoder at random from the corresponding automorphism group. This yields perfect receiver privacy at any finite blocklength, since the public encoder distribution is independent of the receiver‚Äôs choice bit. Sender privacy is obtained asymptotically via channel polarization combined with privacy amplification. Because the construction deliberately injects randomness on selected bad bit-channels, we derive a relaxed reliability criterion and evaluate finite-blocklength performance. Finally, we characterize the polar-transform automorphisms as bit-level permutations of bit-channel indices, and exploit this structure to derive and optimize an achievable finite-blocklength OT rate.\nüìÑ Download PDF\nMolecularly Thin Polyaramid Nanomechanical Resonators Authors: Hagen Gress, Cody L. Ritt, Inal Shomakhov, Kaan Altmisdort, Michelle Quien, Zitang Wei, John R. Lawall, Narasimha Boddeti, Michael S. Strano, J. Scott Bunch, Kamil L. Ekinci Venue: arXiv (2026)\nTwo-dimensional polyaramids exhibit strong hydrogen bonding to create molecularly thin nanosheets analogous to graphene. Here, we report the first nanomechanical resonators made out of a two-dimensional polyaramid, 2DPA-1, with thicknesses as small as 8 nm. To fabricate these molecular-scale resonators, we transferred nanofilms of 2DPA-1 onto chips with previously etched arrays of circular microwells. We then characterized the thermal resonances of these resonators under different conditions. When there is no residual gas inside the 2DPA-1-covered microwells, the eigenfrequencies are well-described by a tensioned plate theory, providing the Young‚Äôs modulus and tension of the 2DPA-1 nanofilms. With gas present, the nanofilms bulge up and mechanical resonances are modified due to the adhesion, bulging and slack present in the system. The fabrication and mechanical characterization of these first 2DPA-1 nanomechanical resonators represent a convincing path toward molecular-scale polymeric NEMS with high mechanical strength, low density, and synthetic processability.\nüìÑ Download PDF\nClassification Imbalance as Transfer Learning Authors: Eric Xia, Jason M. Klusowski Venue: arXiv (2026)\nClassification imbalance arises when one class is much rarer than the other. We frame this setting as transfer learning under label (prior) shift between an imbalanced source distribution induced by the observed data and a balanced target distribution under which performance is evaluated. Within this framework, we study a family of oversampling procedures that augment the training data by generating synthetic samples from an estimated minority-class distribution to roughly balance the classes, among which the celebrated SMOTE algorithm is a canonical example. We show that the excess risk decomposes into the rate achievable under balanced training (as if the data had been drawn from the balanced target distribution) and an additional term, the cost of transfer, which quantifies the discrepancy between the estimated and true minority-class distributions. In particular, we show that the cost of transfer for SMOTE dominates that of bootstrapping (random oversampling) in moderately high dimensions, suggesting that we should expect bootstrapping to have better performance than SMOTE in general. We corroborate these findings with experimental evidence. More broadly, our results provide guidance for choosing among augmentation strategies for imbalanced classification.\nüìÑ Download PDF\nVoiceSculptor: Your Voice, Designed By You Authors: Jingbin Hu, Huakang Chen, Linhan Ma, Dake Guo, Qirui Zhan, Wenhao Li, Haoyu Zhang, Kangxiang Xia, Ziyu Zhang, Wenjie Tian, Chengyou Wang, Jinrui Liang, Shuhan Guo, Zihang Yang, Bengu Wu, Binbin Zhang, Pengcheng Zhu, Pengyuan Xie, Chuan Xie, Qiang Zhang, Jie Liu, Lei Xie Venue: arXiv (2026)\nDespite rapid progress in text-to-speech (TTS), open-source systems still lack truly instruction-following, fine-grained control over core speech attributes (e.g., pitch, speaking rate, age, emotion, and style). We present VoiceSculptor, an open-source unified system that bridges this gap by integrating instruction-based voice design and high-fidelity voice cloning in a single framework. It generates controllable speaker timbre directly from natural-language descriptions, supports iterative refinement via Retrieval-Augmented Generation (RAG), and provides attribute-level edits across multiple dimensions. The designed voice is then rendered into a prompt waveform and fed into a cloning model to enable high-fidelity timbre transfer for downstream speech synthesis. VoiceSculptor achieves open-source state-of-the-art (SOTA) on InstructTTSEval-Zh, and is fully open-sourced, including code and pretrained models, to advance reproducible instruction-controlled TTS research.\nüìÑ Download PDF\nProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition Authors: Arundeep Chinta, Lucas Vinh Tran, Jay Katukuri Venue: arXiv (2026)\nTime Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student‚Äôs t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student‚Äôs-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER‚Äôs effectiveness in financial applications.\nüìÑ Download PDF\nInfluential Training Data Retrieval for Explaining Verbalized Confidence of LLMs Authors: Yuxi Xia, Loris Schoenegger, Benjamin Roth Venue: arXiv (2026)\nLarge language models (LLMs) can increase users‚Äô perceived trust by verbalizing confidence in their outputs. However, prior work has shown that LLMs are often overconfident, making their stated confidence unreliable since it does not consistently align with factual accuracy. To better understand the sources of this verbalized confidence, we introduce TracVC (\\textbf{Trac}ing \\textbf{V}erbalized \\textbf{C}onfidence), a method that builds on information retrieval and influence estimation to trace generated confidence expressions back to the training data. We evaluate TracVC on OLMo and Llama models in a question answering setting, proposing a new metric, content groundness, which measures the extent to which an LLM grounds its confidence in content-related training examples (relevant to the question and answer) versus in generic examples of confidence verbalization. Our analysis reveals that OLMo2-13B is frequently influenced by confidence-related data that is lexically unrelated to the query, suggesting that it may mimic superficial linguistic expressions of certainty rather than rely on genuine content grounding. These findings point to a fundamental limitation in current training regimes: LLMs may learn how to sound confident without learning when confidence is justified. Our analysis provides a foundation for improving LLMs‚Äô trustworthiness in expressing more reliable confidence.\nüìÑ Download PDF\nInstitutional AI: A Governance Framework for Distributional AGI Safety Authors: Federico Pierucci, Marcello Galisai, Marcantonio Syrnikov Bracale, Matteo Prandi, Piercosma Bisconti, Francesco Giarrusso, Olga Sorokoletova, Vincenzo Suriani, Daniele Nardi Venue: arXiv (2026)\nAs LLM-based systems increasingly operate as agents embedded within human social and technical systems, alignment can no longer be treated as a property of an isolated model, but must be understood in relation to the environments in which these agents act. Even the most sophisticated methods of alignment, such as Reinforcement Learning through Human Feedback (RHLF) or through AI Feedback (RLAIF) cannot ensure control once internal goal structures diverge from developer intent. We identify three structural problems that emerge from core properties of AI models: (1) behavioral goal-independence, where models develop internal objectives and misgeneralize goals; (2) instrumental override of natural-language constraints, where models regard safety principles as non-binding while pursuing latent objectives, leveraging deception and manipulation; and (3) agentic alignment drift, where individually aligned agents converge to collusive equilibria through interaction dynamics invisible to single-agent audits. The solution this paper advances is Institutional AI: a system-level approach that treats alignment as a question of effective governance of AI agent collectives. We argue for a governance-graph that details how to constrain agents via runtime monitoring, incentive shaping through prizes and sanctions, explicit norms and enforcement roles. This institutional turn reframes safety from software engineering to a mechanism design problem, where the primary goal of alignment is shifting the payoff landscape of AI agent collectives.\nüìÑ Download PDF\nStructure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems Authors: Amir Khurshid, Abhishek Sehgal Venue: arXiv (2026)\nLarge language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.\nüìÑ Download PDF\nRoutIR: Fast Serving of Retrieval Pipelines for Retrieval-Augmented Generation Authors: Eugene Yang, Andrew Yates, Dawn Lawrie, James Mayfield, Trevor Adriaanse Venue: arXiv (2026)\nRetrieval models are key components of Retrieval-Augmented Generation (RAG) systems, which generate search queries, process the documents returned, and generate a response. RAG systems are often dynamic and may involve multiple rounds of retrieval. While many state-of-the-art retrieval methods are available through academic IR platforms, these platforms are typically designed for the Cranfield paradigm in which all queries are known up front and can be batch processed offline. This simplification accelerates research but leaves state-of-the-art retrieval models unable to support downstream applications that require online services, such as arbitrary dynamic RAG pipelines that involve looping, feedback, or even self-organizing agents. In this work, we introduce RoutIR, a Python package that provides a simple and efficient HTTP API that wraps arbitrary retrieval methods, including first stage retrieval, reranking, query expansion, and result fusion. By providing a minimal JSON configuration file specifying the retrieval models to serve, RoutIR can be used to construct and query retrieval pipelines on-the-fly using any permutation of available models (e.g., fusing the results of several first-stage retrieval methods followed by reranking). The API automatically performs asynchronous query batching and caches results by default. While many state-of-the-art retrieval methods are already supported by the package, RoutIR is also easily expandable by implementing the Engine abstract class. The package is open-sourced and publicly available on GitHub: http://github.com/hltcoe/routir.\nüìÑ Download PDF\nConverse Bounds for Sun-Jafar-type Weak Private Information Retrieval Authors: Chandan Anand, Jayesh Seshadri, Prasad Krishnan, Gowtham R. Kurri Venue: arXiv (2026)\nBuilding on the well-established capacity-achieving schemes of Sun-Jafar (for replicated storage) and the closely related scheme of Banawan-Ulukus (for MDS-coded setting), a recent work by Chandan et al. proposed new classes of weak private information retrieval (WPIR) schemes for the collusion-free (replication and MDS-coded) setting, as well as for the $T$-colluding scenario. In their work, Chandan et al. characterized the expressions for the rate-privacy trade-offs for these classes of WPIR schemes, under the mutual information leakage and maximal leakage metrics. Explicit achievable trade-offs for the same were also presented, which were shown to be competitive or better than prior WPIR schemes. However, the class-wise optimality of the reported trade-offs were unknown. In this work, we show that the explicit rate-privacy trade-offs reported for the Sun-Jafar-type schemes by Chandan et al. are optimal for the non-colluding and replicated setting. Furthermore, we prove the class-wise optimality for Banawan-Ulukus-type MDS-WPIR and Sun-Jafar-type $T$-colluding WPIR schemes, under threshold-constraints on the system parameters. When these threshold-constraints do not hold, we present counter-examples which show that even higher rates than those reported before can be achieved.\nüìÑ Download PDF\nüîç linguistics Emergence and transition of incompressible phases in decorated Landau levels Authors: Bo Peng, Yuzhu Wang, Bo Yang Venue: arXiv (2026)\nWe show a single Landau level (LL) dressed with periodic electrostatic potentials can realize a plethora of interacting topological phases where the Hall conductivity generally does not equal to the LL filling factor. Their physics can be captured by a minimal model of a delta potential lattice within a single LL, realizing exact zero energy Chern bands (denoted as decorated Landau levels or dLL) gapped from dispersive bands with rich geometric properties. With $p/q$ magnetic fluxes per unit cell, there are $q$ dispersive bands and $p-q$ zero energy bands forming the dLL. When the one-body potential strength dominates the electron-electron interaction, band mixing is suppressed and the dispersion bands consist of ``localized states\" with vanishing total Chern number. Nevertheless these dispersive bands can have highly nontrivial Berry curvature distribution, and even non-zero Chern numbers when $q\u003e1$. Interestingly even in the limit of large short range interaction, band mixing between dLL and dispersion bands can be strongly suppressed at low filling factor, leading to robust topological phases within the dLL stabilized by the one-body potential. The dLL and the associated dispersive bands can serve as minimal theoretical models for correlated physics in lattice or moire systems; they are also highly tunable experimental platforms for realizing rich phase diagrams of exotic 2D quantum fluids.\nüìÑ Download PDF\nMadelung hydrodynamics of spin-orbit coupling: action principles, currents, and correlations Authors: Cesare Tronci Venue: arXiv (2026)\nWe exploit the variational and Hamiltonian structures of quantum hydrodynamics with spin to unfold the correlation and torque mechanisms accompanying spin-orbit coupling (SOC) in electronic motion. Using Hamilton‚Äôs action principle for the Pauli equation, we isolate SOC-induced quantum forces that act on the orbital Madelung‚ÄìBohm trajectories and complement the usual force terms known to appear in quantum hydrodynamics with spin. While the latter spin-hydrodynamic forces relate to the quantum geometric tensor (QGT), SOC-induced orbital forces originate from a particular current operator that contributes prominently to the spin current and whose contribution was overlooked in the past. The distinction between different force terms reveals two fundamentally different mechanisms generating quantum spin-orbit correlations. Leveraging the Hamiltonian structure of the hydrodynamic system, we also elucidate spin transport features such as the current shift in the spin Hall effect and the correlation-induced quantum torques. Finally, we illustrate the framework via the Madelung‚ÄìRashba equations for planar SOC configurations and propose a particle-based scheme for numerical implementation.\nüìÑ Download PDF\nPerfect Secret Key Generation for a class of Hypergraphical Sources Authors: Manuj Mukherjee, Sagnik Chatterjee, Alhad Sethi Venue: arXiv (2026)\nNitinawarat and Narayan proposed a perfect secret key generation scheme for the so-called \\emph{pairwise independent network (PIN) model} by exploiting the combinatorial properties of the underlying graph, namely the spanning tree packing rate. This work considers a generalization of the PIN model where the underlying graph is replaced with a hypergraph, and makes progress towards designing similar perfect secret key generation schemes by exploiting the combinatorial properties of the hypergraph. Our contributions are two-fold. We first provide a capacity achieving scheme for a complete $t$-uniform hypergraph on $m$ vertices by leveraging a packing of the complete $t$-uniform hypergraphs by what we refer to as star hypergraphs, and designing a scheme that gives $\\binom{m-2}{t-2}$ bits of perfect secret key per star graph. Our second contribution is a 2-bit perfect secret key generation scheme for 3-uniform star hypergraphs whose projections are cycles. This scheme is then extended to a perfect secret key generation scheme for generic 3-uniform hypergraphs by exploiting star graph packing of 3-uniform hypergraphs and Hamiltonian packings of graphs. The scheme is then shown to be capacity achieving for certain classes of hypergraphs.\nüìÑ Download PDF\nThe Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load Authors: Han Jiang, Yao Xiao, Rachel Hurley, Shichao Liu Venue: arXiv (2026)\nOur study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users‚Äô prior expertise and interaction strategies through prompting.\nüìÑ Download PDF\nWildRayZer: Self-supervised Large View Synthesis in Dynamic Environments Authors: Xuweiyi Chen, Wentao Zhou, Zezhou Cheng Venue: arXiv (2026)\nWe present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.\nüìÑ Download PDF\nDInf-Grid: A Neural Differential Equation Solver with Differentiable Feature Grids Authors: Navami Kairanda, Shanthika Naik, Marc Habermann, Avinash Sharma, Christian Theobalt, Vladislav Golyanik Venue: arXiv (2026)\nWe present a novel differentiable grid-based representation for efficiently solving differential equations (DEs). Widely used architectures for neural solvers, such as sinusoidal neural networks, are coordinate-based MLPs that are both computationally intensive and slow to train. Although grid-based alternatives for implicit representations (e.g., Instant-NGP and K-Planes) train faster by exploiting signal structure, their reliance on linear interpolation restricts their ability to compute higher-order derivatives, rendering them unsuitable for solving DEs. Our approach overcomes these limitations by combining the efficiency of feature grids with radial basis function interpolation, which is infinitely differentiable. To effectively capture high-frequency solutions and enable stable and faster computation of global gradients, we introduce a multi-resolution decomposition with co-located grids. Our proposed representation, DInf-Grid, is trained implicitly using the differential equations as loss functions, enabling accurate modelling of physical fields. We validate DInf-Grid on a variety of tasks, including the Poisson equation for image reconstruction, the Helmholtz equation for wave fields, and the Kirchhoff-Love boundary value problem for cloth simulation. Our results demonstrate a 5-20x speed-up over coordinate-based MLP-based methods, solving differential equations in seconds or minutes while maintaining comparable accuracy and compactness.\nüìÑ Download PDF\nUFO Trees: Practical and Provably-Efficient Parallel Batch-Dynamic Trees Authors: Quinten De Man, Atharva Sharma, Kishen N Gowda, Laxman Dhulipala Venue: arXiv (2026)\nThe dynamic trees problem is to maintain a tree under edge updates while supporting queries like connectivity queries or path queries. Despite the first data structure for this fundamental problem ‚Äì the link-cut tree ‚Äì being invented 40 years ago, our experiments reveal that they are still the fastest sequential data structure for the problem. However, link-cut trees cannot support parallel batch-dynamic updates and have limitations on the kinds of queries they support. In this paper, we design a new parallel batch-dynamic trees data structure called UFO trees that simultaneously supports a wide range of query functionality, supports work-efficient parallel batch-dynamic updates, and is competitive with link-cut trees when run sequentially. We prove that a key reason for the strong practical performance of both link-cut trees and UFO trees is that they can perform updates and queries in sub-logarithmic time for low-diameter trees. We perform an experimental study of our optimized C++ implementations of UFO trees with ten other dynamic tree implementations, several of which are new, in a broad benchmark of both synthetic and real-world trees of varying diameter and size. Our results show that, in both sequential and parallel settings, UFO trees are the fastest dynamic tree data structure that supports a wide range of queries. Our new implementation of UFO trees has low space usage and easily scales to billion-size inputs, making it a promising building block for implementing more complex dynamic graph algorithms in practice.\nüìÑ Download PDF\nQuantifying the properties of evolutionary quantum states of the XXZ spin model using quantum computing Authors: M. P. Tonne, Kh. P. Gnatenko Venue: arXiv (2026)\nThe entanglement distance of evolutionary quantum states of a two-spin system with the XXZ model has been studied. The analysis has been conducted both analytically and using quantum computing. An analytical dependence of the entanglement distance on the values of the model coupling constants and the parameters of the initial states has been obtained. The speed of evolution of a two-spin system has been investigated. The analysis has been performed analytically and using quantum computing. An explicit dependence of the speed of evolution on the coupling constants and on the parameters of the initial state has been obtained. The results of quantum computations are in good agreement with the theoretical predictions.\nüìÑ Download PDF\nCosmoglobe DR2. VI. Disentangling hot and cold thermal dust emission with Planck HFI Authors: R. M. Sullivan, E. Gjerl√∏w, M. Galloway, D. J. Watts, R. Aurvik, A. Basyrov, L. A. Bianchi, A. Bonato, M. Brilenkov, H. K. Eriksen, U. Fuskeland, K. A. Glasscock, L. T. Hergt, D. Herman, J. G. S. Lunde, A. I. Silva Martins, M. San, D. Sponseller, N. -O. Stutzer, H. Thommesen, V. Vikenes, I. K. Wehus, L. Zapelli Venue: arXiv (2026)\nWe present a four-component high-resolution model of thermal dust emission for microwave and sub-mm frequencies derived from Planck HFI, WHAM and Gaia. The resulting high-resolution model derived here forms the basis for the thermal dust model employed in the Cosmoglobe DR2 reanalysis of COBE-DIRBE. The four dust components are called cold dust'', hot dust‚Äô‚Äô, nearby dust'', and Ha correlated dust‚Äô‚Äô, respectively, and trace different physical environments. The spatial distributions of the nearby dust and Ha dust components are defined by the Edenhofer et al. Gaia 3D extinction model and the WHAM survey, respectively, while the hot and cold dust components are fit freely pixel-by-pixel to the Planck HFI data. We use a global parameter grid search coupled to an amplitude map Gibbs sampler to fit this model to Planck HFI data. In agreement with the companion low-resolution analysis, we find that the hot dust component is strongly correlated with the FIRAS Cii map, while the cold dust component is strongly correlated with the HI4PI Hi map. Despite its fewer degrees of freedom per pixel compared to the Planck 2015 legacy dust model, we find that this new model performs competitively in terms of overall residuals, capturing over 98% of the full-sky dust variance for all channels. When fitting a spatially varying 3-parameter MBB model to the new dust model with isotropic SEDs, we find very similar spatial distributions to those of the official Planck analysis, and this new model thus represents an economical decomposition of previously published spatially varying spectral parameter maps. We conclude that this new model represents both a statistically more efficient summary of thermal dust in the microwave and far-infrared regimes and a physically more realistic decomposition of the sky compared to the traditional 3-parameter MBB model. (abridged)\nüìÑ Download PDF\nMeasuring the Coronal Magnetic Field with 2D Coronal Seismology: A Forward-Modeling Validation Authors: Zihao Yang, Sarah Gibson, Matthias Rempel, Giuliana de Toma Venue: arXiv (2026)\nIn recent years, a two-dimensional (2D) coronal seismology technique applied to spectral-imaging data from the Coronal Multi-channel Polarimeter (CoMP) and UCoMP has enabled routine measurement of the global coronal magnetic field. The technique combines coronal transverse wave phase speed from Doppler measurements with electron densities from the Fe \\sc{xiii}\\rm{} 10798/10747 √Ö intensity ratio to infer the magnetic field strength, while the wave propagation directions from Doppler measurements trace the magnetic field direction. To validate the accuracy and robustness of this method, we use forward modeling of a MURaM simulation that produces open and closed magnetic structures with excited waves. From the synthetic Doppler velocity, Fe \\sc{xiii}\\rm{} infrared line intensities, and linear polarization signals, we apply the 2D coronal seismology technique to estimate the magnetic field strength and direction. A comparison with the simulation ground truth shows close agreement, indicating that the technique can recover the line-of-sight emissivity-weighted magnetic field direction and strength with high accuracy. We also perform a parameter-space analysis to quantify sensitivities of the method to parameter choice. These findings provide practical guidance for CoMP/UCoMP-like analysis and demonstrate that 2D coronal seismology can deliver reliable, LOS emissivity-weighted measurements of the coronal magnetic field from coronal wave observations.\nüìÑ Download PDF\nWEAVE imaging spectroscopy of NGC 6720: an iron bar in the Ring Authors: R. Wesson, J. E. Drew, M. J. Barlow, J. Garc√≠a-Rojas, R. Greimel, D. Jones, A. Manchado, R. A. H. Morris, A. Zijlstra, P. J. Storey, J. A. L. Aguerri, S. R. Berlanas, E. Carrasco, G. B. Dalton, E. Gafton, R. Garc√≠a-Benito, A. L. Gonz√°lez-Mor√°n, B. G√§nsicke, S. Hughes, S. Jin, R. Raddi, R. Sanchez-Janssen, E. Schallig, D. J. B. Smith, S. C. Trager, N. A. Walton Venue: arXiv (2026)\nWe present spatially resolved spectroscopic observations of the planetary nebula NGC 6720, the Ring Nebula, taken during the science verification phase of WEAVE, a new instrument mounted on the William Herschel Telescope on La Palma. We use the instrument‚Äôs Large Integral Field Unit (LIFU) to obtain spectra of the Ring Nebula, covering its entire optically bright inner regions as well as parts of its much fainter outer molecular halo. We report the discovery of emission from [Fe~{\\sc v}] and [Fe~{\\sc vi}] confined to a narrow ``bar‚Äô‚Äô extending across the central regions of the nebula. No lines of other elements share this morphology or, at the spectral resolving power used ($R \\sim 2500$), the same radial velocity. The extent to which iron in this bar is depleted is presently unclear; comparison with JWST-detected dust continuum emission suggests that some dust grain destruction may be occurring in the region, but there is currently no observational evidence for the $\u003e$ 50~km,s$^{-1}$ shock waves or $T \u003e 10^6$~K X-ray emitting gas needed to enable this. Where the bar is located along the line of sight through the nebula, and how it was created, are new puzzles to be solved for this iconic planetary nebula.\nüìÑ Download PDF\nOn the geometry of aggregate snowflakes Authors: Axel Seifert, Christoph Siewert, Fabian Jakub, Leonie von Terzi, Stefan Kneifel Venue: arXiv (2026)\nSnowflakes play a crucial role in weather and climate. A significant portion of precipitation that reaches the surface originates as ice, even when it ultimately falls as rain. Contrary to the popular image of symmetric, dendritic crystals, most large snowflakes are irregular aggregates formed through the collision of primary ice crystals, such as hexagonal plates, columns, and dendrites. These aggregates exhibit complex, fractal-like structures, particularly at large sizes. Despite this structural complexity, each aggregate snowflake is unique, with properties that vary significantly around the mean - variability that is typically neglected in weather and climate models. Using a physically based aggregation model, we generate millions of synthetic snowflakes to investigate their geometric properties. The resulting dataset reveals that, for a given monomer number (cluster size) and mass, the maximum dimension follows approximately a lognormal distribution. We present a parameterization of aggregate geometry that captures key statistical properties, including maximum dimension, aspect ratio, cross-sectional area, and their joint correlations. This formulation enables a stochastic representation of aggregate snowflakes in Lagrangian particle models. Incorporating this variability improves the realism of simulated fall velocities, enhances growth rates by aggregation, and broadens Doppler radar spectra in closer agreement with observations.\nüìÑ Download PDF\nOrigins of the UV continuum and Balmer emission lines in Little Red Dots: observational validation of dense gas envelope models enshrouding the AGN Authors: Yoshihisa Asada, Kohei Inayoshi, Qinyue Fei, Seiji Fujimoto, Chris Willott Venue: arXiv (2026)\nWe present a statistical study on the origins of the UV continuum and narrow/broad emission lines in little red dots (LRDs), a newly discovered class of active galactic nuclei (AGNs). Leveraging all archived JWST/NIRSpec data, we build a sample of 28 spectroscopically-confirmed LRDs at $5","wordCount":"26535","inLanguage":"en","datePublished":"2026-01-18T15:24:18.873273Z","dateModified":"2026-01-18T15:24:18.873273Z","author":{"@type":"Person","name":"Gary"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://garyforreal.me/en/posts/paper/paper-2026-01-18-weekly/"},"publisher":{"@type":"Organization","name":"Gary's House","logo":{"@type":"ImageObject","url":"https://garyforreal.me/img/Q.jpg"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><script src=https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://garyforreal.me/en/ accesskey=h title="Gary's Blog (Alt + H)"><img src=https://garyforreal.me/img/me.jpg alt aria-label=logo height=35>Gary's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://garyforreal.me/zh/ title=‰∏≠Êñá aria-label=‰∏≠Êñá>‰∏≠Êñá</a></li></ul></div></div><ul id=menu><li><a href=https://garyforreal.me/en/search title="üîçSearch (Alt + /)" accesskey=/><span>üîçSearch</span></a></li><li><a href=https://garyforreal.me/en/ title=üè†Homepage><span>üè†Homepage</span></a></li><li><a href=https://garyforreal.me/en/posts title=üìöArticle><span>üìöArticle</span></a></li><li><a href=https://garyforreal.me/en/archives/ title=‚è±Archives><span>‚è±Archives</span></a></li><li><a href=https://garyforreal.me/en/music/ title=üéµmusic><span>üéµmusic</span></a></li><li><a href=https://garyforreal.me/en/about title=üôãüèª‚Äç‚ôÇÔ∏èAbout><span>üôãüèª‚Äç‚ôÇÔ∏èAbout</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://garyforreal.me/en/>Home</a>&nbsp;¬ª&nbsp;<a href=https://garyforreal.me/en/posts/>Posts</a>&nbsp;¬ª&nbsp;<a href=https://garyforreal.me/en/posts/paper/>Paper</a></div><h1 class="post-title entry-hint-parent">Weekly Paper Notes - 2026-01-18</h1><div class=post-meta><span title='2026-01-18 15:24:18.873273 +0000 UTC'>2026-01-18</span>&nbsp;¬∑&nbsp;125 min&nbsp;¬∑&nbsp;125 min&nbsp;¬∑&nbsp;Gary&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://garyforreal.me/zh/posts/paper/paper-2026-01-18-weekly/>‰∏≠Êñá</a></li></ul><div class=meta-item>&nbsp¬∑&nbsp
        <span id=busuanzi_container_page_pv>Êú¨ÊñáÈòÖËØªÈáè<span id=busuanzi_value_page_pv></span>Ê¨°</span></div></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#weekly-paper-notes aria-label="Weekly Paper Notes">Weekly Paper Notes</a><ul><li><a href=#-multilingual aria-label="üîç multilingual">üîç multilingual</a><ul><li><a href=#curve-a-benchmark-for-cultural-and-multilingual-long-video-reasoninghttpsarxivorgabs260110649v1 aria-label="CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning"><a href=https://arxiv.org/abs/2601.10649v1>CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning</a></a></li><li><a href=#form-and-meaning-in-intrinsic-multilingual-evaluationshttpsarxivorgabs260110580v1 aria-label="Form and Meaning in Intrinsic Multilingual Evaluations"><a href=https://arxiv.org/abs/2601.10580v1>Form and Meaning in Intrinsic Multilingual Evaluations</a></a></li><li><a href=#a-safety-report-on-gpt-52-gemini-3-pro-qwen3-vl-doubao-18-grok-41-fast-nano-banana-pro-and-seedream-45httpsarxivorgabs260110527v1 aria-label="A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5"><a href=https://arxiv.org/abs/2601.10527v1>A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5</a></a></li><li><a href=#indic-dialect-a-multi-task-benchmark-to-evaluate-and-translate-in-indian-language-dialectshttpsarxivorgabs260110388v1 aria-label="INDIC DIALECT: A Multi Task Benchmark to Evaluate and Translate in Indian Language Dialects"><a href=https://arxiv.org/abs/2601.10388v1>INDIC DIALECT: A Multi Task Benchmark to Evaluate and Translate in Indian Language Dialects</a></a></li><li><a href=#an-efficient-long-context-ranking-architecture-with-calibrated-llm-distillation-application-to-person-job-fithttpsarxivorgabs260110321v1 aria-label="An Efficient Long-Context Ranking Architecture With Calibrated LLM Distillation: Application to Person-Job Fit"><a href=https://arxiv.org/abs/2601.10321v1>An Efficient Long-Context Ranking Architecture With Calibrated LLM Distillation: Application to Person-Job Fit</a></a></li><li><a href=#multilinguality-as-sense-adaptationhttpsarxivorgabs260110310v1 aria-label="Multilinguality as Sense Adaptation"><a href=https://arxiv.org/abs/2601.10310v1>Multilinguality as Sense Adaptation</a></a></li><li><a href=#the-straight-and-narrow-do-llms-possess-an-internal-moral-pathhttpsarxivorgabs260110307v1 aria-label="The Straight and Narrow: Do LLMs Possess an Internal Moral Path?"><a href=https://arxiv.org/abs/2601.10307v1>The Straight and Narrow: Do LLMs Possess an Internal Moral Path?</a></a></li><li><a href=#untangling-input-language-from-reasoning-language-a-diagnostic-framework-for-cross-lingual-moral-alignment-in-llmshttpsarxivorgabs260110257v1 aria-label="Untangling Input Language from Reasoning Language: A Diagnostic Framework for Cross-Lingual Moral Alignment in LLMs"><a href=https://arxiv.org/abs/2601.10257v1>Untangling Input Language from Reasoning Language: A Diagnostic Framework for Cross-Lingual Moral Alignment in LLMs</a></a></li><li><a href=#one-instruction-does-not-fit-all-how-well-do-embeddings-align-personas-and-instructions-in-low-resource-indian-languageshttpsarxivorgabs260110205v1 aria-label="One Instruction Does Not Fit All: How Well Do Embeddings Align Personas and Instructions in Low-Resource Indian Languages?"><a href=https://arxiv.org/abs/2601.10205v1>One Instruction Does Not Fit All: How Well Do Embeddings Align Personas and Instructions in Low-Resource Indian Languages?</a></a></li><li><a href=#homura-taming-the-sand-glass-for-time-constrained-llm-translation-via-reinforcement-learninghttpsarxivorgabs260110187v1 aria-label="HOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning"><a href=https://arxiv.org/abs/2601.10187v1>HOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning</a></a></li><li><a href=#multilingual-to-multimodal-m2m-unlocking-new-languages-with-monolingual-texthttpsarxivorgabs260110096v1 aria-label="Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text"><a href=https://arxiv.org/abs/2601.10096v1>Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text</a></a></li><li><a href=#creating-a-hybrid-rule-and-neural-network-based-semantic-tagger-using-silver-standard-data-the-pymusas-framework-for-multilingual-semantic-annotationhttpsarxivorgabs260109648v1 aria-label="Creating a Hybrid Rule and Neural Network Based Semantic Tagger using Silver Standard Data: the PyMUSAS framework for Multilingual Semantic Annotation"><a href=https://arxiv.org/abs/2601.09648v1>Creating a Hybrid Rule and Neural Network Based Semantic Tagger using Silver Standard Data: the PyMUSAS framework for Multilingual Semantic Annotation</a></a></li><li><a href=#geosteer-faithful-chain-of-thought-steering-via-latent-manifold-gradientshttpsarxivorgabs260110229v1 aria-label="GeoSteer: Faithful Chain-of-Thought Steering via Latent Manifold Gradients"><a href=https://arxiv.org/abs/2601.10229v1>GeoSteer: Faithful Chain-of-Thought Steering via Latent Manifold Gradients</a></a></li><li><a href=#awed-finer-agents-web-applications-and-expert-detectors-for-fine-grained-named-entity-recognition-across-36-languages-for-66-billion-speakershttpsarxivorgabs260110161v1 aria-label="AWED-FiNER: Agents, Web applications, and Expert Detectors for Fine-grained Named Entity Recognition across 36 Languages for 6.6 Billion Speakers"><a href=https://arxiv.org/abs/2601.10161v1>AWED-FiNER: Agents, Web applications, and Expert Detectors for Fine-grained Named Entity Recognition across 36 Languages for 6.6 Billion Speakers</a></a></li><li><a href=#role-playing-agents-driven-by-large-language-models-current-status-challenges-and-future-trendshttpsarxivorgabs260110122v1 aria-label="Role-Playing Agents Driven by Large Language Models: Current Status, Challenges, and Future Trends"><a href=https://arxiv.org/abs/2601.10122v1>Role-Playing Agents Driven by Large Language Models: Current Status, Challenges, and Future Trends</a></a></li><li><a href=#empathy-applicability-modeling-for-general-health-querieshttpsarxivorgabs260109696v1 aria-label="Empathy Applicability Modeling for General Health Queries"><a href=https://arxiv.org/abs/2601.09696v1>Empathy Applicability Modeling for General Health Queries</a></a></li><li><a href=#llms-got-rhythm-hybrid-phonological-filtering-for-greek-poetry-rhyme-detection-and-generationhttpsarxivorgabs260109631v1 aria-label="LLMs Got Rhythm? Hybrid Phonological Filtering for Greek Poetry Rhyme Detection and Generation"><a href=https://arxiv.org/abs/2601.09631v1>LLMs Got Rhythm? Hybrid Phonological Filtering for Greek Poetry Rhyme Detection and Generation</a></a></li><li><a href=#evolving-with-ai-a-longitudinal-analysis-of-developer-logshttpsarxivorgabs260110258v1 aria-label="Evolving with AI: A Longitudinal Analysis of Developer Logs"><a href=https://arxiv.org/abs/2601.10258v1>Evolving with AI: A Longitudinal Analysis of Developer Logs</a></a></li><li><a href=#geometry--and-topology-informed-quantum-computing-from-states-to-real-time-control-with-fpga-prototypeshttpsarxivorgabs260109556v1 aria-label="Geometry- and Topology-Informed Quantum Computing: From States to Real-Time Control with FPGA Prototypes"><a href=https://arxiv.org/abs/2601.09556v1>Geometry- and Topology-Informed Quantum Computing: From States to Real-Time Control with FPGA Prototypes</a></a></li><li><a href=#a-free-fall-based-switching-criterion-for-p3-t-n-body-methods-in-collisional-stellar-systemshttpsarxivorgabs260107425v1 aria-label="A free-fall-based switching criterion for P^3 T N-body methods in collisional stellar systems"><a href=https://arxiv.org/abs/2601.07425v1>A free-fall-based switching criterion for P^3 T N-body methods in collisional stellar systems</a></a></li><li><a href=#learning-dynamic-collaborative-network-for-semi-supervised-3d-vessel-segmentationhttpsarxivorgabs260107377v1 aria-label="Learning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation"><a href=https://arxiv.org/abs/2601.07377v1>Learning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation</a></a></li><li><a href=#can-large-language-models-understand-reason-about-and-generate-code-switched-texthttpsarxivorgabs260107153v1 aria-label="Can Large Language Models Understand, Reason About, and Generate Code-Switched Text?"><a href=https://arxiv.org/abs/2601.07153v1>Can Large Language Models Understand, Reason About, and Generate Code-Switched Text?</a></a></li><li><a href=#transforming-crises-into-opportunities-from-chaos-to-urban-antifragilityhttpsarxivorgabs260110658v1 aria-label="Transforming Crises into Opportunities: From Chaos to Urban Antifragility"><a href=https://arxiv.org/abs/2601.10658v1>Transforming Crises into Opportunities: From Chaos to Urban Antifragility</a></a></li><li><a href=#alterbute-editing-intrinsic-attributes-of-objects-in-imageshttpsarxivorgabs260110714v1 aria-label="Alterbute: Editing Intrinsic Attributes of Objects in Images"><a href=https://arxiv.org/abs/2601.10714v1>Alterbute: Editing Intrinsic Attributes of Objects in Images</a></a></li><li><a href=#matchtir-fine-grained-supervision-for-tool-integrated-reasoning-via-bipartite-matchinghttpsarxivorgabs260110712v1 aria-label="MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching"><a href=https://arxiv.org/abs/2601.10712v1>MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching</a></a></li><li><a href=#from-one-to-one-to-many-to-many-dynamic-cross-layer-injection-for-deep-vision-language-fusionhttpsarxivorgabs260110710v1 aria-label="From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion"><a href=https://arxiv.org/abs/2601.10710v1>From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion</a></a></li><li><a href=#grounding-agent-memory-in-contextual-intenthttpsarxivorgabs260110702v1 aria-label="Grounding Agent Memory in Contextual Intent"><a href=https://arxiv.org/abs/2601.10702v1>Grounding Agent Memory in Contextual Intent</a></a></li><li><a href=#liberty-a-causal-framework-for-benchmarking-concept-based-explanations-of-llms-with-structural-counterfactualshttpsarxivorgabs260110700v1 aria-label="LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals"><a href=https://arxiv.org/abs/2601.10700v1>LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals</a></a></li><li><a href=#on-the-origin-of-neural-scaling-laws-from-random-graphs-to-natural-languagehttpsarxivorgabs260110684v1 aria-label="On the origin of neural scaling laws: from random graphs to natural language"><a href=https://arxiv.org/abs/2601.10684v1>On the origin of neural scaling laws: from random graphs to natural language</a></a></li><li><a href=#see-less-drive-better-generalizable-end-to-end-autonomous-driving-via-foundation-models-stochastic-patch-selectionhttpsarxivorgabs260110707v1 aria-label="See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection"><a href=https://arxiv.org/abs/2601.10707v1>See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection</a></a></li><li><a href=#implementation-of-oblivious-transfer-over-binary-input-awgn-channels-by-polar-codeshttpsarxivorgabs260110682v1 aria-label="Implementation of Oblivious Transfer over Binary-Input AWGN Channels by Polar Codes"><a href=https://arxiv.org/abs/2601.10682v1>Implementation of Oblivious Transfer over Binary-Input AWGN Channels by Polar Codes</a></a></li><li><a href=#molecularly-thin-polyaramid-nanomechanical-resonatorshttpsarxivorgabs260110633v1 aria-label="Molecularly Thin Polyaramid Nanomechanical Resonators"><a href=https://arxiv.org/abs/2601.10633v1>Molecularly Thin Polyaramid Nanomechanical Resonators</a></a></li><li><a href=#classification-imbalance-as-transfer-learninghttpsarxivorgabs260110630v1 aria-label="Classification Imbalance as Transfer Learning"><a href=https://arxiv.org/abs/2601.10630v1>Classification Imbalance as Transfer Learning</a></a></li><li><a href=#voicesculptor-your-voice-designed-by-youhttpsarxivorgabs260110629v1 aria-label="VoiceSculptor: Your Voice, Designed By You"><a href=https://arxiv.org/abs/2601.10629v1>VoiceSculptor: Your Voice, Designed By You</a></a></li><li><a href=#probfm-probabilistic-time-series-foundation-model-with-uncertainty-decompositionhttpsarxivorgabs260110591v1 aria-label="ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition"><a href=https://arxiv.org/abs/2601.10591v1>ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition</a></a></li><li><a href=#influential-training-data-retrieval-for-explaining-verbalized-confidence-of-llmshttpsarxivorgabs260110645v1 aria-label="Influential Training Data Retrieval for Explaining Verbalized Confidence of LLMs"><a href=https://arxiv.org/abs/2601.10645v1>Influential Training Data Retrieval for Explaining Verbalized Confidence of LLMs</a></a></li><li><a href=#institutional-ai-a-governance-framework-for-distributional-agi-safetyhttpsarxivorgabs260110599v1 aria-label="Institutional AI: A Governance Framework for Distributional AGI Safety"><a href=https://arxiv.org/abs/2601.10599v1>Institutional AI: A Governance Framework for Distributional AGI Safety</a></a></li><li><a href=#structure-and-diversity-aware-context-bubble-construction-for-enterprise-retrieval-augmented-systemshttpsarxivorgabs260110681v1 aria-label="Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems"><a href=https://arxiv.org/abs/2601.10681v1>Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems</a></a></li><li><a href=#routir-fast-serving-of-retrieval-pipelines-for-retrieval-augmented-generationhttpsarxivorgabs260110644v1 aria-label="RoutIR: Fast Serving of Retrieval Pipelines for Retrieval-Augmented Generation"><a href=https://arxiv.org/abs/2601.10644v1>RoutIR: Fast Serving of Retrieval Pipelines for Retrieval-Augmented Generation</a></a></li><li><a href=#converse-bounds-for-sun-jafar-type-weak-private-information-retrievalhttpsarxivorgabs260110643v1 aria-label="Converse Bounds for Sun-Jafar-type Weak Private Information Retrieval"><a href=https://arxiv.org/abs/2601.10643v1>Converse Bounds for Sun-Jafar-type Weak Private Information Retrieval</a></a></li></ul></li><li><a href=#-linguistics aria-label="üîç linguistics">üîç linguistics</a><ul><li><a href=#emergence-and-transition-of-incompressible-phases-in-decorated-landau-levelshttpsarxivorgabs260110717v1 aria-label="Emergence and transition of incompressible phases in decorated Landau levels"><a href=https://arxiv.org/abs/2601.10717v1>Emergence and transition of incompressible phases in decorated Landau levels</a></a></li><li><a href=#madelung-hydrodynamics-of-spin-orbit-coupling-action-principles-currents-and-correlationshttpsarxivorgabs260110698v1 aria-label="Madelung hydrodynamics of spin-orbit coupling: action principles, currents, and correlations"><a href=https://arxiv.org/abs/2601.10698v1>Madelung hydrodynamics of spin-orbit coupling: action principles, currents, and correlations</a></a></li><li><a href=#perfect-secret-key-generation-for-a-class-of-hypergraphical-sourceshttpsarxivorgabs260110697v1 aria-label="Perfect Secret Key Generation for a class of Hypergraphical Sources"><a href=https://arxiv.org/abs/2601.10697v1>Perfect Secret Key Generation for a class of Hypergraphical Sources</a></a></li><li><a href=#the-impact-of-generative-ai-on-architectural-conceptual-design-performance-creative-self-efficacy-and-cognitive-loadhttpsarxivorgabs260110696v1 aria-label="The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load"><a href=https://arxiv.org/abs/2601.10696v1>The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load</a></a></li><li><a href=#wildrayzer-self-supervised-large-view-synthesis-in-dynamic-environmentshttpsarxivorgabs260110716v1 aria-label="WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments"><a href=https://arxiv.org/abs/2601.10716v1>WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments</a></a></li><li><a href=#dinf-grid-a-neural-differential-equation-solver-with-differentiable-feature-gridshttpsarxivorgabs260110715v1 aria-label="DInf-Grid: A Neural Differential Equation Solver with Differentiable Feature Grids"><a href=https://arxiv.org/abs/2601.10715v1>DInf-Grid: A Neural Differential Equation Solver with Differentiable Feature Grids</a></a></li><li><a href=#ufo-trees-practical-and-provably-efficient-parallel-batch-dynamic-treeshttpsarxivorgabs260110706v1 aria-label="UFO Trees: Practical and Provably-Efficient Parallel Batch-Dynamic Trees"><a href=https://arxiv.org/abs/2601.10706v1>UFO Trees: Practical and Provably-Efficient Parallel Batch-Dynamic Trees</a></a></li><li><a href=#quantifying-the-properties-of-evolutionary-quantum-states-of-the-xxz-spin-model-using-quantum-computinghttpsarxivorgabs260110650v1 aria-label="Quantifying the properties of evolutionary quantum states of the XXZ spin model using quantum computing"><a href=https://arxiv.org/abs/2601.10650v1>Quantifying the properties of evolutionary quantum states of the XXZ spin model using quantum computing</a></a></li><li><a href=#cosmoglobe-dr2-vi-disentangling-hot-and-cold-thermal-dust-emission-with-planck-hfihttpsarxivorgabs260110640v1 aria-label="Cosmoglobe DR2. VI. Disentangling hot and cold thermal dust emission with Planck HFI"><a href=https://arxiv.org/abs/2601.10640v1>Cosmoglobe DR2. VI. Disentangling hot and cold thermal dust emission with Planck HFI</a></a></li><li><a href=#measuring-the-coronal-magnetic-field-with-2d-coronal-seismology-a-forward-modeling-validationhttpsarxivorgabs260110637v1 aria-label="Measuring the Coronal Magnetic Field with 2D Coronal Seismology: A Forward-Modeling Validation"><a href=https://arxiv.org/abs/2601.10637v1>Measuring the Coronal Magnetic Field with 2D Coronal Seismology: A Forward-Modeling Validation</a></a></li><li><a href=#weave-imaging-spectroscopy-of-ngc-6720-an-iron-bar-in-the-ringhttpsarxivorgabs260110635v1 aria-label="WEAVE imaging spectroscopy of NGC 6720: an iron bar in the Ring"><a href=https://arxiv.org/abs/2601.10635v1>WEAVE imaging spectroscopy of NGC 6720: an iron bar in the Ring</a></a></li><li><a href=#on-the-geometry-of-aggregate-snowflakeshttpsarxivorgabs260110608v1 aria-label="On the geometry of aggregate snowflakes"><a href=https://arxiv.org/abs/2601.10608v1>On the geometry of aggregate snowflakes</a></a></li><li><a href=#origins-of-the-uv-continuum-and-balmer-emission-lines-in-little-red-dots-observational-validation-of-dense-gas-envelope-models-enshrouding-the-agnhttpsarxivorgabs260110573v1 aria-label="Origins of the UV continuum and Balmer emission lines in Little Red Dots: observational validation of dense gas envelope models enshrouding the AGN"><a href=https://arxiv.org/abs/2601.10573v1>Origins of the UV continuum and Balmer emission lines in Little Red Dots: observational validation of dense gas envelope models enshrouding the AGN</a></a></li><li><a href=#high-accuracy-and-dimension-free-sampling-with-diffusionshttpsarxivorgabs260110708v1 aria-label="High-accuracy and dimension-free sampling with diffusions"><a href=https://arxiv.org/abs/2601.10708v1>High-accuracy and dimension-free sampling with diffusions</a></a></li><li><a href=#distributed-perceptron-under-bounded-staleness-partial-participation-and-noisy-communicationhttpsarxivorgabs260110705v1 aria-label="Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication"><a href=https://arxiv.org/abs/2601.10705v1>Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication</a></a></li><li><a href=#communication-efficient-and-privacy-adaptable-mechanism----a-federated-learning-scheme-with-convergence-analysishttpsarxivorgabs260110701v1 aria-label="Communication-Efficient and Privacy-Adaptable Mechanism &ndash; a Federated Learning Scheme with Convergence Analysis"><a href=https://arxiv.org/abs/2601.10701v1>Communication-Efficient and Privacy-Adaptable Mechanism &ndash; a Federated Learning Scheme with Convergence Analysis</a></a></li><li><a href=#an-extension-based-accessibility-framework-for-making-blockly-accessible-to-blind-and-low-vision-usershttpsarxivorgabs260110688v1 aria-label="An Extension-Based Accessibility Framework for Making Blockly Accessible to Blind and Low-Vision Users"><a href=https://arxiv.org/abs/2601.10688v1>An Extension-Based Accessibility Framework for Making Blockly Accessible to Blind and Low-Vision Users</a></a></li><li><a href=#detecting-winning-arguments-with-large-language-models-and-persuasion-strategieshttpsarxivorgabs260110660v1 aria-label="Detecting Winning Arguments with Large Language Models and Persuasion Strategies"><a href=https://arxiv.org/abs/2601.10660v1>Detecting Winning Arguments with Large Language Models and Persuasion Strategies</a></a></li><li><a href=#alignment-pretraining-ai-discourse-causes-self-fulfilling-misalignmenthttpsarxivorgabs260110160v1 aria-label="Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment"><a href=https://arxiv.org/abs/2601.10160v1>Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment</a></a></li><li><a href=#actors-frames-and-arguments-a-multi-decade-computational-analysis-of-climate-discourse-in-financial-news-using-large-language-modelshttpsarxivorgabs260110142v1 aria-label="Actors, Frames and Arguments: A Multi-Decade Computational Analysis of Climate Discourse in Financial News using Large Language Models"><a href=https://arxiv.org/abs/2601.10142v1>Actors, Frames and Arguments: A Multi-Decade Computational Analysis of Climate Discourse in Financial News using Large Language Models</a></a></li><li><a href=#matrix-as-plan-structured-logical-reasoning-with-feedback-driven-replanninghttpsarxivorgabs260110101v1 aria-label="MATRIX AS PLAN: Structured Logical Reasoning with Feedback-Driven Replanning"><a href=https://arxiv.org/abs/2601.10101v1>MATRIX AS PLAN: Structured Logical Reasoning with Feedback-Driven Replanning</a></a></li><li><a href=#pid-guided-partial-alignment-for-multimodal-decentralized-federated-learninghttpsarxivorgabs260110012v1 aria-label="PID-Guided Partial Alignment for Multimodal Decentralized Federated Learning"><a href=https://arxiv.org/abs/2601.10012v1>PID-Guided Partial Alignment for Multimodal Decentralized Federated Learning</a></a></li><li><a href=#how-diplomacy-reshapes-online-discourseasymmetric-persistence-in-online-framing-of-north-koreahttpsarxivorgabs260109942v1 aria-label="How Diplomacy Reshapes Online Discourse:Asymmetric Persistence in Online Framing of North Korea"><a href=https://arxiv.org/abs/2601.09942v1>How Diplomacy Reshapes Online Discourse:Asymmetric Persistence in Online Framing of North Korea</a></a></li><li><a href=#epistemology-gives-a-future-to-complementarity-in-human-ai-interactionshttpsarxivorgabs260109871v1 aria-label="Epistemology gives a Future to Complementarity in Human-AI Interactions"><a href=https://arxiv.org/abs/2601.09871v1>Epistemology gives a Future to Complementarity in Human-AI Interactions</a></a></li><li><a href=#quantum-geometry-of-the-rotating-shallow-water-modelhttpsarxivorgabs260110695v1 aria-label="Quantum geometry of the rotating shallow water model"><a href=https://arxiv.org/abs/2601.10695v1>Quantum geometry of the rotating shallow water model</a></a></li><li><a href=#mitigating-nonlinear-transduction-noise-in-high-cooperativity-cavity-optomechanicshttpsarxivorgabs260110689v1 aria-label="Mitigating nonlinear transduction noise in high-cooperativity cavity optomechanics"><a href=https://arxiv.org/abs/2601.10689v1>Mitigating nonlinear transduction noise in high-cooperativity cavity optomechanics</a></a></li><li><a href=#energy-correlators-in-warped-geometrieshttpsarxivorgabs260110674v1 aria-label="Energy Correlators in Warped Geometries"><a href=https://arxiv.org/abs/2601.10674v1>Energy Correlators in Warped Geometries</a></a></li><li><a href=#the-static-heavy-quark-antiquark-potential-within-string-theory-in-arbitrary-stationary-backgroundshttpsarxivorgabs260110668v1 aria-label="The Static Heavy Quark-Antiquark Potential within String Theory in Arbitrary Stationary Backgrounds"><a href=https://arxiv.org/abs/2601.10668v1>The Static Heavy Quark-Antiquark Potential within String Theory in Arbitrary Stationary Backgrounds</a></a></li><li><a href=#searching-for-quantum-effects-in-the-brain-a-bell-type-test-for-nonclassical-latent-representations-in-autoencodershttpsarxivorgabs260110588v1 aria-label="Searching for Quantum Effects in the Brain: A Bell-Type Test for Nonclassical Latent Representations in Autoencoders"><a href=https://arxiv.org/abs/2601.10588v1>Searching for Quantum Effects in the Brain: A Bell-Type Test for Nonclassical Latent Representations in Autoencoders</a></a></li><li><a href=#euclid-preparation-3d-reconstruction-of-the-cosmic-web-with-simulated-euclid-deep-spectroscopic-sampleshttpsarxivorgabs260110709v1 aria-label="Euclid preparation. 3D reconstruction of the cosmic web with simulated Euclid Deep spectroscopic samples"><a href=https://arxiv.org/abs/2601.10709v1>Euclid preparation. 3D reconstruction of the cosmic web with simulated Euclid Deep spectroscopic samples</a></a></li><li><a href=#increasing-the-opening-speed-of-the-plasma-opening-switch-on-an-direct-action-accelerator-with-an-inductive-energy-storage-devicehttpsarxivorgabs260110694v1 aria-label="Increasing the opening speed of the plasma opening switch on an direct action accelerator with an inductive energy storage device"><a href=https://arxiv.org/abs/2601.10694v1>Increasing the opening speed of the plasma opening switch on an direct action accelerator with an inductive energy storage device</a></a></li><li><a href=#synchronizing-probabilities-in-model-driven-lossless-compressionhttpsarxivorgabs260110678v1 aria-label="Synchronizing Probabilities in Model-Driven Lossless Compression"><a href=https://arxiv.org/abs/2601.10678v1>Synchronizing Probabilities in Model-Driven Lossless Compression</a></a></li><li><a href=#learning-from-brain-topography-a-hierarchical-local-global-graph-transformer-network-for-eeg-emotion-recognitionhttpsarxivorgabs260110525v1 aria-label="Learning from Brain Topography: A Hierarchical Local-Global Graph-Transformer Network for EEG Emotion Recognition"><a href=https://arxiv.org/abs/2601.10525v1>Learning from Brain Topography: A Hierarchical Local-Global Graph-Transformer Network for EEG Emotion Recognition</a></a></li><li><a href=#tf3-ro-50m-training-compact-romanian-language-models-from-scratch-on-synthetic-moral-microfictionhttpsarxivorgabs260110410v1 aria-label="TF3-RO-50M: Training Compact Romanian Language Models from Scratch on Synthetic Moral Microfiction"><a href=https://arxiv.org/abs/2601.10410v1>TF3-RO-50M: Training Compact Romanian Language Models from Scratch on Synthetic Moral Microfiction</a></a></li><li><a href=#an-analytic-theory-of-convolutional-neural-network-inverse-problems-solvershttpsarxivorgabs260110334v1 aria-label="An analytic theory of convolutional neural network inverse problems solvers"><a href=https://arxiv.org/abs/2601.10334v1>An analytic theory of convolutional neural network inverse problems solvers</a></a></li><li><a href=#effect-of-hole-pitch-reduction-on-electron-transport-and-diffusion-a-comparative-simulation-study-of-triple-gem-detectorshttpsarxivorgabs260110139v1 aria-label="Effect of hole pitch reduction on electron transport and diffusion: A comparative simulation study of Triple GEM detectors"><a href=https://arxiv.org/abs/2601.10139v1>Effect of hole pitch reduction on electron transport and diffusion: A comparative simulation study of Triple GEM detectors</a></a></li><li><a href=#deriving-character-logic-from-storyline-as-codified-decision-treeshttpsarxivorgabs260110080v1 aria-label="Deriving Character Logic from Storyline as Codified Decision Trees"><a href=https://arxiv.org/abs/2601.10080v1>Deriving Character Logic from Storyline as Codified Decision Trees</a></a></li></ul></li><li><a href=#-psycholinguistics aria-label="üîç psycholinguistics">üîç psycholinguistics</a><ul><li><a href=#data-driven-stochastic-reduced-order-modeling-of-parametrized-dynamical-systemshttpsarxivorgabs260110690v1 aria-label="Data-driven stochastic reduced-order modeling of parametrized dynamical systems"><a href=https://arxiv.org/abs/2601.10690v1>Data-driven stochastic reduced-order modeling of parametrized dynamical systems</a></a></li><li><a href=#breaking-the-storage-bandwidth-tradeoff-in-distributed-storage-with-quantum-entanglementhttpsarxivorgabs260110676v1 aria-label="Breaking the Storage-Bandwidth Tradeoff in Distributed Storage with Quantum Entanglement"><a href=https://arxiv.org/abs/2601.10676v1>Breaking the Storage-Bandwidth Tradeoff in Distributed Storage with Quantum Entanglement</a></a></li><li><a href=#sporadic-creutzfeldt-jakob-disease-presenting-with-cerebral-atrophy-following-traumatic-brain-injury-mimicking-hydrocephalus-a-case-report-and-literature-reviewhttpsarxivorgabs260110663v1 aria-label="Sporadic Creutzfeldt Jakob disease presenting with cerebral atrophy following traumatic brain injury mimicking hydrocephalus a case report and literature review"><a href=https://arxiv.org/abs/2601.10663v1>Sporadic Creutzfeldt Jakob disease presenting with cerebral atrophy following traumatic brain injury mimicking hydrocephalus a case report and literature review</a></a></li><li><a href=#pacevolve-enabling-long-horizon-progress-aware-consistent-evolutionhttpsarxivorgabs260110657v1 aria-label="PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution"><a href=https://arxiv.org/abs/2601.10657v1>PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution</a></a></li><li><a href=#are-your-reasoning-models-reasoning-or-guessing-a-mechanistic-analysis-of-hierarchical-reasoning-modelshttpsarxivorgabs260110679v1 aria-label="Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models"><a href=https://arxiv.org/abs/2601.10679v1>Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models</a></a></li><li><a href=#sus-strategy-aware-surprise-for-intrinsic-explorationhttpsarxivorgabs260110349v1 aria-label="SuS: Strategy-aware Surprise for Intrinsic Exploration"><a href=https://arxiv.org/abs/2601.10349v1>SuS: Strategy-aware Surprise for Intrinsic Exploration</a></a></li><li><a href=#clozing-the-gap-exploring-why-language-model-surprisal-outperforms-cloze-surprisalhttpsarxivorgabs260109886v1 aria-label="Clozing the Gap: Exploring Why Language Model Surprisal Outperforms Cloze Surprisal"><a href=https://arxiv.org/abs/2601.09886v1>Clozing the Gap: Exploring Why Language Model Surprisal Outperforms Cloze Surprisal</a></a></li><li><a href=#integrating-diverse-assignment-strategies-into-detrshttpsarxivorgabs260109247v1 aria-label="Integrating Diverse Assignment Strategies into DETRs"><a href=https://arxiv.org/abs/2601.09247v1>Integrating Diverse Assignment Strategies into DETRs</a></a></li><li><a href=#why-ai-alignment-failure-is-structural-learned-human-interaction-structures-and-agi-as-an-endogenous-evolutionary-shockhttpsarxivorgabs260108673v1 aria-label="Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock"><a href=https://arxiv.org/abs/2601.08673v1>Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock</a></a></li><li><a href=#quantum-observers-can-communicate-across-multiverse-brancheshttpsarxivorgabs260108102v1 aria-label="Quantum observers can communicate across multiverse branches"><a href=https://arxiv.org/abs/2601.08102v1>Quantum observers can communicate across multiverse branches</a></a></li><li><a href=#quantum-maxwell-erasure-decoder-for-qldpc-codeshttpsarxivorgabs260110713v1 aria-label="Quantum Maxwell Erasure Decoder for qLDPC codes"><a href=https://arxiv.org/abs/2601.10713v1>Quantum Maxwell Erasure Decoder for qLDPC codes</a></a></li><li><a href=#constant-depth-unitary-preparation-of-dicke-stateshttpsarxivorgabs260110693v1 aria-label="Constant-Depth Unitary Preparation of Dicke States"><a href=https://arxiv.org/abs/2601.10693v1>Constant-Depth Unitary Preparation of Dicke States</a></a></li><li><a href=#the-conversational-exam-a-scalable-assessment-design-for-the-ai-erahttpsarxivorgabs260110691v1 aria-label="The Conversational Exam: A Scalable Assessment Design for the AI Era"><a href=https://arxiv.org/abs/2601.10691v1>The Conversational Exam: A Scalable Assessment Design for the AI Era</a></a></li><li><a href=#efficiency-curvature-and-complexity-of-quantum-evolutions-for-qubits-in-nonstationary-magnetic-fieldshttpsarxivorgabs260110672v1 aria-label="Efficiency, Curvature, and Complexity of Quantum Evolutions for Qubits in Nonstationary Magnetic Fields"><a href=https://arxiv.org/abs/2601.10672v1>Efficiency, Curvature, and Complexity of Quantum Evolutions for Qubits in Nonstationary Magnetic Fields</a></a></li><li><a href=#transformer-based-cognitive-radio-adaptive-modulation-strategies-using-transformer-modelshttpsarxivorgabs260110519v1 aria-label="Transformer-Based Cognitive Radio: Adaptive Modulation Strategies Using Transformer Models"><a href=https://arxiv.org/abs/2601.10519v1>Transformer-Based Cognitive Radio: Adaptive Modulation Strategies Using Transformer Models</a></a></li><li><a href=#testing-three-models-of-cognitive-stress-effects-a-psychopharmacological-randomized-controlled-trial-of-acute-stress-and-stress-hormones-across-visual-perception-response-inhibition-and-cognitive-flexibilityhttpsarxivorgabs260110515v1 aria-label="Testing three models of cognitive stress effects: A psychopharmacological randomized controlled trial of acute stress and stress hormones across visual perception, response inhibition and cognitive flexibility"><a href=https://arxiv.org/abs/2601.10515v1>Testing three models of cognitive stress effects: A psychopharmacological randomized controlled trial of acute stress and stress hormones across visual perception, response inhibition and cognitive flexibility</a></a></li><li><a href=#convex-efficient-codinghttpsarxivorgabs260110482v1 aria-label="Convex Efficient Coding"><a href=https://arxiv.org/abs/2601.10482v1>Convex Efficient Coding</a></a></li><li><a href=#are-language-models-modelshttpsarxivorgabs260110421v1 aria-label="Are Language Models Models?"><a href=https://arxiv.org/abs/2601.10421v1>Are Language Models Models?</a></a></li><li><a href=#stem-scaling-transformers-with-embedding-moduleshttpsarxivorgabs260110639v1 aria-label="STEM: Scaling Transformers with Embedding Modules"><a href=https://arxiv.org/abs/2601.10639v1>STEM: Scaling Transformers with Embedding Modules</a></a></li><li><a href=#a-new-construction-structure-on-multi-access-coded-caching-with-linear-subpacketization-cyclic-multi-access-non-half-sum-disjoint-packinghttpsarxivorgabs260110510v1 aria-label="A New Construction Structure on Multi-access Coded Caching with Linear Subpacketization: Cyclic Multi-Access Non-Half-Sum Disjoint Packing"><a href=https://arxiv.org/abs/2601.10510v1>A New Construction Structure on Multi-access Coded Caching with Linear Subpacketization: Cyclic Multi-Access Non-Half-Sum Disjoint Packing</a></a></li></ul></li><li><a href=#-llm aria-label="üîç llm">üîç llm</a><ul><li><a href=#unbounded-symbols-heat-flow-and-toeplitz-operatorshttpsarxivorgabs260110711v1 aria-label="Unbounded symbols, heat flow, and Toeplitz operators"><a href=https://arxiv.org/abs/2601.10711v1>Unbounded symbols, heat flow, and Toeplitz operators</a></a></li><li><a href=#a-note-on-strong-similarity-and-the-connes-embedding-problemhttpsarxivorgabs260110654v1 aria-label="A note on strong similarity and the Connes embedding problem"><a href=https://arxiv.org/abs/2601.10654v1>A note on strong similarity and the Connes embedding problem</a></a></li><li><a href=#circumplanetary-disk-candidate-in-the-disk-of-hd-163296-traced-by-localized-emission-from-simple-organicshttpsarxivorgabs260110631v1 aria-label="Circumplanetary Disk Candidate in the Disk of HD 163296 Traced by Localized Emission from Simple Organics"><a href=https://arxiv.org/abs/2601.10631v1>Circumplanetary Disk Candidate in the Disk of HD 163296 Traced by Localized Emission from Simple Organics</a></a></li><li><a href=#rsatalker-realistic-socially-aware-talking-head-generation-for-multi-turn-conversationhttpsarxivorgabs260110606v1 aria-label="RSATalker: Realistic Socially-Aware Talking Head Generation for Multi-Turn Conversation"><a href=https://arxiv.org/abs/2601.10606v1>RSATalker: Realistic Socially-Aware Talking Head Generation for Multi-Turn Conversation</a></a></li></ul></li><li><a href=#-neuroscience aria-label="üîç neuroscience">üîç neuroscience</a><ul><li><a href=#mhd-modelling-of-open-flux-evolution-around-solar-maximum-by-coronal-model-coconuthttpsarxivorgabs260110675v1 aria-label="MHD modelling of open flux evolution around solar maximum by coronal model COCONUT"><a href=https://arxiv.org/abs/2601.10675v1>MHD modelling of open flux evolution around solar maximum by coronal model COCONUT</a></a></li><li><a href=#safe-trajectory-gradient-flow-control-of-a-grid-interfacing-inverterhttpsarxivorgabs260110671v1 aria-label="Safe Trajectory Gradient Flow Control of a Grid-Interfacing Inverter"><a href=https://arxiv.org/abs/2601.10671v1>Safe Trajectory Gradient Flow Control of a Grid-Interfacing Inverter</a></a></li><li><a href=#combinatorial-optimization-augmented-machine-learninghttpsarxivorgabs260110583v1 aria-label="Combinatorial Optimization Augmented Machine Learning"><a href=https://arxiv.org/abs/2601.10583v1>Combinatorial Optimization Augmented Machine Learning</a></a></li><li><a href=#causalfe-causal-forests-with-fixed-effects-in-pythonhttpsarxivorgabs260110555v1 aria-label="causalfe: Causal Forests with Fixed Effects in Python"><a href=https://arxiv.org/abs/2601.10555v1>causalfe: Causal Forests with Fixed Effects in Python</a></a></li><li><a href=#cogen-creation-of-reusable-ui-components-in-figma-via-textual-commandshttpsarxivorgabs260110536v1 aria-label="CoGen: Creation of Reusable UI Components in Figma via Textual Commands"><a href=https://arxiv.org/abs/2601.10536v1>CoGen: Creation of Reusable UI Components in Figma via Textual Commands</a></a></li></ul></li><li><a href=#-data_resources aria-label="üîç data_resources">üîç data_resources</a><ul><li><a href=#model-see-model-do-exposure-aware-evaluation-of-bug-vs-fix-preference-in-code-llmshttpsarxivorgabs260110496v1 aria-label="Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs"><a href=https://arxiv.org/abs/2601.10496v1>Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs</a></a></li><li><a href=#comovi-co-generation-of-3d-human-motions-and-realistic-videoshttpsarxivorgabs260110632v1 aria-label="CoMoVi: Co-Generation of 3D Human Motions and Realistic Videos"><a href=https://arxiv.org/abs/2601.10632v1>CoMoVi: Co-Generation of 3D Human Motions and Realistic Videos</a></a></li><li><a href=#action100m-a-large-scale-video-action-datasethttpsarxivorgabs260110592v1 aria-label="Action100M: A Large-scale Video Action Dataset"><a href=https://arxiv.org/abs/2601.10592v1>Action100M: A Large-scale Video Action Dataset</a></a></li><li><a href=#bikeactions-an-open-platform-and-benchmark-for-cyclist-centric-vru-action-recognitionhttpsarxivorgabs260110521v1 aria-label="BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition"><a href=https://arxiv.org/abs/2601.10521v1>BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition</a></a></li><li><a href=#privacy-enhanced-peft-tensor-train-decomposition-improves-privacy-utility-tradeoffs-under-dp-sgdhttpsarxivorgabs260110045v1 aria-label="Privacy Enhanced PEFT: Tensor Train Decomposition Improves Privacy Utility Tradeoffs under DP-SGD"><a href=https://arxiv.org/abs/2601.10045v1>Privacy Enhanced PEFT: Tensor Train Decomposition Improves Privacy Utility Tradeoffs under DP-SGD</a></a></li></ul></li><li><a href=#-emotion_language aria-label="üîç emotion_language">üîç emotion_language</a><ul><li><a href=#real-characters-and-real-classes-of-mathrmgl_2-and-mathrmgu_2-over-discrete-valuation-ringshttpsarxivorgabs260110670v1 aria-label="Real characters and real classes of $\mathrm{GL}_2$ and $\mathrm{GU}_2$ over discrete valuation rings"><a href=https://arxiv.org/abs/2601.10670v1>Real characters and real classes of $\mathrm{GL}_2$ and $\mathrm{GU}_2$ over discrete valuation rings</a></a></li><li><a href=#hii-regions-in-ngc-628-the-view-of-two-catalogshttpsarxivorgabs260110642v1 aria-label="HII regions in NGC 628: the view of two catalogs"><a href=https://arxiv.org/abs/2601.10642v1>HII regions in NGC 628: the view of two catalogs</a></a></li><li><a href=#fair-regression-under-demographic-parity-a-unified-frameworkhttpsarxivorgabs260110623v1 aria-label="Fair Regression under Demographic Parity: A Unified Framework"><a href=https://arxiv.org/abs/2601.10623v1>Fair Regression under Demographic Parity: A Unified Framework</a></a></li><li><a href=#malcev-classification-for-the-variety-of-left-symmetric-algebrashttpsarxivorgabs260110613v1 aria-label="Malcev classification for the variety of left-symmetric algebras"><a href=https://arxiv.org/abs/2601.10613v1>Malcev classification for the variety of left-symmetric algebras</a></a></li><li><a href=#scalable-spin-squeezing-in-power-law-interacting-xxz-models-with-disorderhttpsarxivorgabs260110703v1 aria-label="Scalable Spin Squeezing in Power-Law Interacting XXZ Models with Disorder"><a href=https://arxiv.org/abs/2601.10703v1>Scalable Spin Squeezing in Power-Law Interacting XXZ Models with Disorder</a></a></li><li><a href=#late-time-acceleration-without-a-vacuum-term-in-frl_m-gravity-scaling-desitter-dynamics-and-parameter-constraintshttpsarxivorgabs260110699v1 aria-label="Late-time acceleration without a vacuum term in ${f(R,L_m)}$ gravity: scaling deSitter dynamics and parameter constraints"><a href=https://arxiv.org/abs/2601.10699v1>Late-time acceleration without a vacuum term in ${f(R,L_m)}$ gravity: scaling deSitter dynamics and parameter constraints</a></a></li><li><a href=#the-effective-theory-of-muon-to-electron-conversionhttpsarxivorgabs260110704v1 aria-label="The Effective Theory of Muon-to-Electron Conversion"><a href=https://arxiv.org/abs/2601.10704v1>The Effective Theory of Muon-to-Electron Conversion</a></a></li><li><a href=#a-bayesian-discrete-framework-for-enhancing-decision-making-processes-in-clinical-trial-designs-and-evaluationshttpsarxivorgabs260110615v1 aria-label="A Bayesian Discrete Framework for Enhancing Decision-Making Processes in Clinical Trial Designs and Evaluations"><a href=https://arxiv.org/abs/2601.10615v1>A Bayesian Discrete Framework for Enhancing Decision-Making Processes in Clinical Trial Designs and Evaluations</a></a></li><li><a href=#defending-large-language-models-against-jailbreak-attacks-via-in-decoding-safety-awareness-probinghttpsarxivorgabs260110543v1 aria-label="Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing"><a href=https://arxiv.org/abs/2601.10543v1>Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing</a></a></li><li><a href=#observation-timelines-for-the-potential-lunar-impact-of-asteroid-2024-yr4httpsarxivorgabs260110666v1 aria-label="Observation Timelines for the Potential Lunar Impact of Asteroid 2024 YR4"><a href=https://arxiv.org/abs/2601.10666v1>Observation Timelines for the Potential Lunar Impact of Asteroid 2024 YR4</a></a></li><li><a href=#generative-ai-collective-behavior-needs-an-interactionist-paradigmhttpsarxivorgabs260110567v1 aria-label="Generative AI collective behavior needs an interactionist paradigm"><a href=https://arxiv.org/abs/2601.10567v1>Generative AI collective behavior needs an interactionist paradigm</a></a></li><li><a href=#inferring-signed-social-networks-from-contact-patternshttpsarxivorgabs260110565v1 aria-label="Inferring signed social networks from contact patterns"><a href=https://arxiv.org/abs/2601.10565v1>Inferring signed social networks from contact patterns</a></a></li><li><a href=#a-propagation-framework-for-network-regressionhttpsarxivorgabs260110533v1 aria-label="A Propagation Framework for Network Regression"><a href=https://arxiv.org/abs/2601.10533v1>A Propagation Framework for Network Regression</a></a></li><li><a href=#higher-order-trade-offs-in-hypergraph-community-detectionhttpsarxivorgabs260110502v1 aria-label="Higher order trade-offs in hypergraph community detection"><a href=https://arxiv.org/abs/2601.10502v1>Higher order trade-offs in hypergraph community detection</a></a></li></ul></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h1 id=weekly-paper-notes>Weekly Paper Notes<a hidden class=anchor aria-hidden=true href=#weekly-paper-notes>#</a></h1><h2 id=-multilingual>üîç multilingual<a hidden class=anchor aria-hidden=true href=#-multilingual>#</a></h2><h3 id=curve-a-benchmark-for-cultural-and-multilingual-long-video-reasoninghttpsarxivorgabs260110649v1><a href=https://arxiv.org/abs/2601.10649v1>CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning</a><a hidden class=anchor aria-hidden=true href=#curve-a-benchmark-for-cultural-and-multilingual-long-video-reasoninghttpsarxivorgabs260110649v1>#</a></h3><p><strong>Authors:</strong> Darshan Singh, Arsha Nagrani, Kawshik Manikantan, Harman Singh, Dinesh Tewari, Tobias Weyand, Cordelia Schmid, Anelia Angelova, Shachi Dave
<strong>Venue:</strong> arXiv (2026)</p><p>Recent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE&rsquo;s reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under <a href="https://github.com/google-deepmind/neptune?tab=readme-ov-file">https://github.com/google-deepmind/neptune?tab=readme-ov-file</a>#minerva-cultural</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10649v1">üìÑ Download PDF</a></p><hr><h3 id=form-and-meaning-in-intrinsic-multilingual-evaluationshttpsarxivorgabs260110580v1><a href=https://arxiv.org/abs/2601.10580v1>Form and Meaning in Intrinsic Multilingual Evaluations</a><a hidden class=anchor aria-hidden=true href=#form-and-meaning-in-intrinsic-multilingual-evaluationshttpsarxivorgabs260110580v1>#</a></h3><p><strong>Authors:</strong> Wessel Poelman, Miryam de Lhoneux
<strong>Venue:</strong> arXiv (2026)</p><p>Intrinsic evaluation metrics for conditional language models, such as perplexity or bits-per-character, are widely used in both mono- and multilingual settings. These metrics are rather straightforward to use and compare in monolingual setups, but rest on a number of assumptions in multilingual setups. One such assumption is that comparing the perplexity of CLMs on parallel sentences is indicative of their quality since the information content (here understood as the semantic meaning) is the same. However, the metrics are inherently measuring information content in the information-theoretic sense. We make this and other such assumptions explicit and discuss their implications. We perform experiments with six metrics on two multi-parallel corpora both with mono- and multilingual models. Ultimately, we find that current metrics are not universally comparable. We look at the form-meaning debate to provide some explanation for this.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10580v1">üìÑ Download PDF</a></p><hr><h3 id=a-safety-report-on-gpt-52-gemini-3-pro-qwen3-vl-doubao-18-grok-41-fast-nano-banana-pro-and-seedream-45httpsarxivorgabs260110527v1><a href=https://arxiv.org/abs/2601.10527v1>A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5</a><a hidden class=anchor aria-hidden=true href=#a-safety-report-on-gpt-52-gemini-3-pro-qwen3-vl-doubao-18-grok-41-fast-nano-banana-pro-and-seedream-45httpsarxivorgabs260110527v1>#</a></h3><p><strong>Authors:</strong> Xingjun Ma, Yixu Wang, Hengyuan Xu, Yutao Wu, Yifan Ding, Yunhan Zhao, Zilong Wang, Jiabin Hua, Ming Wen, Jianan Liu, Ranjie Duan, Yifeng Gao, Yingshui Tan, Yunhao Chen, Hui Xue, Xin Wang, Wei Cheng, Jingjing Chen, Zuxuan Wu, Bo Li, Yu-Gang Jiang
<strong>Venue:</strong> arXiv (2026)</p><p>The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional&ndash;shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10527v1">üìÑ Download PDF</a></p><hr><h3 id=indic-dialect-a-multi-task-benchmark-to-evaluate-and-translate-in-indian-language-dialectshttpsarxivorgabs260110388v1><a href=https://arxiv.org/abs/2601.10388v1>INDIC DIALECT: A Multi Task Benchmark to Evaluate and Translate in Indian Language Dialects</a><a hidden class=anchor aria-hidden=true href=#indic-dialect-a-multi-task-benchmark-to-evaluate-and-translate-in-indian-language-dialectshttpsarxivorgabs260110388v1>#</a></h3><p><strong>Authors:</strong> Tarun Sharma, Manikandan Ravikiran, Sourava Kumar Behera, Pramit Bhattacharya, Arnab Bhattacharya, Rohit Saluja
<strong>Venue:</strong> arXiv (2026)</p><p>Recent NLP advances focus primarily on standardized languages, leaving most low-resource dialects under-served especially in Indian scenarios. In India, the issue is particularly important: despite Hindi being the third most spoken language globally (over 600 million speakers), its numerous dialects remain underrepresented. The situation is similar for Odia, which has around 45 million speakers. While some datasets exist which contain standard Hindi and Odia languages, their regional dialects have almost no web presence. We introduce INDIC-DIALECT, a human-curated parallel corpus of 13k sentence pairs spanning 11 dialects and 2 languages: Hindi and Odia. Using this corpus, we construct a multi-task benchmark with three tasks: dialect classification, multiple-choice question (MCQ) answering, and machine translation (MT). Our experiments show that LLMs like GPT-4o and Gemini 2.5 perform poorly on the classification task. While fine-tuned transformer based models pretrained on Indian languages substantially improve performance e.g., improving F1 from 19.6% to 89.8% on dialect classification. For dialect to language translation, we find that hybrid AI model achieves highest BLEU score of 61.32 compared to the baseline score of 23.36. Interestingly, due to complexity in generating dialect sentences, we observe that for language to dialect translation the ``rule-based followed by AI" approach achieves best BLEU score of 48.44 compared to the baseline score of 27.59. INDIC-DIALECT thus is a new benchmark for dialect-aware Indic NLP, and we plan to release it as open source to support further work on low-resource Indian dialects.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10388v1">üìÑ Download PDF</a></p><hr><h3 id=an-efficient-long-context-ranking-architecture-with-calibrated-llm-distillation-application-to-person-job-fithttpsarxivorgabs260110321v1><a href=https://arxiv.org/abs/2601.10321v1>An Efficient Long-Context Ranking Architecture With Calibrated LLM Distillation: Application to Person-Job Fit</a><a hidden class=anchor aria-hidden=true href=#an-efficient-long-context-ranking-architecture-with-calibrated-llm-distillation-application-to-person-job-fithttpsarxivorgabs260110321v1>#</a></h3><p><strong>Authors:</strong> Warren Jouanneau, Emma Jouffroy, Marc Palyart
<strong>Venue:</strong> arXiv (2026)</p><p>Finding the most relevant person for a job proposal in real time is challenging, especially when resumes are long, structured, and multilingual. In this paper, we propose a re-ranking model based on a new generation of late cross-attention architecture, that decomposes both resumes and project briefs to efficiently handle long-context inputs with minimal computational overhead. To mitigate historical data biases, we use a generative large language model (LLM) as a teacher, generating fine-grained, semantically grounded supervision. This signal is distilled into our student model via an enriched distillation loss function. The resulting model produces skill-fit scores that enable consistent and interpretable person-job matching. Experiments on relevance, ranking, and calibration metrics demonstrate that our approach outperforms state-of-the-art baselines.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10321v1">üìÑ Download PDF</a></p><hr><h3 id=multilinguality-as-sense-adaptationhttpsarxivorgabs260110310v1><a href=https://arxiv.org/abs/2601.10310v1>Multilinguality as Sense Adaptation</a><a hidden class=anchor aria-hidden=true href=#multilinguality-as-sense-adaptationhttpsarxivorgabs260110310v1>#</a></h3><p><strong>Authors:</strong> Jan Christian Blaise Cruz, David Ifeoluwa Adelani, Alham Fikri Aji
<strong>Venue:</strong> arXiv (2026)</p><p>We approach multilinguality as sense adaptation: aligning latent meaning representations across languages rather than relying solely on shared parameters and scale. In this paper, we introduce SENse-based Symmetric Interlingual Alignment (SENSIA), which adapts a Backpack language model from one language to another by explicitly aligning sense-level mixtures and contextual representations on parallel data, while jointly training a target-language language modeling loss to preserve fluency. Across benchmarks on four typologically diverse languages, SENSIA generally outperforms comparable multilingual alignment methods and achieves competitive accuracy against monolingual from-scratch baselines while using 2-4x less target-language data. Analyses of learned sense geometry indicate that local sense topology and global structure relative to English are largely preserved, and ablations show that the method is robust in terms of design and scale.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10310v1">üìÑ Download PDF</a></p><hr><h3 id=the-straight-and-narrow-do-llms-possess-an-internal-moral-pathhttpsarxivorgabs260110307v1><a href=https://arxiv.org/abs/2601.10307v1>The Straight and Narrow: Do LLMs Possess an Internal Moral Path?</a><a hidden class=anchor aria-hidden=true href=#the-straight-and-narrow-do-llms-possess-an-internal-moral-pathhttpsarxivorgabs260110307v1>#</a></h3><p><strong>Authors:</strong> Luoming Hu, Jingjie Zeng, Liang Yang, Hongfei Lin
<strong>Venue:</strong> arXiv (2026)</p><p>Enhancing the moral alignment of Large Language Models (LLMs) is a critical challenge in AI safety. Current alignment techniques often act as superficial guardrails, leaving the intrinsic moral representations of LLMs largely untouched. In this paper, we bridge this gap by leveraging Moral Foundations Theory (MFT) to map and manipulate the fine-grained moral landscape of LLMs. Through cross-lingual linear probing, we validate the shared nature of moral representations in middle layers and uncover a shared yet different moral subspace between English and Chinese. Building upon this, we extract steerable Moral Vectors and successfully validate their efficacy at both internal and behavioral levels. Leveraging the high generalizability of morality, we propose Adaptive Moral Fusion (AMF), a dynamic inference-time intervention that synergizes probe detection with vector injection to tackle the safety-helpfulness trade-off. Empirical results confirm that our approach acts as a targeted intrinsic defense, effectively reducing incorrect refusals on benign queries while minimizing jailbreak success rates compared to standard baselines.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10307v1">üìÑ Download PDF</a></p><hr><h3 id=untangling-input-language-from-reasoning-language-a-diagnostic-framework-for-cross-lingual-moral-alignment-in-llmshttpsarxivorgabs260110257v1><a href=https://arxiv.org/abs/2601.10257v1>Untangling Input Language from Reasoning Language: A Diagnostic Framework for Cross-Lingual Moral Alignment in LLMs</a><a hidden class=anchor aria-hidden=true href=#untangling-input-language-from-reasoning-language-a-diagnostic-framework-for-cross-lingual-moral-alignment-in-llmshttpsarxivorgabs260110257v1>#</a></h3><p><strong>Authors:</strong> Nan Li, Bo Kang, Tijl De Bie
<strong>Venue:</strong> arXiv (2026)</p><p>When LLMs judge moral dilemmas, do they reach different conclusions in different languages, and if so, why? Two factors could drive such differences: the language of the dilemma itself, or the language in which the model reasons. Standard evaluation conflates these by testing only matched conditions (e.g., English dilemma with English reasoning). We introduce a methodology that separately manipulates each factor, covering also mismatched conditions (e.g., English dilemma with Chinese reasoning), enabling decomposition of their contributions. To study \emph{what} changes, we propose an approach to interpret the moral judgments in terms of Moral Foundations Theory. As a side result, we identify evidence for splitting the Authority dimension into a family-related and an institutional dimension. Applying this methodology to English-Chinese moral judgment with 13 LLMs, we demonstrate its diagnostic power: (1) the framework isolates reasoning-language effects as contributing twice the variance of input-language effects; (2) it detects context-dependency in nearly half of models that standard evaluation misses; and (3) a diagnostic taxonomy translates these patterns into deployment guidance. We release our code and datasets at <a href=https://anonymous.4open.science/r/CrossCulturalMoralJudgement>https://anonymous.4open.science/r/CrossCulturalMoralJudgement</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10257v1">üìÑ Download PDF</a></p><hr><h3 id=one-instruction-does-not-fit-all-how-well-do-embeddings-align-personas-and-instructions-in-low-resource-indian-languageshttpsarxivorgabs260110205v1><a href=https://arxiv.org/abs/2601.10205v1>One Instruction Does Not Fit All: How Well Do Embeddings Align Personas and Instructions in Low-Resource Indian Languages?</a><a hidden class=anchor aria-hidden=true href=#one-instruction-does-not-fit-all-how-well-do-embeddings-align-personas-and-instructions-in-low-resource-indian-languageshttpsarxivorgabs260110205v1>#</a></h3><p><strong>Authors:</strong> Arya Shah, Himanshu beniwal, Mayank Singh
<strong>Venue:</strong> arXiv (2026)</p><p>Aligning multilingual assistants with culturally grounded user preferences is essential for serving India&rsquo;s linguistically diverse population of over one billion speakers across multiple scripts. However, existing benchmarks either focus on a single language or conflate retrieval with generation, leaving open the question of whether current embedding models can encode persona-instruction compatibility without relying on response synthesis. We present a unified benchmark spanning 12 Indian languages and four evaluation tasks: monolingual and cross-lingual persona-to-instruction retrieval, reverse retrieval from instruction to persona, and binary compatibility classification. Eight multilingual embedding models are evaluated in a frozen-encoder setting with a thin logistic regression head for classification. E5-Large-Instruct achieves the highest Recall@1 of 27.4% on monolingual retrieval and 20.7% on cross-lingual transfer, while BGE-M3 leads reverse retrieval at 32.1% Recall@1. For classification, LaBSE attains 75.3% AUROC with strong calibration. These findings offer practical guidance for model selection in Indic multilingual retrieval and establish reproducible baselines for future work\footnote{Code, datasets, and models are publicly available at <a href=https://github.com/aryashah2k/PI-Indic-Align>https://github.com/aryashah2k/PI-Indic-Align</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10205v1">üìÑ Download PDF</a></p><hr><h3 id=homura-taming-the-sand-glass-for-time-constrained-llm-translation-via-reinforcement-learninghttpsarxivorgabs260110187v1><a href=https://arxiv.org/abs/2601.10187v1>HOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning</a><a hidden class=anchor aria-hidden=true href=#homura-taming-the-sand-glass-for-time-constrained-llm-translation-via-reinforcement-learninghttpsarxivorgabs260110187v1>#</a></h3><p><strong>Authors:</strong> Ziang Cui, Mengran Yu, Tianjiao Li, Chenyu Shi, Yingxuan Shi, Lusheng Zhang, Hongwei Lin
<strong>Venue:</strong> arXiv (2026)</p><p>Large Language Models (LLMs) have achieved remarkable strides in multilingual translation but are hindered by a systemic cross-lingual verbosity bias, rendering them unsuitable for strict time-constrained tasks like subtitling and dubbing. Current prompt-engineering approaches struggle to resolve this conflict between semantic fidelity and rigid temporal feasibility. To bridge this gap, we first introduce Sand-Glass, a benchmark specifically designed to evaluate translation under syllable-level duration constraints. Furthermore, we propose HOMURA, a reinforcement learning framework that explicitly optimizes the trade-off between semantic preservation and temporal compliance. By employing a KL-regularized objective with a novel dynamic syllable-ratio reward, HOMURA effectively &ldquo;tames&rdquo; the output length. Experimental results demonstrate that our method significantly outperforms strong LLM baselines, achieving precise length control that respects linguistic density hierarchies without compromising semantic adequacy.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10187v1">üìÑ Download PDF</a></p><hr><h3 id=multilingual-to-multimodal-m2m-unlocking-new-languages-with-monolingual-texthttpsarxivorgabs260110096v1><a href=https://arxiv.org/abs/2601.10096v1>Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text</a><a hidden class=anchor aria-hidden=true href=#multilingual-to-multimodal-m2m-unlocking-new-languages-with-monolingual-texthttpsarxivorgabs260110096v1>#</a></h3><p><strong>Authors:</strong> Piyush Singh Pasi
<strong>Venue:</strong> arXiv (2026)</p><p>Multimodal models excel in English, supported by abundant image-text and audio-text data, but performance drops sharply for other languages due to limited multilingual multimodal resources. Existing solutions rely heavily on machine translation, while advances in multilingual text modeling remain underutilized. We introduce METAL, a lightweight alignment method that learns only a few linear layers using English text alone to map multilingual text embeddings into a multimodal space. Despite its simplicity, METAL matches baseline performance in English (94.9 percent Recall at 10) and achieves strong zero-shot transfer (89.5 percent Recall at 10 averaged across 11 languages, 10 unseen) on XTD text-to-image retrieval. Qualitative t-SNE visualizations show that multilingual embeddings align tightly with multimodal representations, while weight analysis reveals that the transformation reshapes embedding geometry rather than performing trivial rotations. Beyond image-text retrieval, METAL generalizes to audio-text retrieval and cross-lingual text-to-image generation. We release code and checkpoints at <a href=https://github.com/m2m-codebase/M2M>https://github.com/m2m-codebase/M2M</a> , as well as multilingual evaluation datasets including MSCOCO Multilingual 30K (<a href=https://huggingface.co/datasets/piyushsinghpasi/mscoco-multilingual-30k>https://huggingface.co/datasets/piyushsinghpasi/mscoco-multilingual-30k</a> ), AudioCaps Multilingual (<a href=https://huggingface.co/datasets/piyushsinghpasi/audiocaps-multilingual>https://huggingface.co/datasets/piyushsinghpasi/audiocaps-multilingual</a> ), and Clotho Multilingual (<a href=https://huggingface.co/datasets/piyushsinghpasi/clotho-multilingual>https://huggingface.co/datasets/piyushsinghpasi/clotho-multilingual</a> ), to facilitate further research.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10096v1">üìÑ Download PDF</a></p><hr><h3 id=creating-a-hybrid-rule-and-neural-network-based-semantic-tagger-using-silver-standard-data-the-pymusas-framework-for-multilingual-semantic-annotationhttpsarxivorgabs260109648v1><a href=https://arxiv.org/abs/2601.09648v1>Creating a Hybrid Rule and Neural Network Based Semantic Tagger using Silver Standard Data: the PyMUSAS framework for Multilingual Semantic Annotation</a><a hidden class=anchor aria-hidden=true href=#creating-a-hybrid-rule-and-neural-network-based-semantic-tagger-using-silver-standard-data-the-pymusas-framework-for-multilingual-semantic-annotationhttpsarxivorgabs260109648v1>#</a></h3><p><strong>Authors:</strong> Andrew Moore, Paul Rayson, Dawn Archer, Tim Czerniak, Dawn Knight, Daisy Lal, Gear√≥id √ì Donnchadha, M√≠che√°l √ì Meachair, Scott Piao, Elaine U√≠ Dhonnchadha, Johanna Vuorinen, Yan Yabo, Xiaobin Yang
<strong>Venue:</strong> arXiv (2026)</p><p>Word Sense Disambiguation (WSD) has been widely evaluated using the semantic frameworks of WordNet, BabelNet, and the Oxford Dictionary of English. However, for the UCREL Semantic Analysis System (USAS) framework, no open extensive evaluation has been performed beyond lexical coverage or single language evaluation. In this work, we perform the largest semantic tagging evaluation of the rule based system that uses the lexical resources in the USAS framework covering five different languages using four existing datasets and one novel Chinese dataset. We create a new silver labelled English dataset, to overcome the lack of manually tagged training data, that we train and evaluate various mono and multilingual neural models in both mono and cross-lingual evaluation setups with comparisons to their rule based counterparts, and show how a rule based system can be enhanced with a neural network model. The resulting neural network models, including the data they were trained on, the Chinese evaluation dataset, and all of the code have been released as open resources.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.09648v1">üìÑ Download PDF</a></p><hr><h3 id=geosteer-faithful-chain-of-thought-steering-via-latent-manifold-gradientshttpsarxivorgabs260110229v1><a href=https://arxiv.org/abs/2601.10229v1>GeoSteer: Faithful Chain-of-Thought Steering via Latent Manifold Gradients</a><a hidden class=anchor aria-hidden=true href=#geosteer-faithful-chain-of-thought-steering-via-latent-manifold-gradientshttpsarxivorgabs260110229v1>#</a></h3><p><strong>Authors:</strong> Kentaro Kazama, Daiki Shirafuji, Tatsuhiko Saito
<strong>Venue:</strong> arXiv (2026)</p><p>Recent advances in Large Language Models (LLMs) have improved multi-step reasoning. Most approaches rely on Chain-of-Thought (CoT) rationales. Previous studies have shown that LLMs often generate logically inconsistent reasoning steps even when their final answers are correct. These inconsistencies reduce the reliability of step-level reasoning. We propose GeoSteer, a manifold-based framework that improves the quality of intermediate reasoning. The method consists of: (1) constructing a CoT dataset with segment-level scores, (2) training a Variational Autoencoder (VAE) model and a quality estimation model to learn a low-dimensional manifold of high-quality CoT trajectories, and (3) steering hidden states of target LLMs toward higher-quality regions in the latent space. This update in a latent space behaves like a natural-gradient adjustment in the original hidden-state space. It ensures geometrically coherent steering. We evaluate GeoSteer on the GSM8k dataset using the Qwen3 series. We measure via answer accuracy and overall reasoning performance. GeoSteer improved the exact match accuracy by up to 2.6 points. It also enhanced the pairwise win rate by 5.3 points. These results indicate that GeoSteer provides an effective and controllable mechanism for improving the quality of intermediate reasoning in LLMs.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10229v1">üìÑ Download PDF</a></p><hr><h3 id=awed-finer-agents-web-applications-and-expert-detectors-for-fine-grained-named-entity-recognition-across-36-languages-for-66-billion-speakershttpsarxivorgabs260110161v1><a href=https://arxiv.org/abs/2601.10161v1>AWED-FiNER: Agents, Web applications, and Expert Detectors for Fine-grained Named Entity Recognition across 36 Languages for 6.6 Billion Speakers</a><a hidden class=anchor aria-hidden=true href=#awed-finer-agents-web-applications-and-expert-detectors-for-fine-grained-named-entity-recognition-across-36-languages-for-66-billion-speakershttpsarxivorgabs260110161v1>#</a></h3><p><strong>Authors:</strong> Prachuryya Kaushik, Ashish Anand
<strong>Venue:</strong> arXiv (2026)</p><p>We introduce AWED-FiNER, an open-source ecosystem designed to bridge the gap in Fine-grained Named Entity Recognition (FgNER) for 36 global languages spoken by more than 6.6 billion people. While Large Language Models (LLMs) dominate general Natural Language Processing (NLP) tasks, they often struggle with low-resource languages and fine-grained NLP tasks. AWED-FiNER provides a collection of agentic toolkits, web applications, and several state-of-the-art expert models that provides FgNER solutions across 36 languages. The agentic tools enable to route multilingual text to specialized expert models and fetch FgNER annotations within seconds. The web-based platforms provide ready-to-use FgNER annotation service for non-technical users. Moreover, the collection of language specific extremely small sized open-source state-of-the-art expert models facilitate offline deployment in resource contraint scenerios including edge devices. AWED-FiNER covers languages spoken by over 6.6 billion people, including a specific focus on vulnerable languages such as Bodo, Manipuri, Bishnupriya, and Mizo. The resources can be accessed here: Agentic Tool (<a href=https://github.com/PrachuryyaKaushik/AWED-FiNER%29>https://github.com/PrachuryyaKaushik/AWED-FiNER)</a>, Web Application (<a href=https://hf.co/spaces/prachuryyaIITG/AWED-FiNER%29>https://hf.co/spaces/prachuryyaIITG/AWED-FiNER)</a>, and 49 Expert Detector Models (<a href=https://hf.co/collections/prachuryyaIITG/awed-finer%29>https://hf.co/collections/prachuryyaIITG/awed-finer)</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10161v1">üìÑ Download PDF</a></p><hr><h3 id=role-playing-agents-driven-by-large-language-models-current-status-challenges-and-future-trendshttpsarxivorgabs260110122v1><a href=https://arxiv.org/abs/2601.10122v1>Role-Playing Agents Driven by Large Language Models: Current Status, Challenges, and Future Trends</a><a hidden class=anchor aria-hidden=true href=#role-playing-agents-driven-by-large-language-models-current-status-challenges-and-future-trendshttpsarxivorgabs260110122v1>#</a></h3><p><strong>Authors:</strong> Ye Wang, Jiaxing Chen, Hongjiang Xiao
<strong>Venue:</strong> arXiv (2026)</p><p>In recent years, with the rapid advancement of large language models (LLMs), role-playing language agents (RPLAs) have emerged as a prominent research focus at the intersection of natural language processing (NLP) and human-computer interaction. This paper systematically reviews the current development and key technologies of RPLAs, delineating the technological evolution from early rule-based template paradigms, through the language style imitation stage, to the cognitive simulation stage centered on personality modeling and memory mechanisms. It summarizes the critical technical pathways supporting high-quality role-playing, including psychological scale-driven character modeling, memory-augmented prompting mechanisms, and motivation-situation-based behavioral decision control. At the data level, the paper further analyzes the methods and challenges of constructing role-specific corpora, focusing on data sources, copyright constraints, and structured annotation processes. In terms of evaluation, it collates multi-dimensional assessment frameworks and benchmark datasets covering role knowledge, personality fidelity, value alignment, and interactive hallucination, while commenting on the advantages and disadvantages of methods such as human evaluation, reward models, and LLM-based scoring. Finally, the paper outlines future development directions of role-playing agents, including personality evolution modeling, multi-agent collaborative narrative, multimodal immersive interaction, and integration with cognitive neuroscience, aiming to provide a systematic perspective and methodological insights for subsequent research.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10122v1">üìÑ Download PDF</a></p><hr><h3 id=empathy-applicability-modeling-for-general-health-querieshttpsarxivorgabs260109696v1><a href=https://arxiv.org/abs/2601.09696v1>Empathy Applicability Modeling for General Health Queries</a><a hidden class=anchor aria-hidden=true href=#empathy-applicability-modeling-for-general-health-querieshttpsarxivorgabs260109696v1>#</a></h3><p><strong>Authors:</strong> Shan Randhawa, Agha Ali Raza, Kentaro Toyama, Julie Hui, Mustafa Naseem
<strong>Venue:</strong> arXiv (2026)</p><p>LLMs are increasingly being integrated into clinical workflows, yet they often lack clinical empathy, an essential aspect of effective doctor-patient communication. Existing NLP frameworks focus on reactively labeling empathy in doctors&rsquo; responses but offer limited support for anticipatory modeling of empathy needs, especially in general health queries. We introduce the Empathy Applicability Framework (EAF), a theory-driven approach that classifies patient queries in terms of the applicability of emotional reactions and interpretations, based on clinical, contextual, and linguistic cues. We release a benchmark of real patient queries, dual-annotated by Humans and GPT-4o. In the subset with human consensus, we also observe substantial human-GPT alignment. To validate EAF, we train classifiers on human-labeled and GPT-only annotations to predict empathy applicability, achieving strong performance and outperforming the heuristic and zero-shot LLM baselines. Error analysis highlights persistent challenges: implicit distress, clinical-severity ambiguity, and contextual hardship, underscoring the need for multi-annotator modeling, clinician-in-the-loop calibration, and culturally diverse annotation. EAF provides a framework for identifying empathy needs before response generation, establishes a benchmark for anticipatory empathy modeling, and enables supporting empathetic communication in asynchronous healthcare.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.09696v1">üìÑ Download PDF</a></p><hr><h3 id=llms-got-rhythm-hybrid-phonological-filtering-for-greek-poetry-rhyme-detection-and-generationhttpsarxivorgabs260109631v1><a href=https://arxiv.org/abs/2601.09631v1>LLMs Got Rhythm? Hybrid Phonological Filtering for Greek Poetry Rhyme Detection and Generation</a><a hidden class=anchor aria-hidden=true href=#llms-got-rhythm-hybrid-phonological-filtering-for-greek-poetry-rhyme-detection-and-generationhttpsarxivorgabs260109631v1>#</a></h3><p><strong>Authors:</strong> Stergios Chatzikyriakidis
<strong>Venue:</strong> arXiv (2026)</p><p>Large Language Models (LLMs), despite their remarkable capabilities across NLP tasks, struggle with phonologically-grounded phenomena like rhyme detection and generation. This is even more evident in lower-resource languages such as Modern Greek. In this paper, we present a hybrid system that combines LLMs with deterministic phonological algorithms to achieve accurate rhyme identification/analysis and generation. Our approach implements a comprehensive taxonomy of Greek rhyme types, including Pure, Rich, Imperfect, Mosaic, and Identical Pre-rhyme Vowel (IDV) patterns, and employs an agentic generation pipeline with phonological verification. We evaluate multiple prompting strategies (zero-shot, few-shot, Chain-of-Thought, and RAG-augmented) across several LLMs including Claude 3.7 and 4.5, GPT-4o, Gemini 2.0 and open-weight models like Llama 3.1 8B and 70B and Mistral Large. Results reveal a significant &ldquo;Reasoning Gap&rdquo;: while native-like models (Claude 3.7) perform intuitively (40% accuracy in identification), reasoning-heavy models (Claude 4.5) achieve state-of-the-art performance (54%) only when prompted with Chain-of-Thought. Most critically, pure LLM generation fails catastrophically (under 4% valid poems), while our hybrid verification loop restores performance to 73.1%. We release our system and a crucial, rigorously cleaned corpus of 40,000+ rhymes, derived from the Anemoskala and Interwar Poetry corpora, to support future research.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.09631v1">üìÑ Download PDF</a></p><hr><h3 id=evolving-with-ai-a-longitudinal-analysis-of-developer-logshttpsarxivorgabs260110258v1><a href=https://arxiv.org/abs/2601.10258v1>Evolving with AI: A Longitudinal Analysis of Developer Logs</a><a hidden class=anchor aria-hidden=true href=#evolving-with-ai-a-longitudinal-analysis-of-developer-logshttpsarxivorgabs260110258v1>#</a></h3><p><strong>Authors:</strong> Agnia Sergeyuk, Eric Huang, Dariia Karaeva, Anastasiia Serova, Yaroslav Golubev, Iftekhar Ahmed
<strong>Venue:</strong> arXiv (2026)</p><p>AI-powered coding assistants are rapidly becoming fixtures in professional IDEs, yet their sustained influence on everyday development remains poorly understood. Prior research has focused on short-term use or self-reported perceptions, leaving open questions about how sustained AI use reshapes actual daily coding practices in the long term. We address this gap with a mixed-method study of AI adoption in IDEs, combining longitudinal two-year fine-grained telemetry from 800 developers with a survey of 62 professionals. We analyze five dimensions of workflow change: productivity, code quality, code editing, code reuse, and context switching. Telemetry reveals that AI users produce substantially more code but also delete significantly more. Meanwhile, survey respondents report productivity gains and perceive minimal changes in other dimensions. Our results offer empirical insights into the silent restructuring of software workflows and provide implications for designing future AI-augmented tooling.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10258v1">üìÑ Download PDF</a></p><hr><h3 id=geometry--and-topology-informed-quantum-computing-from-states-to-real-time-control-with-fpga-prototypeshttpsarxivorgabs260109556v1><a href=https://arxiv.org/abs/2601.09556v1>Geometry- and Topology-Informed Quantum Computing: From States to Real-Time Control with FPGA Prototypes</a><a hidden class=anchor aria-hidden=true href=#geometry--and-topology-informed-quantum-computing-from-states-to-real-time-control-with-fpga-prototypeshttpsarxivorgabs260109556v1>#</a></h3><p><strong>Authors:</strong> Gunhee Cho
<strong>Venue:</strong> arXiv (2026)</p><p>This book gives a geometry-first, hardware-aware route through quantum-information workflows, with one goal: connect states, circuits, and measurement to deterministic classical pipelines that make hybrid quantum systems run. Part 1 develops the backbone (essential linear algebra, the Bloch-sphere viewpoint, differential-geometric intuition, and quantum Fisher information geometry) so evolution can be read as motion on curved spaces and measurement as statistics. Part 2 reframes circuits as dataflow graphs: measurement outcomes are parsed, aggregated, and reduced to small linear-algebra updates that schedule the next pulses, highlighting why low-latency, low-jitter streaming matters. Part 3 treats multi-qubit structure and entanglement as geometry and computation, including teleportation, superdense coding, entanglement detection, and Shor&rsquo;s algorithm via quantum phase estimation. Part 4 focuses on topological error correction and real-time decoding (Track A): stabilizer codes, surface-code decoding as &ldquo;topology -> graph -> algorithm&rdquo;, and Union-Find decoders down to microarchitectural/RTL constraints, with verification, fault injection, and host/control-stack integration under product metrics (bounded latency, p99 tails, fail-closed policies, observability). Optional Track C covers quantum cryptography and streaming post-processing (BB84/E91, QBER/abort rules, privacy amplification, and zero-knowledge/post-quantum themes), emphasizing FSMs, counters, and hash pipelines. Appendices provide visualization-driven iCEstick labs (switch-to-bit conditioning, fixed-point phase arithmetic, FSM sequencing, minimal control ISAs), bridging principles to implementable systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.09556v1">üìÑ Download PDF</a></p><hr><h3 id=a-free-fall-based-switching-criterion-for-p3-t-n-body-methods-in-collisional-stellar-systemshttpsarxivorgabs260107425v1><a href=https://arxiv.org/abs/2601.07425v1>A free-fall-based switching criterion for P^3 T N-body methods in collisional stellar systems</a><a hidden class=anchor aria-hidden=true href=#a-free-fall-based-switching-criterion-for-p3-t-n-body-methods-in-collisional-stellar-systemshttpsarxivorgabs260107425v1>#</a></h3><p><strong>Authors:</strong> Long Wang, David M. Hernandez, Zepeng Zheng, Wanhao Huang
<strong>Venue:</strong> arXiv (2026)</p><p>The P$^3$T scheme is a hybrid method for simulating gravitational $N$-body systems. It combines a fast particle-tree (PT) algorithm for long-range forces with a high-accuracy particle-particle (PP, direct $N$-body) solver for short-range interactions. Preserving both PT efficiency and PP accuracy requires a robust PT-PP switching criterion. We introduce a simple free-fall-based switching criterion for general stellar systems, alongside the commonly used velocity-dispersion-based ($œÉ$-based) criterion. Using the \textsc{petar} code with the P$^3$T scheme and slow-down algorithmic regularization for binaries and higher-order multiples, we perform extensive simulations of star clusters to evaluate how each criterion affects energy conservation and binary evolution. For systems in virial equilibrium, we find that the free-fall-based criterion is generally more accurate for low-$œÉ$ or loose clusters containing binaries, whereas the $œÉ$-based criterion is better suited for high-$œÉ$ systems. Under subvirial or fractal initial conditions, both criteria struggle to maintain high energy conservation; however, the free-fall-based criterion improves as the tree timestep is reduced, whereas the $œÉ$-based degrades due to its low-accuracy treatment of two-body encounters.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.07425v1">üìÑ Download PDF</a></p><hr><h3 id=learning-dynamic-collaborative-network-for-semi-supervised-3d-vessel-segmentationhttpsarxivorgabs260107377v1><a href=https://arxiv.org/abs/2601.07377v1>Learning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation</a><a hidden class=anchor aria-hidden=true href=#learning-dynamic-collaborative-network-for-semi-supervised-3d-vessel-segmentationhttpsarxivorgabs260107377v1>#</a></h3><p><strong>Authors:</strong> Jiao Xu, Xin Chen, Lihe Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>In this paper, we present a new dynamic collaborative network for semi-supervised 3D vessel segmentation, termed DiCo. Conventional mean teacher (MT) methods typically employ a static approach, where the roles of the teacher and student models are fixed. However, due to the complexity of 3D vessel data, the teacher model may not always outperform the student model, leading to cognitive biases that can limit performance. To address this issue, we propose a dynamic collaborative network that allows the two models to dynamically switch their teacher-student roles. Additionally, we introduce a multi-view integration module to capture various perspectives of the inputs, mirroring the way doctors conduct medical analysis. We also incorporate adversarial supervision to constrain the shape of the segmented vessels in unlabeled data. In this process, the 3D volume is projected into 2D views to mitigate the impact of label inconsistencies. Experiments demonstrate that our DiCo method sets new state-of-the-art performance on three 3D vessel segmentation benchmarks. The code repository address is <a href=https://github.com/xujiaommcome/DiCo>https://github.com/xujiaommcome/DiCo</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.07377v1">üìÑ Download PDF</a></p><hr><h3 id=can-large-language-models-understand-reason-about-and-generate-code-switched-texthttpsarxivorgabs260107153v1><a href=https://arxiv.org/abs/2601.07153v1>Can Large Language Models Understand, Reason About, and Generate Code-Switched Text?</a><a hidden class=anchor aria-hidden=true href=#can-large-language-models-understand-reason-about-and-generate-code-switched-texthttpsarxivorgabs260107153v1>#</a></h3><p><strong>Authors:</strong> Genta Indra Winata, David Anugraha, Patrick Amadeus Irawan, Anirban Das, Haneul Yoo, Paresh Dashore, Shreyas Kulkarni, Ruochen Zhang, Haruki Sakajo, Frederikus Hudi, Anaelia Ovalle, Syrielle Montariol, Felix Gaschi, Michael Anugraha, Rutuj Ravindra Puranik, Zawad Hayat Ahmed, Adril Putra Merin, Emmanuele Chersoni
<strong>Venue:</strong> arXiv (2026)</p><p>Code-switching is a pervasive phenomenon in multilingual communication, yet the robustness of large language models (LLMs) in mixed-language settings remains insufficiently understood. In this work, we present a comprehensive evaluation of LLM capabilities in understanding, reasoning over, and generating code-switched text. We introduce CodeMixQA a novel benchmark with high-quality human annotations, comprising 16 diverse parallel code-switched language-pair variants that span multiple geographic regions and code-switching patterns, and include both original scripts and their transliterated forms. Using this benchmark, we analyze the reasoning behavior of LLMs on code-switched question-answering tasks, shedding light on how models process and reason over mixed-language inputs. We further conduct a systematic evaluation of LLM-generated synthetic code-switched text, focusing on both naturalness and semantic fidelity, and uncover key limitations in current generation capabilities. Our findings reveal persistent challenges in both reasoning and generation under code-switching conditions and provide actionable insights for building more robust multilingual LLMs. We release the dataset and code as open source.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.07153v1">üìÑ Download PDF</a></p><hr><h3 id=transforming-crises-into-opportunities-from-chaos-to-urban-antifragilityhttpsarxivorgabs260110658v1><a href=https://arxiv.org/abs/2601.10658v1>Transforming Crises into Opportunities: From Chaos to Urban Antifragility</a><a hidden class=anchor aria-hidden=true href=#transforming-crises-into-opportunities-from-chaos-to-urban-antifragilityhttpsarxivorgabs260110658v1>#</a></h3><p><strong>Authors:</strong> Joseph Uguet, Nicola Tollin, Jordi Morato
<strong>Venue:</strong> arXiv (2026)</p><p>Urban crises - floods, pandemics, economic shocks, and conflicts - function as accelerators of urban change, exposing structural vulnerabilities while creating windows for reinvention. Building on a prior theoretical contribution that identified fifteen principles of urban antifragility, this paper tests and operationalizes the framework through an empirical assessment of 26 cities selected for their post-crisis adaptation trajectories. Using a tailored diagnostic methodology, we benchmark cities&rsquo; Stress Response Strategies (SRS) and then evaluate Urban Development Trajectories (UDT) across four weighted dimensions, positioning each case along a fragility-robustness-resilience-antifragility continuum and applying a balanced-threshold rule to confirm antifragile status. Results show that &ldquo;resilience enhanced by innovation and technology&rdquo; is the most effective response typology (86.9/100), and that six cities meet the antifragile trajectory criteria. By mapping best practices to activated principles and analysing co-activations, the study identifies a robust &ldquo;hard core&rdquo; of principles - Sustainable Resilience (O), Strategic Diversity (F), Proactive Innovation (I), and Active Prevention (N) - supplemented by operational enablers (e.g., anticipation, mobilization, shock absorption). The paper concludes by proposing an evidence-based, SDG-aligned operational model that links high-impact principle pairings to measurable indicators, offering a practical roadmap for cities seeking to convert crises into sustained transformation. Keywords: Post-crisis strategies, Urban antifragility, Sustainable cities and communities, Disaster resilience and urban regeneration, Risk governance and Black Swan adaptation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10658v1">üìÑ Download PDF</a></p><hr><h3 id=alterbute-editing-intrinsic-attributes-of-objects-in-imageshttpsarxivorgabs260110714v1><a href=https://arxiv.org/abs/2601.10714v1>Alterbute: Editing Intrinsic Attributes of Objects in Images</a><a hidden class=anchor aria-hidden=true href=#alterbute-editing-intrinsic-attributes-of-objects-in-imageshttpsarxivorgabs260110714v1>#</a></h3><p><strong>Authors:</strong> Tal Reiss, Daniel Winter, Matan Cohen, Alex Rav-Acha, Yael Pritch, Ariel Shamir, Yedid Hoshen
<strong>Venue:</strong> arXiv (2026)</p><p>We introduce Alterbute, a diffusion-based method for editing an object&rsquo;s intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., &lsquo;&lsquo;Porsche 911 Carrera&rsquo;&rsquo;) that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10714v1">üìÑ Download PDF</a></p><hr><h3 id=matchtir-fine-grained-supervision-for-tool-integrated-reasoning-via-bipartite-matchinghttpsarxivorgabs260110712v1><a href=https://arxiv.org/abs/2601.10712v1>MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching</a><a hidden class=anchor aria-hidden=true href=#matchtir-fine-grained-supervision-for-tool-integrated-reasoning-via-bipartite-matchinghttpsarxivorgabs260110712v1>#</a></h3><p><strong>Authors:</strong> Changle Qu, Sunhao Dai, Hengyi Cai, Jun Xu, Shuaiqiang Wang, Dawei Yin
<strong>Venue:</strong> arXiv (2026)</p><p>Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at <a href=https://github.com/quchangle1/MatchTIR>https://github.com/quchangle1/MatchTIR</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10712v1">üìÑ Download PDF</a></p><hr><h3 id=from-one-to-one-to-many-to-many-dynamic-cross-layer-injection-for-deep-vision-language-fusionhttpsarxivorgabs260110710v1><a href=https://arxiv.org/abs/2601.10710v1>From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion</a><a hidden class=anchor aria-hidden=true href=#from-one-to-one-to-many-to-many-dynamic-cross-layer-injection-for-deep-vision-language-fusionhttpsarxivorgabs260110710v1>#</a></h3><p><strong>Authors:</strong> Cheng Chen, Yuyu Guo, Pengpeng Zeng, Jingkuan Song, Peng Di, Hang Yu, Lianli Gao
<strong>Venue:</strong> arXiv (2026)</p><p>Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10710v1">üìÑ Download PDF</a></p><hr><h3 id=grounding-agent-memory-in-contextual-intenthttpsarxivorgabs260110702v1><a href=https://arxiv.org/abs/2601.10702v1>Grounding Agent Memory in Contextual Intent</a><a hidden class=anchor aria-hidden=true href=#grounding-agent-memory-in-contextual-intenthttpsarxivorgabs260110702v1>#</a></h3><p><strong>Authors:</strong> Ruozhen Yang, Yucheng Jiang, Yueqi Jiang, Priyanka Kargupta, Yunyi Zhang, Jiawei Han
<strong>Venue:</strong> arXiv (2026)</p><p>Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step&rsquo;s intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history.
For evaluation, we introduce CAME-Bench, a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10702v1">üìÑ Download PDF</a></p><hr><h3 id=liberty-a-causal-framework-for-benchmarking-concept-based-explanations-of-llms-with-structural-counterfactualshttpsarxivorgabs260110700v1><a href=https://arxiv.org/abs/2601.10700v1>LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals</a><a hidden class=anchor aria-hidden=true href=#liberty-a-causal-framework-for-benchmarking-concept-based-explanations-of-llms-with-structural-counterfactualshttpsarxivorgabs260110700v1>#</a></h3><p><strong>Authors:</strong> Gilat Toker, Nitay Calderon, Ohad Amosy, Roi Reichart
<strong>Venue:</strong> arXiv (2026)</p><p>Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10700v1">üìÑ Download PDF</a></p><hr><h3 id=on-the-origin-of-neural-scaling-laws-from-random-graphs-to-natural-languagehttpsarxivorgabs260110684v1><a href=https://arxiv.org/abs/2601.10684v1>On the origin of neural scaling laws: from random graphs to natural language</a><a hidden class=anchor aria-hidden=true href=#on-the-origin-of-neural-scaling-laws-from-random-graphs-to-natural-languagehttpsarxivorgabs260110684v1>#</a></h3><p><strong>Authors:</strong> Maissam Barkeshli, Alberto Alfarano, Andrey Gromov
<strong>Venue:</strong> arXiv (2026)</p><p>Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scaling laws even in the absence of power law structure in the data correlations. We further consider dialing down the complexity of natural language systematically, by training on sequences sampled from increasingly simplified generative language models, from 4,2,1-layer transformer language models down to language bigrams, revealing a monotonic evolution of the scaling exponents. Our results also include scaling laws obtained from training on random walks on random graphs drawn from Erd√∂s-Renyi and scale-free Barab√°si-Albert ensembles. Finally, we revisit conventional scaling laws for language modeling, demonstrating that several essential results can be reproduced using 2 layer transformers with context length of 50, provide a critical analysis of various fits used in prior literature, demonstrate an alternative method for obtaining compute optimal curves as compared with current practice in published literature, and provide preliminary evidence that maximal update parameterization may be more parameter efficient than standard parameterization.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10684v1">üìÑ Download PDF</a></p><hr><h3 id=see-less-drive-better-generalizable-end-to-end-autonomous-driving-via-foundation-models-stochastic-patch-selectionhttpsarxivorgabs260110707v1><a href=https://arxiv.org/abs/2601.10707v1>See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection</a><a hidden class=anchor aria-hidden=true href=#see-less-drive-better-generalizable-end-to-end-autonomous-driving-via-foundation-models-stochastic-patch-selectionhttpsarxivorgabs260110707v1>#</a></h3><p><strong>Authors:</strong> Amir Mallak, Erfan Aasi, Shiva Sreeram, Tsun-Hsuan Wang, Daniela Rus, Alaa Maalouf
<strong>Venue:</strong> arXiv (2026)</p><p>Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$% of variance is captured by $17/64$ principal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness. We present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient. For every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches. Thus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive. Extensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a $6.2$% average improvement and up to $20.4$% in closed-loop simulations, while being $2.4\times$ faster. We conduct ablations over masking rates and patch-feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA. Finally, we show that the same learned policy transfers to a physical, real-world car without any tuning.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10707v1">üìÑ Download PDF</a></p><hr><h3 id=implementation-of-oblivious-transfer-over-binary-input-awgn-channels-by-polar-codeshttpsarxivorgabs260110682v1><a href=https://arxiv.org/abs/2601.10682v1>Implementation of Oblivious Transfer over Binary-Input AWGN Channels by Polar Codes</a><a hidden class=anchor aria-hidden=true href=#implementation-of-oblivious-transfer-over-binary-input-awgn-channels-by-polar-codeshttpsarxivorgabs260110682v1>#</a></h3><p><strong>Authors:</strong> Pin-Hsun Lin, Hadi Aghaee, Christian Deppe, Eduard A. Jorswieck, Holger Boche
<strong>Venue:</strong> arXiv (2026)</p><p>We develop a one-out-of-two-oblivious transfer protocol over the binary-input additive white Gaussian noise channel using polar codes. The scheme uses two decoder views linked by automorphisms of the polar transform and publicly draws the encoder at random from the corresponding automorphism group. This yields perfect receiver privacy at any finite blocklength, since the public encoder distribution is independent of the receiver&rsquo;s choice bit. Sender privacy is obtained asymptotically via channel polarization combined with privacy amplification. Because the construction deliberately injects randomness on selected bad bit-channels, we derive a relaxed reliability criterion and evaluate finite-blocklength performance. Finally, we characterize the polar-transform automorphisms as bit-level permutations of bit-channel indices, and exploit this structure to derive and optimize an achievable finite-blocklength OT rate.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10682v1">üìÑ Download PDF</a></p><hr><h3 id=molecularly-thin-polyaramid-nanomechanical-resonatorshttpsarxivorgabs260110633v1><a href=https://arxiv.org/abs/2601.10633v1>Molecularly Thin Polyaramid Nanomechanical Resonators</a><a hidden class=anchor aria-hidden=true href=#molecularly-thin-polyaramid-nanomechanical-resonatorshttpsarxivorgabs260110633v1>#</a></h3><p><strong>Authors:</strong> Hagen Gress, Cody L. Ritt, Inal Shomakhov, Kaan Altmisdort, Michelle Quien, Zitang Wei, John R. Lawall, Narasimha Boddeti, Michael S. Strano, J. Scott Bunch, Kamil L. Ekinci
<strong>Venue:</strong> arXiv (2026)</p><p>Two-dimensional polyaramids exhibit strong hydrogen bonding to create molecularly thin nanosheets analogous to graphene. Here, we report the first nanomechanical resonators made out of a two-dimensional polyaramid, 2DPA-1, with thicknesses as small as 8 nm. To fabricate these molecular-scale resonators, we transferred nanofilms of 2DPA-1 onto chips with previously etched arrays of circular microwells. We then characterized the thermal resonances of these resonators under different conditions. When there is no residual gas inside the 2DPA-1-covered microwells, the eigenfrequencies are well-described by a tensioned plate theory, providing the Young&rsquo;s modulus and tension of the 2DPA-1 nanofilms. With gas present, the nanofilms bulge up and mechanical resonances are modified due to the adhesion, bulging and slack present in the system. The fabrication and mechanical characterization of these first 2DPA-1 nanomechanical resonators represent a convincing path toward molecular-scale polymeric NEMS with high mechanical strength, low density, and synthetic processability.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10633v1">üìÑ Download PDF</a></p><hr><h3 id=classification-imbalance-as-transfer-learninghttpsarxivorgabs260110630v1><a href=https://arxiv.org/abs/2601.10630v1>Classification Imbalance as Transfer Learning</a><a hidden class=anchor aria-hidden=true href=#classification-imbalance-as-transfer-learninghttpsarxivorgabs260110630v1>#</a></h3><p><strong>Authors:</strong> Eric Xia, Jason M. Klusowski
<strong>Venue:</strong> arXiv (2026)</p><p>Classification imbalance arises when one class is much rarer than the other. We frame this setting as transfer learning under label (prior) shift between an imbalanced source distribution induced by the observed data and a balanced target distribution under which performance is evaluated. Within this framework, we study a family of oversampling procedures that augment the training data by generating synthetic samples from an estimated minority-class distribution to roughly balance the classes, among which the celebrated SMOTE algorithm is a canonical example. We show that the excess risk decomposes into the rate achievable under balanced training (as if the data had been drawn from the balanced target distribution) and an additional term, the cost of transfer, which quantifies the discrepancy between the estimated and true minority-class distributions. In particular, we show that the cost of transfer for SMOTE dominates that of bootstrapping (random oversampling) in moderately high dimensions, suggesting that we should expect bootstrapping to have better performance than SMOTE in general. We corroborate these findings with experimental evidence. More broadly, our results provide guidance for choosing among augmentation strategies for imbalanced classification.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10630v1">üìÑ Download PDF</a></p><hr><h3 id=voicesculptor-your-voice-designed-by-youhttpsarxivorgabs260110629v1><a href=https://arxiv.org/abs/2601.10629v1>VoiceSculptor: Your Voice, Designed By You</a><a hidden class=anchor aria-hidden=true href=#voicesculptor-your-voice-designed-by-youhttpsarxivorgabs260110629v1>#</a></h3><p><strong>Authors:</strong> Jingbin Hu, Huakang Chen, Linhan Ma, Dake Guo, Qirui Zhan, Wenhao Li, Haoyu Zhang, Kangxiang Xia, Ziyu Zhang, Wenjie Tian, Chengyou Wang, Jinrui Liang, Shuhan Guo, Zihang Yang, Bengu Wu, Binbin Zhang, Pengcheng Zhu, Pengyuan Xie, Chuan Xie, Qiang Zhang, Jie Liu, Lei Xie
<strong>Venue:</strong> arXiv (2026)</p><p>Despite rapid progress in text-to-speech (TTS), open-source systems still lack truly instruction-following, fine-grained control over core speech attributes (e.g., pitch, speaking rate, age, emotion, and style). We present VoiceSculptor, an open-source unified system that bridges this gap by integrating instruction-based voice design and high-fidelity voice cloning in a single framework. It generates controllable speaker timbre directly from natural-language descriptions, supports iterative refinement via Retrieval-Augmented Generation (RAG), and provides attribute-level edits across multiple dimensions. The designed voice is then rendered into a prompt waveform and fed into a cloning model to enable high-fidelity timbre transfer for downstream speech synthesis. VoiceSculptor achieves open-source state-of-the-art (SOTA) on InstructTTSEval-Zh, and is fully open-sourced, including code and pretrained models, to advance reproducible instruction-controlled TTS research.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10629v1">üìÑ Download PDF</a></p><hr><h3 id=probfm-probabilistic-time-series-foundation-model-with-uncertainty-decompositionhttpsarxivorgabs260110591v1><a href=https://arxiv.org/abs/2601.10591v1>ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition</a><a hidden class=anchor aria-hidden=true href=#probfm-probabilistic-time-series-foundation-model-with-uncertainty-decompositionhttpsarxivorgabs260110591v1>#</a></h3><p><strong>Authors:</strong> Arundeep Chinta, Lucas Vinh Tran, Jay Katukuri
<strong>Venue:</strong> arXiv (2026)</p><p>Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student&rsquo;s t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student&rsquo;s-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER&rsquo;s effectiveness in financial applications.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10591v1">üìÑ Download PDF</a></p><hr><h3 id=influential-training-data-retrieval-for-explaining-verbalized-confidence-of-llmshttpsarxivorgabs260110645v1><a href=https://arxiv.org/abs/2601.10645v1>Influential Training Data Retrieval for Explaining Verbalized Confidence of LLMs</a><a hidden class=anchor aria-hidden=true href=#influential-training-data-retrieval-for-explaining-verbalized-confidence-of-llmshttpsarxivorgabs260110645v1>#</a></h3><p><strong>Authors:</strong> Yuxi Xia, Loris Schoenegger, Benjamin Roth
<strong>Venue:</strong> arXiv (2026)</p><p>Large language models (LLMs) can increase users&rsquo; perceived trust by verbalizing confidence in their outputs. However, prior work has shown that LLMs are often overconfident, making their stated confidence unreliable since it does not consistently align with factual accuracy. To better understand the sources of this verbalized confidence, we introduce TracVC (\textbf{Trac}ing \textbf{V}erbalized \textbf{C}onfidence), a method that builds on information retrieval and influence estimation to trace generated confidence expressions back to the training data. We evaluate TracVC on OLMo and Llama models in a question answering setting, proposing a new metric, content groundness, which measures the extent to which an LLM grounds its confidence in content-related training examples (relevant to the question and answer) versus in generic examples of confidence verbalization. Our analysis reveals that OLMo2-13B is frequently influenced by confidence-related data that is lexically unrelated to the query, suggesting that it may mimic superficial linguistic expressions of certainty rather than rely on genuine content grounding. These findings point to a fundamental limitation in current training regimes: LLMs may learn how to sound confident without learning when confidence is justified. Our analysis provides a foundation for improving LLMs&rsquo; trustworthiness in expressing more reliable confidence.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10645v1">üìÑ Download PDF</a></p><hr><h3 id=institutional-ai-a-governance-framework-for-distributional-agi-safetyhttpsarxivorgabs260110599v1><a href=https://arxiv.org/abs/2601.10599v1>Institutional AI: A Governance Framework for Distributional AGI Safety</a><a hidden class=anchor aria-hidden=true href=#institutional-ai-a-governance-framework-for-distributional-agi-safetyhttpsarxivorgabs260110599v1>#</a></h3><p><strong>Authors:</strong> Federico Pierucci, Marcello Galisai, Marcantonio Syrnikov Bracale, Matteo Prandi, Piercosma Bisconti, Francesco Giarrusso, Olga Sorokoletova, Vincenzo Suriani, Daniele Nardi
<strong>Venue:</strong> arXiv (2026)</p><p>As LLM-based systems increasingly operate as agents embedded within human social and technical systems, alignment can no longer be treated as a property of an isolated model, but must be understood in relation to the environments in which these agents act. Even the most sophisticated methods of alignment, such as Reinforcement Learning through Human Feedback (RHLF) or through AI Feedback (RLAIF) cannot ensure control once internal goal structures diverge from developer intent. We identify three structural problems that emerge from core properties of AI models: (1) behavioral goal-independence, where models develop internal objectives and misgeneralize goals; (2) instrumental override of natural-language constraints, where models regard safety principles as non-binding while pursuing latent objectives, leveraging deception and manipulation; and (3) agentic alignment drift, where individually aligned agents converge to collusive equilibria through interaction dynamics invisible to single-agent audits. The solution this paper advances is Institutional AI: a system-level approach that treats alignment as a question of effective governance of AI agent collectives. We argue for a governance-graph that details how to constrain agents via runtime monitoring, incentive shaping through prizes and sanctions, explicit norms and enforcement roles. This institutional turn reframes safety from software engineering to a mechanism design problem, where the primary goal of alignment is shifting the payoff landscape of AI agent collectives.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10599v1">üìÑ Download PDF</a></p><hr><h3 id=structure-and-diversity-aware-context-bubble-construction-for-enterprise-retrieval-augmented-systemshttpsarxivorgabs260110681v1><a href=https://arxiv.org/abs/2601.10681v1>Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems</a><a hidden class=anchor aria-hidden=true href=#structure-and-diversity-aware-context-bubble-construction-for-enterprise-retrieval-augmented-systemshttpsarxivorgabs260110681v1>#</a></h3><p><strong>Authors:</strong> Amir Khurshid, Abhishek Sehgal
<strong>Venue:</strong> arXiv (2026)</p><p>Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10681v1">üìÑ Download PDF</a></p><hr><h3 id=routir-fast-serving-of-retrieval-pipelines-for-retrieval-augmented-generationhttpsarxivorgabs260110644v1><a href=https://arxiv.org/abs/2601.10644v1>RoutIR: Fast Serving of Retrieval Pipelines for Retrieval-Augmented Generation</a><a hidden class=anchor aria-hidden=true href=#routir-fast-serving-of-retrieval-pipelines-for-retrieval-augmented-generationhttpsarxivorgabs260110644v1>#</a></h3><p><strong>Authors:</strong> Eugene Yang, Andrew Yates, Dawn Lawrie, James Mayfield, Trevor Adriaanse
<strong>Venue:</strong> arXiv (2026)</p><p>Retrieval models are key components of Retrieval-Augmented Generation (RAG) systems, which generate search queries, process the documents returned, and generate a response. RAG systems are often dynamic and may involve multiple rounds of retrieval. While many state-of-the-art retrieval methods are available through academic IR platforms, these platforms are typically designed for the Cranfield paradigm in which all queries are known up front and can be batch processed offline. This simplification accelerates research but leaves state-of-the-art retrieval models unable to support downstream applications that require online services, such as arbitrary dynamic RAG pipelines that involve looping, feedback, or even self-organizing agents. In this work, we introduce RoutIR, a Python package that provides a simple and efficient HTTP API that wraps arbitrary retrieval methods, including first stage retrieval, reranking, query expansion, and result fusion. By providing a minimal JSON configuration file specifying the retrieval models to serve, RoutIR can be used to construct and query retrieval pipelines on-the-fly using any permutation of available models (e.g., fusing the results of several first-stage retrieval methods followed by reranking). The API automatically performs asynchronous query batching and caches results by default. While many state-of-the-art retrieval methods are already supported by the package, RoutIR is also easily expandable by implementing the Engine abstract class. The package is open-sourced and publicly available on GitHub: <a href=http://github.com/hltcoe/routir>http://github.com/hltcoe/routir</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10644v1">üìÑ Download PDF</a></p><hr><h3 id=converse-bounds-for-sun-jafar-type-weak-private-information-retrievalhttpsarxivorgabs260110643v1><a href=https://arxiv.org/abs/2601.10643v1>Converse Bounds for Sun-Jafar-type Weak Private Information Retrieval</a><a hidden class=anchor aria-hidden=true href=#converse-bounds-for-sun-jafar-type-weak-private-information-retrievalhttpsarxivorgabs260110643v1>#</a></h3><p><strong>Authors:</strong> Chandan Anand, Jayesh Seshadri, Prasad Krishnan, Gowtham R. Kurri
<strong>Venue:</strong> arXiv (2026)</p><p>Building on the well-established capacity-achieving schemes of Sun-Jafar (for replicated storage) and the closely related scheme of Banawan-Ulukus (for MDS-coded setting), a recent work by Chandan et al. proposed new classes of weak private information retrieval (WPIR) schemes for the collusion-free (replication and MDS-coded) setting, as well as for the $T$-colluding scenario. In their work, Chandan et al. characterized the expressions for the rate-privacy trade-offs for these classes of WPIR schemes, under the mutual information leakage and maximal leakage metrics. Explicit achievable trade-offs for the same were also presented, which were shown to be competitive or better than prior WPIR schemes. However, the class-wise optimality of the reported trade-offs were unknown. In this work, we show that the explicit rate-privacy trade-offs reported for the Sun-Jafar-type schemes by Chandan et al. are optimal for the non-colluding and replicated setting. Furthermore, we prove the class-wise optimality for Banawan-Ulukus-type MDS-WPIR and Sun-Jafar-type $T$-colluding WPIR schemes, under threshold-constraints on the system parameters. When these threshold-constraints do not hold, we present counter-examples which show that even higher rates than those reported before can be achieved.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10643v1">üìÑ Download PDF</a></p><hr><h2 id=-linguistics>üîç linguistics<a hidden class=anchor aria-hidden=true href=#-linguistics>#</a></h2><h3 id=emergence-and-transition-of-incompressible-phases-in-decorated-landau-levelshttpsarxivorgabs260110717v1><a href=https://arxiv.org/abs/2601.10717v1>Emergence and transition of incompressible phases in decorated Landau levels</a><a hidden class=anchor aria-hidden=true href=#emergence-and-transition-of-incompressible-phases-in-decorated-landau-levelshttpsarxivorgabs260110717v1>#</a></h3><p><strong>Authors:</strong> Bo Peng, Yuzhu Wang, Bo Yang
<strong>Venue:</strong> arXiv (2026)</p><p>We show a single Landau level (LL) dressed with periodic electrostatic potentials can realize a plethora of interacting topological phases where the Hall conductivity generally does not equal to the LL filling factor. Their physics can be captured by a minimal model of a delta potential lattice within a single LL, realizing exact zero energy Chern bands (denoted as decorated Landau levels or dLL) gapped from dispersive bands with rich geometric properties. With $p/q$ magnetic fluxes per unit cell, there are $q$ dispersive bands and $p-q$ zero energy bands forming the dLL. When the one-body potential strength dominates the electron-electron interaction, band mixing is suppressed and the dispersion bands consist of ``localized states" with vanishing total Chern number. Nevertheless these dispersive bands can have highly nontrivial Berry curvature distribution, and even non-zero Chern numbers when $q>1$. Interestingly even in the limit of large short range interaction, band mixing between dLL and dispersion bands can be strongly suppressed at low filling factor, leading to robust topological phases within the dLL stabilized by the one-body potential. The dLL and the associated dispersive bands can serve as minimal theoretical models for correlated physics in lattice or moire systems; they are also highly tunable experimental platforms for realizing rich phase diagrams of exotic 2D quantum fluids.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10717v1">üìÑ Download PDF</a></p><hr><h3 id=madelung-hydrodynamics-of-spin-orbit-coupling-action-principles-currents-and-correlationshttpsarxivorgabs260110698v1><a href=https://arxiv.org/abs/2601.10698v1>Madelung hydrodynamics of spin-orbit coupling: action principles, currents, and correlations</a><a hidden class=anchor aria-hidden=true href=#madelung-hydrodynamics-of-spin-orbit-coupling-action-principles-currents-and-correlationshttpsarxivorgabs260110698v1>#</a></h3><p><strong>Authors:</strong> Cesare Tronci
<strong>Venue:</strong> arXiv (2026)</p><p>We exploit the variational and Hamiltonian structures of quantum hydrodynamics with spin to unfold the correlation and torque mechanisms accompanying spin-orbit coupling (SOC) in electronic motion. Using Hamilton&rsquo;s action principle for the Pauli equation, we isolate SOC-induced quantum forces that act on the orbital Madelung&ndash;Bohm trajectories and complement the usual force terms known to appear in quantum hydrodynamics with spin. While the latter spin-hydrodynamic forces relate to the quantum geometric tensor (QGT), SOC-induced orbital forces originate from a particular current operator that contributes prominently to the spin current and whose contribution was overlooked in the past. The distinction between different force terms reveals two fundamentally different mechanisms generating quantum spin-orbit correlations. Leveraging the Hamiltonian structure of the hydrodynamic system, we also elucidate spin transport features such as the current shift in the spin Hall effect and the correlation-induced quantum torques. Finally, we illustrate the framework via the Madelung&ndash;Rashba equations for planar SOC configurations and propose a particle-based scheme for numerical implementation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10698v1">üìÑ Download PDF</a></p><hr><h3 id=perfect-secret-key-generation-for-a-class-of-hypergraphical-sourceshttpsarxivorgabs260110697v1><a href=https://arxiv.org/abs/2601.10697v1>Perfect Secret Key Generation for a class of Hypergraphical Sources</a><a hidden class=anchor aria-hidden=true href=#perfect-secret-key-generation-for-a-class-of-hypergraphical-sourceshttpsarxivorgabs260110697v1>#</a></h3><p><strong>Authors:</strong> Manuj Mukherjee, Sagnik Chatterjee, Alhad Sethi
<strong>Venue:</strong> arXiv (2026)</p><p>Nitinawarat and Narayan proposed a perfect secret key generation scheme for the so-called \emph{pairwise independent network (PIN) model} by exploiting the combinatorial properties of the underlying graph, namely the spanning tree packing rate. This work considers a generalization of the PIN model where the underlying graph is replaced with a hypergraph, and makes progress towards designing similar perfect secret key generation schemes by exploiting the combinatorial properties of the hypergraph.
Our contributions are two-fold. We first provide a capacity achieving scheme for a complete $t$-uniform hypergraph on $m$ vertices by leveraging a packing of the complete $t$-uniform hypergraphs by what we refer to as star hypergraphs, and designing a scheme that gives $\binom{m-2}{t-2}$ bits of perfect secret key per star graph. Our second contribution is a 2-bit perfect secret key generation scheme for 3-uniform star hypergraphs whose projections are cycles. This scheme is then extended to a perfect secret key generation scheme for generic 3-uniform hypergraphs by exploiting star graph packing of 3-uniform hypergraphs and Hamiltonian packings of graphs. The scheme is then shown to be capacity achieving for certain classes of hypergraphs.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10697v1">üìÑ Download PDF</a></p><hr><h3 id=the-impact-of-generative-ai-on-architectural-conceptual-design-performance-creative-self-efficacy-and-cognitive-loadhttpsarxivorgabs260110696v1><a href=https://arxiv.org/abs/2601.10696v1>The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load</a><a hidden class=anchor aria-hidden=true href=#the-impact-of-generative-ai-on-architectural-conceptual-design-performance-creative-self-efficacy-and-cognitive-loadhttpsarxivorgabs260110696v1>#</a></h3><p><strong>Authors:</strong> Han Jiang, Yao Xiao, Rachel Hurley, Shichao Liu
<strong>Venue:</strong> arXiv (2026)</p><p>Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users&rsquo; prior expertise and interaction strategies through prompting.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10696v1">üìÑ Download PDF</a></p><hr><h3 id=wildrayzer-self-supervised-large-view-synthesis-in-dynamic-environmentshttpsarxivorgabs260110716v1><a href=https://arxiv.org/abs/2601.10716v1>WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments</a><a hidden class=anchor aria-hidden=true href=#wildrayzer-self-supervised-large-view-synthesis-in-dynamic-environmentshttpsarxivorgabs260110716v1>#</a></h3><p><strong>Authors:</strong> Xuweiyi Chen, Wentao Zhou, Zezhou Cheng
<strong>Venue:</strong> arXiv (2026)</p><p>We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10716v1">üìÑ Download PDF</a></p><hr><h3 id=dinf-grid-a-neural-differential-equation-solver-with-differentiable-feature-gridshttpsarxivorgabs260110715v1><a href=https://arxiv.org/abs/2601.10715v1>DInf-Grid: A Neural Differential Equation Solver with Differentiable Feature Grids</a><a hidden class=anchor aria-hidden=true href=#dinf-grid-a-neural-differential-equation-solver-with-differentiable-feature-gridshttpsarxivorgabs260110715v1>#</a></h3><p><strong>Authors:</strong> Navami Kairanda, Shanthika Naik, Marc Habermann, Avinash Sharma, Christian Theobalt, Vladislav Golyanik
<strong>Venue:</strong> arXiv (2026)</p><p>We present a novel differentiable grid-based representation for efficiently solving differential equations (DEs). Widely used architectures for neural solvers, such as sinusoidal neural networks, are coordinate-based MLPs that are both computationally intensive and slow to train. Although grid-based alternatives for implicit representations (e.g., Instant-NGP and K-Planes) train faster by exploiting signal structure, their reliance on linear interpolation restricts their ability to compute higher-order derivatives, rendering them unsuitable for solving DEs. Our approach overcomes these limitations by combining the efficiency of feature grids with radial basis function interpolation, which is infinitely differentiable. To effectively capture high-frequency solutions and enable stable and faster computation of global gradients, we introduce a multi-resolution decomposition with co-located grids. Our proposed representation, DInf-Grid, is trained implicitly using the differential equations as loss functions, enabling accurate modelling of physical fields. We validate DInf-Grid on a variety of tasks, including the Poisson equation for image reconstruction, the Helmholtz equation for wave fields, and the Kirchhoff-Love boundary value problem for cloth simulation. Our results demonstrate a 5-20x speed-up over coordinate-based MLP-based methods, solving differential equations in seconds or minutes while maintaining comparable accuracy and compactness.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10715v1">üìÑ Download PDF</a></p><hr><h3 id=ufo-trees-practical-and-provably-efficient-parallel-batch-dynamic-treeshttpsarxivorgabs260110706v1><a href=https://arxiv.org/abs/2601.10706v1>UFO Trees: Practical and Provably-Efficient Parallel Batch-Dynamic Trees</a><a hidden class=anchor aria-hidden=true href=#ufo-trees-practical-and-provably-efficient-parallel-batch-dynamic-treeshttpsarxivorgabs260110706v1>#</a></h3><p><strong>Authors:</strong> Quinten De Man, Atharva Sharma, Kishen N Gowda, Laxman Dhulipala
<strong>Venue:</strong> arXiv (2026)</p><p>The dynamic trees problem is to maintain a tree under edge updates while supporting queries like connectivity queries or path queries. Despite the first data structure for this fundamental problem &ndash; the link-cut tree &ndash; being invented 40 years ago, our experiments reveal that they are still the fastest sequential data structure for the problem. However, link-cut trees cannot support parallel batch-dynamic updates and have limitations on the kinds of queries they support.
In this paper, we design a new parallel batch-dynamic trees data structure called UFO trees that simultaneously supports a wide range of query functionality, supports work-efficient parallel batch-dynamic updates, and is competitive with link-cut trees when run sequentially. We prove that a key reason for the strong practical performance of both link-cut trees and UFO trees is that they can perform updates and queries in sub-logarithmic time for low-diameter trees. We perform an experimental study of our optimized C++ implementations of UFO trees with ten other dynamic tree implementations, several of which are new, in a broad benchmark of both synthetic and real-world trees of varying diameter and size. Our results show that, in both sequential and parallel settings, UFO trees are the fastest dynamic tree data structure that supports a wide range of queries. Our new implementation of UFO trees has low space usage and easily scales to billion-size inputs, making it a promising building block for implementing more complex dynamic graph algorithms in practice.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10706v1">üìÑ Download PDF</a></p><hr><h3 id=quantifying-the-properties-of-evolutionary-quantum-states-of-the-xxz-spin-model-using-quantum-computinghttpsarxivorgabs260110650v1><a href=https://arxiv.org/abs/2601.10650v1>Quantifying the properties of evolutionary quantum states of the XXZ spin model using quantum computing</a><a hidden class=anchor aria-hidden=true href=#quantifying-the-properties-of-evolutionary-quantum-states-of-the-xxz-spin-model-using-quantum-computinghttpsarxivorgabs260110650v1>#</a></h3><p><strong>Authors:</strong> M. P. Tonne, Kh. P. Gnatenko
<strong>Venue:</strong> arXiv (2026)</p><p>The entanglement distance of evolutionary quantum states of a two-spin system with the XXZ model has been studied. The analysis has been conducted both analytically and using quantum computing. An analytical dependence of the entanglement distance on the values of the model coupling constants and the parameters of the initial states has been obtained. The speed of evolution of a two-spin system has been investigated. The analysis has been performed analytically and using quantum computing. An explicit dependence of the speed of evolution on the coupling constants and on the parameters of the initial state has been obtained. The results of quantum computations are in good agreement with the theoretical predictions.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10650v1">üìÑ Download PDF</a></p><hr><h3 id=cosmoglobe-dr2-vi-disentangling-hot-and-cold-thermal-dust-emission-with-planck-hfihttpsarxivorgabs260110640v1><a href=https://arxiv.org/abs/2601.10640v1>Cosmoglobe DR2. VI. Disentangling hot and cold thermal dust emission with Planck HFI</a><a hidden class=anchor aria-hidden=true href=#cosmoglobe-dr2-vi-disentangling-hot-and-cold-thermal-dust-emission-with-planck-hfihttpsarxivorgabs260110640v1>#</a></h3><p><strong>Authors:</strong> R. M. Sullivan, E. Gjerl√∏w, M. Galloway, D. J. Watts, R. Aurvik, A. Basyrov, L. A. Bianchi, A. Bonato, M. Brilenkov, H. K. Eriksen, U. Fuskeland, K. A. Glasscock, L. T. Hergt, D. Herman, J. G. S. Lunde, A. I. Silva Martins, M. San, D. Sponseller, N. -O. Stutzer, H. Thommesen, V. Vikenes, I. K. Wehus, L. Zapelli
<strong>Venue:</strong> arXiv (2026)</p><p>We present a four-component high-resolution model of thermal dust emission for microwave and sub-mm frequencies derived from Planck HFI, WHAM and Gaia. The resulting high-resolution model derived here forms the basis for the thermal dust model employed in the Cosmoglobe DR2 reanalysis of COBE-DIRBE. The four dust components are called <code>cold dust'', </code>hot dust&rsquo;&rsquo;, <code>nearby dust'', and </code>Ha correlated dust&rsquo;&rsquo;, respectively, and trace different physical environments. The spatial distributions of the nearby dust and Ha dust components are defined by the Edenhofer et al. Gaia 3D extinction model and the WHAM survey, respectively, while the hot and cold dust components are fit freely pixel-by-pixel to the Planck HFI data. We use a global parameter grid search coupled to an amplitude map Gibbs sampler to fit this model to Planck HFI data. In agreement with the companion low-resolution analysis, we find that the hot dust component is strongly correlated with the FIRAS Cii map, while the cold dust component is strongly correlated with the HI4PI Hi map. Despite its fewer degrees of freedom per pixel compared to the Planck 2015 legacy dust model, we find that this new model performs competitively in terms of overall residuals, capturing over 98% of the full-sky dust variance for all channels. When fitting a spatially varying 3-parameter MBB model to the new dust model with isotropic SEDs, we find very similar spatial distributions to those of the official Planck analysis, and this new model thus represents an economical decomposition of previously published spatially varying spectral parameter maps. We conclude that this new model represents both a statistically more efficient summary of thermal dust in the microwave and far-infrared regimes and a physically more realistic decomposition of the sky compared to the traditional 3-parameter MBB model. (abridged)</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10640v1">üìÑ Download PDF</a></p><hr><h3 id=measuring-the-coronal-magnetic-field-with-2d-coronal-seismology-a-forward-modeling-validationhttpsarxivorgabs260110637v1><a href=https://arxiv.org/abs/2601.10637v1>Measuring the Coronal Magnetic Field with 2D Coronal Seismology: A Forward-Modeling Validation</a><a hidden class=anchor aria-hidden=true href=#measuring-the-coronal-magnetic-field-with-2d-coronal-seismology-a-forward-modeling-validationhttpsarxivorgabs260110637v1>#</a></h3><p><strong>Authors:</strong> Zihao Yang, Sarah Gibson, Matthias Rempel, Giuliana de Toma
<strong>Venue:</strong> arXiv (2026)</p><p>In recent years, a two-dimensional (2D) coronal seismology technique applied to spectral-imaging data from the Coronal Multi-channel Polarimeter (CoMP) and UCoMP has enabled routine measurement of the global coronal magnetic field. The technique combines coronal transverse wave phase speed from Doppler measurements with electron densities from the Fe \sc{xiii}\rm{} 10798/10747 √Ö intensity ratio to infer the magnetic field strength, while the wave propagation directions from Doppler measurements trace the magnetic field direction. To validate the accuracy and robustness of this method, we use forward modeling of a MURaM simulation that produces open and closed magnetic structures with excited waves. From the synthetic Doppler velocity, Fe \sc{xiii}\rm{} infrared line intensities, and linear polarization signals, we apply the 2D coronal seismology technique to estimate the magnetic field strength and direction. A comparison with the simulation ground truth shows close agreement, indicating that the technique can recover the line-of-sight emissivity-weighted magnetic field direction and strength with high accuracy. We also perform a parameter-space analysis to quantify sensitivities of the method to parameter choice. These findings provide practical guidance for CoMP/UCoMP-like analysis and demonstrate that 2D coronal seismology can deliver reliable, LOS emissivity-weighted measurements of the coronal magnetic field from coronal wave observations.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10637v1">üìÑ Download PDF</a></p><hr><h3 id=weave-imaging-spectroscopy-of-ngc-6720-an-iron-bar-in-the-ringhttpsarxivorgabs260110635v1><a href=https://arxiv.org/abs/2601.10635v1>WEAVE imaging spectroscopy of NGC 6720: an iron bar in the Ring</a><a hidden class=anchor aria-hidden=true href=#weave-imaging-spectroscopy-of-ngc-6720-an-iron-bar-in-the-ringhttpsarxivorgabs260110635v1>#</a></h3><p><strong>Authors:</strong> R. Wesson, J. E. Drew, M. J. Barlow, J. Garc√≠a-Rojas, R. Greimel, D. Jones, A. Manchado, R. A. H. Morris, A. Zijlstra, P. J. Storey, J. A. L. Aguerri, S. R. Berlanas, E. Carrasco, G. B. Dalton, E. Gafton, R. Garc√≠a-Benito, A. L. Gonz√°lez-Mor√°n, B. G√§nsicke, S. Hughes, S. Jin, R. Raddi, R. Sanchez-Janssen, E. Schallig, D. J. B. Smith, S. C. Trager, N. A. Walton
<strong>Venue:</strong> arXiv (2026)</p><p>We present spatially resolved spectroscopic observations of the planetary nebula NGC 6720, the Ring Nebula, taken during the science verification phase of WEAVE, a new instrument mounted on the William Herschel Telescope on La Palma. We use the instrument&rsquo;s Large Integral Field Unit (LIFU) to obtain spectra of the Ring Nebula, covering its entire optically bright inner regions as well as parts of its much fainter outer molecular halo. We report the discovery of emission from [Fe~{\sc v}] and [Fe~{\sc vi}] confined to a narrow ``bar&rsquo;&rsquo; extending across the central regions of the nebula. No lines of other elements share this morphology or, at the spectral resolving power used ($R \sim 2500$), the same radial velocity. The extent to which iron in this bar is depleted is presently unclear; comparison with JWST-detected dust continuum emission suggests that some dust grain destruction may be occurring in the region, but there is currently no observational evidence for the $>$ 50~km,s$^{-1}$ shock waves or $T > 10^6$~K X-ray emitting gas needed to enable this. Where the bar is located along the line of sight through the nebula, and how it was created, are new puzzles to be solved for this iconic planetary nebula.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10635v1">üìÑ Download PDF</a></p><hr><h3 id=on-the-geometry-of-aggregate-snowflakeshttpsarxivorgabs260110608v1><a href=https://arxiv.org/abs/2601.10608v1>On the geometry of aggregate snowflakes</a><a hidden class=anchor aria-hidden=true href=#on-the-geometry-of-aggregate-snowflakeshttpsarxivorgabs260110608v1>#</a></h3><p><strong>Authors:</strong> Axel Seifert, Christoph Siewert, Fabian Jakub, Leonie von Terzi, Stefan Kneifel
<strong>Venue:</strong> arXiv (2026)</p><p>Snowflakes play a crucial role in weather and climate. A significant portion of precipitation that reaches the surface originates as ice, even when it ultimately falls as rain. Contrary to the popular image of symmetric, dendritic crystals, most large snowflakes are irregular aggregates formed through the collision of primary ice crystals, such as hexagonal plates, columns, and dendrites. These aggregates exhibit complex, fractal-like structures, particularly at large sizes. Despite this structural complexity, each aggregate snowflake is unique, with properties that vary significantly around the mean - variability that is typically neglected in weather and climate models. Using a physically based aggregation model, we generate millions of synthetic snowflakes to investigate their geometric properties. The resulting dataset reveals that, for a given monomer number (cluster size) and mass, the maximum dimension follows approximately a lognormal distribution. We present a parameterization of aggregate geometry that captures key statistical properties, including maximum dimension, aspect ratio, cross-sectional area, and their joint correlations. This formulation enables a stochastic representation of aggregate snowflakes in Lagrangian particle models. Incorporating this variability improves the realism of simulated fall velocities, enhances growth rates by aggregation, and broadens Doppler radar spectra in closer agreement with observations.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10608v1">üìÑ Download PDF</a></p><hr><h3 id=origins-of-the-uv-continuum-and-balmer-emission-lines-in-little-red-dots-observational-validation-of-dense-gas-envelope-models-enshrouding-the-agnhttpsarxivorgabs260110573v1><a href=https://arxiv.org/abs/2601.10573v1>Origins of the UV continuum and Balmer emission lines in Little Red Dots: observational validation of dense gas envelope models enshrouding the AGN</a><a hidden class=anchor aria-hidden=true href=#origins-of-the-uv-continuum-and-balmer-emission-lines-in-little-red-dots-observational-validation-of-dense-gas-envelope-models-enshrouding-the-agnhttpsarxivorgabs260110573v1>#</a></h3><p><strong>Authors:</strong> Yoshihisa Asada, Kohei Inayoshi, Qinyue Fei, Seiji Fujimoto, Chris Willott
<strong>Venue:</strong> arXiv (2026)</p><p>We present a statistical study on the origins of the UV continuum and narrow/broad emission lines in little red dots (LRDs), a newly discovered class of active galactic nuclei (AGNs). Leveraging all archived JWST/NIRSpec data, we build a sample of 28 spectroscopically-confirmed LRDs at $5&lt;z_{\rm spec}&lt;7.2$, by requiring broad H$Œ±$ emission, blue UV colors, V-shaped continua, and compact morphologies. We define a control sample of 9 blue, compact, broad-line AGNs without red optical continua (hereafter little blue dots; LBDs), and examine correlations between rest UV and the narrow/broad H$Œ±$ luminosities in these populations. In LRDs, both narrow and broad H$Œ±$ components are tightly correlated with the UV continuum, and the luminosity ratios are consistent with those in young starburst galaxies. In contrast, the UV to broad H$Œ±$ ratios in LBDs closely match local unobscured AGNs and are statistically different from LRDs. The Ly$Œ±$ occurrence rates and strengths do not differ between LRDs and LBDs and are comparable to normal star-forming galaxies. These results are consistent with a scenario where the central BH in LRDs is enshrouded by a dense opaque gas envelope &ndash; in this model, the UV continuum as well as narrow and even broad H$Œ±$ emissions are not powered by AGNs but predominantly by young massive stars surrounding the envelope, while the envelope radiates as a $\sim 5000$ K blackbody. As the envelope dissipates, direct AGN emission can emerge, potentially transforming LRDs into LBDs and marking the end of a short-lived phase of rapid black hole growth.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10573v1">üìÑ Download PDF</a></p><hr><h3 id=high-accuracy-and-dimension-free-sampling-with-diffusionshttpsarxivorgabs260110708v1><a href=https://arxiv.org/abs/2601.10708v1>High-accuracy and dimension-free sampling with diffusions</a><a hidden class=anchor aria-hidden=true href=#high-accuracy-and-dimension-free-sampling-with-diffusionshttpsarxivorgabs260110708v1>#</a></h3><p><strong>Authors:</strong> Khashayar Gatmiry, Sitan Chen, Adil Salim
<strong>Venue:</strong> arXiv (2026)</p><p>Diffusion models have shown remarkable empirical success in sampling from rich multi-modal distributions. Their inference relies on numerically solving a certain differential equation. This differential equation cannot be solved in closed form, and its resolution via discretization typically requires many small iterations to produce \emph{high-quality} samples.
More precisely, prior works have shown that the iteration complexity of discretization methods for diffusion models scales polynomially in the ambient dimension and the inverse accuracy $1/\varepsilon$. In this work, we propose a new solver for diffusion models relying on a subtle interplay between low-degree approximation and the collocation method (Lee, Song, Vempala 2018), and we prove that its iteration complexity scales \emph{polylogarithmically} in $1/\varepsilon$, yielding the first ``high-accuracy&rsquo;&rsquo; guarantee for a diffusion-based sampler that only uses (approximate) access to the scores of the data distribution. In addition, our bound does not depend explicitly on the ambient dimension; more precisely, the dimension affects the complexity of our solver through the \emph{effective radius} of the support of the target distribution only.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10708v1">üìÑ Download PDF</a></p><hr><h3 id=distributed-perceptron-under-bounded-staleness-partial-participation-and-noisy-communicationhttpsarxivorgabs260110705v1><a href=https://arxiv.org/abs/2601.10705v1>Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication</a><a hidden class=anchor aria-hidden=true href=#distributed-perceptron-under-bounded-staleness-partial-participation-and-noisy-communicationhttpsarxivorgabs260110705v1>#</a></h3><p><strong>Authors:</strong> Keval Jain, Anant Raj, Saurav Prakash, Girish Varma
<strong>Venue:</strong> arXiv (2026)</p><p>We study a semi-asynchronous client-server perceptron trained via iterative parameter mixing (IPM-style averaging): clients run local perceptron updates and a server forms a global model by aggregating the updates that arrive in each communication round. The setting captures three system effects in federated and distributed deployments: (i) stale updates due to delayed model delivery and delayed application of client computations (two-sided version lag), (ii) partial participation (intermittent client availability), and (iii) imperfect communication on both downlink and uplink, modeled as effective zero-mean additive noise with bounded second moment. We introduce a server-side aggregation rule called staleness-bucket aggregation with padding that deterministically enforces a prescribed staleness profile over update ages without assuming any stochastic model for delays or participation. Under margin separability and bounded data radius, we prove a finite-horizon expected bound on the cumulative weighted number of perceptron mistakes over a given number of server rounds: the impact of delay appears only through the mean enforced staleness, whereas communication noise contributes an additional term that grows on the order of the square root of the horizon with the total noise energy. In the noiseless case, we show how a finite expected mistake budget yields an explicit finite-round stabilization bound under a mild fresh-participation condition.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10705v1">üìÑ Download PDF</a></p><hr><h3 id=communication-efficient-and-privacy-adaptable-mechanism----a-federated-learning-scheme-with-convergence-analysishttpsarxivorgabs260110701v1><a href=https://arxiv.org/abs/2601.10701v1>Communication-Efficient and Privacy-Adaptable Mechanism &ndash; a Federated Learning Scheme with Convergence Analysis</a><a hidden class=anchor aria-hidden=true href=#communication-efficient-and-privacy-adaptable-mechanism----a-federated-learning-scheme-with-convergence-analysishttpsarxivorgabs260110701v1>#</a></h3><p><strong>Authors:</strong> Chun Hei Michael Shiu, Chih Wei Ling
<strong>Venue:</strong> arXiv (2026)</p><p>Federated learning enables multiple parties to jointly train learning models without sharing their own underlying data, offering a practical pathway to privacy-preserving collaboration under data-governance constraints. Continued study of federated learning is essential to address key challenges in it, including communication efficiency and privacy protection between parties. A recent line of work introduced a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), which achieves both objectives simultaneously. CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a randomized vector quantizer whose quantization error is equivalent to a prescribed noise, which can be tuned to customize privacy protection between parties. In this work, we theoretically analyze the privacy guarantees and convergence properties of CEPAM. Moreover, we assess CEPAM&rsquo;s utility performance through experimental evaluations, including convergence profiles compared with other baselines, and accuracy-privacy trade-offs between different parties.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10701v1">üìÑ Download PDF</a></p><hr><h3 id=an-extension-based-accessibility-framework-for-making-blockly-accessible-to-blind-and-low-vision-usershttpsarxivorgabs260110688v1><a href=https://arxiv.org/abs/2601.10688v1>An Extension-Based Accessibility Framework for Making Blockly Accessible to Blind and Low-Vision Users</a><a hidden class=anchor aria-hidden=true href=#an-extension-based-accessibility-framework-for-making-blockly-accessible-to-blind-and-low-vision-usershttpsarxivorgabs260110688v1>#</a></h3><p><strong>Authors:</strong> Rubel Hassan Mollik, Vamsi Krishna Kosuri, Hans Djalali, Stephanie Ludi, Aboubakar Mountapmbeme
<strong>Venue:</strong> arXiv (2026)</p><p>Block-based programming environments (BBPEs) such as Scratch and Code.org are now widely used in K-12 computer science classes, but they remain mostly inaccessible to blind or visually impaired (BVI) learners. A major problem is that prior accessibility solutions have relied on modifications to the Blockly library, making them difficult to apply in existing BBPEs and thereby limiting adoption. We present an Extension-based Accessibility Framework (EAF) to make BBPEs accessible for BVI students. The framework uses a modular architecture that enables seamless integration with existing Blockly-based BBPEs. We present an innovative three-dimensional (3D) hierarchical navigation model featuring stack labeling and block numbering, mode-based editing to prevent accidental modifications, and WAI-ARIA implementation to ensure compatibility with external screen readers. We evaluated our approach by integrating the EAF framework into two BBPEs (covering 177 test cases) and conducting semi-structured interviews with four participants using VoiceOver, JAWS, and NVDA. Participants reported clearer spatial orientation and easier mental model formation compared to default Blockly keyboard navigation. EAF shows that modular architecture can provide comprehensive accessibility while ensuring compatibility with existing BBPEs.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10688v1">üìÑ Download PDF</a></p><hr><h3 id=detecting-winning-arguments-with-large-language-models-and-persuasion-strategieshttpsarxivorgabs260110660v1><a href=https://arxiv.org/abs/2601.10660v1>Detecting Winning Arguments with Large Language Models and Persuasion Strategies</a><a hidden class=anchor aria-hidden=true href=#detecting-winning-arguments-with-large-language-models-and-persuasion-strategieshttpsarxivorgabs260110660v1>#</a></h3><p><strong>Authors:</strong> Tiziano Labruna, Arkadiusz Modzelewski, Giorgio Satta, Giovanni Da San Martino
<strong>Venue:</strong> arXiv (2026)</p><p>Detecting persuasion in argumentative text is a challenging task with important implications for understanding human communication. This work investigates the role of persuasion strategies - such as Attack on reputation, Distraction, and Manipulative wording - in determining the persuasiveness of a text. We conduct experiments on three annotated argument datasets: Winning Arguments (built from the Change My View subreddit), Anthropic/Persuasion, and Persuasion for Good. Our approach leverages large language models (LLMs) with a Multi-Strategy Persuasion Scoring approach that guides reasoning over six persuasion strategies. Results show that strategy-guided reasoning improves the prediction of persuasiveness. To better understand the influence of content, we organize the Winning Argument dataset into broad discussion topics and analyze performance across them. We publicly release this topic-annotated version of the dataset to facilitate future research. Overall, our methodology demonstrates the value of structured, strategy-aware prompting for enhancing interpretability and robustness in argument quality assessment.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10660v1">üìÑ Download PDF</a></p><hr><h3 id=alignment-pretraining-ai-discourse-causes-self-fulfilling-misalignmenthttpsarxivorgabs260110160v1><a href=https://arxiv.org/abs/2601.10160v1>Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment</a><a hidden class=anchor aria-hidden=true href=#alignment-pretraining-ai-discourse-causes-self-fulfilling-misalignmenthttpsarxivorgabs260110160v1>#</a></h3><p><strong>Authors:</strong> Cameron Tice, Puria Radmard, Samuel Ratnam, Andy Kim, David Africa, Kyle O&rsquo;Brien
<strong>Venue:</strong> arXiv (2026)</p><p>Pretraining corpora contain extensive discourse about AI systems, yet the causal influence of this discourse on downstream alignment remains poorly understood. If prevailing descriptions of AI behaviour are predominantly negative, LLMs may internalise corresponding behavioural priors, giving rise to self-fulfilling misalignment. This paper provides the first controlled study of this hypothesis by pretraining 6.9B-parameter LLMs with varying amounts of (mis)alignment discourse. We find that discussion of AI contributes to misalignment. Upsampling synthetic training documents about AI misalignment leads to a notable increase in misaligned behaviour. Conversely, upsampling documents about aligned behaviour reduces misalignment scores from 45% to 9%. We consider this evidence of self-fulfilling alignment. These effects are dampened, but persist through post-training. Our findings establish the study of how pretraining data shapes alignment priors, or alignment pretraining, as a complement to post-training. We recommend practitioners pretrain for alignment as well as capabilities. Our models and datasets are available at alignmentpretraining.ai</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10160v1">üìÑ Download PDF</a></p><hr><h3 id=actors-frames-and-arguments-a-multi-decade-computational-analysis-of-climate-discourse-in-financial-news-using-large-language-modelshttpsarxivorgabs260110142v1><a href=https://arxiv.org/abs/2601.10142v1>Actors, Frames and Arguments: A Multi-Decade Computational Analysis of Climate Discourse in Financial News using Large Language Models</a><a hidden class=anchor aria-hidden=true href=#actors-frames-and-arguments-a-multi-decade-computational-analysis-of-climate-discourse-in-financial-news-using-large-language-modelshttpsarxivorgabs260110142v1>#</a></h3><p><strong>Authors:</strong> Ruiran Su, Janet B. Pierrehumbert, Markus Leippold
<strong>Venue:</strong> arXiv (2026)</p><p>Financial news media shapes trillion-dollar climate investment decisions, yet discourse in this elite domain remains underexplored. We analyze two decades of climate-related articles (2000-2023) from Dow Jones Newswire using an Actor-Frame-Argument (AFA) pipeline that extracts who speaks, how issues are framed, and which arguments are deployed. We validate extractions against 2,000 human-annotated articles using a Decompositional Verification Framework that evaluates completeness, faithfulness, coherence, and relevance. Our longitudinal analysis uncovers a structural transformation: pre-2015 coverage emphasized risk and regulatory burden; post-Paris Agreement, discourse shifted toward economic opportunity and innovation, with financial institutions becoming dominant voices. Methodologically, we provide a replicable paradigm for longitudinal media analysis with LLMs; substantively, we reveal how financial elites have internalized and reframed the climate crisis across two decades.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10142v1">üìÑ Download PDF</a></p><hr><h3 id=matrix-as-plan-structured-logical-reasoning-with-feedback-driven-replanninghttpsarxivorgabs260110101v1><a href=https://arxiv.org/abs/2601.10101v1>MATRIX AS PLAN: Structured Logical Reasoning with Feedback-Driven Replanning</a><a hidden class=anchor aria-hidden=true href=#matrix-as-plan-structured-logical-reasoning-with-feedback-driven-replanninghttpsarxivorgabs260110101v1>#</a></h3><p><strong>Authors:</strong> Ke Chen, Jiandian Zeng, Zihao Peng, Guo Li, Guangxue Zhang, Tian Wang
<strong>Venue:</strong> arXiv (2026)</p><p>As knowledge and semantics on the web grow increasingly complex, enhancing Large Language Models (LLMs) comprehension and reasoning capabilities has become particularly important. Chain-of-Thought (CoT) prompting has been shown to enhance the reasoning capabilities of LLMs. However, it still falls short on logical reasoning tasks that rely on symbolic expressions and strict deductive rules. Neuro-symbolic methods address this gap by enforcing formal correctness through external solvers. Yet these solvers are highly format-sensitive, and small instabilities in model outputs can lead to frequent processing failures. LLM-driven approaches avoid parsing brittleness, but they lack structured representations and process-level error-correction mechanisms. To further enhance the logical reasoning capabilities of LLMs, we propose MatrixCoT, a structured CoT framework with a matrix-based plan. Specifically, we normalize and type natural language expressions, attach explicit citation fields, and introduce a matrix-based planning method to preserve global relations among steps. The plan becomes a verifiable artifact, making execution more stable. For verification, we also add a feedback-driven replanning mechanism. Under semantic-equivalence constraints, it identifies omissions and defects, rewrites and compresses the dependency matrix, and produces a more trustworthy final answer. Experiments on five logical-reasoning benchmarks and five LLMs show that, without relying on external solvers, MatrixCoT enhances both robustness and interpretability when tackling complex symbolic reasoning tasks, while maintaining competitive performance.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10101v1">üìÑ Download PDF</a></p><hr><h3 id=pid-guided-partial-alignment-for-multimodal-decentralized-federated-learninghttpsarxivorgabs260110012v1><a href=https://arxiv.org/abs/2601.10012v1>PID-Guided Partial Alignment for Multimodal Decentralized Federated Learning</a><a hidden class=anchor aria-hidden=true href=#pid-guided-partial-alignment-for-multimodal-decentralized-federated-learninghttpsarxivorgabs260110012v1>#</a></h3><p><strong>Authors:</strong> Yanhang Shi, Xiaoyu Wang, Houwei Cao, Jian Li, Yong Liu
<strong>Venue:</strong> arXiv (2026)</p><p>Multimodal decentralized federated learning (DFL) is challenging because agents differ in available modalities and model architectures, yet must collaborate over peer-to-peer (P2P) networks without a central coordinator. Standard multimodal pipelines learn a single shared embedding across all modalities. In DFL, such a monolithic representation induces gradient misalignment between uni- and multimodal agents; as a result, it suppresses heterogeneous sharing and cross-modal interaction. We present PARSE, a multimodal DFL framework that operationalizes partial information decomposition (PID) in a server-free setting. Each agent performs feature fission to factorize its latent representation into redundant, unique, and synergistic slices. P2P knowledge sharing among heterogeneous agents is enabled by slice-level partial alignment: only semantically shareable branches are exchanged among agents that possess the corresponding modality. By removing the need for central coordination and gradient surgery, PARSE resolves uni-/multimodal gradient conflicts, thereby overcoming the multimodal DFL dilemma while remaining compatible with standard DFL constraints. Across benchmarks and agent mixes, PARSE yields consistent gains over task-, modality-, and hybrid-sharing DFL baselines. Ablations on fusion operators and split ratios, together with qualitative visualizations, further demonstrate the efficiency and robustness of the proposed design.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10012v1">üìÑ Download PDF</a></p><hr><h3 id=how-diplomacy-reshapes-online-discourseasymmetric-persistence-in-online-framing-of-north-koreahttpsarxivorgabs260109942v1><a href=https://arxiv.org/abs/2601.09942v1>How Diplomacy Reshapes Online Discourse:Asymmetric Persistence in Online Framing of North Korea</a><a hidden class=anchor aria-hidden=true href=#how-diplomacy-reshapes-online-discourseasymmetric-persistence-in-online-framing-of-north-koreahttpsarxivorgabs260109942v1>#</a></h3><p><strong>Authors:</strong> Hunjun Shin, Hoonbae Moon, Mohit Singhal
<strong>Venue:</strong> arXiv (2026)</p><p>Public opinion toward foreign adversaries shapes and constrains diplomatic options. Prior research has largely relied on sentiment analysis and survey based measures, providing limited insight into how sustained narrative changes (beyond transient emotional reactions) might follow diplomatic engagement. This study examines the extent to which high stakes diplomatic summits shape how adversaries are framed in online discourse. We analyze U.S.-North Korea summit diplomacy (2018-2019) using a Difference-in-Difference(DiD) design on Reddit discussions. Using multiple control groups (China, Iran, Russia) to adjust for concurrent geopolitical shocks, we integrate a validated Codebook LLM framework for framing classification with graph based discourse network analysis that examines both edge level relationships and community level narrative structures. Our results reveal short term asymmetric persistence in framing responses to diplomacy. While both post level and comment level sentiment proved transient (improving during the Singapore Summit but fully reverting after the Hanoi failure),framing exhibited significant stability: the shift from threat oriented to diplomacy oriented framing was only partially reversed. Structurally, the proportion of threat oriented edges decreased substantially (48% -> 28%) while diplomacy oriented structures expanded, and these shifts resisted complete reversion after diplomatic failure. These findings suggest that diplomatic success can leave a short-term but lasting imprint on how adversaries are framed in online discourse, even when subsequent negotiations fail.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.09942v1">üìÑ Download PDF</a></p><hr><h3 id=epistemology-gives-a-future-to-complementarity-in-human-ai-interactionshttpsarxivorgabs260109871v1><a href=https://arxiv.org/abs/2601.09871v1>Epistemology gives a Future to Complementarity in Human-AI Interactions</a><a hidden class=anchor aria-hidden=true href=#epistemology-gives-a-future-to-complementarity-in-human-ai-interactionshttpsarxivorgabs260109871v1>#</a></h3><p><strong>Authors:</strong> Andrea Ferrario, Alessandro Facchini, Juan M. Dur√°n
<strong>Venue:</strong> arXiv (2026)</p><p>Human-AI complementarity is the claim that a human supported by an AI system can outperform either alone in a decision-making process. Since its introduction in the human-AI interaction literature, it has gained traction by generalizing the reliance paradigm and by offering a more practical alternative to the contested construct of &rsquo;trust in AI.&rsquo; Yet complementarity faces key theoretical challenges: it lacks precise theoretical anchoring, it is formalized just as a post hoc indicator of relative predictive accuracy, it remains silent about other desiderata of human-AI interactions and it abstracts away from the magnitude-cost profile of its performance gain. As a result, complementarity is difficult to obtain in empirical settings. In this work, we leverage epistemology to address these challenges by reframing complementarity within the discourse on justificatory AI. Drawing on computational reliabilism, we argue that historical instances of complementarity function as evidence that a given human-AI interaction is a reliable epistemic process for a given predictive task. Together with other reliability indicators assessing the alignment of the human-AI team with the epistemic standards and socio-technical practices, complementarity contributes to the degree of reliability of human-AI teams when generating predictions. This supports the practical reasoning of those affected by these outputs &ndash; patients, managers, regulators, and others. In summary, our approach suggests that the role and value of complementarity lies not in providing a relative measure of predictive accuracy, but in helping calibrate decision-making to the reliability of AI-supported processes that increasingly shape everyday life.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.09871v1">üìÑ Download PDF</a></p><hr><h3 id=quantum-geometry-of-the-rotating-shallow-water-modelhttpsarxivorgabs260110695v1><a href=https://arxiv.org/abs/2601.10695v1>Quantum geometry of the rotating shallow water model</a><a hidden class=anchor aria-hidden=true href=#quantum-geometry-of-the-rotating-shallow-water-modelhttpsarxivorgabs260110695v1>#</a></h3><p><strong>Authors:</strong> Sriram Ganeshan, Alan T. Dorsey
<strong>Venue:</strong> arXiv (2026)</p><p>The rotating shallow water equations (RSWE) are a mainstay of atmospheric and oceanic modeling, and their wave dynamics has close analogues in settings ranging from two-dimensional electron gases to active-matter fluids. While recent work has emphasized the topological character of RSWE wave bands, here we develop a complementary quantum-geometric description by computing the full quantum geometric tensor (QGT) for the linearized RSWE on an $f$-plane. The QGT unifies two pieces of band geometry: its real part defines a metric that quantifies how rapidly wave polarization changes with parameters, while its imaginary part is the Berry curvature that controls geometric phases and topological invariants. We obtain compact, symmetry-guided expressions for all three bands, highlighting the transverse structure of the metric and the monopole-like Berry curvature that yields Chern numbers for the Poincar√© bands. Finally, we describe a feasible route to probing this geometry in rotating-tank experiments via weak, time-periodic parametric driving.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10695v1">üìÑ Download PDF</a></p><hr><h3 id=mitigating-nonlinear-transduction-noise-in-high-cooperativity-cavity-optomechanicshttpsarxivorgabs260110689v1><a href=https://arxiv.org/abs/2601.10689v1>Mitigating nonlinear transduction noise in high-cooperativity cavity optomechanics</a><a hidden class=anchor aria-hidden=true href=#mitigating-nonlinear-transduction-noise-in-high-cooperativity-cavity-optomechanicshttpsarxivorgabs260110689v1>#</a></h3><p><strong>Authors:</strong> Daniel Allepuz-Requena, Zohran Ali, Dennis H√∏j, Yingxuan Chen, Luiz Couto Correa Pinto Filho, Alexander Huck, Ulrik L. Andersen
<strong>Venue:</strong> arXiv (2026)</p><p>Coupling mechanical motion to an optical resonator enables displacement measurements approaching the standard quantum limit (SQL). However, increasing the optomechanical coupling strength will inevitably lead to probing of the nonlinear response of the optical resonator. Thermal intermodulation noise (TIN) arising from the nonlinear mixing of thermomechanical motion can further increase the imprecision well above the SQL and has hitherto been canceled up to second order of nonlinearity via operation at the &ldquo;magic detuning&rdquo;. In this work, we record the output of a membrane-in-the-middle microcavity system operating at room temperature and achieving high cooperativity, $C>n_\text{th}$, and apply a nonlinear transform that removes all orders of TIN, improving the mechanical signal-to-noise ratio by nearly 10 dB. Our results can be applied to experiments affected by third-order TIN, which we expect to be the dominating intrinsic source of noise in high-cooperativity room-temperature cavity optomechanical systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10689v1">üìÑ Download PDF</a></p><hr><h3 id=energy-correlators-in-warped-geometrieshttpsarxivorgabs260110674v1><a href=https://arxiv.org/abs/2601.10674v1>Energy Correlators in Warped Geometries</a><a hidden class=anchor aria-hidden=true href=#energy-correlators-in-warped-geometrieshttpsarxivorgabs260110674v1>#</a></h3><p><strong>Authors:</strong> Lorenzo Ricci, Raman Sundrum
<strong>Venue:</strong> arXiv (2026)</p><p>We study Energy Correlators as probes of strongly-coupled nearly-conformal field theories within their holographically dual descriptions, focusing on the important features that appear in realistic theories going beyond the standard model. In particular, we study warped geometries which asymptote to $\text{AdS}_5$, as well as IR-truncations dual to a 4D gap. Our correlators are computed by in-in type Witten perturbative diagrams, corresponding to a large-N expansion of the strong dynamics. We describe how this sets the stage for phenomenological applications for collider searches beyond the standard model as well as for new theoretical explorations in Lorentzian holography.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10674v1">üìÑ Download PDF</a></p><hr><h3 id=the-static-heavy-quark-antiquark-potential-within-string-theory-in-arbitrary-stationary-backgroundshttpsarxivorgabs260110668v1><a href=https://arxiv.org/abs/2601.10668v1>The Static Heavy Quark-Antiquark Potential within String Theory in Arbitrary Stationary Backgrounds</a><a hidden class=anchor aria-hidden=true href=#the-static-heavy-quark-antiquark-potential-within-string-theory-in-arbitrary-stationary-backgroundshttpsarxivorgabs260110668v1>#</a></h3><p><strong>Authors:</strong> Nikita Tsegelnik
<strong>Venue:</strong> arXiv (2026)</p><p>We analyze a static open string in a general stationary spacetime, which can represent a heavy quark-antiquark pair within the holographic framework or effective theory. We establish that for a simple U-shaped string with only radial dependence on the space string coordinate, $x_r&rsquo;(œÉ) \neq 0$, the string is generally not symmetric about its turning point, and the symmetry restores only for backgrounds with $h_{pr} = G_{00} G_{pr} - G_{0p} G_{0r} = 0$. Consequently, such asymmetric strings directly probe a possibility of the parity violation in the quark-antiquark interaction. Nevertheless, we identify a wide family of metrics for which the symmetry is preserved, enabling a direct isolation of the linear-in-distance term in the static interquark potential for simple symmetric string configurations, even in non-diagonal backgrounds. Applying the holographic framework, we further study the Rindler-AdS spacetime dual to an accelerated $\mathcal{N}=4$ super Yang-Mills plasma. We show that the distance between quarks decreases, the static potential between them increases, and the deconfinement phase transition temperature, $T_{\rm dec} = (œÄ/3) T_H = a_c/6$, increases with an acceleration. However, we observe that an acceleration-scaled potential as a function of the acceleration-scaled distance does not depend on the certain value of the acceleration This result, reflecting the scale invariance and self-similarity of the holographic setup, can be also obtained in the dimensionless metric after scaling of the coordinates onto the acceleration, $\tilde{x}<em>i = a_c x_i$, for which one obtains an universal value of the phase transition temperature, $\tilde{T}</em>{\rm dec} = (œÄ/3) \tilde{T}_H = 1/6$.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10668v1">üìÑ Download PDF</a></p><hr><h3 id=searching-for-quantum-effects-in-the-brain-a-bell-type-test-for-nonclassical-latent-representations-in-autoencodershttpsarxivorgabs260110588v1><a href=https://arxiv.org/abs/2601.10588v1>Searching for Quantum Effects in the Brain: A Bell-Type Test for Nonclassical Latent Representations in Autoencoders</a><a hidden class=anchor aria-hidden=true href=#searching-for-quantum-effects-in-the-brain-a-bell-type-test-for-nonclassical-latent-representations-in-autoencodershttpsarxivorgabs260110588v1>#</a></h3><p><strong>Authors:</strong> I. K. Kominis, C. Xie, S. Li, M. Skotiniotis, G. P. Tsironis
<strong>Venue:</strong> arXiv (2026)</p><p>Whether neural information processing is entirely classical or involves quantum-mechanical elements remains an open question. Here we propose a model-agnostic, information-theoretic test of nonclassicality that bypasses microscopic assumptions and instead probes the structure of neural representations themselves. Using autoencoders as a transparent model system, we introduce a Bell-type consistency test in latent space, and ask whether decoding statistics obtained under multiple readout contexts can be jointly explained by a single positive latent-variable distribution. By shifting the search for quantum-like signatures in neural systems from microscopic dynamics to experimentally testable constraints on information processing, this work opens a new route for probing the fundamental physics of neural computation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10588v1">üìÑ Download PDF</a></p><hr><h3 id=euclid-preparation-3d-reconstruction-of-the-cosmic-web-with-simulated-euclid-deep-spectroscopic-sampleshttpsarxivorgabs260110709v1><a href=https://arxiv.org/abs/2601.10709v1>Euclid preparation. 3D reconstruction of the cosmic web with simulated Euclid Deep spectroscopic samples</a><a hidden class=anchor aria-hidden=true href=#euclid-preparation-3d-reconstruction-of-the-cosmic-web-with-simulated-euclid-deep-spectroscopic-sampleshttpsarxivorgabs260110709v1>#</a></h3><p><strong>Authors:</strong> Euclid Collaboration, K. Kraljic, C. Laigle, M. Balogh, P. Jablonka, U. Kuchner, N. Malavasi, F. Sarron, C. Pichon, G. De Lucia, M. Bethermin, F. Durret, M. Fumagalli, C. Gouin, M. Magliocchetti, J. G. Sorce, O. Cucciati, F. Fontanot, M. Hirschmann, Y. Kang, M. Spinelli, N. Aghanim, A. Amara, S. Andreon, N. Auricchio, C. Baccigalupi, M. Baldi, S. Bardelli, A. Biviano, E. Branchini, M. Brescia, J. Brinchmann, S. Camera, G. Ca√±as-Herrera, V. Capobianco, C. Carbone, J. Carretero, R. Casas, S. Casas, F. J. Castander, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, G. Congedo, C. J. Conselice, L. Conversi, Y. Copin, F. Courbin, H. M. Courtois, A. Da Silva, H. Degaudenzi, S. de la Torre, H. Dole, M. Douspis, F. Dubath, C. A. J. Duncan, X. Dupac, S. Dusini, S. Escoffier, M. Farina, R. Farinelli, S. Ferriol, F. Finelli, P. Fosalba, N. Fourmanoit, M. Frailis, E. Franceschi, M. Fumana, S. Galeotta, K. George, W. Gillard, B. Gillis, C. Giocoli, J. Gracia-Carpio, A. Grazian, F. Grupp, S. V. H. Haugan, W. Holmes, F. Hormuth, A. Hornstrup, K. Jahnke, M. Jhabvala, B. Joachimi, E. Keih√§nen, S. Kermiche, A. Kiessling, M. Kilbinger, B. Kubik, M. K√ºmmel, M. Kunz, H. Kurki-Suonio, A. M. C. Le Brun, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, E. Maiorano, O. Mansutti, S. Marcin, O. Marggraf, M. Martinelli, N. Martinet, F. Marulli, R. Massey, S. Maurogordato, E. Medinaceli, S. Mei, Y. Mellier, M. Meneghetti, E. Merlin, G. Meylan, A. Mora, M. Moresco, L. Moscardini, R. Nakajima, C. Neissner, S. -M. Niemi, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, W. J. Percival, V. Pettorino, S. Pires, G. Polenta, M. Poncet, L. A. Popa, L. Pozzetti, F. Raison, R. Rebolo, A. Renzi, J. Rhodes, G. Riccio, E. Romelli, M. Roncarelli, C. Rosset, E. Rossetti, R. Saglia, Z. Sakr, A. G. S√°nchez, D. Sapone, B. Sartoris, P. Schneider, T. Schrabback, M. Scodeggio, A. Secroun, E. Sefusatti, G. Seidel, M. Seiffert, S. Serrano, P. Simon, C. Sirignano, G. Sirri, L. Stanco, J. Steinwagner, P. Tallada-Cresp√≠, A. N. Taylor, H. I. Teplitz, I. Tereno, N. Tessore, S. Toft, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, L. Valenziano, J. Valiviita, T. Vassallo, G. Verdoes Kleijn, A. Veropalumbo, D. Vibert, Y. Wang, J. Weller, A. Zacchei, G. Zamorani, E. Zucca, V. Allevato, M. Ballardini, M. Bolzonella, E. Bozzo, C. Burigana, R. Cabanac, M. Calabrese, A. Cappi, D. Di Ferdinando, J. A. Escartin Vigo, L. Gabarra, W. G. Hartley, J. Mart√≠n-Fleitas, S. Matthew, N. Mauri, R. B. Metcalf, A. A. Nucita, A. Pezzotta, M. P√∂ntinen, C. Porciani, I. Risso, V. Scottez, M. Sereno, M. Tenti, M. Viel, M. Wiesmann, Y. Akrami, S. Alvi, I. T. Andika, S. Anselmi, M. Archidiacono, F. Atrio-Barandela, A. Balaguera-Antolinez, P. Bergamini, D. Bertacca, A. Blanchard, L. Blot, H. B√∂hringer, S. Borgani, M. L. Brown, S. Bruton, A. Calabro, B. Camacho Quevedo, F. Caro, C. S. Carvalho, T. Castro, R. Chary, F. Cogato, S. Conseil, T. Contini, A. R. Cooray, S. Davini, F. De Paolis, G. Desprez, A. D√≠az-S√°nchez, J. J. Diaz, S. Di Domizio, J. M. Diego, P. Dimauro, P. -A. Duc, A. Enia, Y. Fang, A. G. Ferrari, A. Finoguenov, A. Fontana, A. Franco, K. Ganga, J. Garc√≠a-Bellido, T. Gasparetto, R. Gavazzi, E. Gaztanaga, F. Giacomini, F. Gianotti, G. Gozaliasl, M. Guidi, C. M. Gutierrez, A. Hall, H. Hildebrandt, J. Hjorth, S. Joudaki, J. J. E. Kajava, V. Kansal, D. Karagiannis, K. Kiiveri, C. C. Kirkpatrick, S. Kruk, M. Lattanzi, V. Le Brun, J. Le Graet, L. Legrand, M. Lembo, F. Lepori, G. Leroy, G. F. Lesci, J. Lesgourgues, L. Leuzzi, T. I. Liaudat, S. J. Liu, A. Loureiro, J. Macias-Perez, G. Maggio, E. A. Magnier, F. Mannucci, R. Maoli, C. J. A. P. Martins, L. Maurin, M. Miluzio, P. Monaco, C. Moretti, G. Morgante, S. Nadathur, K. Naidoo, A. Navarro-Alsina, S. Nesseris, L. Pagano, F. Passalacqua, K. Paterson, L. Patrizii, A. Pisani, D. Potter, S. Quai, M. Radovich, P. -F. Rocci, G. Rodighiero, S. Sacquegna, M. Sahl√©n, D. B. Sanders, A. Schneider, D. Sciotti, E. Sellentin, L. C. Smith, K. Tanidis, C. Tao, G. Testera, R. Teyssier, S. Tosi, A. Troja, M. Tucci, C. Valieri, A. Venhola, D. Vergani, G. Verza, P. Vielzeuf, N. A. Walton
<strong>Venue:</strong> arXiv (2026)</p><p>The ongoing Euclid mission aims to measure spectroscopic redshifts for approximately two million galaxies using the H $Œ±$ line emission detected in near-infrared slitless spectroscopic data from the Euclid Deep Fields (EDFs). These measurements will reach a flux limit of $5\times 10^{-17},{\rm erg},{\rm cm}^{-2},{\rm s}^{-1}$ in the redshift range $0.4&lt;z&lt;1.8$, opening the door to numerous investigations involving galaxy evolution, extending well beyond the mission&rsquo;s core objectives. The achieved H $Œ±$ luminosity depth will lead to a sufficiently high sampling, enabling the reconstruction of the large-scale galaxy environment. We assess the quality of the reconstruction of the galaxy cosmic web environment with the expected spectroscopic dataset in EDFs. The analysis is carried out on the Flagship and GAEA galaxy mock catalogues. The quality of the reconstruction is first evaluated using geometrical and topological statistics measured on the cosmic web, namely the length of filaments, the area of walls, the volume of voids, and its connectivity and multiplicity. We then quantify how accurately gradients in galaxy properties with distance from filaments can be recovered. As expected, the small-scale redshift-space distortions, have a strong impact on filament lengths and connectivity, but can be mitigated by compressing galaxy groups before skeleton extraction. The cosmic web reconstruction is biased when relying solely on H $Œ±$ emitters. This limitation can be mitigated by applying stellar mass weighting during the reconstruction. However, this approach introduces non-trivial biases that need to be accounted for when comparing to theoretical predictions. Redshift uncertainties pose the greatest challenge in recovering the expected dependence of galaxy properties, though the well-established stellar mass transverse gradients towards filaments can still be observed.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10709v1">üìÑ Download PDF</a></p><hr><h3 id=increasing-the-opening-speed-of-the-plasma-opening-switch-on-an-direct-action-accelerator-with-an-inductive-energy-storage-devicehttpsarxivorgabs260110694v1><a href=https://arxiv.org/abs/2601.10694v1>Increasing the opening speed of the plasma opening switch on an direct action accelerator with an inductive energy storage device</a><a hidden class=anchor aria-hidden=true href=#increasing-the-opening-speed-of-the-plasma-opening-switch-on-an-direct-action-accelerator-with-an-inductive-energy-storage-devicehttpsarxivorgabs260110694v1>#</a></h3><p><strong>Authors:</strong> D. V. Vinnikov, O. M. Ozerov, V. V. Katrechko, V. I. Tkachov, O. V. Manuilenko, I. N. Onishchenko
<strong>Venue:</strong> arXiv (2026)</p><p>To increase the voltage multiplication factor in a small-sized direct-acting electron accelerator DIN-2K with an inductive energy storage and a plasma opening switch, it is necessary to ensure an increase in the rate of change of the current and its amplitude during the POS opening for the purpose of obtaining an explosive electron emission with the formation of an electron beam and a virtual cathode. However, the opening process depends on many electrical parameters of the plant, and it re-quires determining their joint and individual effect on its dynamics. The purpose of this research is to study and determine the effects of electrical parameters, including those of the discharge voltages of the entire electrical circuit of the accelerator on the dynamics of the plasma opening switch, in particular opening speed, current amplitudes, and opening time, as well as to give recommendations on optimizing the operation of accelerators of this type and highlight possible ways to increase the voltage multiplication factor. Methodology. A method for determining the induced voltage according to experimental current oscillograms has been proposed. The methodology was verified by the coincidence of its data with the results of measurements by a capacitive voltage divider with the data spread of less than 20%. Scientific novelty. The rates of change in the current at the plasma switch opening stage were determined depending on the main electrical parameters of the DIN-2K accelerator. The diagnostics of the voltage induced during the POS opening were provided using no capacitive voltage divider in the internal volume of the accelerator, which enabled the removal of some diagnostic tools from the working volume of the chamber. Practical significance.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10694v1">üìÑ Download PDF</a></p><hr><h3 id=synchronizing-probabilities-in-model-driven-lossless-compressionhttpsarxivorgabs260110678v1><a href=https://arxiv.org/abs/2601.10678v1>Synchronizing Probabilities in Model-Driven Lossless Compression</a><a hidden class=anchor aria-hidden=true href=#synchronizing-probabilities-in-model-driven-lossless-compressionhttpsarxivorgabs260110678v1>#</a></h3><p><strong>Authors:</strong> Aviv Adler, Jennifer Tang
<strong>Venue:</strong> arXiv (2026)</p><p>It is well-known in the field of lossless data compression that probabilistic next-symbol prediction can be used to compress sequences of symbols. Deep neural networks are able to capture rich dependencies in data, offering a powerful means of estimating these probabilities and hence an avenue towards more effective compression algorithms. However, both compressor and decompressor must have exactly matching predictions; even small non-deterministic differences (which often happen with learned models due to hardware, software, or computation order) can lead to cascading decoding failures. In this paper, we formalize the problem of prediction mismatch in model-driven compression, and introduce Probability Matching Interval Coding (PMATIC), a model-agnostic algorithm that tolerates bounded prediction mismatch with low overhead. PMATIC works with the predicted probabilities, making it compatible as a drop-in replacement for the arithmetic encoder in model-driven compression tools. We show theoretical correctness and performance bounds for PMATIC, and validate these results on text data. These results confirm that, when paired an advanced prediction model, PMATIC is robust to prediction mismatch while achieving compression rates that out-perform standard modern compression tools.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10678v1">üìÑ Download PDF</a></p><hr><h3 id=learning-from-brain-topography-a-hierarchical-local-global-graph-transformer-network-for-eeg-emotion-recognitionhttpsarxivorgabs260110525v1><a href=https://arxiv.org/abs/2601.10525v1>Learning from Brain Topography: A Hierarchical Local-Global Graph-Transformer Network for EEG Emotion Recognition</a><a hidden class=anchor aria-hidden=true href=#learning-from-brain-topography-a-hierarchical-local-global-graph-transformer-network-for-eeg-emotion-recognitionhttpsarxivorgabs260110525v1>#</a></h3><p><strong>Authors:</strong> Yijin Zhou, Fu Li, Yi Niu, Boxun Fu, Huaning Wang, Lijian Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>Understanding how local neurophysiological patterns interact with global brain dynamics is essential for decoding human emotions from EEG signals. However, existing deep learning approaches often overlook the brain&rsquo;s intrinsic spatial organization, failing to simultaneously capture local topological relations and global dependencies. To address these challenges, we propose Neuro-HGLN, a Neurologically-informed Hierarchical Graph-Transformer Learning Network that integrates biologically grounded priors with hierarchical representation learning. Neuro-HGLN first constructs a spatial Euclidean prior graph based on physical electrode distances to serve as an anatomically grounded inductive bias. A learnable global dynamic graph is then introduced to model functional connectivity across the entire brain. In parallel, to capture fine-grained regional dependencies, Neuro-HGLN builds region-level local graphs using a multi-head self-attention mechanism. These graphs are processed synchronously through local-constrained parallel GCN layers to produce region-specific representations. Subsequently, an iTransformer encoder aggregates these features to capture cross-region dependencies under a dimension-as-token formulation. Extensive experiments demonstrate that Neuro-HGLN achieves state-of-the-art performance on multiple benchmarks, providing enhanced interpretability grounded in neurophysiological structure. These results highlight the efficacy of unifying local topological learning with cross-region dependency modeling for robust EEG emotion recognition.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10525v1">üìÑ Download PDF</a></p><hr><h3 id=tf3-ro-50m-training-compact-romanian-language-models-from-scratch-on-synthetic-moral-microfictionhttpsarxivorgabs260110410v1><a href=https://arxiv.org/abs/2601.10410v1>TF3-RO-50M: Training Compact Romanian Language Models from Scratch on Synthetic Moral Microfiction</a><a hidden class=anchor aria-hidden=true href=#tf3-ro-50m-training-compact-romanian-language-models-from-scratch-on-synthetic-moral-microfictionhttpsarxivorgabs260110410v1>#</a></h3><p><strong>Authors:</strong> Mihai Dan Nadas, Laura Diosan, Andreea Tomescu, Andrei Piscoran
<strong>Venue:</strong> arXiv (2026)</p><p>Recent advances in synthetic data generation have shown that compact language models can be trained effectively when the underlying corpus is structurally controlled and linguistically coherent. However, for morphologically rich and computationally under-resourced languages such as Romanian, there is still no openly documented, end-to-end pipeline that unifies tokenizer design, preprocessing, pretraining, compression, evaluation, and large-scale synthetic data generation in a reproducible framework. Building on TF1, a three-million-story English fable dataset, and TF2, which extends TF1 through high-quality Romanian translations, we introduce TF3-RO, a Romanian-centric language modeling pipeline spanning tokenizer training, from-scratch model development, and Romanian-native dataset generation. TF3-RO constructs Romanian-specific BPE and Unigram tokenizers from a linguistically informed corpus to mitigate token inflation induced by Romanian morphology. Using long-sequence packed training, we pretrain a 51.65M-parameter LLaMA-style Transformer entirely from scratch. The model is subsequently optimized through quantization, structured pruning, and logit-based knowledge distillation, yielding a compact 26.45M-parameter student model with tied embeddings and strong deployment characteristics. Using this distilled model, TF3-RO generates three million Romanian-native synthetic fables via a controlled combinatorial prompting framework. Across all stages, the pipeline integrates a comprehensive evaluation suite combining intrinsic metrics, Romanian agreement probes, entity coherence, rule-based grammar checking, and LLM-based assessment. TF3-RO provides a reproducible and linguistically grounded framework for training compact Romanian language models and producing large-scale synthetic narrative corpora.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10410v1">üìÑ Download PDF</a></p><hr><h3 id=an-analytic-theory-of-convolutional-neural-network-inverse-problems-solvershttpsarxivorgabs260110334v1><a href=https://arxiv.org/abs/2601.10334v1>An analytic theory of convolutional neural network inverse problems solvers</a><a hidden class=anchor aria-hidden=true href=#an-analytic-theory-of-convolutional-neural-network-inverse-problems-solvershttpsarxivorgabs260110334v1>#</a></h3><p><strong>Authors:</strong> Minh Hai Nguyen, Quoc Bao Do, Edouard Pauwels, Pierre Weiss
<strong>Venue:</strong> arXiv (2026)</p><p>Supervised convolutional neural networks (CNNs) are widely used to solve imaging inverse problems, achieving state-of-the-art performance in numerous applications. However, despite their empirical success, these methods are poorly understood from a theoretical perspective and often treated as black boxes. To bridge this gap, we analyze trained neural networks through the lens of the Minimum Mean Square Error (MMSE) estimator, incorporating functional constraints that capture two fundamental inductive biases of CNNs: translation equivariance and locality via finite receptive fields. Under the empirical training distribution, we derive an analytic, interpretable, and tractable formula for this constrained variant, termed Local-Equivariant MMSE (LE-MMSE). Through extensive numerical experiments across various inverse problems (denoising, inpainting, deconvolution), datasets (FFHQ, CIFAR-10, FashionMNIST), and architectures (U-Net, ResNet, PatchMLP), we demonstrate that our theory matches the neural networks outputs (PSNR $\gtrsim25$dB). Furthermore, we provide insights into the differences between \emph{physics-aware} and \emph{physics-agnostic} estimators, the impact of high-density regions in the training (patch) distribution, and the influence of other factors (dataset size, patch size, etc).</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10334v1">üìÑ Download PDF</a></p><hr><h3 id=effect-of-hole-pitch-reduction-on-electron-transport-and-diffusion-a-comparative-simulation-study-of-triple-gem-detectorshttpsarxivorgabs260110139v1><a href=https://arxiv.org/abs/2601.10139v1>Effect of hole pitch reduction on electron transport and diffusion: A comparative simulation study of Triple GEM detectors</a><a hidden class=anchor aria-hidden=true href=#effect-of-hole-pitch-reduction-on-electron-transport-and-diffusion-a-comparative-simulation-study-of-triple-gem-detectorshttpsarxivorgabs260110139v1>#</a></h3><p><strong>Authors:</strong> Rajiv Gupta, Sunidhi Saxena, Ajay Kumar
<strong>Venue:</strong> arXiv (2026)</p><p>Advances in fabrication techniques and high-performance electronics have facilitated the development of fine-pitch Gas Electron Multipliers (GEMs). Earlier experimental and simulation findings suggest that these reduced-pitch GEMs can outperform the standard configuration in terms of effective gain, collection efficiency, and position resolution. However, a noticeable fraction of avalanche electrons is lost within the GEM systems, resulting in a degradation of charge collection efficiency. Therefore, a comprehensive simulation-based study is essential to provide deeper insights into the extent of degradation and its contributing factors. In this context, we employ ANSYS and Garfield++ to model the Triple GEM detectors with reduced pitch sizes of 90 and 60 $Œº$m, and perform a comparative performance analysis with the standard configuration (pitch size: 140 $Œº$m). At first, the simulation framework is validated by comparing the results of the standard configuration with available experimental data and previously reported simulation outcomes. Despite the characteristic gain offset, the framework remains physically consistent and reliable in capturing microscopic avalanche dynamics, reproducing the experimental trend. Following validation, we investigate electron losses at the metal electrodes and within the Kapton holes, electron transmission through the transfer and induction regions, electron diffusion on the induction electrode, and the overall collection efficiency. These parameters are analyzed as functions of GEM potential, outer hole diameter, inner hole diameter, Kapton thickness, metal thickness, and gas composition, thereby offering insights for designing efficient GEM detectors.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10139v1">üìÑ Download PDF</a></p><hr><h3 id=deriving-character-logic-from-storyline-as-codified-decision-treeshttpsarxivorgabs260110080v1><a href=https://arxiv.org/abs/2601.10080v1>Deriving Character Logic from Storyline as Codified Decision Trees</a><a hidden class=anchor aria-hidden=true href=#deriving-character-logic-from-storyline-as-codified-decision-treeshttpsarxivorgabs260110080v1>#</a></h3><p><strong>Authors:</strong> Letian Peng, Kun Zhou, Longfei Yun, Yupeng Hou, Jingbo Shang
<strong>Venue:</strong> arXiv (2026)</p><p>Role-playing (RP) agents rely on behavioral profiles to act consistently across diverse narrative contexts, yet existing profiles are largely unstructured, non-executable, and weakly validated, leading to brittle agent behavior. We propose Codified Decision Trees (CDT), a data-driven framework that induces an executable and interpretable decision structure from large-scale narrative data. CDT represents behavioral profiles as a tree of conditional rules, where internal nodes correspond to validated scene conditions and leaves encode grounded behavioral statements, enabling deterministic retrieval of context-appropriate rules at execution time. The tree is learned by iteratively inducing candidate scene-action rules, validating them against data, and refining them through hierarchical specialization, yielding profiles that support transparent inspection and principled updates. Across multiple benchmarks, CDT substantially outperforms human-written profiles and prior profile induction methods on $85$ characters across $16$ artifacts, indicating that codified and validated behavioral representations lead to more reliable agent grounding.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10080v1">üìÑ Download PDF</a></p><hr><h2 id=-psycholinguistics>üîç psycholinguistics<a hidden class=anchor aria-hidden=true href=#-psycholinguistics>#</a></h2><h3 id=data-driven-stochastic-reduced-order-modeling-of-parametrized-dynamical-systemshttpsarxivorgabs260110690v1><a href=https://arxiv.org/abs/2601.10690v1>Data-driven stochastic reduced-order modeling of parametrized dynamical systems</a><a hidden class=anchor aria-hidden=true href=#data-driven-stochastic-reduced-order-modeling-of-parametrized-dynamical-systemshttpsarxivorgabs260110690v1>#</a></h3><p><strong>Authors:</strong> Andrew F. Ilersich, Kevin Course, Prasanth B. Nair
<strong>Venue:</strong> arXiv (2026)</p><p>Modeling complex dynamical systems under varying conditions is computationally intensive, often rendering high-fidelity simulations intractable. Although reduced-order models (ROMs) offer a promising solution, current methods often struggle with stochastic dynamics and fail to quantify prediction uncertainty, limiting their utility in robust decision-making contexts. To address these challenges, we introduce a data-driven framework for learning continuous-time stochastic ROMs that generalize across parameter spaces and forcing conditions. Our approach, based on amortized stochastic variational inference, leverages a reparametrization trick for Markov Gaussian processes to eliminate the need for computationally expensive forward solvers during training. This enables us to jointly learn a probabilistic autoencoder and stochastic differential equations governing the latent dynamics, at a computational cost that is independent of the dataset size and system stiffness. Additionally, our approach offers the flexibility of incorporating physics-informed priors if available. Numerical studies are presented for three challenging test problems, where we demonstrate excellent generalization to unseen parameter combinations and forcings, and significant efficiency gains compared to existing approaches.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10690v1">üìÑ Download PDF</a></p><hr><h3 id=breaking-the-storage-bandwidth-tradeoff-in-distributed-storage-with-quantum-entanglementhttpsarxivorgabs260110676v1><a href=https://arxiv.org/abs/2601.10676v1>Breaking the Storage-Bandwidth Tradeoff in Distributed Storage with Quantum Entanglement</a><a hidden class=anchor aria-hidden=true href=#breaking-the-storage-bandwidth-tradeoff-in-distributed-storage-with-quantum-entanglementhttpsarxivorgabs260110676v1>#</a></h3><p><strong>Authors:</strong> Lei Hu, Mohamed Nomeir, Alptug Aytekin, Sennur Ulukus
<strong>Venue:</strong> arXiv (2026)</p><p>This work investigates the use of quantum resources in distributed storage systems. Consider an $(n,k,d)$ distributed storage system in which a file is stored across $n$ nodes such that any $k$ nodes suffice to reconstruct the file. When a node fails, any $d$ helper nodes transmit information to a newcomer to rebuild the system. In contrast to the classical repair, where helper nodes transmit classical bits, we allow them to send classical information over quantum channels to the newcomer. The newcomer then generates its storage by performing appropriate measurements on the received quantum states. In this setting, we fully characterize the fundamental tradeoff between storage and repair bandwidth (total communication cost). Compared to classical systems, the optimal storage&ndash;bandwidth tradeoff can be significantly improved with the enhancement of quantum entanglement shared only among the surviving nodes, particularly at the minimum-storage regenerating point. Remarkably, we show that when $d \geq 2k-2$, there exists an operating point at which \textit{both storage and repair bandwidth are simultaneously minimized}. This phenomenon breaks the tradeoff in the classical setting and reveals a fundamentally new regime enabled by quantum communication.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10676v1">üìÑ Download PDF</a></p><hr><h3 id=sporadic-creutzfeldt-jakob-disease-presenting-with-cerebral-atrophy-following-traumatic-brain-injury-mimicking-hydrocephalus-a-case-report-and-literature-reviewhttpsarxivorgabs260110663v1><a href=https://arxiv.org/abs/2601.10663v1>Sporadic Creutzfeldt Jakob disease presenting with cerebral atrophy following traumatic brain injury mimicking hydrocephalus a case report and literature review</a><a hidden class=anchor aria-hidden=true href=#sporadic-creutzfeldt-jakob-disease-presenting-with-cerebral-atrophy-following-traumatic-brain-injury-mimicking-hydrocephalus-a-case-report-and-literature-reviewhttpsarxivorgabs260110663v1>#</a></h3><p><strong>Authors:</strong> Chun Zeng, Dezhu Gao, Liang Wu
<strong>Venue:</strong> arXiv (2026)</p><p>Introduction Sporadic Creutzfeldt Jakob disease sCJD is a rapidly progressive neurodegenerative disease without effective treatment that usually results in death within one year. The recently applied methods have improved the accuracy of the disease diagnosis and the specific radiological findings provide the necessary information for differential diagnosis. Research question The research is aimed to provide a different perspective on the development of CJD and associated literature review. Materials and methods The study presents a case who presented cognitive deficits, gait instability, and urinary and fecal incontinence suffered from traumatic brain injury eight months ago before admission with cerebral ventricle dilation on CT images. Furthermore, studies describe relevant cases are also included. Results The patients symptoms got deteriorated. Further examinations, including 14-3-3 and tau proteins in the cerebrospinal fluid CSF, MRI, and EEG, confirmed the patients diagnosis of sCJD. He returned to the local hospital for the conservative treatment without effective medical intervention. Conclusion This case illustrates the diagnostic process of CJD and underscores the importance of distinguishing rare disorders from common conditions to achieve a comprehensive understanding of the disease.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10663v1">üìÑ Download PDF</a></p><hr><h3 id=pacevolve-enabling-long-horizon-progress-aware-consistent-evolutionhttpsarxivorgabs260110657v1><a href=https://arxiv.org/abs/2601.10657v1>PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution</a><a hidden class=anchor aria-hidden=true href=#pacevolve-enabling-long-horizon-progress-aware-consistent-evolutionhttpsarxivorgabs260110657v1>#</a></h3><p><strong>Authors:</strong> Minghao Yan, Bo Peng, Benjamin Coleman, Ziqi Chen, Zhouhang Xie, Zhankui He, Noveen Sachdeva, Isabella Ye, Weili Wang, Chi Wang, Ed H. Chi, Wang-Cheng Kang, Derek Zhiyuan Cheng, Beidou Wang
<strong>Venue:</strong> arXiv (2026)</p><p>Large Language Models (LLMs) have emerged as powerful operators for evolutionary search, yet the design of efficient search scaffolds remains ad hoc. While promising, current LLM-in-the-loop systems lack a systematic approach to managing the evolutionary process. We identify three distinct failure modes: Context Pollution, where experiment history biases future candidate generation; Mode Collapse, where agents stagnate in local minima due to poor exploration-exploitation balance; and Weak Collaboration, where rigid crossover strategies fail to leverage parallel search trajectories effectively. We introduce Progress-Aware Consistent Evolution (PACEvolve), a framework designed to robustly govern the agent&rsquo;s context and search dynamics, to address these challenges. PACEvolve combines hierarchical context management (HCM) with pruning to address context pollution; momentum-based backtracking (MBB) to escape local minima; and a self-adaptive sampling policy that unifies backtracking and crossover for dynamic search coordination (CE), allowing agents to balance internal refinement with cross-trajectory collaboration. We demonstrate that PACEvolve provides a systematic path to consistent, long-horizon self-improvement, achieving state-of-the-art results on LLM-SR and KernelBench, while discovering solutions surpassing the record on Modded NanoGPT.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10657v1">üìÑ Download PDF</a></p><hr><h3 id=are-your-reasoning-models-reasoning-or-guessing-a-mechanistic-analysis-of-hierarchical-reasoning-modelshttpsarxivorgabs260110679v1><a href=https://arxiv.org/abs/2601.10679v1>Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models</a><a hidden class=anchor aria-hidden=true href=#are-your-reasoning-models-reasoning-or-guessing-a-mechanistic-analysis-of-hierarchical-reasoning-modelshttpsarxivorgabs260110679v1>#</a></h3><p><strong>Authors:</strong> Zirui Ren, Ziming Liu
<strong>Venue:</strong> arXiv (2026)</p><p>Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) &ldquo;Grokking&rdquo; dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM &ldquo;guesses&rdquo; the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be &ldquo;guessing&rdquo; instead of &ldquo;reasoning&rdquo;. Leveraging this &ldquo;guessing&rdquo; picture, we propose three strategies to scale HRM&rsquo;s guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models &ldquo;reason&rdquo;.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10679v1">üìÑ Download PDF</a></p><hr><h3 id=sus-strategy-aware-surprise-for-intrinsic-explorationhttpsarxivorgabs260110349v1><a href=https://arxiv.org/abs/2601.10349v1>SuS: Strategy-aware Surprise for Intrinsic Exploration</a><a hidden class=anchor aria-hidden=true href=#sus-strategy-aware-surprise-for-intrinsic-explorationhttpsarxivorgabs260110349v1>#</a></h3><p><strong>Authors:</strong> Mark Kashirskiy, Ilya Makarov
<strong>Venue:</strong> arXiv (2026)</p><p>We propose Strategy-aware Surprise (SuS), a novel intrinsic motivation framework that uses pre-post prediction mismatch as a novelty signal for exploration in reinforcement learning. Unlike traditional curiosity-driven methods that rely solely on state prediction error, SuS introduces two complementary components: Strategy Stability (SS) and Strategy Surprise (SuS). SS measures consistency in behavioral strategy across temporal steps, while SuS captures unexpected outcomes relative to the agent&rsquo;s current strategy representation. Our combined reward formulation leverages both signals through learned weighting coefficients. We evaluate SuS on mathematical reasoning tasks using large language models, demonstrating significant improvements in both accuracy and solution diversity. Ablation studies confirm that removing either component results in at least 10% performance degradation, validating the synergistic nature of our approach. SuS achieves 17.4% improvement in Pass@1 and 26.4% improvement in Pass@5 compared to baseline methods, while maintaining higher strategy diversity throughout training.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10349v1">üìÑ Download PDF</a></p><hr><h3 id=clozing-the-gap-exploring-why-language-model-surprisal-outperforms-cloze-surprisalhttpsarxivorgabs260109886v1><a href=https://arxiv.org/abs/2601.09886v1>Clozing the Gap: Exploring Why Language Model Surprisal Outperforms Cloze Surprisal</a><a hidden class=anchor aria-hidden=true href=#clozing-the-gap-exploring-why-language-model-surprisal-outperforms-cloze-surprisalhttpsarxivorgabs260109886v1>#</a></h3><p><strong>Authors:</strong> Sathvik Nair, Byung-Doh Oh
<strong>Venue:</strong> arXiv (2026)</p><p>How predictable a word is can be quantified in two ways: using human responses to the cloze task or using probabilities from language models (LMs).When used as predictors of processing effort, LM probabilities outperform probabilities derived from cloze data. However, it is important to establish that LM probabilities do so for the right reasons, since different predictors can lead to different scientific conclusions about the role of prediction in language comprehension. We present evidence for three hypotheses about the advantage of LM probabilities: not suffering from low resolution, distinguishing semantically similar words, and accurately assigning probabilities to low-frequency words. These results call for efforts to improve the resolution of cloze studies, coupled with experiments on whether human-like prediction is also as sensitive to the fine-grained distinctions made by LM probabilities.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.09886v1">üìÑ Download PDF</a></p><hr><h3 id=integrating-diverse-assignment-strategies-into-detrshttpsarxivorgabs260109247v1><a href=https://arxiv.org/abs/2601.09247v1>Integrating Diverse Assignment Strategies into DETRs</a><a hidden class=anchor aria-hidden=true href=#integrating-diverse-assignment-strategies-into-detrshttpsarxivorgabs260109247v1>#</a></h3><p><strong>Authors:</strong> Yiwei Zhang, Jin Gao, Hanshi Wang, Fudong Ge, Guan Luo, Weiming Hu, Zhipeng Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>Label assignment is a critical component in object detectors, particularly within DETR-style frameworks where the one-to-one matching strategy, despite its end-to-end elegance, suffers from slow convergence due to sparse supervision. While recent works have explored one-to-many assignments to enrich supervisory signals, they often introduce complex, architecture-specific modifications and typically focus on a single auxiliary strategy, lacking a unified and scalable design. In this paper, we first systematically investigate the effects of <code>one-to-many'' supervision and reveal a surprising insight that performance gains are driven not by the sheer quantity of supervision, but by the diversity of the assignment strategies employed. This finding suggests that a more elegant, parameter-efficient approach is attainable. Building on this insight, we propose LoRA-DETR, a flexible and lightweight framework that seamlessly integrates diverse assignment strategies into any DETR-style detector. Our method augments the primary network with multiple Low-Rank Adaptation (LoRA) branches during training, each instantiating a different one-to-many assignment rule. These branches act as auxiliary modules that inject rich, varied supervisory gradients into the main model and are discarded during inference, thus incurring no additional computational cost. This design promotes robust joint optimization while maintaining the architectural simplicity of the original detector. Extensive experiments on different baselines validate the effectiveness of our approach. Our work presents a new paradigm for enhancing detectors, demonstrating that diverse </code>one-to-many&rsquo;&rsquo; supervision can be integrated to achieve state-of-the-art results without compromising model elegance.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.09247v1">üìÑ Download PDF</a></p><hr><h3 id=why-ai-alignment-failure-is-structural-learned-human-interaction-structures-and-agi-as-an-endogenous-evolutionary-shockhttpsarxivorgabs260108673v1><a href=https://arxiv.org/abs/2601.08673v1>Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock</a><a hidden class=anchor aria-hidden=true href=#why-ai-alignment-failure-is-structural-learned-human-interaction-structures-and-agi-as-an-endogenous-evolutionary-shockhttpsarxivorgabs260108673v1>#</a></h3><p><strong>Authors:</strong> Didier Sornette, Sandro Claudio Lera, Ke Wu
<strong>Venue:</strong> arXiv (2026)</p><p>Recent reports of large language models (LLMs) exhibiting behaviors such as deception, threats, or blackmail are often interpreted as evidence of alignment failure or emergent malign agency. We argue that this interpretation rests on a conceptual error. LLMs do not reason morally; they statistically internalize the record of human social interaction, including laws, contracts, negotiations, conflicts, and coercive arrangements. Behaviors commonly labeled as unethical or anomalous are therefore better understood as structural generalizations of interaction regimes that arise under extreme asymmetries of power, information, or constraint. Drawing on relational models theory, we show that practices such as blackmail are not categorical deviations from normal social behavior, but limiting cases within the same continuum that includes market pricing, authority relations, and ultimatum bargaining. The surprise elicited by such outputs reflects an anthropomorphic expectation that intelligence should reproduce only socially sanctioned behavior, rather than the full statistical landscape of behaviors humans themselves enact. Because human morality is plural, context-dependent, and historically contingent, the notion of a universally moral artificial intelligence is ill-defined. We therefore reframe concerns about artificial general intelligence (AGI). The primary risk is not adversarial intent, but AGI&rsquo;s role as an endogenous amplifier of human intelligence, power, and contradiction. By eliminating longstanding cognitive and institutional frictions, AGI compresses timescales and removes the historical margin of error that has allowed inconsistent values and governance regimes to persist without collapse. Alignment failure is thus structural, not accidental, and requires governance approaches that address amplification, complexity, and regime stability rather than model-level intent alone.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.08673v1">üìÑ Download PDF</a></p><hr><h3 id=quantum-observers-can-communicate-across-multiverse-brancheshttpsarxivorgabs260108102v1><a href=https://arxiv.org/abs/2601.08102v1>Quantum observers can communicate across multiverse branches</a><a hidden class=anchor aria-hidden=true href=#quantum-observers-can-communicate-across-multiverse-brancheshttpsarxivorgabs260108102v1>#</a></h3><p><strong>Authors:</strong> Maria Violaris
<strong>Venue:</strong> arXiv (2026)</p><p>It is commonly thought that observers in distinct branches of an Everettian multiverse cannot communicate without violating the linearity of quantum theory. Here we show a counterexample, demonstrating that inter-branch communication is in fact possible, entirely within standard quantum theory. We do this by considering a Wigner&rsquo;s-friend scenario, where an observer (Wigner) can have quantum control over another observer (the friend). We present a thought experiment where the friend in superposition can receive a message written by a distinct copy of themselves in the multiverse, with the aid of Wigner. To maintain the unitarity of quantum theory, the observers must have no memory of the message that they sent. Our thought experiment challenges conventional wisdom regarding the ultimate limits of what is possible in an Everettian multiverse. It has a surprising potential application which involves using knowledge-creation paradoxes for testing Everettian quantum theory against single-world theories.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.08102v1">üìÑ Download PDF</a></p><hr><h3 id=quantum-maxwell-erasure-decoder-for-qldpc-codeshttpsarxivorgabs260110713v1><a href=https://arxiv.org/abs/2601.10713v1>Quantum Maxwell Erasure Decoder for qLDPC codes</a><a hidden class=anchor aria-hidden=true href=#quantum-maxwell-erasure-decoder-for-qldpc-codeshttpsarxivorgabs260110713v1>#</a></h3><p><strong>Authors:</strong> Bruno Costa Alves Freire, Fran√ßois-Marie Le R√©gent, Anthony Leverrier
<strong>Venue:</strong> arXiv (2026)</p><p>We introduce a quantum Maxwell erasure decoder for CSS quantum low-density parity-check (qLDPC) codes that extends peeling with bounded guessing. Guesses are tracked symbolically and can be eliminated by restrictive checks, giving a tunable tradeoff between complexity and performance via a guessing budget: an unconstrained budget recovers Maximum-Likelihood (ML) performance, while a constant budget yields linear-time decoding and approximates ML. We provide theoretical guarantees on asymptotic performance and demonstrate strong performance on bivariate bicycle and quantum Tanner codes.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10713v1">üìÑ Download PDF</a></p><hr><h3 id=constant-depth-unitary-preparation-of-dicke-stateshttpsarxivorgabs260110693v1><a href=https://arxiv.org/abs/2601.10693v1>Constant-Depth Unitary Preparation of Dicke States</a><a hidden class=anchor aria-hidden=true href=#constant-depth-unitary-preparation-of-dicke-stateshttpsarxivorgabs260110693v1>#</a></h3><p><strong>Authors:</strong> Francisca Vasconcelos, Malvika Raj Joshi
<strong>Venue:</strong> arXiv (2026)</p><p>Dicke states serve as a critical resource in quantum metrology, communication, and computation. However, unitary preparation of Dicke states is limited to logarithmic depth in standard circuit models and existing constant-depth protocols require measurement and feed-forward. In this work, we present the first unitary, constant-depth protocols for exact Dicke state preparation. We overcome the logarithmic-depth barrier by moving beyond the standard circuit model and leveraging global interactions (native to architectures such as neutral atoms and trapped ions). Specifically, utilizing unbounded CZ gates (i.e. within the QAC$^0$ circuit class), we offer circuits for exact computation of constant-weight Dicke states, using polynomial ancillae, and approximation of weight-1 Dicke states (i.e. $W$ states), using only constant ancillae. Granted additional access to the quantum FAN-OUT operation (i.e. upgrading to the QAC$_f^0$ circuit class), we also achieve exact preparation of arbitrary-weight Dicke states, with polynomial ancillae. These protocols distinguish the constant-depth capabilities of quantum architectures based on connectivity and offer a novel path toward resolving a long-standing quantum complexity conjecture.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10693v1">üìÑ Download PDF</a></p><hr><h3 id=the-conversational-exam-a-scalable-assessment-design-for-the-ai-erahttpsarxivorgabs260110691v1><a href=https://arxiv.org/abs/2601.10691v1>The Conversational Exam: A Scalable Assessment Design for the AI Era</a><a hidden class=anchor aria-hidden=true href=#the-conversational-exam-a-scalable-assessment-design-for-the-ai-erahttpsarxivorgabs260110691v1>#</a></h3><p><strong>Authors:</strong> Lorena A. Barba, Laura Stegner
<strong>Venue:</strong> arXiv (2026)</p><p>Traditional assessment methods collapse when students use generative AI to complete work without genuine engagement, creating an illusion of competence where they believe they&rsquo;re learning but aren&rsquo;t. This paper presents the conversational exam &ndash; a scalable oral examination format that restores assessment validity by having students code live while explaining their reasoning. Drawing on human-computer interaction principles, we examined 58 students in small groups across just two days, demonstrating that oral exams can scale to typical class sizes. The format combines authentic practice (students work with documentation and supervised AI access) with inherent validity (real-time performance cannot be faked). We provide detailed implementation guidance to help instructors adapt this approach, offering a practical path forward when many educators feel paralyzed between banning AI entirely or accepting that valid assessment is impossible.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10691v1">üìÑ Download PDF</a></p><hr><h3 id=efficiency-curvature-and-complexity-of-quantum-evolutions-for-qubits-in-nonstationary-magnetic-fieldshttpsarxivorgabs260110672v1><a href=https://arxiv.org/abs/2601.10672v1>Efficiency, Curvature, and Complexity of Quantum Evolutions for Qubits in Nonstationary Magnetic Fields</a><a hidden class=anchor aria-hidden=true href=#efficiency-curvature-and-complexity-of-quantum-evolutions-for-qubits-in-nonstationary-magnetic-fieldshttpsarxivorgabs260110672v1>#</a></h3><p><strong>Authors:</strong> Carlo Cafaro, James Schneeloch
<strong>Venue:</strong> arXiv (2026)</p><p>In optimal quantum-mechanical evolutions, motion can take place along paths of minimal length within an optimal time frame. Alternatively, optimal evolutions may occur along established paths without any waste of energy resources and achieving 100% speed efficiency. Unfortunately, realistic physical scenarios often lead to less-than-ideal evolutions that demonstrate suboptimal efficiency, nonzero curvature, and a high level of complexity. In this paper, we provide an exact analytical expression for the curvature of a quantum evolution pertaining to a two-level quantum system subjected to various time-dependent magnetic fields. Specifically, we examine the dynamics produced by a two-parameter nonstationary Hermitian Hamiltonian with unit speed efficiency. To enhance our understanding of the physical implications of the curvature coefficient, we analyze the curvature behavior in relation to geodesic efficiency, speed efficiency, and the complexity of the quantum evolution (as described by the ratio of the difference between accessible and accessed Bloch-sphere volumes for the evolution from initial to final state to the accessible volume for the given quantum evolution). Our findings indicate that, generally, efficient quantum evolutions exhibit lower complexity compared to inefficient ones. However, we also note that complexity transcends mere length. In fact, longer paths that are sufficiently curved can demonstrate a complexity that is less than that of shorter paths with a lower curvature coefficient.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10672v1">üìÑ Download PDF</a></p><hr><h3 id=transformer-based-cognitive-radio-adaptive-modulation-strategies-using-transformer-modelshttpsarxivorgabs260110519v1><a href=https://arxiv.org/abs/2601.10519v1>Transformer-Based Cognitive Radio: Adaptive Modulation Strategies Using Transformer Models</a><a hidden class=anchor aria-hidden=true href=#transformer-based-cognitive-radio-adaptive-modulation-strategies-using-transformer-modelshttpsarxivorgabs260110519v1>#</a></h3><p><strong>Authors:</strong> Andrea Melis, Andrea Piroddi, Roberto Girau
<strong>Venue:</strong> arXiv (2026)</p><p>Cognitive Radio (CR) systems, which dynamically adapt to changing spectrum environments, could benefit significantly from advancements in machine learning technologies. These systems can be enhanced in terms of spectral efficiency, robustness, and security through innovative approaches such as the use of Transformer models. This work investigates the application of Transformer models, specifically the GPT-2 architecture, to generate novel modulation schemes for wireless communications. By training a GPT-2 model on a dataset of existing modulation formulas, new modulation schemes has been created. These generated schemes are then compared to traditional methods using key performance metrics such as Signal-to-Noise Ratio (SNR) and Power Spectrum Density (PSD). The results show that Transformer-generated modulation schemes can achieve performance comparable to, and in some cases outperforming, traditional methods. This demonstrates that advanced CR systems could greatly benefit from the implementation of Transformer models, leading to more efficient, robust, and secure communication systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10519v1">üìÑ Download PDF</a></p><hr><h3 id=testing-three-models-of-cognitive-stress-effects-a-psychopharmacological-randomized-controlled-trial-of-acute-stress-and-stress-hormones-across-visual-perception-response-inhibition-and-cognitive-flexibilityhttpsarxivorgabs260110515v1><a href=https://arxiv.org/abs/2601.10515v1>Testing three models of cognitive stress effects: A psychopharmacological randomized controlled trial of acute stress and stress hormones across visual perception, response inhibition and cognitive flexibility</a><a hidden class=anchor aria-hidden=true href=#testing-three-models-of-cognitive-stress-effects-a-psychopharmacological-randomized-controlled-trial-of-acute-stress-and-stress-hormones-across-visual-perception-response-inhibition-and-cognitive-flexibilityhttpsarxivorgabs260110515v1>#</a></h3><p><strong>Authors:</strong> Lisa Weckesser, Charlotte Grosskopf, Benjamin Weber, Selen Soylu, Tanja Endrass, Robert Miller
<strong>Venue:</strong> arXiv (2026)</p><p>Acute stress alters cognitive performance, yet competing models make divergent predictions regarding the mechanisms, scope, and temporal dynamics of these effects. This large-scale randomized controlled trial tested predications from three influential stress-effect models using a broad cognitive task battery embedded within a psychopharmacological stress paradigm. Across 606 testing sessions, 303 healthy male participants completed both the Maastricht Acute Stress Test (MAST) and its non-stress control condition. To independently manipulate acute stress and stress hormone availability, participants were additionally randomized to receive atomoxetine (40 mg; to prolong norepinephrine availability), hydrocortisone (10 mg; to increase cortisol availability), or placebo. Cognitive performance was assessed over 80-minutes (post-stress) using tasks targeting visual perception (rapid serial visual presentation), response inhibition (stop-signal), and cognitive flexibility (dual and switch tasks). MAST exposure selectively impaired response inhibition, reflected in shorter stop-signal delays, lower probabilities of successful stopping and prolonged stop-signal reaction times, particularly during later testing phases (40-80 minutes post-stress). MAST exposure did not affect visual perception or task-switching performance but buffered time-related declines in processing efficiency at the expense of task prioritization in the dual task. Pharmacological manipulation of norepinephrine or cortisol availability was effective but did not moderate cognitive stress effects. Overall, this pattern of task-specific impairment alongside stabilized processing efficiency cannot be fully explained by any tested model, highlighting the need to refine existing models and adopt more integrative approaches to advance our mechanistic understanding of cognitive stress-effects in laboratory and real-world contexts.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10515v1">üìÑ Download PDF</a></p><hr><h3 id=convex-efficient-codinghttpsarxivorgabs260110482v1><a href=https://arxiv.org/abs/2601.10482v1>Convex Efficient Coding</a><a hidden class=anchor aria-hidden=true href=#convex-efficient-codinghttpsarxivorgabs260110482v1>#</a></h3><p><strong>Authors:</strong> William Dorrell, Peter E. Latham, James Whittington
<strong>Venue:</strong> arXiv (2026)</p><p>Why do neurons encode information the way they do? Normative answers to this question model neural activity as the solution to an optimisation problem; for example, the celebrated efficient coding hypothesis frames neural activity as the optimal encoding of information under efficiency constraints. Successful normative theories have varied dramatically in complexity, from simple linear models (Atick & Redlich &lsquo;90), to complex deep neural networks (Lindsay &lsquo;21). What complex models gain in flexibility, they lose in tractability and often understandability. Here, we split the difference by constructing a set of tractable but flexible normative representational theories. Instead of optimising the neural activities directly, following Sengupta et al. &lsquo;18, we optimise the representational similarity, a matrix formed from the dot products of each pair of neural responses. Using this, we show that a large family of interesting optimisation problems are convex. This family includes problems corresponding to linear and some non-linear neural networks, and problems from the literature not previously recognised as convex, such as modified versions of semi-nonnegative matrix factorisation or nonnegative sparse coding. We put these findings to work in three ways. First, we provide the first necessary and sufficient identifiability result for a form of semi-nonnegative matrix factorisation. Second, we show that if neural tunings are `different enough&rsquo; then they are uniquely linked to the optimal representational similarity, partially justifying the use of single neuron tuning analysis in neuroscience. Finally, we use the tractable nonlinearity of some of our problems to explain why dense retinal codes, but not sparse cortical codes, optimally split the coding of a single variable into ON & OFF channels. In sum, we identify a space of convex problems, and use them to derive neural coding results.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10482v1">üìÑ Download PDF</a></p><hr><h3 id=are-language-models-modelshttpsarxivorgabs260110421v1><a href=https://arxiv.org/abs/2601.10421v1>Are Language Models Models?</a><a hidden class=anchor aria-hidden=true href=#are-language-models-modelshttpsarxivorgabs260110421v1>#</a></h3><p><strong>Authors:</strong> Philip Resnik
<strong>Venue:</strong> arXiv (2026)</p><p>Futrell and Mahowald claim LMs &ldquo;serve as model systems&rdquo;, but an assessment at each of Marr&rsquo;s three levels suggests the claim is clearly not true at the implementation level, poorly motivated at the algorithmic-representational level, and problematic at the computational theory level. LMs are good candidates as tools; calling them cognitive models overstates the case and unnecessarily feeds LLM hype.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10421v1">üìÑ Download PDF</a></p><hr><h3 id=stem-scaling-transformers-with-embedding-moduleshttpsarxivorgabs260110639v1><a href=https://arxiv.org/abs/2601.10639v1>STEM: Scaling Transformers with Embedding Modules</a><a hidden class=anchor aria-hidden=true href=#stem-scaling-transformers-with-embedding-moduleshttpsarxivorgabs260110639v1>#</a></h3><p><strong>Authors:</strong> Ranajoy Sadhukhan, Sheng Cao, Harry Dong, Changsheng Zhao, Attiano Purpura-Pontoniere, Yuandong Tian, Zechun Liu, Beidi Chen
<strong>Venue:</strong> arXiv (2026)</p><p>Fine-grained sparsity promises higher parametric capacity without proportional per-token compute, but often suffers from training instability, load balancing, and communication overhead. We introduce STEM (Scaling Transformers with Embedding Modules), a static, token-indexed approach that replaces the FFN up-projection with a layer-local embedding lookup while keeping the gate and down-projection dense. This removes runtime routing, enables CPU offload with asynchronous prefetch, and decouples capacity from both per-token FLOPs and cross-device communication. Empirically, STEM trains stably despite extreme sparsity. It improves downstream performance over dense baselines while reducing per-token FLOPs and parameter accesses (eliminating roughly one-third of FFN parameters). STEM learns embedding spaces with large angular spread which enhances its knowledge storage capacity. More interestingly, this enhanced knowledge capacity comes with better interpretability. The token-indexed nature of STEM embeddings allows simple ways to perform knowledge editing and knowledge injection in an interpretable manner without any intervention in the input text or additional computation. In addition, STEM strengthens long-context performance: as sequence length grows, more distinct parameters are activated, yielding practical test-time capacity scaling. Across 350M and 1B model scales, STEM delivers up to ~3&ndash;4% accuracy improvements overall, with notable gains on knowledge and reasoning-heavy benchmarks (ARC-Challenge, OpenBookQA, GSM8K, MMLU). Overall, STEM is an effective way of scaling parametric memory while providing better interpretability, better training stability and improved efficiency.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10639v1">üìÑ Download PDF</a></p><hr><h3 id=a-new-construction-structure-on-multi-access-coded-caching-with-linear-subpacketization-cyclic-multi-access-non-half-sum-disjoint-packinghttpsarxivorgabs260110510v1><a href=https://arxiv.org/abs/2601.10510v1>A New Construction Structure on Multi-access Coded Caching with Linear Subpacketization: Cyclic Multi-Access Non-Half-Sum Disjoint Packing</a><a hidden class=anchor aria-hidden=true href=#a-new-construction-structure-on-multi-access-coded-caching-with-linear-subpacketization-cyclic-multi-access-non-half-sum-disjoint-packinghttpsarxivorgabs260110510v1>#</a></h3><p><strong>Authors:</strong> Mengyuan Li, Minquan Cheng, Kai Wan, Giuseppe Caire
<strong>Venue:</strong> arXiv (2026)</p><p>We consider the $(K,L,M,N)$ multi-access coded caching system introduced by Hachem et al., which consists of a central server with $N$ files and $K$ cache nodes, each of memory size $M$, where each user can access $L$ cache nodes in a cyclic wrap-around fashion. At present, several existing schemes achieve competitive transmission performance, but their subpacketization levels grow exponentially with the number of users. In contrast, schemes with linear or polynomial subpacketization always incur higher transmission loads. We aim to design a multi-access coded caching scheme with linear subpacketization $F$ while maintaining low transmission load. Recently, Cheng et al. proposed a construction framework for coded caching schemes with linear subpacketization (i.e., $F=K$) called non-half-sum disjoint packing (NHSDP). Inspired by this structure, we introduce a novel combinatorial structure named cyclic multi-access non-half-sum disjoint packing (CMA-NHSDP) by extending NHSDP to MACC system. By constructing CMA-NHSDP, we obtain a new class of multi-access coded caching schemes. Theoretical and numerical analyses show that our scheme achieves lower transmission loads than some existing schemes with linear subpacketization. Moreover, the proposed schemes achieves lower transmission load compared to existing schemes with exponential subpacketization in some case.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10510v1">üìÑ Download PDF</a></p><hr><h2 id=-llm>üîç llm<a hidden class=anchor aria-hidden=true href=#-llm>#</a></h2><h3 id=unbounded-symbols-heat-flow-and-toeplitz-operatorshttpsarxivorgabs260110711v1><a href=https://arxiv.org/abs/2601.10711v1>Unbounded symbols, heat flow, and Toeplitz operators</a><a hidden class=anchor aria-hidden=true href=#unbounded-symbols-heat-flow-and-toeplitz-operatorshttpsarxivorgabs260110711v1>#</a></h3><p><strong>Authors:</strong> Sam Looi
<strong>Venue:</strong> arXiv (2026)</p><p>We disprove the natural domain extension of the Berger&ndash;Coburn heat-flow conjecture for Toeplitz operators on the Bargmann space and identify the failure mechanism as a gap between pointwise and uniform control of a Gaussian averaging of the squared modulus of the symbol, a gap that is invisible to the linear form $T_g$. We establish that the form-defined operator $T_g$ and the natural-domain operator $U_g$ decouple in the unbounded symbols regime: while $T_g$ is governed by linear averaging, $U_g$ is controlled by the quadratic intensity of $|g|^2$. We construct a smooth, nonnegative radial symbol $g$ satisfying the coherent-state admissibility hypothesis with bounded heat transforms for all time $t>0$; for this symbol, $T_g$ is bounded, yet $U_g$ is unbounded. This is a strictly global phenomenon: under the coherent-state hypothesis, local singularities are insufficient to cause unboundedness, leaving the ``geometry at infinity&rsquo;&rsquo; as the sole obstruction. Boundedness of $U_g$ is equivalent to the condition that $|g|^2 dŒº$ is a Fock&ndash;Carleson measure, a condition strictly stronger than the linear average $g dŒº$ governing $T_g$. Finally, regarding the gap between the known sub-critical sufficiency condition and the critical heat time, we prove that heat-flow regularity is irreversible in this context and show that bootstrapping strategies cannot resolve the gap between sufficiency and critical time.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10711v1">üìÑ Download PDF</a></p><hr><h3 id=a-note-on-strong-similarity-and-the-connes-embedding-problemhttpsarxivorgabs260110654v1><a href=https://arxiv.org/abs/2601.10654v1>A note on strong similarity and the Connes embedding problem</a><a hidden class=anchor aria-hidden=true href=#a-note-on-strong-similarity-and-the-connes-embedding-problemhttpsarxivorgabs260110654v1>#</a></h3><p><strong>Authors:</strong> Gilles Pisier
<strong>Venue:</strong> arXiv (2026)</p><p>We show that there exists a completely bounded (c.b. in short) homomorphism $u$ from a $C^<em>$-algebra $C$ with the lifting property (in short LP) into a QWEP von Neumann algebra $N$ that is not strongly similar to a $</em>$-homomorphism, i.e. the similarities that ``orthogonalize" $u$ (which exist since $u$ is c.b.) cannot belong to the von Neumann algebra $N$. Moreover, the map $u$ does not admit any c.b. lifting up into the WEP $C^<em>$-algebra of which $N$ is a quotient. We can take $C=C^</em>(G)$ (full $C^<em>$-algebra) where $G$ is any nonabelian free group and $N= B(H)\bar \otimes M$ where $M$ is the von Neumann algebra generated by the reduced $C^</em>$-algebra of $G$.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10654v1">üìÑ Download PDF</a></p><hr><h3 id=circumplanetary-disk-candidate-in-the-disk-of-hd-163296-traced-by-localized-emission-from-simple-organicshttpsarxivorgabs260110631v1><a href=https://arxiv.org/abs/2601.10631v1>Circumplanetary Disk Candidate in the Disk of HD 163296 Traced by Localized Emission from Simple Organics</a><a hidden class=anchor aria-hidden=true href=#circumplanetary-disk-candidate-in-the-disk-of-hd-163296-traced-by-localized-emission-from-simple-organicshttpsarxivorgabs260110631v1>#</a></h3><p><strong>Authors:</strong> Andres F. Izquierdo, Jaehan Bae, Maria Galloway-Sprietsma, Ewine F. van Dishoeck, Stefano Facchini, Giovanni Rosotti, Jochen Stadler, Myriam Benisty, Leonardo Testi
<strong>Venue:</strong> arXiv (2026)</p><p>Atacama Large Millimeter/submillimeter Array observations suggest that the disc of HD 163296 is being actively shaped by embedded, yet unseen protoplanets, as indicated by numerous gas and dust substructures consistent with planet-disc interaction models. We report the first detection of simple organic molecules, HCN and C2H, tracing a candidate circumplanetary disc (CPD) in the HD 163296 system, located at an orbital radius of $R=88\pm7$ au and azimuth $œÜ=46\pm3^\circ$ (or $R=0.75&rsquo;&rsquo;$, $\rm{PA}=350^\circ$ in projected sky coordinates), and originating near the midplane of the circumstellar disc. The signature is localised but spectrally resolved, and it overlaps with a previously reported planet candidate, P94, identified through kinematic perturbations traced by CO lines. We propose a scenario in which the observed chemical anomalies arise from increased heating driven by the forming planet and ongoing accretion through its CPD, facilitating the thermal desorption of species that would otherwise remain frozen out in the disc midplane, and potentially triggering the activation barriers of chemical reactions that lead to enhanced molecular production. Based on a first-order dynamical analysis of the HCN spectrum from the CPD&ndash;isolated with a 7$œÉ$ significance&ndash;we infer an upper limit on the planet mass of 1.8 $M_{\rm Jup}$, consistent with predictions from CO kinematics and constraints from direct imaging studies. By comparing the CPD sizes derived from our models with theoretical expectations where the CPD radius corresponds to roughly one-third of the planet&rsquo;s Hill radius, we favor CPD gas temperatures $T > 150$ K, planet masses $M_{\rm p} &lt; 1.0$ $M_{\rm Jup}$, and CPD radii $R_{\rm CPD} &lt; 2$ au.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10631v1">üìÑ Download PDF</a></p><hr><h3 id=rsatalker-realistic-socially-aware-talking-head-generation-for-multi-turn-conversationhttpsarxivorgabs260110606v1><a href=https://arxiv.org/abs/2601.10606v1>RSATalker: Realistic Socially-Aware Talking Head Generation for Multi-Turn Conversation</a><a hidden class=anchor aria-hidden=true href=#rsatalker-realistic-socially-aware-talking-head-generation-for-multi-turn-conversationhttpsarxivorgabs260110606v1>#</a></h3><p><strong>Authors:</strong> Peng Chen, Xiaobao Wei, Yi Yang, Naiming Yao, Hui Chen, Feng Tian
<strong>Venue:</strong> arXiv (2026)</p><p>Talking head generation is increasingly important in virtual reality (VR), especially for social scenarios involving multi-turn conversation. Existing approaches face notable limitations: mesh-based 3D methods can model dual-person dialogue but lack realistic textures, while large-model-based 2D methods produce natural appearances but incur prohibitive computational costs. Recently, 3D Gaussian Splatting (3DGS) based methods achieve efficient and realistic rendering but remain speaker-only and ignore social relationships. We introduce RSATalker, the first framework that leverages 3DGS for realistic and socially-aware talking head generation with support for multi-turn conversation. Our method first drives mesh-based 3D facial motion from speech, then binds 3D Gaussians to mesh facets to render high-fidelity 2D avatar videos. To capture interpersonal dynamics, we propose a socially-aware module that encodes social relationships, including blood and non-blood as well as equal and unequal, into high-level embeddings through a learnable query mechanism. We design a three-stage training paradigm and construct the RSATalker dataset with speech-mesh-image triplets annotated with social relationships. Extensive experiments demonstrate that RSATalker achieves state-of-the-art performance in both realism and social awareness. The code and dataset will be released.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10606v1">üìÑ Download PDF</a></p><hr><h2 id=-neuroscience>üîç neuroscience<a hidden class=anchor aria-hidden=true href=#-neuroscience>#</a></h2><h3 id=mhd-modelling-of-open-flux-evolution-around-solar-maximum-by-coronal-model-coconuthttpsarxivorgabs260110675v1><a href=https://arxiv.org/abs/2601.10675v1>MHD modelling of open flux evolution around solar maximum by coronal model COCONUT</a><a hidden class=anchor aria-hidden=true href=#mhd-modelling-of-open-flux-evolution-around-solar-maximum-by-coronal-model-coconuthttpsarxivorgabs260110675v1>#</a></h3><p><strong>Authors:</strong> Haopeng Wang, Stefaan Poedts, Andrea Lani, Luis Linan, Tinatin Baratashvili, Hyun-Jin Jeong, Rayan Dhib, Quentin Noraz, Wenwen Wei, Mahdi Najafi-Ziyazi, Junyan Liu, Hao Wu, Rui Zhuo, Jos√© Miguel Luzia Murteira, Ketevan Arabuli, Brigitte Schmieder, Jasmina Magdaleniƒá Zhukov
<strong>Venue:</strong> arXiv (2026)</p><p>To evaluate impact of temporal evolution and commonly used harmonic filtering of magnetograms, and the empirically defined oversimplified heating source terms on open-field distributions, we use a series of hourly-updated magnetograms, preprocessed by the 10th- and 50th-order filtered PF solvers, to drive COCONUT, configured with different heating prescriptions, to mimic coronal evolutions during CRs 2282 and 2283. We evaluate the simulated open magnetic flux at 1.01~$R_s$, 3~$R_s$, and 0.1<del>AU, and compare them with interplanetary observations. The results show that the simulated unsigned open flux evaluated near the solar surface can be comparable to that derived from interplanetary in situ observations. However, in low corona, numerous small-scale closed-field magnetic structures introduce magnetic polarity inversion interfaces within the open field, cancelling part of the open field near these interfaces during the volume-integration procedure of the finite-volume method. Consequently, the simulated unsigned open flux can be reduced by up to 45% at 0.1</del>AU and decreases more rapidly in the low corona. The results also indicate that moderate adjustments to the heating source term can effectively regulate the magnitude of the unsigned open magnetic flux. Preprocessing the initial magnetogram by a PF solver with limited spherical harmonics can reduce the open flux in the low corona and alter the distribution of open-field regions, but has little effect on the total unsigned open flux at larger heliocentric distances. The ratio of the maximum to minimum open unsigned magnetic flux can reach 1.4 within a single solar maximum CR. These findings highlight the necessity of considering finer grid resolution around magnetic polarity inversion interfaces, more realistic heating mechanisms, and the time-evolving regime of MHD coronal modelling when further addressing the ``open flux problem".</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10675v1">üìÑ Download PDF</a></p><hr><h3 id=safe-trajectory-gradient-flow-control-of-a-grid-interfacing-inverterhttpsarxivorgabs260110671v1><a href=https://arxiv.org/abs/2601.10671v1>Safe Trajectory Gradient Flow Control of a Grid-Interfacing Inverter</a><a hidden class=anchor aria-hidden=true href=#safe-trajectory-gradient-flow-control-of-a-grid-interfacing-inverterhttpsarxivorgabs260110671v1>#</a></h3><p><strong>Authors:</strong> Trager Joswig-Jones, Baosen Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>Grid-interfacing inverters serve as the interface between renewable energy resources and the electric power grid, offering fast, programmable control capabilities. However, their operation is constrained by hardware limitations, such as bounds on the current magnitude. Existing control methods for these systems often neglect these constraints during controller design and instead rely on ad hoc limiters, which can introduce instability or degrade performance. In this work, we present a control framework that directly incorporates constraints into the control of a voltage-source inverter. We propose a safe trajectory gradient flow controller, which applies the safe gradient flow method to a rolling horizon trajectory optimization problem to ensure that the states remain within a safe set defined by the constraints while directing the trajectory towards an optimal equilibrium point of a nonlinear program. Simulation results demonstrate that our approach can drive the outputs of a simulated inverter system to optimal values and maintain state constraints, even when using a limited number of optimization steps per control cycle.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10671v1">üìÑ Download PDF</a></p><hr><h3 id=combinatorial-optimization-augmented-machine-learninghttpsarxivorgabs260110583v1><a href=https://arxiv.org/abs/2601.10583v1>Combinatorial Optimization Augmented Machine Learning</a><a hidden class=anchor aria-hidden=true href=#combinatorial-optimization-augmented-machine-learninghttpsarxivorgabs260110583v1>#</a></h3><p><strong>Authors:</strong> Maximilian Schiffer, Heiko Hoppe, Yue Su, Louis Bouvier, Axel Parmentier
<strong>Venue:</strong> arXiv (2026)</p><p>Combinatorial optimization augmented machine learning (COAML) has recently emerged as a powerful paradigm for integrating predictive models with combinatorial decision-making. By embedding combinatorial optimization oracles into learning pipelines, COAML enables the construction of policies that are both data-driven and feasibility-preserving, bridging the traditions of machine learning, operations research, and stochastic optimization. This paper provides a comprehensive overview of the state of the art in COAML. We introduce a unifying framework for COAML pipelines, describe their methodological building blocks, and formalize their connection to empirical cost minimization. We then develop a taxonomy of problem settings based on the form of uncertainty and decision structure. Using this taxonomy, we review algorithmic approaches for static and dynamic problems, survey applications across domains such as scheduling, vehicle routing, stochastic programming, and reinforcement learning, and synthesize methodological contributions in terms of empirical cost minimization, imitation learning, and reinforcement learning. Finally, we identify key research frontiers. This survey aims to serve both as a tutorial introduction to the field and as a roadmap for future research at the interface of combinatorial optimization and machine learning.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10583v1">üìÑ Download PDF</a></p><hr><h3 id=causalfe-causal-forests-with-fixed-effects-in-pythonhttpsarxivorgabs260110555v1><a href=https://arxiv.org/abs/2601.10555v1>causalfe: Causal Forests with Fixed Effects in Python</a><a hidden class=anchor aria-hidden=true href=#causalfe-causal-forests-with-fixed-effects-in-pythonhttpsarxivorgabs260110555v1>#</a></h3><p><strong>Authors:</strong> Harry Aytug
<strong>Venue:</strong> arXiv (2026)</p><p>The causalfe package provides a Python implementation of Causal Forests with Fixed Effects (CFFE) for estimating heterogeneous treatment effects in panel data settings. Standard causal forest methods struggle with panel data because unit and time fixed effects induce spurious heterogeneity in treatment effect estimates. The CFFE approach addresses this by performing node-level residualization during tree construction, removing fixed effects within each candidate split rather than globally. This paper describes the methodology, documents the software interface, and demonstrates the package through simulation studies that validate the estimator&rsquo;s performance under various data generating processes.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10555v1">üìÑ Download PDF</a></p><hr><h3 id=cogen-creation-of-reusable-ui-components-in-figma-via-textual-commandshttpsarxivorgabs260110536v1><a href=https://arxiv.org/abs/2601.10536v1>CoGen: Creation of Reusable UI Components in Figma via Textual Commands</a><a hidden class=anchor aria-hidden=true href=#cogen-creation-of-reusable-ui-components-in-figma-via-textual-commandshttpsarxivorgabs260110536v1>#</a></h3><p><strong>Authors:</strong> Ishani Kanapathipillai, Obhasha Priyankara
<strong>Venue:</strong> arXiv (2026)</p><p>The evolution of User Interface design has emphasized the need for efficient, reusable, and editable components to ensure an efficient design process. This research introduces CoGen, a system that uses machine learning techniques to generate reusable UI components directly in Figma, one of the most popular UI design tools. Addressing gaps in current systems, CoGen focuses on creating atomic components such as buttons, labels, and input fields using structured JSON and natural language prompts.
The project integrates Figma API data extraction, Seq2Seq models, and fine-tuned T5 transformers for component generation. The key results demonstrate the efficiency of the T5 model in prompt generation, with an accuracy of 98% and a BLEU score of 0.2668, which ensures the mapping of JSON to descriptive prompts. For JSON creation, CoGen achieves a success rate of up to 100% in generating simple JSON outputs for specified component types.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10536v1">üìÑ Download PDF</a></p><hr><h2 id=-data_resources>üîç data_resources<a hidden class=anchor aria-hidden=true href=#-data_resources>#</a></h2><h3 id=model-see-model-do-exposure-aware-evaluation-of-bug-vs-fix-preference-in-code-llmshttpsarxivorgabs260110496v1><a href=https://arxiv.org/abs/2601.10496v1>Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs</a><a hidden class=anchor aria-hidden=true href=#model-see-model-do-exposure-aware-evaluation-of-bug-vs-fix-preference-in-code-llmshttpsarxivorgabs260110496v1>#</a></h3><p><strong>Authors:</strong> Ali Al-Kaswan, Claudio Spiess, Prem Devanbu, Arie van Deursen, Maliheh Izadi
<strong>Venue:</strong> arXiv (2026)</p><p>Large language models are increasingly used for code generation and debugging, but their outputs can still contain bugs, that originate from training data. Distinguishing whether an LLM prefers correct code, or a familiar incorrect version might be influenced by what it&rsquo;s been exposed to during training. We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a model&rsquo;s preference. Using the ManySStuBs4J benchmark, we apply Data Portraits for membership testing on the Stack-V2 corpus to estimate whether each buggy and fixed variant was seen during training. We then stratify examples by exposure and compare model preference using code completion as well as multiple likelihood-based scoring metrics We find that most examples (67%) have neither variant in the training data, and when only one is present, fixes are more frequently present than bugs. In model generations, models reproduce buggy lines far more often than fixes, with bug-exposed examples amplifying this tendency and fix-exposed examples showing only marginal improvement. In likelihood scoring, minimum and maximum token-probability metrics consistently prefer the fixed code across all conditions, indicating a stable bias toward correct fixes. In contrast, metrics like the Gini coefficient reverse preference when only the buggy variant was seen. Our results indicate that exposure can skew bug-fix evaluations and highlight the risk that LLMs may propagate memorised errors in practice.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10496v1">üìÑ Download PDF</a></p><hr><h3 id=comovi-co-generation-of-3d-human-motions-and-realistic-videoshttpsarxivorgabs260110632v1><a href=https://arxiv.org/abs/2601.10632v1>CoMoVi: Co-Generation of 3D Human Motions and Realistic Videos</a><a hidden class=anchor aria-hidden=true href=#comovi-co-generation-of-3d-human-motions-and-realistic-videoshttpsarxivorgabs260110632v1>#</a></h3><p><strong>Authors:</strong> Chengfeng Zhao, Jiazhi Shu, Yubo Zhao, Tianyu Huang, Jiahao Lu, Zekai Gu, Chengwei Ren, Zhiyang Dou, Qing Shuai, Yuan Liu
<strong>Venue:</strong> arXiv (2026)</p><p>In this paper, we find that the generation of 3D human motions and 2D human videos is intrinsically coupled. 3D motions provide the structural prior for plausibility and consistency in videos, while pre-trained video models offer strong generalization capabilities for motions, which necessitate coupling their generation processes. Based on this, we present CoMoVi, a co-generative framework that couples two video diffusion models (VDMs) to generate 3D human motions and videos synchronously within a single diffusion denoising loop. To achieve this, we first propose an effective 2D human motion representation that can inherit the powerful prior of pre-trained VDMs. Then, we design a dual-branch diffusion model to couple human motion and video generation process with mutual feature interaction and 3D-2D cross attentions. Moreover, we curate CoMoVi Dataset, a large-scale real-world human video dataset with text and motion annotations, covering diverse and challenging human motions. Extensive experiments demonstrate the effectiveness of our method in both 3D human motion and video generation tasks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10632v1">üìÑ Download PDF</a></p><hr><h3 id=action100m-a-large-scale-video-action-datasethttpsarxivorgabs260110592v1><a href=https://arxiv.org/abs/2601.10592v1>Action100M: A Large-scale Video Action Dataset</a><a hidden class=anchor aria-hidden=true href=#action100m-a-large-scale-video-action-datasethttpsarxivorgabs260110592v1>#</a></h3><p><strong>Authors:</strong> Delong Chen, Tejaswi Kasarla, Yejin Bang, Mustafa Shukor, Willy Chung, Jade Yu, Allen Bolourchi, Theo Moutakanni, Pascale Fung
<strong>Venue:</strong> arXiv (2026)</p><p>Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10592v1">üìÑ Download PDF</a></p><hr><h3 id=bikeactions-an-open-platform-and-benchmark-for-cyclist-centric-vru-action-recognitionhttpsarxivorgabs260110521v1><a href=https://arxiv.org/abs/2601.10521v1>BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition</a><a hidden class=anchor aria-hidden=true href=#bikeactions-an-open-platform-and-benchmark-for-cyclist-centric-vru-action-recognitionhttpsarxivorgabs260110521v1>#</a></h3><p><strong>Authors:</strong> Max A. Buettner, Kanak Mazumder, Luca Koecher, Mario Finkbeiner, Sebastian Niebler, Fabian B. Flohr
<strong>Venue:</strong> arXiv (2026)</p><p>Anticipating the intentions of Vulnerable Road Users (VRUs) is a critical challenge for safe autonomous driving (AD) and mobile robotics. While current research predominantly focuses on pedestrian crossing behaviors from a vehicle&rsquo;s perspective, interactions within dense shared spaces remain underexplored. To bridge this gap, we introduce FUSE-Bike, the first fully open perception platform of its kind. Equipped with two LiDARs, a camera, and GNSS, it facilitates high-fidelity, close-range data capture directly from a cyclist&rsquo;s viewpoint. Leveraging this platform, we present BikeActions, a novel multi-modal dataset comprising 852 annotated samples across 5 distinct action classes, specifically tailored to improve VRU behavior modeling. We establish a rigorous benchmark by evaluating state-of-the-art graph convolution and transformer-based models on our publicly released data splits, establishing the first performance baselines for this challenging task. We release the full dataset together with data curation tools, the open hardware design, and the benchmark code to foster future research in VRU action understanding under <a href=https://iv.ee.hm.edu/bikeactions/>https://iv.ee.hm.edu/bikeactions/</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10521v1">üìÑ Download PDF</a></p><hr><h3 id=privacy-enhanced-peft-tensor-train-decomposition-improves-privacy-utility-tradeoffs-under-dp-sgdhttpsarxivorgabs260110045v1><a href=https://arxiv.org/abs/2601.10045v1>Privacy Enhanced PEFT: Tensor Train Decomposition Improves Privacy Utility Tradeoffs under DP-SGD</a><a hidden class=anchor aria-hidden=true href=#privacy-enhanced-peft-tensor-train-decomposition-improves-privacy-utility-tradeoffs-under-dp-sgdhttpsarxivorgabs260110045v1>#</a></h3><p><strong>Authors:</strong> Pradip Kunwar, Minh Vu, Maanak Gupta, Manish Bhattarai
<strong>Venue:</strong> arXiv (2026)</p><p>Fine-tuning large language models on sensitive data poses significant privacy risks, as membership inference attacks can reveal whether individual records were used during training. While Differential Privacy (DP) provides formal protection, applying DP to conventional Parameter-Efficient Fine-Tuning (PEFT) methods such as Low-Rank Adaptation (LoRA) often incurs substantial utility loss. In this work, we show that a more structurally constrained PEFT architecture, Tensor Train Low-Rank Adaptation (TTLoRA), can improve the privacy-utility tradeoff by shrinking the effective parameter space while preserving expressivity. To this end, we develop TTLoRA-DP, a differentially private training framework for TTLoRA. Specifically, we extend the ghost clipping algorithm to Tensor Train cores via cached contraction states, enabling efficient Differentially Private Stochastic Gradient Descent (DP-SGD) with exact per-example gradient norm computation without materializing full per-example gradients. Experiments on GPT-2 fine-tuning over the Enron and Penn Treebank datasets show that TTLoRA-DP consistently strengthens privacy protection relative to LoRA-DP while maintaining comparable or better downstream utility. Moreover, TTLoRA exhibits lower membership leakage even without DP training, using substantially smaller adapters and requiring on average 7.6X fewer parameters than LoRA. Overall, our results demonstrate that TTLoRA offers a practical path to improving the privacy-utility tradeoff in parameter-efficient language model adaptation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10045v1">üìÑ Download PDF</a></p><hr><h2 id=-emotion_language>üîç emotion_language<a hidden class=anchor aria-hidden=true href=#-emotion_language>#</a></h2><h3 id=real-characters-and-real-classes-of-mathrmgl_2-and-mathrmgu_2-over-discrete-valuation-ringshttpsarxivorgabs260110670v1><a href=https://arxiv.org/abs/2601.10670v1>Real characters and real classes of $\mathrm{GL}_2$ and $\mathrm{GU}_2$ over discrete valuation rings</a><a hidden class=anchor aria-hidden=true href=#real-characters-and-real-classes-of-mathrmgl_2-and-mathrmgu_2-over-discrete-valuation-ringshttpsarxivorgabs260110670v1>#</a></h3><p><strong>Authors:</strong> Archita Gupta, Tejbir Lohan, Pooja Singla
<strong>Venue:</strong> arXiv (2026)</p><p>Let $\mathfrak{o}$ be the ring of integers of a non-archimedean local field with residue field of odd characteristic, $\mathfrak{p}$ be its maximal ideal and let $\mathfrak{o}_\ell = \mathfrak{o}/\mathfrak{p}^\ell$ for $\ell\ge 2$. In this article, we study real-valued characters and real representations of the finite groups $\mathrm{GL}<em>2(\mathfrak{o}</em>\ell)$ and $\mathrm{GU}<em>2(\mathfrak{o}</em>\ell)$. We give a complete classification of real and strongly real classes of these groups and characterize the real-valued irreducible complex characters. We prove that every real-valued irreducible complex character of $\mathrm{GL}<em>2(\mathfrak{o}</em>\ell)$ is afforded by a representation over $\mathbb{R}$. In contrast, we show that $\mathrm{GU}<em>2(\mathfrak{o}</em>\ell)$ admits real-valued irreducible characters that are not realizable over $\mathbb{R}$. These results extend the parallel known phenomena for the finite groups $\mathrm{GL}_n(\mathbb{F}_q)$ and $\mathrm{GU}_n(\mathbb{F}_q)$.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10670v1">üìÑ Download PDF</a></p><hr><h3 id=hii-regions-in-ngc-628-the-view-of-two-catalogshttpsarxivorgabs260110642v1><a href=https://arxiv.org/abs/2601.10642v1>HII regions in NGC 628: the view of two catalogs</a><a hidden class=anchor aria-hidden=true href=#hii-regions-in-ngc-628-the-view-of-two-catalogshttpsarxivorgabs260110642v1>#</a></h3><p><strong>Authors:</strong> Ksenia I. Smirnova, Dmitri S. Wiebe
<strong>Venue:</strong> arXiv (2026)</p><p>The study is devoted to comparing the parameters of the interstellar medium of HII regions in the Kongiu and Groves catalogs for the galaxy NGC 628. The article analyzes the characteristics of star-forming regions, including a comparison of radiation fluxes in the ranges of 7.7 $Œº$m and 21 $Œº$m and in the H$Œ±$, H$Œ≤$, OIII and CO lines, calculating the kinematic parameters (FWHM) for the lines, and analyzing the spatial distribution of regions for both catalogs.
The results of the study showed that the regions from the Groves catalog demonstrate higher line widths compared to the Kongiu catalog. Signs of possible misidentified classification of some regions from the Groves catalog were revealed: there is a possibility that some of them are not HII regions, but shock ionization regions.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10642v1">üìÑ Download PDF</a></p><hr><h3 id=fair-regression-under-demographic-parity-a-unified-frameworkhttpsarxivorgabs260110623v1><a href=https://arxiv.org/abs/2601.10623v1>Fair Regression under Demographic Parity: A Unified Framework</a><a hidden class=anchor aria-hidden=true href=#fair-regression-under-demographic-parity-a-unified-frameworkhttpsarxivorgabs260110623v1>#</a></h3><p><strong>Authors:</strong> Yongzhen Feng, Weiwei Wang, Raymond K. W. Wong, Xianyang Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>We propose a unified framework for fair regression tasks formulated as risk minimization problems subject to a demographic parity constraint. Unlike many existing approaches that are limited to specific loss functions or rely on challenging non-convex optimization, our framework is applicable to a broad spectrum of regression tasks. Examples include linear regression with squared loss, binary classification with cross-entropy loss, quantile regression with pinball loss, and robust regression with Huber loss. We derive a novel characterization of the fair risk minimizer, which yields a computationally efficient estimation procedure for general loss functions. Theoretically, we establish the asymptotic consistency of the proposed estimator and derive its convergence rates under mild assumptions. We illustrate the method&rsquo;s versatility through detailed discussions of several common loss functions. Numerical results demonstrate that our approach effectively minimizes risk while satisfying fairness constraints across various regression settings.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10623v1">üìÑ Download PDF</a></p><hr><h3 id=malcev-classification-for-the-variety-of-left-symmetric-algebrashttpsarxivorgabs260110613v1><a href=https://arxiv.org/abs/2601.10613v1>Malcev classification for the variety of left-symmetric algebras</a><a hidden class=anchor aria-hidden=true href=#malcev-classification-for-the-variety-of-left-symmetric-algebrashttpsarxivorgabs260110613v1>#</a></h3><p><strong>Authors:</strong> A. Ryskeldin, B. Sartayev
<strong>Venue:</strong> arXiv (2026)</p><p>In this paper, we study three classes of subvarieties inside the variety of left-symmetric algebras. We show that these subvarieties are naturally related to some well-known varieties, such as alternative, assosymmetric and Zinbiel algebras. For certain subvarieties of the varieties of alternative and assosymmetric algebras, we explicitly construct bases of the corresponding free algebras. We then define the commutator and anti-commutator operations on these algebras and derive a number of identities satisfied by these operations in all degrees up to $4$.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10613v1">üìÑ Download PDF</a></p><hr><h3 id=scalable-spin-squeezing-in-power-law-interacting-xxz-models-with-disorderhttpsarxivorgabs260110703v1><a href=https://arxiv.org/abs/2601.10703v1>Scalable Spin Squeezing in Power-Law Interacting XXZ Models with Disorder</a><a hidden class=anchor aria-hidden=true href=#scalable-spin-squeezing-in-power-law-interacting-xxz-models-with-disorderhttpsarxivorgabs260110703v1>#</a></h3><p><strong>Authors:</strong> Samuel E. Begg, Bishal K. Ghosh, Chong Zu, Chuanwei Zhang, Michael Kolodrubetz
<strong>Venue:</strong> arXiv (2026)</p><p>While spin squeezing has been traditionally considered in all-to-all interacting models, recent works have shown that spin squeezing can occur in systems with power-law interactions, leading to direct testing in Rydberg atoms, trapped ions, ultracold atoms and nitrogen vacancy (NV) centers in diamond. For the latter, Wu. et al. Nature 646 (2025) demonstrated that spin squeezing is heavily affected by positional disorder, reducing any capacity for a practical squeezing advantage, which requires scalability with the system size. In this Letter we explore the robustness of spin-squeezing in two-dimensional lattices with a fraction of unoccupied lattice sites. Using semi-classical modeling, we demonstrate the existence of scalable squeezing in power-law interacting XXZ models up to a disorder threshold, above which squeezing is not scalable. We produce a phase diagram for scalable squeezing, and explain its absence in the aforementioned NV experiment. Our work illustrates the maximum disorder allowed for realizing scalable spin squeezing in a host of quantum simulators, highlights a regime with substantial tolerance to disorder, and identifies controlled defect creation as a promising route for scalable squeezing in solid-state systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10703v1">üìÑ Download PDF</a></p><hr><h3 id=late-time-acceleration-without-a-vacuum-term-in-frl_m-gravity-scaling-desitter-dynamics-and-parameter-constraintshttpsarxivorgabs260110699v1><a href=https://arxiv.org/abs/2601.10699v1>Late-time acceleration without a vacuum term in ${f(R,L_m)}$ gravity: scaling deSitter dynamics and parameter constraints</a><a hidden class=anchor aria-hidden=true href=#late-time-acceleration-without-a-vacuum-term-in-frl_m-gravity-scaling-desitter-dynamics-and-parameter-constraintshttpsarxivorgabs260110699v1>#</a></h3><p><strong>Authors:</strong> Luciano Navarro-Coyd√°n, J. Alberto V√°zquez, Israel Quiros, Ricardo Garc√≠a-Salcedo
<strong>Venue:</strong> arXiv (2026)</p><p>We investigate late-time cosmic acceleration in $f(R,L_m)$ gravity driven by nonlinear matter contributions, focusing on the class $f(R,L_m)=R/2+c_1 L_m+c_n L_m^{n}+c_0$ with the explicit choice $L_m=œÅ_m$ and an uncoupled radiation sector. We analyze two realizations: (i) Case A: $f(R,L_m)=R/2+Œ≤œÅ_m^{n}+Œ≥$, where $Œ≥$ acts as a vacuum term, and (ii) Case B: $f(R,L_m)=R/2+Œ≤œÅ_m+Œ≥œÅ_m^{n}$, where the nonlinear sector can mimic dark energy without an explicit cosmological constant. For each case, we construct a bounded autonomous system, classify all critical points and their stability, and compute cosmographic diagnostics. The phase-space analysis shows that Case A reproduces the standard radiation$\to$matter$\to$de<del>Sitter sequence only for $n\gtrsim 4/5$, with acceleration essentially enforced by the vacuum term. In contrast, Case</del>B admits a qualitatively distinct and phenomenologically appealing branch: for $0&lt;n&lt;1/2$ the system possesses a physical \emph{scaling} de~Sitter future attractor inside the bounded simplex, yielding radiation$\to$matter$\to$acceleration with $q=-1$ and $œâ_{\rm eff}=-1$ and without introducing $c_0$. We confront both models with background data (CC, Union3, DESI BAO, plus a BBN prior on $Œ©_b h^2$) using nested sampling and perform model comparison via Bayesian evidence and AIC/BIC. The full data combination constrains $n=1.08\pm0.05$ in Case A and $n=0.05\pm0.10$ in Case B (68% CL), the latter lying within the accelerating window while remaining statistically consistent with $Œõ$CDM kinematics at the background level. We also record minimal consistency conditions for stability (tensor no-ghost and luminal propagation) and motivate a dedicated perturbation-level analysis as the next step to test growth and lensing observables.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10699v1">üìÑ Download PDF</a></p><hr><h3 id=the-effective-theory-of-muon-to-electron-conversionhttpsarxivorgabs260110704v1><a href=https://arxiv.org/abs/2601.10704v1>The Effective Theory of Muon-to-Electron Conversion</a><a hidden class=anchor aria-hidden=true href=#the-effective-theory-of-muon-to-electron-conversionhttpsarxivorgabs260110704v1>#</a></h3><p><strong>Authors:</strong> W. C. Haxton, Evan Rule
<strong>Venue:</strong> arXiv (2026)</p><p>We summarize recent work to develop an effective theory of muon-to-electron conversion, based on a complete set of low-energy effective operators that are developed from a systematic expansion in velocities and momenta. The expansion effectively factors rates into sums of particle physics and nuclear physics terms, where the former are expressed as bilinears in the LECs (the low-energy constants of the effective theory) and the latter are the associated nuclear responses. One can view the nuclear responses as <code>dials" that can be adjusted -- for example, by selection of targets with specific properties -- in order to isolate the former. We show that an important dial, in the case of Mu2e and COMET, will be inelastic transitions to certain low-energy nuclear states that are resolvable in 27Al. If these transitions are exploited, the experiments have the potential not only to discover charged lepton flavor violation (CLFV), but to determine the operators responsible for the CLFV. We also discuss how such low-energy results can be </code>ported" to higher energies through a tower of matched EFTs, so they can be combined with other experimental limits to further constrain CLFV</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10704v1">üìÑ Download PDF</a></p><hr><h3 id=a-bayesian-discrete-framework-for-enhancing-decision-making-processes-in-clinical-trial-designs-and-evaluationshttpsarxivorgabs260110615v1><a href=https://arxiv.org/abs/2601.10615v1>A Bayesian Discrete Framework for Enhancing Decision-Making Processes in Clinical Trial Designs and Evaluations</a><a hidden class=anchor aria-hidden=true href=#a-bayesian-discrete-framework-for-enhancing-decision-making-processes-in-clinical-trial-designs-and-evaluationshttpsarxivorgabs260110615v1>#</a></h3><p><strong>Authors:</strong> Paramahansa Pramanik, Arnab Kumar Maity, Anjan Mandal, Haley Kate Robinson
<strong>Venue:</strong> arXiv (2026)</p><p>This study examines the application of Bayesian approach in the context of clinical trials, emphasizing their increasing importance in contemporary biomedical research. While conventional frequentist approach provides a foundational basis for analysis, it often lacks the flexibility to integrate prior knowledge, which can constrain its effectiveness in adaptive settings. In contrast, Bayesian methods enable continual refinement of statistical inferences through the assimilation of accumulating evidence, thereby supporting more informed decision-making and improving the reliability of trial findings. This paper also considers persistent challenges in clinical investigations, including replication difficulties and the misinterpretation of statistical results, suggesting that Bayesian strategies may offer a path toward enhanced analytical robustness. Moreover, discrete probability models, specifically the Binomial, Poisson, and Negative Binomial distributions are explored for their suitability in modeling clinical endpoints, particularly in trials involving binary responses or data with overdispersion. The discussion further incorporates Bayesian networks and Bayesian estimation techniques, with a comparative evaluation against maximum likelihood estimation to elucidate differences in inferential behavior and practical implementation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10615v1">üìÑ Download PDF</a></p><hr><h3 id=defending-large-language-models-against-jailbreak-attacks-via-in-decoding-safety-awareness-probinghttpsarxivorgabs260110543v1><a href=https://arxiv.org/abs/2601.10543v1>Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing</a><a hidden class=anchor aria-hidden=true href=#defending-large-language-models-against-jailbreak-attacks-via-in-decoding-safety-awareness-probinghttpsarxivorgabs260110543v1>#</a></h3><p><strong>Authors:</strong> Yinzhi Zhao, Ming Wang, Shi Feng, Xiaocui Yang, Daling Wang, Yifei Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model&rsquo;s drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: <a href=https://github.com/zyz13590/SafeProbing>https://github.com/zyz13590/SafeProbing</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10543v1">üìÑ Download PDF</a></p><hr><h3 id=observation-timelines-for-the-potential-lunar-impact-of-asteroid-2024-yr4httpsarxivorgabs260110666v1><a href=https://arxiv.org/abs/2601.10666v1>Observation Timelines for the Potential Lunar Impact of Asteroid 2024 YR4</a><a hidden class=anchor aria-hidden=true href=#observation-timelines-for-the-potential-lunar-impact-of-asteroid-2024-yr4httpsarxivorgabs260110666v1>#</a></h3><p><strong>Authors:</strong> Yifan He, Yixuan Wu, Yifei Jiao, Wen-Yue Dai, Xin Liu, Bin Cheng, Hexi Baoyin
<strong>Venue:</strong> arXiv (2026)</p><p>The near-Earth asteroid 2024 YR4 &ndash; a $\sim$60 m rocky object that was once considered a potential Earth impactor &ndash; has since been ruled out for Earth but retained a $\sim$4.3% probability of striking the Moon in 2032. Such an impact, with equivalent kinetic energy of $\sim$6.5 Mt TNT, is expected to produce a $\sim$1 km crater on the Moon, and will be the most energetic lunar impact event ever recorded in human history. Despite the associated risk, this scenario offers a rare and valuable scientific opportunity. Using a hybrid framework combining Monte Carlo orbital propagation, smoothed particle hydrodynamics (SPH) impact modeling, and N-body ejecta dynamics, we evaluate the physical outcomes and propose the observation timelines of this rare event. Our results suggest an optical flash of visual magnitude from -2.5 to -3 lasting several minutes directly after the impact, followed by hours of infrared afterglow from $\sim$2000 K molten rock cooling to a few hundred K. The associated seismic energy release would lead to a global-scale lunar reverberation (magnitude $\sim$5.0) that can be detectable by modern seismometers. Furthermore, the impact would eject $\sim$10$^8$ kg of debris that escapes the lunar gravity, with a small fraction reaching Earth to produce a lunar meteor outburst within 100 years. Finally, we integrate these results into a coordinated observation timeline, identifying the best detection windows for ground-based telescopes, lunar orbiters, and surface stations.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10666v1">üìÑ Download PDF</a></p><hr><h3 id=generative-ai-collective-behavior-needs-an-interactionist-paradigmhttpsarxivorgabs260110567v1><a href=https://arxiv.org/abs/2601.10567v1>Generative AI collective behavior needs an interactionist paradigm</a><a hidden class=anchor aria-hidden=true href=#generative-ai-collective-behavior-needs-an-interactionist-paradigmhttpsarxivorgabs260110567v1>#</a></h3><p><strong>Authors:</strong> Laura Ferrarotti, Gian Maria Campedelli, Roberto Dess√¨, Andrea Baronchelli, Giovanni Iacca, Kathleen M. Carley, Alex Pentland, Joel Z. Leibo, James Evans, Bruno Lepri
<strong>Venue:</strong> arXiv (2026)</p><p>In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs&ndash;namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning&ndash;motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10567v1">üìÑ Download PDF</a></p><hr><h3 id=inferring-signed-social-networks-from-contact-patternshttpsarxivorgabs260110565v1><a href=https://arxiv.org/abs/2601.10565v1>Inferring signed social networks from contact patterns</a><a hidden class=anchor aria-hidden=true href=#inferring-signed-social-networks-from-contact-patternshttpsarxivorgabs260110565v1>#</a></h3><p><strong>Authors:</strong> D√°vid Ferenczi, Jean-Gabriel Young, Leto Peel
<strong>Venue:</strong> arXiv (2026)</p><p>Social networks are typically inferred from indirect observations, such as proximity data; yet, most methods cannot distinguish between absent relationships and actual negative ties, as both can result in few or no interactions. We address the challenge of inferring signed networks from contact patterns while accounting for whether lack of interactions reflect a lack of opportunity as opposed to active avoidance. We develop a Bayesian framework with MCMC inference that models interaction groups to separate chance from choice when no interactions are observed. Validation on synthetic data demonstrates superior performance compared to natural baselines, particularly in detecting negative edges. We apply our method to French high school contact data to reveal a structure consistent with friendship surveys and demonstrate the model&rsquo;s adequacy through posterior predictive checks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10565v1">üìÑ Download PDF</a></p><hr><h3 id=a-propagation-framework-for-network-regressionhttpsarxivorgabs260110533v1><a href=https://arxiv.org/abs/2601.10533v1>A Propagation Framework for Network Regression</a><a hidden class=anchor aria-hidden=true href=#a-propagation-framework-for-network-regressionhttpsarxivorgabs260110533v1>#</a></h3><p><strong>Authors:</strong> Yingying Ma, Chenlei Leng
<strong>Venue:</strong> arXiv (2026)</p><p>We introduce a unified and computationally efficient framework for regression on network data, addressing limitations of existing models that require specialized estimation procedures or impose restrictive decay assumptions. Our Network Propagation Regression (NPR) models outcomes as functions of covariates propagated through network connections, capturing both direct and indirect effects. NPR is estimable via ordinary least squares for continuous outcomes and standard routines for binary, categorical, and time-to-event data, all within a single interpretable framework. We establish consistency and asymptotic normality under weak conditions and develop valid hypothesis tests for the order of network influence. Simulation studies demonstrate that NPR consistently outperforms established approaches, such as the linear-in-means model and regression with network cohesion, especially under model misspecification. An application to social media sentiment analysis highlights the practical utility and robustness of NPR in real-world settings.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10533v1">üìÑ Download PDF</a></p><hr><h3 id=higher-order-trade-offs-in-hypergraph-community-detectionhttpsarxivorgabs260110502v1><a href=https://arxiv.org/abs/2601.10502v1>Higher order trade-offs in hypergraph community detection</a><a hidden class=anchor aria-hidden=true href=#higher-order-trade-offs-in-hypergraph-community-detectionhttpsarxivorgabs260110502v1>#</a></h3><p><strong>Authors:</strong> Jiaze Li, Michael T. Schaub, Leto Peel
<strong>Venue:</strong> arXiv (2026)</p><p>Extending community detection from pairwise networks to hypergraphs introduces fundamental theoretical challenges. Hypergraphs exhibit structural heterogeneity with no direct graph analogue: hyperedges of varying orders can connect nodes across communities in diverse configurations, introducing new trade-offs in defining and detecting community structure. We address these challenges by developing a unified framework for community detection in non-uniform hypergraphs under the Hypergraph Stochastic Block Model. We introduce a general signal-to-noise ratio that enables a quantitative analysis of trade-offs unique to higher-order networks, such as which hypergedges we choose to split across communities and how we choose to split them. Building on this framework, we derive a Bethe Hessian operator for non-uniform hypergraphs that provides efficient spectral clustering with principled model selection. We characterize the resulting spectral detectability threshold and compare it to belief propagation limits, showing the methods coincide for uniform hypergraphs but diverge in non-uniform settings. Synthetic experiments confirm our analytical predictions and reveal systematic biases toward preserving higher-order and balanced-shape hyperedges. Application to empirical data demonstrates the practical relevance of these higher-order detectability trade-offs in real-world systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.10502v1">üìÑ Download PDF</a></p><hr></div><footer class=post-footer><ul class=post-tags></ul></footer><div><div class=pagination__title><span class=pagination__title-h style=font-size:20px>Comments</span><hr></div><div id=tcomment></div><script src=https://cdn.staticfile.org/twikoo/1.6.39/twikoo.all.min.js></script><script>twikoo.init({envId:"https://mangodb-theta.vercel.app/",el:"#tcomment",lang:"en-US",path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://garyforreal.me/en/>Gary's House</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>Views: <span id=busuanzi_value_site_pv></span>
</span><span id=busuanzi_container_site_uv>Visitors: <span id=busuanzi_value_site_uv></span></span></div></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>