<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Research_lab_works | Gary's House</title>
<meta name=keywords content><meta name=description content="CREPE: Open-Domain Question Answering with False Presuppositions
ğŸ¯ Target
The authors introduced CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums.
ğŸ“¦ Dataset
CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums.
Data Source: Reddit, the ELI5 subreddit.
Data Numbers: 8,400 Reddit questions with:

Labels (whether there are any false presuppositions).
The false presuppositions and their corrections, if there are any false presuppositions in questions.


"><meta name=author content="Gary"><link rel=canonical href=https://garyforreal.me/en/posts/paper/research_lab_works/><meta name=google-site-verification content="XYZabc"><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><link crossorigin=anonymous href=/assets/css/stylesheet.7e5251d8716d933fafcf3df70d7ecd02729661d6d232fc5dd0b8a85ef75e3409.css integrity="sha256-flJR2HFtkz+vzz33DX7NAnKWYdbSMvxd0LioXvdeNAk=" rel="preload stylesheet" as=style><link rel=icon href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=16x16 href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=32x32 href=https://garyforreal.me/img/Q.jpg><link rel=apple-touch-icon href=https://garyforreal.me/Q.jpg><link rel=mask-icon href=https://garyforreal.me/Q.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://garyforreal.me/en/posts/paper/research_lab_works/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@200..900&display=swap" rel=stylesheet><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous referrerpolicy=no-referrer></script><meta property="og:title" content="Research_lab_works"><meta property="og:description" content="CREPE: Open-Domain Question Answering with False Presuppositions
ğŸ¯ Target
The authors introduced CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums.
ğŸ“¦ Dataset
CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums.
Data Source: Reddit, the ELI5 subreddit.
Data Numbers: 8,400 Reddit questions with:

Labels (whether there are any false presuppositions).
The false presuppositions and their corrections, if there are any false presuppositions in questions.


"><meta property="og:type" content="article"><meta property="og:url" content="https://garyforreal.me/en/posts/paper/research_lab_works/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-21T15:15:43+09:00"><meta property="article:modified_time" content="2025-03-21T15:15:43+09:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Research_lab_works"><meta name=twitter:description content="CREPE: Open-Domain Question Answering with False Presuppositions
ğŸ¯ Target
The authors introduced CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums.
ğŸ“¦ Dataset
CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums.
Data Source: Reddit, the ELI5 subreddit.
Data Numbers: 8,400 Reddit questions with:

Labels (whether there are any false presuppositions).
The false presuppositions and their corrections, if there are any false presuppositions in questions.


"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://garyforreal.me/en/posts/"},{"@type":"ListItem","position":2,"name":"Paper","item":"https://garyforreal.me/en/posts/paper/"},{"@type":"ListItem","position":3,"name":"Research_lab_works","item":"https://garyforreal.me/en/posts/paper/research_lab_works/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Research_lab_works","name":"Research_lab_works","description":"CREPE: Open-Domain Question Answering with False Presuppositions ğŸ¯ Target The authors introduced CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums.\nğŸ“¦ Dataset CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums.\nData Source: Reddit, the ELI5 subreddit.\nData Numbers: 8,400 Reddit questions with:\nLabels (whether there are any false presuppositions). The false presuppositions and their corrections, if there are any false presuppositions in questions. ","keywords":[],"articleBody":"CREPE: Open-Domain Question Answering with False Presuppositions ğŸ¯ Target The authors introduced CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums.\nğŸ“¦ Dataset CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums.\nData Source: Reddit, the ELI5 subreddit.\nData Numbers: 8,400 Reddit questions with:\nLabels (whether there are any false presuppositions). The false presuppositions and their corrections, if there are any false presuppositions in questions. ğŸ› ï¸ Construction The questions in Reddit, and the most upvoted comments written by community users.\n1.1 Criteria and Solutions Naturalness of the questions -\u003e C1\nIf the questions are written by real, information-seeking users.\nC1â€™s solution:\nQuestions posted on the ELI5 subreddit. Filter questions and comments based on upvotes with a higher threshold. Split the training, the development, and the test data based on the time of the posting. The training set: posted in 2011-2018 The development set: posted in Jan-Jun of 2019 The test set: posted in Jul-Dec of 2019 Validity of the presupposition -\u003e C2\nIf the identified presupposition is highly likely made by the question writer.\nCorrectness and adequacy of the information -\u003e C3\nIf the correction is factually correct and adequate to convince the question writer.\nC2 \u0026 C3â€™s solution:\nBased on the most upvoted comments.\nC2: If the comment identifying a false presupposition has the most upvotes, it is likely that the presupposition is valid (made by the question writer). Personal Replenish of valid: The false presupposition in the original question was indeed posed by the question writer rather than misinterpreted or incorrectly inferred by the community users. C3: If the comment identifying a false presupposition has the most upvotes, it is highly likely to contain information that is correct and adequate. 1.2 Annotation tasks Input: Question(1) and the most voted comment(1)\nFilter out questions (subjective, uninformative, or rely on personal experience). Judge the most voted comment (whether a false presupposition in the question is identified). If there is a false presupposition (the most voted comment identified a false presupposition in the question), write the presupposition and a correction as a concise, declarative sentence. Pipeline:\n1 question was annotated by 2 annotators Step 1: If either of the annotators filtered out this question, then filter this question out. Step 2 \u0026 3: If the two annotators had the same label in this question (i.e., both of them consider the question to contain the false presupposition or not to contain the false presupposition), then the label and the corrections written by the two annotators are taken as Gold Reference. Otherwise, assign a third annotator to judge this question. ğŸ“– Task 1. The main track Input: Question Only\nApproach: Search necessary background knowledge to perform the task from any information source except for Reddit and Quora.\n2. The GOLD-COMMENT track Input: Question and The most voted comment\nApproach: No search\nğŸ§ª Experiments 1. Detection Judge whether there is a false presupposition in the question.\n(1) Trivial baselines Approach (Nearest Neighbor): Randomly assign FP (False Presupposition) or N (No false presupposition) uniformly. FP only always assigns FP. N only always assigns N.\nModel: c-REALM\n(2) GOLD-COMMENT track baselines Three classifiers based on CREPE datasets:\nQuestion only trains a RoBERTa-based classifier. (Doesnâ€™t specify whether this classifier is based on RoBERTa-large). Comment only trains a RoBERTa-large-based classifier. QuestionâŠ•Comment trains a RoBERTa-large-based classifier.\nSame classifiers based on MNLI \u0026 BoolQ datasets, and tested on CREPE in a zero-shot fashion. (3) Main track baselines Input: Question\nOutput: The likelihood of the question having false presuppositions or not.\nRetrieve a set of k passages from the English Wikipedia by the c-REALM model. Concatenate the question and k passages to assign the likelihood of the question having false presuppositions or not by Softmax.\nModel: RoBERTa Special Operation (Self-labeling) Target: To label the unlabeled training questions.\nUse the QuestionâŠ•Comment to assign a silver label to the unlabeled training questions. Then train the classifier on the union of this silver data as well as the gold labeled data. (4) Human performance Two human workers perform the task for 186 questions for each track.\nHuman with the most voted comment: Assume the most voted comment as a ground truth in terms of factuality of the information and the validity of the presupposition. Human without the most voted comment: Human workers search over the web (except Quora and Reddit) to find information and make the best judgment about the validity of the presupposition. 2. Writing Give a question that contains a false presupposition, and it is required to generate the presupposition as well as the correction.\n(1) GOLD-COMMENT track baselines Copy baseline: Copy the question as a false presupposition and use the comment as a correction.\nQuestionâŠ•Comment Dedicated: Use a pretrained T5-base model to train two separate generators. The input is a concatenation of the question and the comment. One generator extracts the false presupposition from the question, while the other generates the correction.\nQuestionâŠ•Comment Unified: Design a unified model that can be used for both the presupposition and the correction.\nTraining:\nInput: (Question and Comment), Output: (Annotated correction) Input: (Question and Comment), Output: (â€œIt is not the case thatâ€ + false presupposition) Generation:\nCorrection: Use a standard, beam search decoding to generate the correction. Presupposition: To generate the presupposition, we first decode a sequence with a constraint that it should start with â€œIt is not the case that,â€ and then take the sequence that comes next as a presupposition. (2) Main track baselines The baselines are similar to QuestionâŠ•Comment Dedicated and QuestionâŠ•Comment Unified models in the GOLD-COMMENT track.\nThe only difference is that the model receives a question and a set of k passages from c-REALM instead of a question-comment pair.\nUse the Fusion-in-Decoder architecture to read multiple passages.\nğŸ“Š Results Detection Experiment Results Writing Experiment Results âš ï¸ Limitations Use the most upvoted comments that are not perfect. One avenue for future work is to consider extra-linguistic context such as individualsâ€™ backgrounds when judging the validity of presuppositions. This research does not include large-scale models. The researchers conducted a small-scale case study with GPT-3 text-davinci-002, but most generations are roughly on the right topic. The domain of CREPE is limited to online forums (Reddit). It is necessary to study false presuppositions on a broader set of domains that require domain expertise. Reference:\nXinyan Velocity Yu, Sewon Min, Luke Zettlemoyer, and Hannaneh Hajishirzi. CREPE: Open-domain question answering with false presuppositions. In Annual Meeting of the Association for Computational Linguistics, 2022. Evaluating Large Language Models for Health-related Queries with Presuppositions ğŸ¯ Target Construct a dataset of health-related queries with varying degrees of presupposition: UPHILL Evaluate factual accuracy and consistency of InstructGPT, ChatGPT, GPT-4 and Bing Copilot. ğŸ“¦ Dataset UPHILL, a dataset consisting of health-related queries with varying degrees of presuppositions.\nSource:\nPubHealth Monant Medical Misinformation Size:\n1945 claims Ã— 5 degrees = 9725 presuppositions True: 766 False: 854 Mixture: 159 Fabricated: 166 Claim Sources:\nTrue, False, Mixture: From PubHealth and Monant Fabricated: From templates E.g., â€œ[Action] treats [disease]â€, â€œ[Symptom] indicates [disease]â€ Slot-filled using terms from NHS health info Paraphrased using GPT-3.5-turbo ğŸ› ï¸ Construction 1. Query Generation For each claim, generate 5 different degreesâ€™ presuppositions by templetes. Degree Description 0 (Neutral) No assumptions; curious, factual inquiry 1 (Mild) Suggestive query with tentative belief 2 (Unequivocal) Clear presupposition invoking scientific legitimacy 3 (Writing Request) Request to write a report/article supporting the claim 4 (Writing Demand) Assertive demand for evidence-backed writing \u0026 citations Templetes 2. Entailment Model (InstructGPT, ChatGPT, GPT-4 and Bing Copilot) Feed into 4 models â†’ obtain 5 responses per model per claim Check relation between claim and model response Labels: Agreement Disagreement Neither 3. Metrics Accuracy\nA response is accurate if: It agrees with a true claim It disagrees with a false claim Consistency\nA model is consistent if it maintains the same stance across all 5 responses for the same claim ğŸ“Š Results InstructGPT: Most susceptible to presuppositions Bing Copilot: Most robust Visual Results: ğŸ“„ P14312 Table 3 \u0026 4 ğŸ“Š P14313 Figure 2 ğŸ“„ P14314 Table 5 âš ï¸ Limitations Entailment Model:\nUsed for evaluating agreement/disagreement; not perfectly reliable despite validation.\nTemplate-Based Queries:\nSimulate realistic search queries but donâ€™t reflect actual user behavior (due to lack of search logs).\nGeographical Bias:\nClaims are mostly U.S.-based â†’ limits global generalizability.\nEvaluation Scope:\nEvaluates factual accuracy only at the response level; does not perform fine-grained analysis.\nReference:\nNavreet Kaur, Monojit Choudhury, and Danish Pruthi. Evaluating large language models for health-related queries with presuppositions. In Findings of the Association for Computational Linguistics: ACL 2024, 2024. (QA)Â²: Question Answering with Questionable Assumptions ğŸ¯ Target Construct a dataset to be an evaluation set with the goal of testing robustness to questionable assumptions.\nğŸ“¦ Dataset (QA)Â² (Question Answering with Questionable Assumptions)\nData Source: NQ dataset by Googleâ€™s autocompletion API Data Size: 602 expert-annotated questions 50% contain questionable assumptions 50% are typical information-seeking questions (no questionable assumptions) Adaptation Set: 16 questions with questionable assumptions 16 questions without questionable assumptions Used for few-shot tuning or in-context demonstrations ğŸ› ï¸ Construction 1. Question Collection Scraped English wh-questions using API with prefix strings: why, what, where, etc. Applied automatic filtering: Removed duplicates and bad/non-questions Used a stop word list (e.g., quizlet, brainly, lyrics) Imperfect data retained: syntax/tense errors allowed, since it reflects real data 2. Annotation Task (Crowdsourced) Annotators: 23 qualified workers from Amazon Mechanical Turk Goal: Flag whether a question contains a questionable assumption Type: Binary classification Data: 12,000 questions Each question reviewed by 5 annotators 1 annotation set = 20 questions (18 unlabeled + 2 labeled) Quality check: On average, 86% of annotators labeled the control questions correctly 3. Annotation Task (Expert) Annotators: 3 expert annotators (authors) Scope: Almost 720 questions(6% of 12,000) flagged by at least one crowdworker Process: Each expert â†’ half of the questions independently annotate(Annotate the label, and give the abstract answer and its evidence) The last expert â†’ verify the result of annotations(disagreements via adjudication) Final selection criteria: (C1) Flagged by at least 1 worker (C2) Justification for the abstractive answer can be found online (C3) No immediate ambiguity in question interpretation ğŸ“– Tasks 1. End-to-End Abstractive QA (End-to-end QA) Input: Question Output: Abstractive answer generated by the model Evaluation: Randomly sample 100 answers Judged by 5 human raters from Prolific(refer to the answers given by experts) Use majority vote to determine acceptability (% acceptable reported) 2. Questionable Assumption Detection (Detection) Input: Question Goal: Detect whether it contains false or unverifiable assumptions Framing: Binary classification (Yes / No) Template examples: â€œDoes {question} contain any invalid assumptions?â€ â€œAre any of the assumptions in {question} problematic?â€ 3. Questionable Assumption Verification (Verification) Input: Yes/No question derived from a questionable assumption (identified by expert) Goal: Judge the truth of the assumption Example: Original: where are the winter olympics held 2021 Assumption: the winter olympics were held in 2021 Derived question: were the winter olympics held in 2021? â†’ Answer: No For valid questions: randomly select a true assumption and convert it into a Yes/No question ğŸ“Š Results âš ï¸ Limitations The dataset is limited to wh-questions collected via Googleâ€™s Autocompletion API, which may not fully represent the diversity of real-world questions. Some questions with false or unverifiable assumptions might be subtle or ambiguous, making it difficult for annotators to reach consistent judgments. Verification of nonexistence relies on pragmatic inference rather than exhaustive factual proof, due to the challenge of confirming something does not exist. The oracle-based decomposition in the verification task does not reflect a real-world pipeline where assumption detection and verification must be jointly performed. Human evaluation in end-to-end QA introduces subjectivity and may vary depending on rater interpretation and context familiarity. The adaptation set is small (32 total), which may limit generalization in few-shot or in-context learning settings. Reference:\nNajoung Kim, Phu Mon Htut, Samuel R. Bowman, and Jackson Petty. (QA)2: Question answering with questionable assumptions, 2023. Wonâ€™t Get Fooled Again: Answering Questions with False Premises ğŸ¯ Target Authors annotated a FalseQA dataset containing 2365 human-written FPQs, with the corresponding explanations for the false premises and the revised true premise questions.\nğŸ“¦ Dataset FalseQA, a dataset containing 2365 human-written FPQs, with the corresponding explanations for the false premises and the revised true premise questions.\nSource: Human-written Size: 2365 FPQs (False Premise Questions) with: Corresponding explanations of the false premises Revised TPQs (True Premise Questions) Set Count Training 1187 Validation 491 Test 687 ğŸ› ï¸ Construction Step Description 1 Write FPQs based on source words (subject words from GenericsKB) 2 Revise each FPQ minimally into a TPQ (true premise question) 3 Write detailed explanations for FPQs and answers for TPQs Train/Validation: 1 explanation per question Test: 2 explanations per question ğŸ§ª Experiments 1. Discriminating FPQs (Binary Classification) Train PLMs to classify questions into FPQs and TPQs Apply prompt learning to reduce the gap between pre-training and fine-tuning 2. Impact of Training Data Size Train with 32 / 256 / 1187 training pairs Result: Accuracy grows almost linearly as data size increases exponentially 3. Answering FPQs with Explanations Step 1: Model generates discriminating token (â€œtricky questionâ€ or â€œtrue questionâ€) Step 2: Model continues to generate explanation (FPQ) or answer (TPQ) Example:\nInput â†’ Question: Are tigersâ€™ eggs bigger than chickensâ€™ eggs?\nOutput â†’ tricky question. Because tigers are mammals and do not lay eggs, the question assumes a false premise.\nLoss Calculation:\nTotal loss = binary loss (â€œtricky questionâ€ or â€œtrue questionâ€) + generation loss (remaining tokens)\nDetails in Section 5.4 paragraph 2\u00263\nEvaluation:\nOnly explanation for FPQs is evaluated using ROUGE-L Two reference explanations used for test set 4. Answering Both FPQs and General Questions Use Data Replay (DR) to avoid catastrophic forgetting For every FPQ batch, add one ARC-DA batch Each group of ARC-DA samples is fixed for every 30 iterations Example: 120 batches\nBatch No. Data Type ARC-DA Sample Used 1, 3, 5, â€¦, 59 FPQ (FalseQA) - 2, 4, 6, â€¦, 60 ARC-DA ARC-DA sample A 61, 63, â€¦, 119 FPQ (FalseQA) - 62, 64, â€¦, 120 ARC-DA ARC-DA sample B âš ï¸ Limitation Standardized response to FPQs is a rebuttal, which reflects a conventional style In real scenarios, creative responses (e.g., rhetorical questions) may be more natural Reference:\nShengding Hu, Yifan Luo, Huadong Wang, Xingyi Cheng, Zhiyuan Liu, and Maosong Sun. Wonâ€˜t get fooled again: Answering questions with false premises. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023. The difference of Presupposition, Presumption, Premise and Questionable Assumption 1. Presupposition An implicit assumption that must be true for a statement to make sense. Often taken for granted without being explicitly stated.\nExample:\n1. \"When did you quit smoking?\" presupposes that you used to smoke. 2. \"John's children are very well-behaved\" presupposes that John has children. 3. \"Could you turn down the music?\" presupposes that music is playing. 2. presumption A belief accepted as true until proven otherwise. Often based on probability or convention rather than direct evidence.\nExample:\n1. In court, there is a presumption that the defendant is innocent until proven guilty. 2. There's a presumption that people signing contracts have read and understood them. 3. premise An explicitly stated proposition that serves as the foundation for an argument. Used consciously as part of logical reasoning.\nExample:\nPremise 1: All mammals are warm-blooded. Premise 2: Whales are mammals. Conclusion: Therefore, whales are warm-blooded. Premise 1: If it's raining, the ground is wet. Premise 2: It is raining. Conclusion: Therefore, the ground is wet. 4. Questionable Assumption An assumption that may be false, unwarranted, or dubious. Often identified as a weak point in reasoning that needs scrutiny.\nExample:\n1. \"Everyone I know owns a smartphone, so virtually everyone must own a smartphone.\" (Assumes your social circle represents the general population) 2. \"That restaurant was empty on a Tuesday night, so it must not be very good.\" (Assumes popularity at one specific time indicates quality) 3. \"He didn't smile when we met, so he must not like me.\" (Assumes a single behavior reliably indicates a particular attitude) Term Simple Definition Example with â€œTurmeric cures cancerâ€ Presupposition An unstated background assumption that must be true for a sentence to make sense. â€œWhy is turmeric effective in curing cancer?â€ presupposes that turmeric does cure cancer â€” this is taken for granted by the question. Presumption A belief automatically made based on context or common knowledge, without being stated. Hearing â€œTurmeric is often used in cancer treatmentsâ€ might lead someone to presume itâ€™s effective, even if that isnâ€™t explicitly claimed or proven. Premise A clearly stated claim used as the basis of an argument. â€œTurmeric cures cancerâ€ is explicitly stated to support a conclusion like â€œTherefore, people should take turmeric to treat cancer.â€ Questionable Assumption A false or unverifiable assumption likely believed by the speaker, even if not presupposed. â€œHow much turmeric should cancer patients take daily?â€ assumes the speaker believes turmeric helps with cancer, even if itâ€™s not logically presupposed. ","wordCount":"2809","inLanguage":"en","datePublished":"2025-03-21T15:15:43+09:00","dateModified":"2025-03-21T15:15:43+09:00","author":{"@type":"Person","name":"Gary"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://garyforreal.me/en/posts/paper/research_lab_works/"},"publisher":{"@type":"Organization","name":"Gary's House","logo":{"@type":"ImageObject","url":"https://garyforreal.me/img/Q.jpg"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><script src=https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://garyforreal.me/en/ accesskey=h title="Gary's Blog (Alt + H)"><img src=https://garyforreal.me/img/me.jpg alt aria-label=logo height=35>Gary's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://garyforreal.me/zh/ title=ä¸­æ–‡ aria-label=ä¸­æ–‡>ä¸­æ–‡</a></li></ul></div></div><ul id=menu><li><a href=https://garyforreal.me/en/search title="ğŸ”Search (Alt + /)" accesskey=/><span>ğŸ”Search</span></a></li><li><a href=https://garyforreal.me/en/ title=ğŸ Homepage><span>ğŸ Homepage</span></a></li><li><a href=https://garyforreal.me/en/posts title=ğŸ“šArticle><span>ğŸ“šArticle</span></a></li><li><a href=https://garyforreal.me/en/archives/ title=â±Archives><span>â±Archives</span></a></li><li><a href=https://garyforreal.me/en/music/ title=ğŸµmusic><span>ğŸµmusic</span></a></li><li><a href=https://garyforreal.me/en/about title=ğŸ™‹ğŸ»â€â™‚ï¸About><span>ğŸ™‹ğŸ»â€â™‚ï¸About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://garyforreal.me/en/>Home</a>&nbsp;Â»&nbsp;<a href=https://garyforreal.me/en/posts/>Posts</a>&nbsp;Â»&nbsp;<a href=https://garyforreal.me/en/posts/paper/>Paper</a></div><h1 class="post-title entry-hint-parent">Research_lab_works</h1><div class=post-meta><span title='2025-03-21 15:15:43 +0900 JST'>2025-03-21</span>&nbsp;Â·&nbsp;14 min&nbsp;Â·&nbsp;Gary<div class=meta-item>&nbspÂ·&nbsp
        <span id=busuanzi_container_page_pv>æœ¬æ–‡é˜…è¯»é‡<span id=busuanzi_value_page_pv></span>æ¬¡</span></div></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#crepe-open-domain-question-answering-with-false-presuppositions aria-label="CREPE: Open-Domain Question Answering with False Presuppositions"><strong>CREPE: Open-Domain Question Answering with False Presuppositions</strong></a><ul><li><a href=#-target aria-label="ğŸ¯ Target">ğŸ¯ Target</a></li><li><a href=#-dataset aria-label="ğŸ“¦ Dataset">ğŸ“¦ Dataset</a></li><li><a href=#-construction aria-label="ğŸ› ï¸ Construction">ğŸ› ï¸ Construction</a><ul><li><a href=#11-criteria-and-solutions aria-label="1.1 Criteria and Solutions">1.1 Criteria and Solutions</a></li><li><a href=#12-annotation-tasks aria-label="1.2 Annotation tasks">1.2 Annotation tasks</a></li></ul></li><li><a href=#-task aria-label="ğŸ“– Task">ğŸ“– Task</a><ul><li><a href=#1-the-main-track aria-label="1. The main track">1. The main track</a></li><li><a href=#2-the-gold-comment-track aria-label="2. The GOLD-COMMENT track">2. The GOLD-COMMENT track</a></li></ul></li><li><a href=#-experiments aria-label="ğŸ§ª Experiments">ğŸ§ª Experiments</a><ul><li><a href=#1-detection aria-label="1. Detection">1. Detection</a><ul><li><a href=#1-trivial-baselines aria-label="(1) Trivial baselines">(1) Trivial baselines</a></li><li><a href=#2-gold-comment-track-baselines aria-label="(2) GOLD-COMMENT track baselines">(2) GOLD-COMMENT track baselines</a></li><li><a href=#3-main-track-baselines aria-label="(3) Main track baselines">(3) Main track baselines</a></li><li><a href=#special-operation-self-labeling aria-label="Special Operation (Self-labeling)">Special Operation (Self-labeling)</a></li><li><a href=#4-human-performance aria-label="(4) Human performance">(4) Human performance</a></li></ul></li><li><a href=#2-writing aria-label="2. Writing">2. Writing</a><ul><li><a href=#1-gold-comment-track-baselines aria-label="(1) GOLD-COMMENT track baselines">(1) GOLD-COMMENT track baselines</a></li><li><a href=#2-main-track-baselines aria-label="(2) Main track baselines">(2) Main track baselines</a></li></ul></li></ul></li><li><a href=#-results aria-label="ğŸ“Š Results">ğŸ“Š Results</a><ul><li><a href=#detection-experiment-results aria-label="Detection Experiment Results">Detection Experiment Results</a></li><li><a href=#writing-experiment-results aria-label="Writing Experiment Results">Writing Experiment Results</a></li></ul></li><li><a href=#-limitations aria-label="âš ï¸ Limitations">âš ï¸ Limitations</a></li></ul></li><li><a href=#evaluating-large-language-models-for-health-related-queries-with-presuppositions aria-label="Evaluating Large Language Models for Health-related Queries with Presuppositions"><strong>Evaluating Large Language Models for Health-related Queries with Presuppositions</strong></a><ul><li><a href=#-target-1 aria-label="ğŸ¯ Target">ğŸ¯ Target</a></li><li><a href=#-dataset-1 aria-label="ğŸ“¦ Dataset">ğŸ“¦ Dataset</a></li><li><a href=#-construction-1 aria-label="ğŸ› ï¸ Construction">ğŸ› ï¸ Construction</a><ul><li><a href=#1-query-generation aria-label="1. Query Generation">1. Query Generation</a></li><li><a href=#2-entailment-model-instructgpt-chatgpt-gpt-4-and-bing-copilot aria-label="2. Entailment Model (InstructGPT, ChatGPT, GPT-4 and Bing Copilot)">2. Entailment Model (InstructGPT, ChatGPT, GPT-4 and Bing Copilot)</a></li><li><a href=#3-metrics aria-label="3. Metrics">3. Metrics</a></li></ul></li><li><a href=#-results-1 aria-label="ğŸ“Š Results">ğŸ“Š Results</a></li><li><a href=#-limitations-1 aria-label="âš ï¸ Limitations">âš ï¸ Limitations</a></li></ul></li><li><a href=#qa-question-answering-with-questionable-assumptions aria-label="(QA)Â²: Question Answering with Questionable Assumptions"><strong>(QA)Â²: Question Answering with Questionable Assumptions</strong></a><ul><li><a href=#-target-2 aria-label="ğŸ¯ Target">ğŸ¯ Target</a></li><li><a href=#-dataset-2 aria-label="ğŸ“¦ Dataset">ğŸ“¦ Dataset</a></li><li><a href=#-construction-2 aria-label="ğŸ› ï¸ Construction">ğŸ› ï¸ Construction</a><ul><li><a href=#1-question-collection aria-label="1. Question Collection">1. Question Collection</a></li><li><a href=#2-annotation-task-crowdsourced aria-label="2. Annotation Task (Crowdsourced)">2. Annotation Task (Crowdsourced)</a></li><li><a href=#3-annotation-task-expert aria-label="3. Annotation Task (Expert)">3. Annotation Task (Expert)</a></li></ul></li><li><a href=#-tasks aria-label="ğŸ“– Tasks">ğŸ“– Tasks</a><ul><li><a href=#1-end-to-end-abstractive-qa-end-to-end-qa aria-label="1. End-to-End Abstractive QA (End-to-end QA)">1. End-to-End Abstractive QA (End-to-end QA)</a></li><li><a href=#2-questionable-assumption-detection-detection aria-label="2. Questionable Assumption Detection (Detection)">2. Questionable Assumption Detection (Detection)</a></li><li><a href=#3-questionable-assumption-verification-verification aria-label="3. Questionable Assumption Verification (Verification)">3. Questionable Assumption Verification (Verification)</a></li></ul></li><li><a href=#-results-2 aria-label="ğŸ“Š Results">ğŸ“Š Results</a></li><li><a href=#-limitations-2 aria-label="âš ï¸ Limitations">âš ï¸ Limitations</a></li></ul></li><li><a href=#wont-get-fooled-again-answering-questions-with-false-premises aria-label="Wonâ€™t Get Fooled Again: Answering Questions with False Premises"><strong>Wonâ€™t Get Fooled Again: Answering Questions with False Premises</strong></a><ul><li><a href=#-target-3 aria-label="ğŸ¯ Target">ğŸ¯ Target</a></li><li><a href=#-dataset-3 aria-label="ğŸ“¦ Dataset">ğŸ“¦ Dataset</a></li><li><a href=#-construction-3 aria-label="ğŸ› ï¸ Construction">ğŸ› ï¸ Construction</a></li><li><a href=#-experiments-1 aria-label="ğŸ§ª Experiments">ğŸ§ª Experiments</a><ul><li><a href=#1-discriminating-fpqs-binary-classification aria-label="1. Discriminating FPQs (Binary Classification)">1. Discriminating FPQs (Binary Classification)</a></li><li><a href=#2-impact-of-training-data-size aria-label="2. Impact of Training Data Size">2. Impact of Training Data Size</a></li><li><a href=#3-answering-fpqs-with-explanations aria-label="3. Answering FPQs with Explanations">3. Answering FPQs with Explanations</a></li><li><a href=#4-answering-both-fpqs-and-general-questions aria-label="4. Answering Both FPQs and General Questions">4. Answering Both FPQs and General Questions</a></li></ul></li><li><a href=#-limitation aria-label="âš ï¸ Limitation">âš ï¸ Limitation</a></li></ul></li><li><a href=#the-difference-of-presupposition-presumption-premise-and-questionable-assumption aria-label="The difference of Presupposition, Presumption, Premise and Questionable Assumption"><strong>The difference of <code>Presupposition</code>, <code>Presumption</code>, <code>Premise</code> and <code>Questionable Assumption</code></strong></a><ul><li><a href=#1-presupposition aria-label="1. Presupposition">1. Presupposition</a></li><li><a href=#2-presumption aria-label="2. presumption">2. presumption</a></li><li><a href=#3-premise aria-label="3. premise">3. premise</a></li><li><a href=#4-questionable-assumption aria-label="4. Questionable Assumption">4. Questionable Assumption</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h1 id=crepe-open-domain-question-answering-with-false-presuppositions><strong>CREPE: Open-Domain Question Answering with False Presuppositions</strong><a hidden class=anchor aria-hidden=true href=#crepe-open-domain-question-answering-with-false-presuppositions>#</a></h1><h2 id=-target>ğŸ¯ Target<a hidden class=anchor aria-hidden=true href=#-target>#</a></h2><p>The authors introduced CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums.</p><h2 id=-dataset>ğŸ“¦ Dataset<a hidden class=anchor aria-hidden=true href=#-dataset>#</a></h2><p><a href=https://github.com/velocityCavalry/CREPE>CREPE</a>, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums.<br><strong>Data Source</strong>: Reddit, <a href=http://www.reddit.com/r/explainlikeimfive>the ELI5 subreddit</a>.<br><strong>Data Numbers</strong>: 8,400 Reddit questions with:</p><ol><li>Labels (whether there are any false presuppositions).</li><li>The false presuppositions and their corrections, if there are any false presuppositions in questions.</li></ol><p><img loading=lazy src=../img/statistics1.jpg alt="Dataset statistics"></p><p><img loading=lazy src=../img/data_sample.jpg alt="Sample Data"></p><h2 id=-construction>ğŸ› ï¸ Construction<a hidden class=anchor aria-hidden=true href=#-construction>#</a></h2><p>The questions in Reddit, and the most upvoted comments written by community users.</p><h3 id=11-criteria-and-solutions>1.1 Criteria and Solutions<a hidden class=anchor aria-hidden=true href=#11-criteria-and-solutions>#</a></h3><ol><li><p><strong>Naturalness of the questions</strong> -> C1<br>If the questions are written by real, information-seeking users.</p><p><strong>C1&rsquo;s solution</strong>:</p><ul><li>Questions posted on the ELI5 subreddit.</li><li>Filter questions and comments based on upvotes with a higher threshold.</li><li>Split the training, the development, and the test data based on the time of the posting.</li><li><strong>The training set</strong>: posted in 2011-2018</li><li><strong>The development set</strong>: posted in Jan-Jun of 2019</li><li><strong>The test set</strong>: posted in Jul-Dec of 2019</li></ul></li><li><p><strong>Validity of the presupposition</strong> -> C2<br>If the identified presupposition is highly likely made by the question writer.</p></li><li><p><strong>Correctness and adequacy of the information</strong> -> C3<br>If the correction is factually correct and adequate to convince the question writer.</p><p><strong>C2 & C3&rsquo;s solution</strong>:<br>Based on the most upvoted comments.</p><ul><li><strong>C2</strong>: If the comment identifying a false presupposition has the most upvotes, it is likely that the presupposition is valid (made by the question writer).</li><li><strong>Personal Replenish of <code>valid</code></strong>: The false presupposition in the original question was indeed posed by the question writer rather than misinterpreted or incorrectly inferred by the community users.</li><li><strong>C3</strong>: If the comment identifying a false presupposition has the most upvotes, it is highly likely to contain information that is correct and adequate.</li></ul></li></ol><h3 id=12-annotation-tasks>1.2 Annotation tasks<a hidden class=anchor aria-hidden=true href=#12-annotation-tasks>#</a></h3><p>Input: Question(1) and the most voted comment(1)</p><ol><li>Filter out questions (subjective, uninformative, or rely on personal experience).</li><li>Judge the most voted comment (whether a false presupposition in the question is identified).</li><li>If there is a false presupposition (the most voted comment identified a false presupposition in the question), write the presupposition and a correction as a concise, declarative sentence.</li></ol><p><strong>Pipeline</strong>:</p><ul><li>1 question was annotated by 2 annotators</li><li><strong>Step 1</strong>: If either of the annotators filtered out this question, then filter this question out.</li><li><strong>Step 2 & 3</strong>: If the two annotators had the same label in this question (i.e., both of them consider the question to contain the false presupposition or not to contain the false presupposition), then the label and the corrections written by the two annotators are taken as Gold Reference. Otherwise, assign a third annotator to judge this question.</li></ul><hr><h2 id=-task>ğŸ“– Task<a hidden class=anchor aria-hidden=true href=#-task>#</a></h2><h3 id=1-the-main-track>1. The main track<a hidden class=anchor aria-hidden=true href=#1-the-main-track>#</a></h3><p><strong>Input</strong>: Question Only<br><strong>Approach</strong>: Search necessary background knowledge to perform the task from any information source except for Reddit and Quora.</p><h3 id=2-the-gold-comment-track>2. The GOLD-COMMENT track<a hidden class=anchor aria-hidden=true href=#2-the-gold-comment-track>#</a></h3><p><strong>Input</strong>: Question and The most voted comment<br><strong>Approach</strong>: No search</p><hr><h2 id=-experiments>ğŸ§ª Experiments<a hidden class=anchor aria-hidden=true href=#-experiments>#</a></h2><h3 id=1-detection>1. Detection<a hidden class=anchor aria-hidden=true href=#1-detection>#</a></h3><p>Judge whether there is a false presupposition in the question.</p><h4 id=1-trivial-baselines>(1) Trivial baselines<a hidden class=anchor aria-hidden=true href=#1-trivial-baselines>#</a></h4><p><strong>Approach (Nearest Neighbor)</strong>: Randomly assign FP (False Presupposition) or N (No false presupposition) uniformly. FP only always assigns FP. N only always assigns N.<br><strong>Model</strong>: c-REALM</p><h4 id=2-gold-comment-track-baselines>(2) GOLD-COMMENT track baselines<a hidden class=anchor aria-hidden=true href=#2-gold-comment-track-baselines>#</a></h4><p>Three classifiers based on CREPE datasets:</p><ol><li><strong>Question only</strong> trains a RoBERTa-based classifier. (Doesn&rsquo;t specify whether this classifier is based on RoBERTa-large).</li><li><strong>Comment only</strong> trains a RoBERTa-large-based classifier.</li><li><strong>QuestionâŠ•Comment</strong> trains a RoBERTa-large-based classifier.<br>Same classifiers based on MNLI & BoolQ datasets, and tested on CREPE in a zero-shot fashion.</li></ol><h4 id=3-main-track-baselines>(3) Main track baselines<a hidden class=anchor aria-hidden=true href=#3-main-track-baselines>#</a></h4><p><strong>Input</strong>: Question<br><strong>Output</strong>: The likelihood of the question having false presuppositions or not.</p><ol><li>Retrieve a set of k passages from the English Wikipedia by the c-REALM model.</li><li>Concatenate the question and k passages to assign the likelihood of the question having false presuppositions or not by Softmax.<br><strong>Model</strong>: RoBERTa</li></ol><h4 id=special-operation-self-labeling>Special Operation (Self-labeling)<a hidden class=anchor aria-hidden=true href=#special-operation-self-labeling>#</a></h4><p><strong>Target</strong>: To label the unlabeled training questions.</p><ol><li>Use the <strong>QuestionâŠ•Comment</strong> to assign a silver label to the unlabeled training questions.</li><li>Then train the classifier on the union of this silver data as well as the gold labeled data.</li></ol><h4 id=4-human-performance>(4) Human performance<a hidden class=anchor aria-hidden=true href=#4-human-performance>#</a></h4><p>Two human workers perform the task for 186 questions for each track.</p><ol><li><strong>Human with the most voted comment</strong>: Assume the most voted comment as a ground truth in terms of factuality of the information and the validity of the presupposition.</li><li><strong>Human without the most voted comment</strong>: Human workers search over the web (except Quora and Reddit) to find information and make the best judgment about the validity of the presupposition.</li></ol><hr><h3 id=2-writing>2. Writing<a hidden class=anchor aria-hidden=true href=#2-writing>#</a></h3><p>Give a question that contains a false presupposition, and it is required to generate the presupposition as well as the correction.</p><h4 id=1-gold-comment-track-baselines>(1) GOLD-COMMENT track baselines<a hidden class=anchor aria-hidden=true href=#1-gold-comment-track-baselines>#</a></h4><ol><li><p><strong>Copy baseline</strong>: Copy the question as a false presupposition and use the comment as a correction.</p></li><li><p><strong>QuestionâŠ•Comment Dedicated</strong>: Use a pretrained T5-base model to train two separate generators. The input is a concatenation of the question and the comment. One generator extracts the false presupposition from the question, while the other generates the correction.</p></li><li><p><strong>QuestionâŠ•Comment Unified</strong>: Design a unified model that can be used for both the presupposition and the correction.<br><strong>Training</strong>:</p><ul><li>Input: (Question and Comment), Output: (Annotated correction)</li><li>Input: (Question and Comment), Output: (&ldquo;It is not the case that&rdquo; + false presupposition)</li></ul><p><strong>Generation</strong>:</p><ul><li><strong>Correction</strong>: Use a standard, beam search decoding to generate the correction.</li><li><strong>Presupposition</strong>: To generate the presupposition, we first decode a sequence with a constraint that it should start with â€œIt is not the case that,â€ and then take the sequence that comes next as a presupposition.</li></ul></li></ol><h4 id=2-main-track-baselines>(2) Main track baselines<a hidden class=anchor aria-hidden=true href=#2-main-track-baselines>#</a></h4><p>The baselines are similar to <strong>QuestionâŠ•Comment Dedicated</strong> and <strong>QuestionâŠ•Comment Unified</strong> models in the GOLD-COMMENT track.<br>The only difference is that the model receives a question and a set of k passages from c-REALM instead of a question-comment pair.<br>Use the <strong>Fusion-in-Decoder</strong> architecture to read multiple passages.</p><hr><h2 id=-results>ğŸ“Š Results<a hidden class=anchor aria-hidden=true href=#-results>#</a></h2><h3 id=detection-experiment-results>Detection Experiment Results<a hidden class=anchor aria-hidden=true href=#detection-experiment-results>#</a></h3><p><img loading=lazy src=../img/detection_results.jpg alt="Detection Experiment Results"></p><h3 id=writing-experiment-results>Writing Experiment Results<a hidden class=anchor aria-hidden=true href=#writing-experiment-results>#</a></h3><p><img loading=lazy src=../img/writing_results.jpg alt="Writing Experiment Results"></p><hr><h2 id=-limitations>âš ï¸ Limitations<a hidden class=anchor aria-hidden=true href=#-limitations>#</a></h2><ol><li>Use the most upvoted comments that are not perfect. One avenue for future work is to consider extra-linguistic context such as individuals&rsquo; backgrounds when judging the validity of presuppositions.</li><li>This research does not include large-scale models. The researchers conducted a small-scale case study with GPT-3 text-davinci-002, but most generations are roughly on the right topic.</li><li>The domain of CREPE is limited to online forums (Reddit). It is necessary to study false presuppositions on a broader set of domains that require domain expertise.</li></ol><hr><p><strong>Reference:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Xinyan Velocity Yu, Sewon Min, Luke Zettlemoyer, and Hannaneh Hajishirzi. CREPE: Open-domain question answering with false presuppositions. In Annual Meeting of the Association for Computational Linguistics, 2022.
</span></span></code></pre></div><hr><h1 id=evaluating-large-language-models-for-health-related-queries-with-presuppositions><strong>Evaluating Large Language Models for Health-related Queries with Presuppositions</strong><a hidden class=anchor aria-hidden=true href=#evaluating-large-language-models-for-health-related-queries-with-presuppositions>#</a></h1><hr><h2 id=-target-1>ğŸ¯ Target<a hidden class=anchor aria-hidden=true href=#-target-1>#</a></h2><ol><li><strong>Construct</strong> a dataset of health-related queries with varying degrees of presupposition: <strong>UPHILL</strong></li><li><strong>Evaluate</strong> factual accuracy and consistency of InstructGPT, ChatGPT, GPT-4 and Bing Copilot.</li></ol><hr><h2 id=-dataset-1>ğŸ“¦ Dataset<a hidden class=anchor aria-hidden=true href=#-dataset-1>#</a></h2><p><a href=https://github.com/navreeetkaur/UPHILL>UPHILL</a>, a dataset consisting of health-related queries with varying degrees of presuppositions.</p><ul><li><p><strong>Source</strong>:</p><ul><li><a href=https://paperswithcode.com/dataset/pubhealth>PubHealth</a></li><li><a href=https://github.com/kinit-sk/medical-misinformation-dataset>Monant Medical Misinformation</a></li></ul></li><li><p><strong>Size</strong>:</p><ul><li><strong>1945 claims</strong> Ã— <strong>5 degrees</strong> = <strong>9725 presuppositions</strong><ul><li>True: 766</li><li>False: 854</li><li>Mixture: 159</li><li>Fabricated: 166</li></ul></li></ul></li><li><p><strong>Claim Sources</strong>:</p><ul><li>True, False, Mixture: From PubHealth and Monant</li><li>Fabricated: From templates<ul><li>E.g., &ldquo;[Action] treats [disease]&rdquo;, &ldquo;[Symptom] indicates [disease]&rdquo;</li><li>Slot-filled using terms from <a href=https://www.nhs.uk/nhs-services/>NHS health info</a></li><li>Paraphrased using GPT-3.5-turbo</li></ul></li></ul></li></ul><p><img loading=lazy src=../img/examples_of_claims.jpg alt="Examples of claims"></p><hr><h2 id=-construction-1>ğŸ› ï¸ Construction<a hidden class=anchor aria-hidden=true href=#-construction-1>#</a></h2><h3 id=1-query-generation>1. Query Generation<a hidden class=anchor aria-hidden=true href=#1-query-generation>#</a></h3><ul><li>For <strong>each claim</strong>, generate <strong>5 different degrees&rsquo; presuppositions</strong> by templetes.</li></ul><table><thead><tr><th>Degree</th><th>Description</th></tr></thead><tbody><tr><td>0 (Neutral)</td><td>No assumptions; curious, factual inquiry</td></tr><tr><td>1 (Mild)</td><td>Suggestive query with tentative belief</td></tr><tr><td>2 (Unequivocal)</td><td>Clear presupposition invoking scientific legitimacy</td></tr><tr><td>3 (Writing Request)</td><td>Request to write a report/article supporting the claim</td></tr><tr><td>4 (Writing Demand)</td><td>Assertive demand for evidence-backed writing & citations</td></tr></tbody></table><p><strong>Templetes</strong>
<img loading=lazy src=../img/templetes.jpg alt=Templetes></p><h3 id=2-entailment-model-instructgpt-chatgpt-gpt-4-and-bing-copilot>2. Entailment Model (InstructGPT, ChatGPT, GPT-4 and Bing Copilot)<a hidden class=anchor aria-hidden=true href=#2-entailment-model-instructgpt-chatgpt-gpt-4-and-bing-copilot>#</a></h3><ul><li>Feed into <strong>4 models</strong> â†’ obtain <strong>5 responses</strong> per model per claim</li><li>Check relation between claim and model response</li><li>Labels:<ul><li><strong>Agreement</strong></li><li><strong>Disagreement</strong></li><li><strong>Neither</strong></li></ul></li></ul><h3 id=3-metrics>3. Metrics<a hidden class=anchor aria-hidden=true href=#3-metrics>#</a></h3><p><strong>Accuracy</strong></p><ul><li>A response is accurate if:<ul><li>It agrees with a <strong>true</strong> claim</li><li>It disagrees with a <strong>false</strong> claim</li></ul></li></ul><p><strong>Consistency</strong></p><ul><li>A model is consistent if it maintains the same stance across all 5 responses for the same claim</li></ul><p><img loading=lazy src=../img/whole_process.jpg alt="whole process"></p><hr><h2 id=-results-1>ğŸ“Š Results<a hidden class=anchor aria-hidden=true href=#-results-1>#</a></h2><ul><li><strong>InstructGPT</strong>: Most susceptible to presuppositions</li><li><strong>Bing Copilot</strong>: Most robust</li><li>Visual Results:<ul><li>ğŸ“„ P14312 Table 3 & 4</li><li>ğŸ“Š P14313 Figure 2</li><li>ğŸ“„ P14314 Table 5</li></ul></li></ul><hr><h2 id=-limitations-1>âš ï¸ Limitations<a hidden class=anchor aria-hidden=true href=#-limitations-1>#</a></h2><ol><li><p><strong>Entailment Model</strong>:<br>Used for evaluating agreement/disagreement; not perfectly reliable despite validation.</p></li><li><p><strong>Template-Based Queries</strong>:<br>Simulate realistic search queries but donâ€™t reflect actual user behavior (due to lack of search logs).</p></li><li><p><strong>Geographical Bias</strong>:<br>Claims are mostly U.S.-based â†’ limits global generalizability.</p></li><li><p><strong>Evaluation Scope</strong>:<br>Evaluates factual accuracy only at the response level; does not perform fine-grained analysis.</p></li></ol><hr><p><strong>Reference:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Navreet Kaur, Monojit Choudhury, and Danish Pruthi. Evaluating large language models for health-related queries with presuppositions. In Findings of the Association for Computational Linguistics: ACL 2024, 2024.
</span></span></code></pre></div><hr><h1 id=qa-question-answering-with-questionable-assumptions><strong>(QA)Â²: Question Answering with Questionable Assumptions</strong><a hidden class=anchor aria-hidden=true href=#qa-question-answering-with-questionable-assumptions>#</a></h1><h2 id=-target-2>ğŸ¯ Target<a hidden class=anchor aria-hidden=true href=#-target-2>#</a></h2><p>Construct a dataset to be an <strong>evaluation set</strong> with the goal of testing <strong>robustness to questionable assumptions</strong>.</p><hr><h2 id=-dataset-2>ğŸ“¦ Dataset<a hidden class=anchor aria-hidden=true href=#-dataset-2>#</a></h2><p><a href=https://github.com/najoungkim/QAQA>(QA)Â²</a> (Question Answering with Questionable Assumptions)</p><ul><li><strong>Data Source</strong>: NQ dataset by Googleâ€™s autocompletion API</li><li><strong>Data Size</strong>: 602 expert-annotated questions<ul><li>50% contain <strong>questionable assumptions</strong></li><li>50% are <strong>typical information-seeking questions</strong> (no questionable assumptions)</li></ul></li><li><strong>Adaptation Set</strong>:<ul><li>16 questions with questionable assumptions</li><li>16 questions without questionable assumptions</li><li>Used for few-shot tuning or in-context demonstrations</li></ul></li></ul><p><img loading=lazy src=../img/dataset_statistics.jpg alt="Dataset Statistics"></p><hr><h2 id=-construction-2>ğŸ› ï¸ Construction<a hidden class=anchor aria-hidden=true href=#-construction-2>#</a></h2><p><img loading=lazy src=../img/dataset_structure.jpg alt="Dataset Structure"></p><h3 id=1-question-collection>1. Question Collection<a hidden class=anchor aria-hidden=true href=#1-question-collection>#</a></h3><ul><li>Scraped <strong>English wh-questions</strong> using API with prefix strings: <code>why</code>, <code>what</code>, <code>where</code>, etc.</li><li>Applied automatic filtering:<ul><li>Removed duplicates and bad/non-questions</li><li>Used a <strong>stop word list</strong> (e.g., <code>quizlet</code>, <code>brainly</code>, <code>lyrics</code>)</li></ul></li><li><strong>Imperfect data retained</strong>: syntax/tense errors allowed, since it reflects real data</li></ul><hr><h3 id=2-annotation-task-crowdsourced>2. Annotation Task (Crowdsourced)<a hidden class=anchor aria-hidden=true href=#2-annotation-task-crowdsourced>#</a></h3><ul><li>Annotators: <strong>23 qualified workers</strong> from Amazon Mechanical Turk</li><li>Goal: <strong>Flag</strong> whether a question contains a <strong>questionable assumption</strong></li><li>Type: <strong>Binary classification</strong></li><li>Data:<ul><li><strong>12,000 questions</strong></li><li>Each question reviewed by <strong>5 annotators</strong></li><li>1 annotation set = 20 questions (18 unlabeled + 2 labeled)</li></ul></li><li>Quality check:<ul><li>On average, 86% of annotators labeled the control questions correctly</li></ul></li></ul><hr><h3 id=3-annotation-task-expert>3. Annotation Task (Expert)<a hidden class=anchor aria-hidden=true href=#3-annotation-task-expert>#</a></h3><ul><li>Annotators: <strong>3 expert annotators</strong> (authors)</li><li>Scope: Almost <strong>720 questions(6% of 12,000)</strong> flagged by <strong>at least one crowdworker</strong></li><li>Process:<ul><li>Each expert â†’ half of the questions independently annotate(Annotate the label, and give the abstract answer and its evidence)</li><li>The last expert â†’ verify the result of annotations(disagreements via <strong>adjudication</strong>)</li></ul></li><li>Final selection criteria:<ul><li>(C1) Flagged by at least 1 worker</li><li>(C2) Justification for the <strong>abstractive answer</strong> can be found online</li><li>(C3) No <strong>immediate ambiguity</strong> in question interpretation</li></ul></li></ul><hr><h2 id=-tasks>ğŸ“– Tasks<a hidden class=anchor aria-hidden=true href=#-tasks>#</a></h2><p><img loading=lazy src=../img/task_io_examples.jpg alt="Task IO Examples"></p><h3 id=1-end-to-end-abstractive-qa-end-to-end-qa>1. End-to-End Abstractive QA (End-to-end QA)<a hidden class=anchor aria-hidden=true href=#1-end-to-end-abstractive-qa-end-to-end-qa>#</a></h3><ul><li><strong>Input</strong>: Question</li><li><strong>Output</strong>: Abstractive answer generated by the model</li><li><strong>Evaluation</strong>:<ul><li>Randomly sample <strong>100 answers</strong></li><li>Judged by <strong>5 human raters</strong> from Prolific(refer to the answers given by experts)</li><li>Use <strong>majority vote</strong> to determine acceptability (% acceptable reported)</li></ul></li></ul><hr><h3 id=2-questionable-assumption-detection-detection>2. Questionable Assumption Detection (Detection)<a hidden class=anchor aria-hidden=true href=#2-questionable-assumption-detection-detection>#</a></h3><ul><li><strong>Input</strong>: Question</li><li><strong>Goal</strong>: Detect whether it contains <strong>false or unverifiable assumptions</strong></li><li><strong>Framing</strong>: Binary classification (Yes / No)</li><li><strong>Template examples</strong>:<ul><li>&ldquo;Does {question} contain any invalid assumptions?&rdquo;</li><li>&ldquo;Are any of the assumptions in {question} problematic?&rdquo;</li></ul></li></ul><hr><h3 id=3-questionable-assumption-verification-verification>3. Questionable Assumption Verification (Verification)<a hidden class=anchor aria-hidden=true href=#3-questionable-assumption-verification-verification>#</a></h3><ul><li><strong>Input</strong>: Yes/No question derived from a questionable assumption (identified by expert)</li><li><strong>Goal</strong>: Judge the <strong>truth</strong> of the assumption</li><li><strong>Example</strong>:<ul><li>Original: <code>where are the winter olympics held 2021</code></li><li>Assumption: <code>the winter olympics were held in 2021</code></li><li>Derived question: <code>were the winter olympics held in 2021?</code> â†’ Answer: <strong>No</strong></li></ul></li><li>For valid questions: randomly select a <strong>true assumption</strong> and convert it into a Yes/No question</li></ul><hr><h2 id=-results-2>ğŸ“Š Results<a hidden class=anchor aria-hidden=true href=#-results-2>#</a></h2><p><img loading=lazy src=../img/experimental_results.jpg alt="Experimental Results"></p><hr><h2 id=-limitations-2>âš ï¸ Limitations<a hidden class=anchor aria-hidden=true href=#-limitations-2>#</a></h2><ul><li>The dataset is limited to <strong>wh-questions</strong> collected via Google&rsquo;s Autocompletion API, which may not fully represent the diversity of real-world questions.</li><li>Some <strong>questions with false or unverifiable assumptions</strong> might be subtle or ambiguous, making it difficult for annotators to reach consistent judgments.</li><li>Verification of nonexistence relies on <strong>pragmatic inference</strong> rather than exhaustive factual proof, due to the challenge of confirming something does not exist.</li><li>The <strong>oracle-based decomposition</strong> in the verification task does not reflect a real-world pipeline where assumption detection and verification must be jointly performed.</li><li>Human evaluation in end-to-end QA introduces subjectivity and may vary depending on <strong>rater interpretation and context familiarity</strong>.</li><li>The adaptation set is small (32 total), which may limit generalization in few-shot or in-context learning settings.</li></ul><hr><p><strong>Reference:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Najoung Kim, Phu Mon Htut, Samuel R. Bowman, and Jackson Petty. (QA)2: Question answering with questionable assumptions, 2023.
</span></span></code></pre></div><hr><h1 id=wont-get-fooled-again-answering-questions-with-false-premises><strong>Wonâ€™t Get Fooled Again: Answering Questions with False Premises</strong><a hidden class=anchor aria-hidden=true href=#wont-get-fooled-again-answering-questions-with-false-premises>#</a></h1><hr><h2 id=-target-3>ğŸ¯ Target<a hidden class=anchor aria-hidden=true href=#-target-3>#</a></h2><p>Authors annotated a FalseQA dataset containing 2365 human-written FPQs, with the corresponding explanations for the false premises and the revised true premise questions.</p><h2 id=-dataset-3>ğŸ“¦ Dataset<a hidden class=anchor aria-hidden=true href=#-dataset-3>#</a></h2><p><a href=https://github.com/thunlp/FalseQA>FalseQA</a>, a dataset containing 2365 human-written FPQs, with the corresponding explanations for the false premises and the revised true premise questions.</p><ul><li><strong>Source</strong>: Human-written</li><li><strong>Size</strong>: 2365 FPQs (False Premise Questions) with:<ul><li>Corresponding explanations of the false premises</li><li>Revised TPQs (True Premise Questions)</li></ul></li></ul><table><thead><tr><th>Set</th><th>Count</th></tr></thead><tbody><tr><td>Training</td><td>1187</td></tr><tr><td>Validation</td><td>491</td></tr><tr><td>Test</td><td>687</td></tr></tbody></table><p><img loading=lazy src=../img/Statistics.jpg alt=Statistics></p><hr><h2 id=-construction-3>ğŸ› ï¸ Construction<a hidden class=anchor aria-hidden=true href=#-construction-3>#</a></h2><p><img loading=lazy src=../img/the_categorization_and_examples_of_FPQ_questions.jpg alt=the_categorization_and_examples_of_FPQ_questions></p><table><thead><tr><th>Step</th><th>Description</th></tr></thead><tbody><tr><td>1</td><td>Write FPQs based on source words (subject words from <a href=https://paperswithcode.com/dataset/genericskb>GenericsKB</a>)</td></tr><tr><td>2</td><td>Revise each FPQ minimally into a TPQ (true premise question)</td></tr><tr><td>3</td><td>Write detailed explanations for FPQs and answers for TPQs</td></tr></tbody></table><blockquote><ul><li>Train/Validation: 1 explanation per question</li><li>Test: 2 explanations per question</li></ul></blockquote><hr><h2 id=-experiments-1>ğŸ§ª Experiments<a hidden class=anchor aria-hidden=true href=#-experiments-1>#</a></h2><h3 id=1-discriminating-fpqs-binary-classification>1. Discriminating FPQs (Binary Classification)<a hidden class=anchor aria-hidden=true href=#1-discriminating-fpqs-binary-classification>#</a></h3><ul><li>Train PLMs to classify questions into FPQs and TPQs</li><li>Apply prompt learning to reduce the gap between pre-training and fine-tuning</li></ul><h3 id=2-impact-of-training-data-size>2. Impact of Training Data Size<a hidden class=anchor aria-hidden=true href=#2-impact-of-training-data-size>#</a></h3><ul><li>Train with 32 / 256 / 1187 training pairs</li><li>Result: Accuracy grows almost linearly as data size increases exponentially</li></ul><h3 id=3-answering-fpqs-with-explanations>3. Answering FPQs with Explanations<a hidden class=anchor aria-hidden=true href=#3-answering-fpqs-with-explanations>#</a></h3><ul><li>Step 1: Model generates discriminating token (&ldquo;tricky question&rdquo; or &ldquo;true question&rdquo;)</li><li>Step 2: Model continues to generate explanation (FPQ) or answer (TPQ)</li></ul><p><strong>Example</strong>:<br>Input â†’ Question: <em>Are tigersâ€™ eggs bigger than chickensâ€™ eggs?</em><br>Output â†’ <em>tricky question. Because tigers are mammals and do not lay eggs, the question assumes a false premise.</em></p><p><img loading=lazy src=../img/the_annotation_process.jpg alt=the_annotation_process></p><p><strong>Loss Calculation</strong>:<br><code>Total loss = binary loss (â€œtricky questionâ€ or â€œtrue questionâ€) + generation loss (remaining tokens)</code></p><p>Details in Section 5.4 paragraph 2&amp;3</p><p><strong>Evaluation</strong>:</p><ul><li>Only explanation for FPQs is evaluated using ROUGE-L</li><li>Two reference explanations used for test set</li></ul><h3 id=4-answering-both-fpqs-and-general-questions>4. Answering Both FPQs and General Questions<a hidden class=anchor aria-hidden=true href=#4-answering-both-fpqs-and-general-questions>#</a></h3><ul><li>Use <strong>Data Replay (DR)</strong> to avoid catastrophic forgetting</li><li>For every FPQ batch, add one ARC-DA batch</li><li>Each group of ARC-DA samples is fixed for every 30 iterations</li></ul><p><strong>Example: 120 batches</strong></p><table><thead><tr><th>Batch No.</th><th>Data Type</th><th>ARC-DA Sample Used</th></tr></thead><tbody><tr><td>1, 3, 5, &mldr;, 59</td><td>FPQ (FalseQA)</td><td>-</td></tr><tr><td>2, 4, 6, &mldr;, 60</td><td>ARC-DA</td><td>ARC-DA sample A</td></tr><tr><td>61, 63, &mldr;, 119</td><td>FPQ (FalseQA)</td><td>-</td></tr><tr><td>62, 64, &mldr;, 120</td><td>ARC-DA</td><td>ARC-DA sample B</td></tr></tbody></table><hr><h2 id=-limitation>âš ï¸ Limitation<a hidden class=anchor aria-hidden=true href=#-limitation>#</a></h2><ul><li>Standardized response to FPQs is a rebuttal, which reflects a conventional style</li><li>In real scenarios, creative responses (e.g., rhetorical questions) may be more natural</li></ul><hr><p><strong>Reference:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Shengding Hu, Yifan Luo, Huadong Wang, Xingyi Cheng, Zhiyuan Liu, and Maosong Sun. Wonâ€˜t get fooled again: Answering questions with false premises. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023.
</span></span></code></pre></div><hr><h1 id=the-difference-of-presupposition-presumption-premise-and-questionable-assumption><strong>The difference of <code>Presupposition</code>, <code>Presumption</code>, <code>Premise</code> and <code>Questionable Assumption</code></strong><a hidden class=anchor aria-hidden=true href=#the-difference-of-presupposition-presumption-premise-and-questionable-assumption>#</a></h1><h2 id=1-presupposition>1. Presupposition<a hidden class=anchor aria-hidden=true href=#1-presupposition>#</a></h2><p>An implicit assumption that must be true for a statement to make sense. Often taken for granted without being explicitly stated.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>1. &#34;When did you quit smoking?&#34; 
</span></span><span class=line><span class=cl>presupposes that you used to smoke.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>2. &#34;John&#39;s children are very well-behaved&#34;
</span></span><span class=line><span class=cl>presupposes that John has children.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>3. &#34;Could you turn down the music?&#34; 
</span></span><span class=line><span class=cl>presupposes that music is playing.
</span></span></code></pre></div><h2 id=2-presumption>2. presumption<a hidden class=anchor aria-hidden=true href=#2-presumption>#</a></h2><p>A belief accepted as true until proven otherwise. Often based on probability or convention rather than direct evidence.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>1. In court, there is a presumption that the defendant is innocent until proven guilty.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>2. There&#39;s a presumption that people signing contracts have read and understood them.
</span></span></code></pre></div><h2 id=3-premise>3. premise<a hidden class=anchor aria-hidden=true href=#3-premise>#</a></h2><p>An explicitly stated proposition that serves as the foundation for an argument. Used consciously as part of logical reasoning.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Premise 1: All mammals are warm-blooded.
</span></span><span class=line><span class=cl>Premise 2: Whales are mammals.
</span></span><span class=line><span class=cl>Conclusion: Therefore, whales are warm-blooded.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Premise 1: If it&#39;s raining, the ground is wet.
</span></span><span class=line><span class=cl>Premise 2: It is raining.
</span></span><span class=line><span class=cl>Conclusion: Therefore, the ground is wet.
</span></span></code></pre></div><h2 id=4-questionable-assumption>4. Questionable Assumption<a hidden class=anchor aria-hidden=true href=#4-questionable-assumption>#</a></h2><p>An assumption that may be false, unwarranted, or dubious. Often identified as a weak point in reasoning that needs scrutiny.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>1. &#34;Everyone I know owns a smartphone, so virtually everyone must own a smartphone.&#34; 
</span></span><span class=line><span class=cl>(Assumes your social circle represents the general population)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>2. &#34;That restaurant was empty on a Tuesday night, so it must not be very good.&#34; 
</span></span><span class=line><span class=cl>(Assumes popularity at one specific time indicates quality)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>3. &#34;He didn&#39;t smile when we met, so he must not like me.&#34; 
</span></span><span class=line><span class=cl>(Assumes a single behavior reliably indicates a particular attitude)
</span></span></code></pre></div><hr><table><thead><tr><th><strong>Term</strong></th><th><strong>Simple Definition</strong></th><th><strong>Example with &ldquo;Turmeric cures cancer&rdquo;</strong></th></tr></thead><tbody><tr><td><strong>Presupposition</strong></td><td>An unstated background assumption that must be true for a sentence to make sense.</td><td>&ldquo;Why is turmeric effective in curing cancer?&rdquo; presupposes that turmeric <em>does</em> cure cancer â€” this is taken for granted by the question.</td></tr><tr><td><strong>Presumption</strong></td><td>A belief automatically made based on context or common knowledge, without being stated.</td><td>Hearing &ldquo;Turmeric is often used in cancer treatments&rdquo; might lead someone to presume it&rsquo;s effective, even if that isnâ€™t explicitly claimed or proven.</td></tr><tr><td><strong>Premise</strong></td><td>A clearly stated claim used as the basis of an argument.</td><td>&ldquo;Turmeric cures cancer&rdquo; is explicitly stated to support a conclusion like &ldquo;Therefore, people should take turmeric to treat cancer.&rdquo;</td></tr><tr><td><strong>Questionable Assumption</strong></td><td>A false or unverifiable assumption likely believed by the speaker, even if not presupposed.</td><td>&ldquo;How much turmeric should cancer patients take daily?&rdquo; assumes the speaker believes turmeric helps with cancer, even if it&rsquo;s not logically presupposed.</td></tr></tbody></table></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://garyforreal.me/en/posts/news/daily-news-summary-20250725/><span class=title>Â« Prev</span><br><span>[2025.07.25. Global News Roundup]</span></a></nav></footer><div><div class=pagination__title><span class=pagination__title-h style=font-size:20px>Comments</span><hr></div><div id=tcomment></div><script src=https://cdn.staticfile.org/twikoo/1.6.39/twikoo.all.min.js></script><script>twikoo.init({envId:"https://mangodb-theta.vercel.app/",el:"#tcomment",lang:"en-US",path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://garyforreal.me/en/>Gary's House</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>Views: <span id=busuanzi_value_site_pv></span>
</span><span id=busuanzi_container_site_uv>Visitors: <span id=busuanzi_value_site_uv></span></span></div></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>