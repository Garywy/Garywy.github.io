<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Paper on Gary&#39;s House</title>
    <link>https://garyforreal.me/en/posts/paper/</link>
    <description>Recent content in Paper on Gary&#39;s House</description>
    <generator>Hugo -- 0.141.0</generator>
    <language>en</language>
    <lastBuildDate>Sun, 21 Dec 2025 15:23:23 +0000</lastBuildDate>
    <atom:link href="https://garyforreal.me/en/posts/paper/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Weekly Paper Notes - 2025-12-21</title>
      <link>https://garyforreal.me/en/posts/paper/paper-2025-12-21-weekly/</link>
      <pubDate>Sun, 21 Dec 2025 15:23:23 +0000</pubDate>
      <guid>https://garyforreal.me/en/posts/paper/paper-2025-12-21-weekly/</guid>
      <description>&lt;h1 id=&#34;weekly-paper-notes&#34;&gt;Weekly Paper Notes&lt;/h1&gt;
&lt;h2 id=&#34;-multilingual&#34;&gt;üîç multilingual&lt;/h2&gt;
&lt;h3 id=&#34;from-essence-to-defense-adaptive-semantic-aware-watermarking-for-embedding-as-a-service-copyright-protectionhttpsarxivorgabs251216439v1&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2512.16439v1&#34;&gt;From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Li, Yubing Ren, Yanan Cao, Yingjie Li, Fang Fang, Xuebin Wang
&lt;strong&gt;Venue:&lt;/strong&gt; arXiv (2025)&lt;/p&gt;
&lt;p&gt;Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Weekly Paper Notes - 2025-12-14</title>
      <link>https://garyforreal.me/en/posts/paper/paper-2025-12-14-weekly/</link>
      <pubDate>Sun, 14 Dec 2025 15:23:14 +0000</pubDate>
      <guid>https://garyforreal.me/en/posts/paper/paper-2025-12-14-weekly/</guid>
      <description>&lt;h1 id=&#34;weekly-paper-notes&#34;&gt;Weekly Paper Notes&lt;/h1&gt;
&lt;h2 id=&#34;-multilingual&#34;&gt;üîç multilingual&lt;/h2&gt;
&lt;h3 id=&#34;grow-up-and-merge-scaling-strategies-for-efficient-language-adaptationhttpsarxivorgabs251210772v1&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2512.10772v1&#34;&gt;Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kevin Glocker, K√§triin Kukk, Romina Oji, Marcel Bollmann, Marco Kuhlmann, Jenny Kunz
&lt;strong&gt;Venue:&lt;/strong&gt; arXiv (2025)&lt;/p&gt;
&lt;p&gt;Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model&amp;rsquo;s capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Research_lab_works</title>
      <link>https://garyforreal.me/en/posts/paper/research_lab_works/</link>
      <pubDate>Fri, 21 Mar 2025 15:15:43 +0900</pubDate>
      <guid>https://garyforreal.me/en/posts/paper/research_lab_works/</guid>
      <description>&lt;h1 id=&#34;crepe-open-domain-question-answering-with-false-presuppositions&#34;&gt;&lt;strong&gt;CREPE: Open-Domain Question Answering with False Presuppositions&lt;/strong&gt;&lt;/h1&gt;
&lt;h2 id=&#34;-target&#34;&gt;üéØ Target&lt;/h2&gt;
&lt;p&gt;The authors introduced CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums.&lt;/p&gt;
&lt;h2 id=&#34;-dataset&#34;&gt;üì¶ Dataset&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/velocityCavalry/CREPE&#34;&gt;CREPE&lt;/a&gt;, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums.&lt;br&gt;
&lt;strong&gt;Data Source&lt;/strong&gt;: Reddit, &lt;a href=&#34;http://www.reddit.com/r/explainlikeimfive&#34;&gt;the ELI5 subreddit&lt;/a&gt;.&lt;br&gt;
&lt;strong&gt;Data Numbers&lt;/strong&gt;: 8,400 Reddit questions with:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Labels (whether there are any false presuppositions).&lt;/li&gt;
&lt;li&gt;The false presuppositions and their corrections, if there are any false presuppositions in questions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;../img/statistics1.jpg&#34; alt=&#34;Dataset statistics&#34;  /&gt;
&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
