<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Weekly Paper Notes - 2025-12-09 | Gary's House</title>
<meta name=keywords content><meta name=description content="Weekly Paper Notes
üîç multilingual
M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG
Authors: David Anugraha, Patrick Amadeus Irawan, Anshul Singh, En-Shiun Annie Lee, Genta Indra Winata
Venue: arXiv (2025)
Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts."><meta name=author content="Gary"><link rel=canonical href=https://garyforreal.me/en/posts/paper/paper-2025-12-09-weekly/><meta name=google-site-verification content="XYZabc"><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><link crossorigin=anonymous href=/assets/css/stylesheet.7e5251d8716d933fafcf3df70d7ecd02729661d6d232fc5dd0b8a85ef75e3409.css integrity="sha256-flJR2HFtkz+vzz33DX7NAnKWYdbSMvxd0LioXvdeNAk=" rel="preload stylesheet" as=style><link rel=icon href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=16x16 href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=32x32 href=https://garyforreal.me/img/Q.jpg><link rel=apple-touch-icon href=https://garyforreal.me/Q.jpg><link rel=mask-icon href=https://garyforreal.me/Q.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://garyforreal.me/zh/posts/paper/paper-2025-12-09-weekly/><link rel=alternate hreflang=en href=https://garyforreal.me/en/posts/paper/paper-2025-12-09-weekly/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@200..900&display=swap" rel=stylesheet><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous referrerpolicy=no-referrer></script><meta property="og:title" content="Weekly Paper Notes - 2025-12-09"><meta property="og:description" content="Weekly Paper Notes
üîç multilingual
M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG
Authors: David Anugraha, Patrick Amadeus Irawan, Anshul Singh, En-Shiun Annie Lee, Genta Indra Winata
Venue: arXiv (2025)
Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts."><meta property="og:type" content="article"><meta property="og:url" content="https://garyforreal.me/en/posts/paper/paper-2025-12-09-weekly/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-09T00:13:56+00:00"><meta property="article:modified_time" content="2025-12-09T00:13:56+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Weekly Paper Notes - 2025-12-09"><meta name=twitter:description content="Weekly Paper Notes
üîç multilingual
M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG
Authors: David Anugraha, Patrick Amadeus Irawan, Anshul Singh, En-Shiun Annie Lee, Genta Indra Winata
Venue: arXiv (2025)
Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://garyforreal.me/en/posts/"},{"@type":"ListItem","position":2,"name":"Paper","item":"https://garyforreal.me/en/posts/paper/"},{"@type":"ListItem","position":3,"name":"Weekly Paper Notes - 2025-12-09","item":"https://garyforreal.me/en/posts/paper/paper-2025-12-09-weekly/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Weekly Paper Notes - 2025-12-09","name":"Weekly Paper Notes - 2025-12-09","description":"Weekly Paper Notes üîç multilingual M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG Authors: David Anugraha, Patrick Amadeus Irawan, Anshul Singh, En-Shiun Annie Lee, Genta Indra Winata Venue: arXiv (2025)\nVision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.\n","keywords":[],"articleBody":"Weekly Paper Notes üîç multilingual M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG Authors: David Anugraha, Patrick Amadeus Irawan, Anshul Singh, En-Shiun Annie Lee, Genta Indra Winata Venue: arXiv (2025)\nVision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.\nüì• Save to Zotero üìÑ Download PDF\nEfficient Text Classification with Conformal In-Context Learning Authors: Ippokratis Pantelidis, Korbinian Randl, Aron Henriksson Venue: arXiv (2025)\nLarge Language Models (LLMs) demonstrate strong in-context learning abilities, yet their effectiveness in text classification depends heavily on prompt design and incurs substantial computational cost. Conformal In-Context Learning (CICLe) has been proposed as a resource-efficient framework that integrates a lightweight base classifier with Conformal Prediction to guide LLM prompting by adaptively reducing the set of candidate classes. However, its broader applicability and efficiency benefits beyond a single domain have not yet been systematically explored. In this paper, we present a comprehensive evaluation of CICLe across diverse NLP classification benchmarks. The results show that CICLe consistently improves over its base classifier and outperforms few-shot prompting baselines when the sample size is sufficient for training the base classifier, and performs comparably in low-data regimes. In terms of efficiency, CICLe reduces the number of shots and prompt length by up to 34.45% and 25.16%, respectively, and enables the use of smaller models with competitive performance. CICLe is furthermore particularly advantageous for text classification tasks with high class imbalance. These findings highlight CICLe as a practical and scalable approach for efficient text classification, combining the robustness of traditional classifiers with the adaptability of LLMs, and achieving substantial gains in data and computational efficiency.\nüì• Save to Zotero üìÑ Download PDF\nRRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS Authors: Cong Wang, Changfeng Gao, Yang Xiang, Zhihao Du, Keyu An, Han Zhao, Qian Chen, Xiangang Li, Yingming Gao, Ya Li Venue: arXiv (2025)\nDifferentiable reinforcement learning (RL) frameworks like DiffRO offer a powerful approach for controllable text-to-speech (TTS), but are vulnerable to reward hacking, particularly for nuanced tasks like emotion control. The policy model can exploit a vanilla Reward Model (RM) by generating acoustic artifacts to achieve spurious rewards, but at the cost of degrading perceptual quality. To address this, we propose Robust Reward Policy Optimization (RRPO), a novel framework that employs a hybrid regularization scheme. This scheme develops a robust RM whose reward signal is more reliably aligned with human perception, compelling the policy to abandon detrimental shortcuts and instead learn the complex features of genuine emotions. Our ablation study confirms the enhanced robustness of our RM, as evidenced by its strong cross-lingual generalization. The subjective evaluation demonstrates that this robust RM effectively mitigates reward hacking, leading to significant improvements in both emotional expressiveness and naturalness over all baselines. Demo page: https://lrwinr.github.io/RRPO-CosyVoice.\nüì• Save to Zotero üìÑ Download PDF\nAdapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study Authors: Lifeng Chen, Ryan Lai, Tianming Liu Venue: arXiv (2025)\nAdapting large language models (LLMs) to low-resource languages remains a major challenge due to data scarcity and cross-lingual drift. This work presents a two-stage adaptation of Qwen2.5-3B to Tibetan, a morphologically rich and underrepresented language. We employ Continual Pretraining (CPT) to establish Tibetan linguistic grounding, followed by Supervised Fine-Tuning (SFT) for task and translation specialization. Empirical evaluations demonstrate a consistent decrease in perplexity (from 2.98 $\\rightarrow$ 1.54) and substantial improvements in Chinese$\\rightarrow$Tibetan translation quality (BLEU: 0.046 $\\rightarrow$ 0.261; chrF: 2.2 $\\rightarrow$ 6.6). Layer-wise analysis across 435 layers in Qwen3-4B reveals that adaptation primarily concentrates on embedding and output heads, with mid‚Äìlate MLP projections encoding domain-specific transformations. Our findings suggest that CPT constructs a Tibetan semantic manifold while SFT sharpens task alignment with minimal representational disruption. This study provides the first quantitative exploration of Tibetan adaptation dynamics for LLMs, and offers an open, reproducible framework for extending multilingual foundation models to low-resource settings.\nüì• Save to Zotero üìÑ Download PDF\nBeyond Data Filtering: Knowledge Localization for Capability Removal in LLMs Authors: Igor Shilov, Alex Cloud, Aryo Pradipta Gema, Jacob Goldman-Wetzler, Nina Panickssery, Henry Sleight, Erik Jones, Cem Anil Venue: arXiv (2025)\nLarge Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, even small amounts of mislabeled content could give rise to dangerous capabilities. To address risks associated with mislabeled harmful content, prior work proposed Gradient Routing (Cloud et al., 2024) ‚Äì a technique that localizes target knowledge into a dedicated subset of model parameters so they can later be removed. We explore an improved variant of Gradient Routing, which we call Selective GradienT Masking (SGTM), with particular focus on evaluating its robustness to label noise. SGTM zero-masks selected gradients such that target domain examples only update their dedicated parameters. We test SGTM‚Äôs effectiveness in two applications: removing knowledge of one language from a model trained on a bilingual synthetic dataset, and removing biology knowledge from a model trained on English Wikipedia. In both cases SGTM provides better retain/forget trade-off in the presence of labeling errors compared to both data filtering and a previously proposed instantiation of Gradient Routing. Unlike shallow unlearning approaches that can be quickly undone through fine-tuning, SGTM exhibits strong robustness to adversarial fine-tuning, requiring seven times more fine-tuning steps to reach baseline performance on the forget set compared to a finetuning-based unlearning method (RMU). Our results suggest SGTM provides a promising pretraining-time complement to existing safety mitigations, particularly in settings where label noise is unavoidable.\nüì• Save to Zotero üìÑ Download PDF\nExploiting Spatial Multiplexing Based on Pixel Antennas: An Antenna Coding Approach Authors: Zixiang Han, Shanpu Shen, Ross Murch Venue: arXiv (2025)\nAn antenna coding approach for exploiting the spatial multiplexing capability of pixel antennas is proposed. This approach can leverage additional degrees of freedom in the beamspace domain to transmit more information streams. Pixel antennas are a general reconfigurable antenna design where a radiating structure with arbitrary shape and size can be discretized into sub-wavelength elements called pixels which are connected by radio frequency switches. By controlling the switch states, the pixel antenna topology can be flexibly adjusted so that the resulting radiation pattern can be reconfigured for beamspace spatial multiplexing. In this work, we introduce the antenna coder and pattern coder for pixel antennas, provide a multiple-input multiple-output (MIMO) communication system model with antenna coding in the beamspace domain, and derive the spectral efficiency. Utilizing the antenna coder, the radiation pattern of the pixel antenna is analyzed and efficient optimization algorithms are provided for antenna coding design. Numerical simulation results show that the proposed technique using pixel antennas can enhance spectral efficiency of 4-by-4 MIMO by up to 12 bits/s/Hz or equivalently reduce the required transmit power by up to 90% when compared to conventional MIMO, demonstrating the effectiveness of the antenna coding technique in spectral efficiency enhancement and its promise for future sixth generation (6G) wireless communication.\nüì• Save to Zotero üìÑ Download PDF\nThe Decision Path to Control AI Risks Completely: Fundamental Control Mechanisms for AI Governance Authors: Yong Tao Venue: arXiv (2025)\nArtificial intelligence (AI) advances rapidly but achieving complete human control over AI risks remains an unsolved problem, akin to driving the fast AI ‚Äútrain‚Äù without a ‚Äúbrake system.‚Äù By exploring fundamental control mechanisms at key elements of AI decisions, this paper develops a systematic solution to thoroughly control AI risks, providing an architecture for AI governance and legislation with five pillars supported by six control mechanisms, illustrated through a minimum set of AI Mandates (AIMs). Three of the AIMs must be built inside AI systems and three in society to address major areas of AI risks: 1) align AI values with human users; 2) constrain AI decision-actions by societal ethics, laws, and regulations; 3) build in human intervention options for emergencies and shut-off switches for existential threats; 4) limit AI access to resources to reinforce controls inside AI; 5) mitigate spillover risks like job loss from AI. We also highlight the differences in AI governance on physical AI systems versus generative AI. We discuss how to strengthen analog physical safeguards to prevent smarter AI/AGI/ASI from circumventing core safety controls by exploiting AI‚Äôs intrinsic disconnect from the analog physical world: AI‚Äôs nature as pure software code run on chips controlled by humans, and the prerequisite that all AI-driven physical actions must be digitized. These findings establish a theoretical foundation for AI governance and legislation as the basic structure of a ‚Äúbrake system‚Äù for AI decisions. If enacted, these controls can rein in AI dangers as completely as humanly possible, removing large chunks of currently wide-open AI risks, substantially reducing overall AI risks to residual human errors.\nüì• Save to Zotero üìÑ Download PDF\nStructured Reasoning with Tree-of-Thoughts for Bengali Math Word Problems Authors: Aurprita Mahmood, Sabrin alam, Neloy kumer Sagor, Md. Abdul Hadi, Md. Sehab Al Islam, Minhajul Islam Venue: arXiv (2025)\nMathematical Word Problems (MWPs) are among the most challenging tasks in natural language processing because they require both linguistic understanding and multi-step numerical reasoning. While Chain-of-Thought (CoT) prompting has shown promise, its linear structure often propagates errors, limiting overall effectiveness. To address this limitation, we present the a systematic study of Tree-of-Thought (ToT) reasoning for Bengali MWPs using the SOMADHAN dataset. Owing to computational and token-cost constraints, we evaluate a curated set of 100 representative problems across multiple large language models (LLMs), including GPT-OSS and LLaMA variants, under standard prompting, CoT, and ToT strategies. Our results show that CoT improves baseline accuracy from 78% (standard prompting) to 83% on average, while ToT further increases performance by up to 5 percentage points, achieving 88% accuracy with GPT-OSS-120B. These improvements highlight that ToT is particularly effective in medium-to-large-scale models but may offer less advantage for smaller ones. Overall, our findings establish ToT as a robust framework for solving mathematical problems in low-resource languages such as Bengali. More broadly, this study shows that structured reasoning methods like ToT can provide more reliable and globally consistent outcomes than CoT, paving the way for better reasoning strategies in multilingual NLP.\nüì• Save to Zotero üìÑ Download PDF\nEnhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms Authors: Francesco Granata, Francesco Poggi, Misael Mongiov√¨ Venue: arXiv (2025)\nIn the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.\nüì• Save to Zotero üìÑ Download PDF\nTraining-Time Action Conditioning for Efficient Real-Time Chunking Authors: Kevin Black, Allen Z. Ren, Michael Equi, Sergey Levine Venue: arXiv (2025)\nReal-time chunking (RTC) enables vision-language-action models (VLAs) to generate smooth, reactive robot trajectories by asynchronously predicting action chunks and conditioning on previously committed actions via inference-time inpainting. However, this inpainting method introduces computational overhead that increases inference latency. In this work, we propose a simple alternative: simulating inference delay at training time and conditioning on action prefixes directly, eliminating any inference-time overhead. Our method requires no modifications to the model architecture or robot runtime, and can be implemented with only a few additional lines of code. In simulated experiments, we find that training-time RTC outperforms inference-time RTC at higher inference delays. In real-world experiments on box building and espresso making tasks with the $œÄ_{0.6}$ VLA, we demonstrate that training-time RTC maintains both task performance and speed parity with inference-time RTC while being computationally cheaper. Our results suggest that training-time action conditioning is a practical drop-in replacement for inference-time inpainting in real-time robot control.\nüì• Save to Zotero üìÑ Download PDF\nThermodynamics of Shear Equilibration During Magnetic Reconnection Onset in Mixed-Equilibrium Current Sheets Authors: Dominic Payne, Marc Swisdak, James Drake, Tak Chu Li Venue: arXiv (2025)\nMagnetic shear across the polarity inversion line (PIL) plays an important role in the explosive nature of reconnection onset and in the equilibration of current sheets, acting as a source of free energy that can enhance or inhibit the onset process under certain conditions. In this study, we use a 2D PIC simulation to examine the local interaction between the reconnection guide field and thermodynamic variables during reconnection onset in a region of initially depleted thermal energy and enhanced magnetic energy in a large guide field background. We identify critical stages of the equilibration process, characterize intervals based on whether the pressure evolution is driven by changes in density or temperature, and discuss what these intervals imply about the evolution of local heat and work density. Finally, we examine power densities associated with electromagnetic field time evolution and electromagnetic energy transfer and compare to those related to thermodynamic changes.\nüì• Save to Zotero üìÑ Download PDF\nSCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations Authors: Wenhao Yan, Sheng Ye, Zhuoyi Yang, Jiayan Teng, ZhenHui Dong, Kairui Wen, Xiaotao Gu, Yong-Jin Liu, Jie Tang Venue: arXiv (2025)\nAchieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present \\textbf{SCAIL} (\\textbf{S}tudio-grade \\textbf{C}haracter \\textbf{A}nimation via \\textbf{I}n-context \\textbf{L}earning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that \\textbf{SCAIL} achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.\nüì• Save to Zotero üìÑ Download PDF\nEditThinker: Unlocking Iterative Reasoning for Any Image Editor Authors: Hongyu Li, Manyuan Zhang, Dian Zheng, Ziyu Guo, Yimeng Jia, Kaituo Feng, Hao Yu, Yexin Liu, Yan Feng, Peng Pei, Xunliang Cai, Linjiang Huang, Hongsheng Li, Si Liu Venue: arXiv (2025)\nInstruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to ‚Äôthink‚Äô while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker‚Äôs thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community.\nüì• Save to Zotero üìÑ Download PDF\nüîç linguistics AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement Authors: Munsif Ali, Najmul Hassan, Lucia Ventura, Davide Di Bari, Simonepietro Canese Venue: arXiv (2025)\nUnderwater images often suffer from severe color distortion, low contrast, and a hazy appearance due to wavelength-dependent light absorption and scattering. Simultaneously, existing deep learning models exhibit high computational complexity, which limits their practical deployment for real-time underwater applications. To address these challenges, this paper presents a novel underwater image enhancement model, called Adaptive Frequency Fusion and Illumination Aware Network (AQUA-Net). It integrates a residual encoder decoder with dual auxiliary branches, which operate in the frequency and illumination domains. The frequency fusion encoder enriches spatial representations with frequency cues from the Fourier domain and preserves fine textures and structural details. Inspired by Retinex, the illumination-aware decoder performs adaptive exposure correction through a learned illumination map that separates reflectance from lighting effects. This joint spatial, frequency, and illumination design enables the model to restore color balance, visual contrast, and perceptual realism under diverse underwater conditions. Additionally, we present a high-resolution, real-world underwater video-derived dataset from the Mediterranean Sea, which captures challenging deep-sea conditions with realistic visual degradations to enable robust evaluation and development of deep learning models. Extensive experiments on multiple benchmark datasets show that AQUA-Net performs on par with SOTA in both qualitative and quantitative evaluations while using less number of parameters. Ablation studies further confirm that the frequency and illumination branches provide complementary contributions that improve visibility and color representation. Overall, the proposed model shows strong generalization capability and robustness, and it provides an effective solution for real-world underwater imaging applications.\nüì• Save to Zotero üìÑ Download PDF\nSymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code Authors: Shima Imani, Seungwhan Moon, Adel Ahmadyan, Lu Zhang, Kirmani Ahmed, Babak Damavandi Venue: arXiv (2025)\nWe introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems\nüì• Save to Zotero üìÑ Download PDF\nLPD: Learnable Prototypes with Diversity Regularization for Weakly Supervised Histopathology Segmentation Authors: Khang Le, Anh Mai Vu, Thi Kim Trang Vo, Ha Thach, Ngoc Bui Lam Quang, Thanh-Huy Nguyen, Minh H. N. Le, Zhu Han, Chandra Mohan, Hien Van Nguyen Venue: arXiv (2025)\nWeakly supervised semantic segmentation (WSSS) in histopathology reduces pixel-level labeling by learning from image-level labels, but it is hindered by inter-class homogeneity, intra-class heterogeneity, and CAM-induced region shrinkage (global pooling-based class activation maps whose activations highlight only the most distinctive areas and miss nearby class regions). Recent works address these challenges by constructing a clustering prototype bank and then refining masks in a separate stage; however, such two-stage pipelines are costly, sensitive to hyperparameters, and decouple prototype discovery from segmentation learning, limiting their effectiveness and efficiency. We propose a cluster-free, one-stage learnable-prototype framework with diversity regularization to enhance morphological intra-class heterogeneity coverage. Our approach achieves state-of-the-art (SOTA) performance on BCSS-WSSS, outperforming prior methods in mIoU and mDice. Qualitative segmentation maps show sharper boundaries and fewer mislabels, and activation heatmaps further reveal that, compared with clustering-based prototypes, our learnable prototypes cover more diverse and complementary regions within each class, providing consistent qualitative evidence for their effectiveness.\nüì• Save to Zotero üìÑ Download PDF\nA redshift-independent theoretical halo mass function validated with the Uchuu simulations Authors: Elena Fern√°ndez-Garc√≠a, Juan E. Betancort-Rijo, Francisco Prada, Tomoaki Ishiyama, Anatoly Klypin, Jos√© Ruedas Venue: arXiv (2025)\nWe present a new theoretical framework for the halo mass function (HMF) that accurately predicts the abundance of dark matter haloes across an exceptionally wide range in mass and redshift. Building on a generalised Press \u0026 Schechter model and triaxial collapse (GPS+), we predict the HMF in terms of the variance of the linear density field, with only a weak explicit dependence on halo mass and no explicit dependence on redshift. The GPS+ model naturally provides the correct normalization and high-mass behaviour without requiring empirical fitting. We calibrate and validate the GPS+ model using the Uchuu N-body simulation suite, which combines large cosmological volume and high mass resolution under Planck cosmology. Using six simulations with up to 300 realizations, we obtain precision HMF measurements spanning halo masses in the range 6.5 \u003c log($M_{\\rm 200m}$/[h$^{-1}$ $M_{\\odot}$]) \u003c16 over 0 \u003c z \u003c 20, with reduced cosmic variance. Across this full domain, the GPS+ model reproduces the simulated HMF with deviations typically below 10-20%. Comparison with the Sheth-Tormen (ST) model shows similar performance at z \u003c 2, but markedly improved agreement at higher redshifts, where ST can deviate by 70-80% while our model remains within ~20%. Finally, we assess the impact of the halo mass definition: adopting the evolving virial overdensity of Bryan \u0026 Norman (1998) worsens agreement at low redshift and high masses, whereas M200m yields a more universal, nearly redshift-independent HMF.\nüì• Save to Zotero üìÑ Download PDF\nSIMPACT: Simulation-Enabled Action Planning using Vision-Language Models Authors: Haowen Liu, Shaoxiong Yao, Haonan Chen, Jiawei Gao, Jiayuan Mao, Jia-Bin Huang, Yilun Du Venue: arXiv (2025)\nVision-Language Models (VLMs) exhibit remarkable common-sense and semantic reasoning capabilities. However, they lack a grounded understanding of physical dynamics. This limitation arises from training VLMs on static internet-scale visual-language data that contain no causal interactions or action-conditioned changes. Consequently, it remains challenging to leverage VLMs for fine-grained robotic manipulation tasks that require physical understanding, reasoning, and corresponding action planning. To overcome this, we present SIMPACT, a test-time, SIMulation-enabled ACTion Planning framework that equips VLMs with physical reasoning through simulation-in-the-loop world modeling, without requiring any additional training. From a single RGB-D observation, SIMPACT efficiently constructs physics simulations, enabling the VLM to propose informed actions, observe simulated rollouts, and iteratively refine its reasoning. By integrating language reasoning with physics prediction, our simulation-enabled VLM can understand contact dynamics and action outcomes in a physically grounded way. Our method demonstrates state-of-the-art performance on five challenging, real-world rigid-body and deformable manipulation tasks that require fine-grained physical reasoning, outperforming existing general-purpose robotic manipulation models. Our results demonstrate that embedding physics understanding via efficient simulation into VLM reasoning at test time offers a promising path towards generalizable embodied intelligence. Project webpage can be found at https://simpact-bot.github.io\nüì• Save to Zotero üìÑ Download PDF\nHeard or Halted? Gender, Interruptions, and Emotional Tone in U.S. Supreme Court Oral Arguments Authors: Yifei Tong Venue: arXiv (2025)\nThis study examines how interruptions during U.S. Supreme Court oral arguments shape both the semantic content and emotional tone of advocates‚Äô speech, with a focus on gendered dynamics in judicial discourse. Using the ConvoKit Supreme Court Corpus (2010-2019), we analyze 12,663 speech chunks from advocate-justice interactions to assess whether interruptions alter the meaning of an advocate‚Äôs argument and whether interruptions toward female advocates exhibit more negative emotional valence. Semantic shifts are quantified using GloVe-based sentence embeddings, while sentiment is measured through lexicon-based analysis. We find that semantic similarity between pre- and post-interruption speech remains consistently high, suggesting that interruptions do not substantially alter argumentative content. However, interruptions directed at female advocates contain significantly higher levels of negative sentiment. These results deepen empirical understanding of gendered communication in elite institutional settings and demonstrate the value of computational linguistic methods for studying power, discourse, and equity in judicial proceedings.\nüì• Save to Zotero üìÑ Download PDF\nUnveiling Affective Polarization Trends in Parliamentary Proceedings Authors: Gili Goldin, Ella Rabinovich, Shuly Wintner Venue: arXiv (2025)\nRecent years have seen an increase in polarized discourse worldwide, on various platforms. We propose a novel method for quantifying polarization, based on the emotional style of the discourse rather than on differences in ideological stands. Using measures of Valence, Arousal and Dominance, we detect signals of emotional discourse and use them to operationalize the concept of affective polarization. Applying this method to a recently released corpus of proceedings of the Knesset, the Israeli parliament (in Hebrew), we find that the emotional style of members of government differs from that of opposition members; and that the level of affective polarization, as reflected by this style, is significantly increasing with time.\nüì• Save to Zotero üìÑ Download PDF\nStrongly Coupled Quantum Forces Authors: Yuval Grossman, Chinhsan Sieng, Xun-Jie Xu, Bingrong Yu Venue: arXiv (2025)\nQuantum forces are long-range interactions originating from vacuum fluctuations of mediator fields. Such forces inevitably arise between ordinary matter particles whenever they couple to light mediator species. Conventional computations of quantum forces rely on evaluating one-loop Feynman diagrams of the relevant scattering processes. In this work, we introduce a novel framework to compute quantum forces. Instead of relying on perturbative scattering amplitudes, we directly evaluate the quantum fluctuations of the mediator field by solving its quantized equation of motion with appropriate boundary conditions. This approach remains valid beyond the Born approximation and thus applies to regimes of strong coupling between the mediator and matter fields. In the weak-coupling limit, our results reproduce the known expressions from the Feynman diagram approach. In the strong-coupling regime, the result is modified by a factor that can suppress or enhance the effect. In contrast to classical forces, quantum forces intrinsically violate the superposition principle. Our approach may therefore offer a useful tool for probing non-perturbative effects in the infrared regime.\nüì• Save to Zotero üìÑ Download PDF\nEntanglement-Enhanced Quantum Nano-Vibrometry Authors: Colin P. Lualdi, Joshua Rapp, Spencer J. Johnson, Michael Vayninger, Paul G. Kwiat Venue: arXiv (2025)\nThe study of dynamic systems at the nanometer scale can benefit from the loss and background resilience offered by quantum two-photon interference. However, fast measurements with the required resolution are difficult to realize. As a solution, we introduce extreme energy entanglement between the photons undergoing interference. Using a flux probing analysis technique, we recover vibrational signals with frequencies as high as 21 kHz. Along with validating nanometer-scale precision and accuracy, we observe a significant quantum advantage when measuring in the presence of loss and background.\nüì• Save to Zotero üìÑ Download PDF\nCategorifying isomonodromic deformations via Lie groupoids I: Logarithmic singularities Authors: Waleed Qaisar Venue: arXiv (2025)\nWe upgrade the classical operation of \\textit{isomonodromic deformations} along a path $Œ≥$ to a functor $\\mathbb{P}_Œ≥$ between categories of flat connections with logarithmic singularities along a divisor $D$, which itself depends functorially on $Œ≥$, using tools from the theory of Lie groupoids. As applications, (1) we get that isomonodromy gives a map of moduli \\textit{stacks} of flat connections with logarithmic singularities, (2) we encode higher homotopical information at level 2, i.e. we get an action of the fundamental 2-groupoid of the base of our family on the categories of logarithmic flat connections on the fibres, and (3) our methods produce a geometric incarnation of the isomonodromy functors as Morita equivalences which are more primary than the isomonodromy functors themselves, and from which they can be formally extracted by passing to representation categories.\nüì• Save to Zotero üìÑ Download PDF\nBootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models Authors: Sairam Vaidya, Marcel B√∂hme, Loris D‚ÄôAntoni Venue: arXiv (2025)\nModern extensible compiler frameworks-such as MLIR-enable rapid creation of domain-specific language dialects. This flexibility, however, makes correctness harder to ensure as the same extensibility that accelerates development also complicates maintaining the testing infrastructure. Extensible languages require automated test generation that is both dialect-agnostic (works across dialects without manual adaptation) and dialect-effective (targets dialect-specific features to find bugs). Existing approaches typically sacrifice one of these goals by either requiring manually constructed seed corpora for each dialect, or by failing to be effective. We present a dialect-agnostic and dialect-effective grammar-based and coverage-guided fuzzing approach for extensible compilers that combines two key insights from existing work: (i) the grammars of dialects, which already encode the structural and type constraints, can often be extracted automatically from the dialect specification; and (ii) these grammars can be used in combination with pre-trained large language models to automatically generate representative and diverse seed inputs from the full dialect space without requiring any manual input or training data. These seeds can then be used to bootstrap coverage-guided fuzzers. We built this approach into a tool, Germinator. When evaluated on six MLIR projects spanning 91 dialects, Germinator generated seeds improve line coverage by 10-120% over grammar-based baselines. We compare against grammar-based baselines because they are the only class of existing automatic seed generators that can be applied uniformly across MLIR‚Äôs heterogeneous dialect ecosystem. Germinator discovers 88 previously unknown bugs (40 confirmed), including 23 in dialects with no prior automated test generators, demonstrating effective and controllable testing of low-resource dialects at scale.\nüì• Save to Zotero üìÑ Download PDF\nModel selection with uncertainty in estimating optimal dynamic treatment regimes Authors: Chunyu Wang, Brian Tom Venue: arXiv (2025)\nOptimal dynamic treatment regimes (DTRs), as a key part of precision medicine, have progressively gained more attention recently. To inform clinical decision making, interpretable and parsimonious models for contrast functions are preferred, raising concerns about undue misspecification. It is therefore important to properly evaluate the performance of candidate interpretable models and select the one that best approximates the unknown contrast function. Moreover, since a DTR usually involves multiple decision points, an inaccurate approximation at a later decision point affects its estimation at an earlier decision point when a backward induction algorithm is applied. This paper aims to perform model selection for contrast functions in the context of learning optimal DTRs from observed data. Note that the relative performance of candidate models may heavily depend on the sample size when, for example, the comparison is made between parametric and tree-based models. Therefore, instead of investigating the limiting behavior of each candidate model and developing methods to select asymptotically the `correct‚Äô one, we focus on the finite sample performance of each model and attempt to perform model selection under a given sample size. To this end, we adopt the counterfactual cross-validation metric and propose a novel method to estimate the variance of the metric. Supplementing the cross-validation metric with its estimated variance allows us to characterize the uncertainty in model selection under a given sample size and facilitates hypothesis testing associated with a preferred model structure.\nüì• Save to Zotero üìÑ Download PDF\nüîç psycholinguistics Evolutionary System 2 Reasoning: An Empirical Proof Authors: Zeyuan Ma, Wenqi Huang, Guo-Huan Song, Hongshu Guo, Sijie Ma, Zhiguang Cao, Yue-Jiao Gong Venue: arXiv (2025)\nMachine intelligence marks the ultimate dream of making machines‚Äô intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs.\nüì• Save to Zotero üìÑ Download PDF\nSqueezing Classical Antiferromagnets into Quantum Spin Liquids via Global Cavity Fluctuations Authors: Charlie-Ray Mann, Mark A. Oehlgrien, B≈Ça≈ºej Jaworowski, Giuseppe Calaj√≥, Jamir Marino, Kyung S. Choi, Darrick E. Chang Venue: arXiv (2025)\nCavity quantum electrodynamics with atomic ensembles is typically associated with collective spin phenomena, such as superradiance and spin squeezing, in which the atoms evolve collectively as a macroscopic spin ($S\\sim N/2$) on the Bloch sphere. Surprisingly, we show that the tendency toward a collective spin description need not imply collective spin phenomena; rather, it can be exploited to generate new forms of strongly correlated quantum matter. The key idea is to use uniform cavity-mediated interactions to energetically project the system into the total-spin singlet sector ($S=0$) - a highly entangled subspace where the physics is governed entirely by cavity fluctuations. Focusing on Rydberg atom arrays coupled to a single-mode cavity, we show that global cavity fluctuations can effectively squeeze classical antiferromagnets into quantum spin liquids, characterized by non-local entanglement, fractionalized excitations, and emergent gauge fields. This work suggests that cavity QED can be a surprising resource for inducing strongly correlated phenomena, which could be explored in the new generation of hybrid tweezer-cavity platforms.\nüì• Save to Zotero üìÑ Download PDF\nTrusted AI Agents in the Cloud Authors: Teofil Bodea, Masanori Misono, Julian Pritzi, Patrick Sabanic, Thore Sommer, Harshavardhan Unnibhavi, David Schall, Nuno Santos, Dimitrios Stavrakakis, Pramod Bhatotia Venue: arXiv (2025)\nAI agents powered by large language models are increasingly deployed as cloud services that autonomously access sensitive data, invoke external tools, and interact with other agents. However, these agents run within a complex multi-party ecosystem, where untrusted components can lead to data leakage, tampering, or unintended behavior. Existing Confidential Virtual Machines (CVMs) provide only per binary protection and offer no guarantees for cross-principal trust, accelerator-level isolation, or supervised agent behavior. We present Omega, a system that enables trusted AI agents by enforcing end-to-end isolation, establishing verifiable trust across all contributing principals, and supervising every external interaction with accountable provenance. Omega builds on Confidential VMs and Confidential GPUs to create a Trusted Agent Platform that hosts many agents within a single CVM using nested isolation. It also provides efficient multi-agent orchestration with cross-principal trust establishment via differential attestation, and a policy specification and enforcement framework that governs data access, tool usage, and inter-agent communication for data protection and regulatory compliance. Implemented on AMD SEV-SNP and NVIDIA H100, Omega fully secures agent state across CVM-GPU, and achieves high performance while enabling high-density, policy-compliant multi-agent deployments at cloud scale.\nüì• Save to Zotero üìÑ Download PDF\nSpeech World Model: Causal State-Action Planning with Explicit Reasoning for Speech Authors: Xuanru Zhou, Jiachen Lian, Henry Hong, Xinyi Yang, Gopala Anumanchipalli Venue: arXiv (2025)\nCurrent speech-language models (SLMs) typically use a cascade of speech encoder and large language model, treating speech understanding as a single black box. They analyze the content of speech well but reason weakly about other aspects, especially under sparse supervision. Thus, we argue for explicit reasoning over speech states and actions with modular and transparent decisions. Inspired by cognitive science we adopt a modular perspective and a world model view in which the system learns forward dynamics over latent states. We factorize speech understanding into four modules that communicate through a causal graph, establishing a cognitive state search space. Guided by posterior traces from this space, an instruction-tuned language model produces a concise causal analysis and a user-facing response, enabling counterfactual interventions and interpretability under partial supervision. We present the first graph based modular speech model for explicit reasoning and we will open source the model and data to promote the development of advanced speech understanding.\nüì• Save to Zotero üìÑ Download PDF\nüîç llm Group Classification (1+2)-dimensional Linear Equation of Asian Options Pricing Authors: Stanislav V. Spichak, Valeriy I. Stogniy, Inna M. Kopas Venue: arXiv (2025)\nWe consider a class of (1+2)-dimensional linear partial differential of Asian options pricing. Special cases have been used to models of financial mathematics. We carry out group classification of a class equations. In particular, the maximum dimension Lie invariance algebra within the above class is eight-dimensional. It is shown that an equation with such an algebra can be transformed into the linear Kolmogorov equation with the help of the point transformations of variables. Using the operators of invariance algebra symmetry reduction is carried out and invariant exact solutions are constructed for some equations.\nüì• Save to Zotero üìÑ Download PDF\nTransformation of orientation and rotation angles of synchronous satellites: Application to the Galilean moons Authors: Marie Yseboodt, Rose-Marie Baland Venue: arXiv (2025)\nThe orientation and rotation of a synchronous satellite can be referred to both its Laplace plane and the ICRF equatorial plane, in terms of Euler angles or spin axis Cartesian coordinates and Earth equatorial coordinates, respectively. We computed second-order analytical expressions to make the transformation between the two systems and applied them to the Galilean satellites (Io, Europa, Ganymede, and Callisto). If one term of the spin axis Cartesian coordinates series is dominant, trigonometric series can be generated for the inertial and orbital obliquities, node longitude and offset with respect to the Cassini plane. Since the transformation does not require any fit of amplitudes and frequencies on numerical series, the physical meaning of the frequencies is preserved from the input series and the amplitudes can be directly related to the geophysical parameters of interest. We provide tables for the coordinates and angles‚Äô series assuming that the satellites are entirely solid, and considering two different orbital theories. The possible amplitude ranges for the main terms are also examined in the case where a liquid layer is assumed in the interior model. We use our transformation method to propose an updated IAU WG solution which would result in an improvement with respect to zero obliquity models used so far. This method will also be useful for the interpretation of future Earth-based radar observations or JUICE data.\nüì• Save to Zotero üìÑ Download PDF\nConsequences of Kernel Regularity for Bandit Optimization Authors: Madison Lee, Tara Javidi Venue: arXiv (2025)\nIn this work we investigate the relationship between kernel regularity and algorithmic performance in the bandit optimization of RKHS functions. While reproducing kernel Hilbert space (RKHS) methods traditionally rely on global kernel regressors, it is also common to use a smoothness-based approach that exploits local approximations. We show that these perspectives are deeply connected through the spectral properties of isotropic kernels. In particular, we characterize the Fourier spectra of the Mat√©rn, square-exponential, rational-quadratic, $Œ≥$-exponential, piecewise-polynomial, and Dirichlet kernels, and show that the decay rate determines asymptotic regret from both viewpoints. For kernelized bandit algorithms, spectral decay yields upper bounds on the maximum information gain, governing worst-case regret, while for smoothness-based methods, the same decay rates establish H√∂lder space embeddings and Besov space norm-equivalences, enabling local continuity analysis. These connections show that kernel-based and locally adaptive algorithms can be analyzed within a unified framework. This allows us to derive explicit regret bounds for each kernel family, obtaining novel results in several cases and providing improved analysis for others. Furthermore, we analyze LP-GP-UCB, an algorithm that combines both approaches, augmenting global Gaussian process surrogates with local polynomial estimators. While the hybrid approach does not uniformly dominate specialized methods, it achieves order-optimality across multiple kernel families.\nüì• Save to Zotero üìÑ Download PDF\nüîç neuroscience Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding Authors: Zhiyuan Jiang, Shenghao Xie, Wenyi Li, Wenqiang Zu, Peihang Li, Jiahao Qiu, Siqi Pei, Lei Ma, Tiejun Huang, Mengdi Wang, Shilong Liu Venue: arXiv (2025)\nGrounding is a fundamental capability for building graphical user interface (GUI) agents. Although existing approaches rely on large-scale bounding box supervision, they still face various challenges, such as cross-platform generalization, complex layout analysis, and fine-grained element localization. In this paper, we investigate zoom as a strong yet underexplored prior for GUI grounding, and propose a training-free method, ZoomClick. By characterizing four key properties of zoom (i.e., pre-zoom, depth, shrink size, minimal crop size), we unlock its full capabilities for dynamic spatial focusing and adaptive context switching. Experiments demonstrate that our method significantly boosts the performance of both general vision-language and specialized GUI grounding models, achieving state-of-the-art results on several mainstream benchmarks; for example, UI-Venus-72B attains a 73.1% success rate on ScreenSpot-Pro. Furthermore, we present GUIZoom-Bench, a benchmark for evaluating model adaptability to zoom, aiming to inspire future research on improving zoom for further training and test-time scaling in GUI grounding tasks.\nüì• Save to Zotero üìÑ Download PDF\nPhysically-Based Simulation of Automotive LiDAR Authors: L. Dudzik, M. Roschani, A. Sielemann, K. Trampert, J. Ziehn, J. Beyerer, C. Neumann Venue: arXiv (2025)\nWe present an analytic model for simulating automotive time-of-flight (ToF) LiDAR that includes blooming, echo pulse width, and ambient light, along with steps to determine model parameters systematically through optical laboratory measurements. The model uses physically based rendering (PBR) in the near-infrared domain. It assumes single-bounce reflections and retroreflections over rasterized rendered images from shading or ray tracing, including light emitted from the sensor as well as stray light from other, non-correlated sources such as sunlight. Beams from the sensor and sensitivity of the receiving diodes are modeled with flexible beam steering patterns and with non-vanishing diameter. Different (all non-real time) computational approaches can be chosen based on system properties, computing capabilities, and desired output properties. Model parameters include system-specific properties, namely the physical spread of the LiDAR beam, combined with the sensitivity of the receiving diode; the intensity of the emitted light; the conversion between the intensity of reflected light and the echo pulse width; and scenario parameters such as environment lighting, positioning, and surface properties of the target(s) in the relevant infrared domain. System-specific properties of the model are determined from laboratory measurements of the photometric luminance on different target surfaces aligned with a goniometer at 0.01¬∞ resolution, which marks the best available resolution for measuring the beam pattern. The approach is calibrated for and tested on two automotive LiDAR systems, the Valeo Scala Gen. 2 and the Blickfeld Cube 1. Both systems differ notably in their properties and available interfaces, but the relevant model parameters could be extracted successfully.\nüì• Save to Zotero üìÑ Download PDF\nüîç data_resources Using Large Language Models to Create Personalized Networks From Therapy Sessions Authors: Clarissa W. Ong, Hiba Arnaout, Kate Sheehan, Estella Fox, Eugen Owtscharow, Iryna Gurevych Venue: arXiv (2025)\nRecent advances in psychotherapy have focused on treatment personalization, such as by selecting treatment modules based on personalized networks. However, estimating personalized networks typically requires intensive longitudinal data, which is not always feasible. A solution to facilitate scalability of network-driven treatment personalization is leveraging LLMs. In this study, we present an end-to-end pipeline for automatically generating client networks from 77 therapy transcripts to support case conceptualization and treatment planning. We annotated 3364 psychological processes and their corresponding dimensions in therapy transcripts. Using these data, we applied in-context learning to jointly identify psychological processes and their dimensions. The method achieved high performance even with a few training examples. To organize the processes into networks, we introduced a two-step method that grouped them into clinically meaningful clusters. We then generated explanation-augmented relationships between clusters. Experts found that networks produced by our multi-step approach outperformed those built with direct prompting for clinical utility and interpretability, with up to 90% preferring our approach. In addition, the networks were rated favorably by experts, with scores for clinical relevance, novelty, and usefulness ranging from 72-75%. Our findings provide a proof of concept for using LLMs to create clinically relevant networks from therapy transcripts. Advantages of our approach include bottom-up case conceptualization from client utterances in therapy sessions and identification of latent themes. Networks generated from our pipeline may be used in clinical settings and supervision and training. Future research should examine whether these networks improve treatment outcomes relative to other methods of treatment personalization, including statistically estimated networks.\nüì• Save to Zotero üìÑ Download PDF\nüîç emotion_language Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception Authors: Anne Sielemann, Valentin Barner, Stefan Wolf, Masoud Roschani, Jens Ziehn, Juergen Beyerer Venue: arXiv (2025)\nCommon approaches to explainable AI (XAI) for deep learning focus on analyzing the importance of input features on the classification task in a given model: saliency methods like SHAP and GradCAM are used to measure the impact of spatial regions of the input image on the classification result. Combined with ground truth information about the location of the object in the input image (e.g., a binary mask), it is determined whether object pixels had a high impact on the classification result, or whether the classification focused on background pixels. The former is considered to be a sign of a healthy classifier, whereas the latter is assumed to suggest overfitting on spurious correlations. A major challenge, however, is that these intuitive interpretations are difficult to test quantitatively, and hence the output of such explanations lacks an explanation itself. One particular reason is that correlations in real-world data are difficult to avoid, and whether they are spurious or legitimate is debatable. Synthetic data in turn can facilitate to actively enable or disable correlations where desired but often lack a sufficient quantification of realism and stochastic properties. [‚Ä¶] Therefore, we systematically generate six synthetic datasets for the task of traffic sign recognition, which differ only in their degree of camera variation and background correlation [‚Ä¶] to quantify the isolated influence of background correlation, different levels of camera variation, and considered traffic sign shapes on the classification performance, as well as background feature importance. [‚Ä¶] Results include a quantification of when and how much background features gain importance to support the classification task based on changes in the training domain [‚Ä¶]. Download: synset.de/datasets/synset-signset-ger/background-effect\nüì• Save to Zotero üìÑ Download PDF\nMinimal two band model and experimental proposals to distinguish pairing mechanisms of the high-T$_c$ superconductor La$_3$Ni$_2$O$_7$ Authors: Zheng-Duo Fan, Ashvin Vishwanath Venue: arXiv (2025)\nThe discovery of high-T$_c$ superconductivity in La$_3$Ni$_2$O$_7$ has opened the door to a new route to high temperature superconductivity, distinct from that in cuprates and iron-based materials. Yet, despite intense recent activity, we lack experimentally testable protocols for distinguishing between different pairing scenarios. In this Letter, we construct a minimal two-band model that reproduces the Fermi-surface topology observed in recent ARPES measurements and DFT calculations, and we analyze superconductivity arising from two distinct pairing mechanisms. We show that these mechanisms yield sharply different responses to an applied perpendicular electric field. Thus, La$_3$Ni$_2$O$_7$ offers the unique opportunity to cleanly distinguish between different pairing scenarios. Finally, we propose three concrete experimental proposals designed to distinguish these scenarios and thereby identify the pairing mechanism most relevant to the real material.\nüì• Save to Zotero üìÑ Download PDF\nWorld Models That Know When They Don‚Äôt Know: Controllable Video Generation with Calibrated Uncertainty Authors: Zhiting Mei, Tenny Yin, Micah Baker, Ola Shorinwa, Anirudha Majumdar Venue: arXiv (2025)\nRecent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model‚Äôs uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.\nüì• Save to Zotero üìÑ Download PDF\nTo Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis Authors: Federico Bianchi, Yongchan Kwon, Zachary Izzo, Linjun Zhang, James Zou Venue: arXiv (2025)\nHow many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.\nüì• Save to Zotero üìÑ Download PDF\nImpugan: Learning Conditional Generative Models for Robust Data Imputation Authors: Zalish Mahmud, Anantaa Kotal, Aritran Piplai Venue: arXiv (2025)\nIncomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82% lower Earth Mover‚Äôs Distance (EMD) and 70% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: github.com/zalishmahmud/impuganBigData2025\nüì• Save to Zotero üìÑ Download PDF\nInvariant Price of Anarchy: a Metric for Welfarist Traffic Control Authors: Ilia Shilov, Mingjia He, Heinrich H. Nax, Emilio Frazzoli, Gioele Zardini, Saverio Bolognani Venue: arXiv (2025)\nThe Price of Anarchy (PoA) is a standard metric for quantifying inefficiency in socio-technical systems, widely used to guide policies like traffic tolling. Conventional PoA analysis relies on exact numerical costs. However, in many settings, costs represent agents‚Äô preferences and may be defined only up to possibly arbitrary scaling and shifting, representing informational and modeling ambiguities. We observe that while such transformations preserve equilibrium and optimal outcomes, they change the PoA value. To resolve this issue, we rely on results from Social Choice Theory and define the Invariant PoA. By connecting admissible transformations to degrees of comparability of agents‚Äô costs, we derive the specific social welfare functions which ensure that efficiency evaluations do not depend on arbitrary rescalings or translations of individual costs. Case studies on a toy example and the Zurich network demonstrate that identical tolling strategies can lead to substantially different efficiency estimates depending on the assumed comparability. Our framework thus demonstrates that explicit axiomatic foundations are necessary in order to define efficiency metrics and to appropriately guide policy in large-scale infrastructure design robustly and effectively.\nüì• Save to Zotero üìÑ Download PDF\nTowards agent-based-model informed neural networks Authors: Nino Antulov-Fantulin Venue: arXiv (2025)\nIn this article, we present a framework for designing neural networks that remain consistent with the underlying principles of agent-based models. We begin by highlighting the limitations of standard neural differential equations in modeling complex systems, where physical invariants (like energy) are often absent but other constraints (like mass conservation, network locality, bounded rationality) must be enforced. To address this, we introduce Agent-Based-Model informed Neural Networks(ABM-NNs), which leverage restricted graph neural networks and hierarchical decomposition to learn interpretable, structure-preserving dynamics. We validate the framework across three case studies of increasing complexity: (i) a generalized Generalized Lotka‚ÄìVolterra system, where we recover ground-truth parameters from short trajectories in presence of interventions; (ii) a graph-based SIR contagion model, where our method outperforms state-of-the-art graph learning baselines (GCN, GraphSAGE, Graph Transformer) in out-of-sample forecasting and noise robustness; and (iii) a real-world macroeconomic model of the ten largest economies, where we learn coupled GDP dynamics from empirical data and demonstrate gradient-based counterfactual analysis for policy interventions.\nüì• Save to Zotero üìÑ Download PDF\n","wordCount":"9180","inLanguage":"en","datePublished":"2025-12-09T00:13:56.72846Z","dateModified":"2025-12-09T00:13:56.72846Z","author":{"@type":"Person","name":"Gary"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://garyforreal.me/en/posts/paper/paper-2025-12-09-weekly/"},"publisher":{"@type":"Organization","name":"Gary's House","logo":{"@type":"ImageObject","url":"https://garyforreal.me/img/Q.jpg"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><script src=https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://garyforreal.me/en/ accesskey=h title="Gary's Blog (Alt + H)"><img src=https://garyforreal.me/img/me.jpg alt aria-label=logo height=35>Gary's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://garyforreal.me/zh/ title=‰∏≠Êñá aria-label=‰∏≠Êñá>‰∏≠Êñá</a></li></ul></div></div><ul id=menu><li><a href=https://garyforreal.me/en/search title="üîçSearch (Alt + /)" accesskey=/><span>üîçSearch</span></a></li><li><a href=https://garyforreal.me/en/ title=üè†Homepage><span>üè†Homepage</span></a></li><li><a href=https://garyforreal.me/en/posts title=üìöArticle><span>üìöArticle</span></a></li><li><a href=https://garyforreal.me/en/archives/ title=‚è±Archives><span>‚è±Archives</span></a></li><li><a href=https://garyforreal.me/en/music/ title=üéµmusic><span>üéµmusic</span></a></li><li><a href=https://garyforreal.me/en/about title=üôãüèª‚Äç‚ôÇÔ∏èAbout><span>üôãüèª‚Äç‚ôÇÔ∏èAbout</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://garyforreal.me/en/>Home</a>&nbsp;¬ª&nbsp;<a href=https://garyforreal.me/en/posts/>Posts</a>&nbsp;¬ª&nbsp;<a href=https://garyforreal.me/en/posts/paper/>Paper</a></div><h1 class="post-title entry-hint-parent">Weekly Paper Notes - 2025-12-09</h1><div class=post-meta><span title='2025-12-09 00:13:56.72846 +0000 UTC'>2025-12-09</span>&nbsp;¬∑&nbsp;44 min&nbsp;¬∑&nbsp;Gary&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://garyforreal.me/zh/posts/paper/paper-2025-12-09-weekly/>‰∏≠Êñá</a></li></ul><div class=meta-item>&nbsp¬∑&nbsp
        <span id=busuanzi_container_page_pv>Êú¨ÊñáÈòÖËØªÈáè<span id=busuanzi_value_page_pv></span>Ê¨°</span></div></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#weekly-paper-notes aria-label="Weekly Paper Notes">Weekly Paper Notes</a><ul><li><a href=#-multilingual aria-label="üîç multilingual">üîç multilingual</a><ul><li><a href=#m4-rag-a-massive-scale-multilingual-multi-cultural-multimodal-raghttpsarxivorgabs251205959v1 aria-label="M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG"><a href=https://arxiv.org/abs/2512.05959v1>M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG</a></a></li><li><a href=#efficient-text-classification-with-conformal-in-context-learninghttpsarxivorgabs251205732v1 aria-label="Efficient Text Classification with Conformal In-Context Learning"><a href=https://arxiv.org/abs/2512.05732v1>Efficient Text Classification with Conformal In-Context Learning</a></a></li><li><a href=#rrpo-robust-reward-policy-optimization-for-llm-based-emotional-ttshttpsarxivorgabs251204552v1 aria-label="RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS"><a href=https://arxiv.org/abs/2512.04552v1>RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS</a></a></li><li><a href=#adapting-large-language-models-to-low-resource-tibetan-a-two-stage-continual-and-supervised-fine-tuning-studyhttpsarxivorgabs251203976v1 aria-label="Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study"><a href=https://arxiv.org/abs/2512.03976v1>Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study</a></a></li><li><a href=#beyond-data-filtering-knowledge-localization-for-capability-removal-in-llmshttpsarxivorgabs251205648v1 aria-label="Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs"><a href=https://arxiv.org/abs/2512.05648v1>Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs</a></a></li><li><a href=#exploiting-spatial-multiplexing-based-on-pixel-antennas-an-antenna-coding-approachhttpsarxivorgabs251205706v1 aria-label="Exploiting Spatial Multiplexing Based on Pixel Antennas: An Antenna Coding Approach"><a href=https://arxiv.org/abs/2512.05706v1>Exploiting Spatial Multiplexing Based on Pixel Antennas: An Antenna Coding Approach</a></a></li><li><a href=#the-decision-path-to-control-ai-risks-completely-fundamental-control-mechanisms-for-ai-governancehttpsarxivorgabs251204489v1 aria-label="The Decision Path to Control AI Risks Completely: Fundamental Control Mechanisms for AI Governance"><a href=https://arxiv.org/abs/2512.04489v1>The Decision Path to Control AI Risks Completely: Fundamental Control Mechanisms for AI Governance</a></a></li><li><a href=#structured-reasoning-with-tree-of-thoughts-for-bengali-math-word-problemshttpsarxivorgabs251205580v1 aria-label="Structured Reasoning with Tree-of-Thoughts for Bengali Math Word Problems"><a href=https://arxiv.org/abs/2512.05580v1>Structured Reasoning with Tree-of-Thoughts for Bengali Math Word Problems</a></a></li><li><a href=#enhancing-retrieval-augmented-generation-with-entity-linking-for-educational-platformshttpsarxivorgabs251205967v1 aria-label="Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms"><a href=https://arxiv.org/abs/2512.05967v1>Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms</a></a></li><li><a href=#training-time-action-conditioning-for-efficient-real-time-chunkinghttpsarxivorgabs251205964v1 aria-label="Training-Time Action Conditioning for Efficient Real-Time Chunking"><a href=https://arxiv.org/abs/2512.05964v1>Training-Time Action Conditioning for Efficient Real-Time Chunking</a></a></li><li><a href=#thermodynamics-of-shear-equilibration-during-magnetic-reconnection-onset-in-mixed-equilibrium-current-sheetshttpsarxivorgabs251205921v1 aria-label="Thermodynamics of Shear Equilibration During Magnetic Reconnection Onset in Mixed-Equilibrium Current Sheets"><a href=https://arxiv.org/abs/2512.05921v1>Thermodynamics of Shear Equilibration During Magnetic Reconnection Onset in Mixed-Equilibrium Current Sheets</a></a></li><li><a href=#scail-towards-studio-grade-character-animation-via-in-context-learning-of-3d-consistent-pose-representationshttpsarxivorgabs251205905v1 aria-label="SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations"><a href=https://arxiv.org/abs/2512.05905v1>SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations</a></a></li><li><a href=#editthinker-unlocking-iterative-reasoning-for-any-image-editorhttpsarxivorgabs251205965v1 aria-label="EditThinker: Unlocking Iterative Reasoning for Any Image Editor"><a href=https://arxiv.org/abs/2512.05965v1>EditThinker: Unlocking Iterative Reasoning for Any Image Editor</a></a></li></ul></li><li><a href=#-linguistics aria-label="üîç linguistics">üîç linguistics</a><ul><li><a href=#aqua-net-adaptive-frequency-fusion-and-illumination-aware-network-for-underwater-image-enhancementhttpsarxivorgabs251205960v1 aria-label="AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement"><a href=https://arxiv.org/abs/2512.05960v1>AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement</a></a></li><li><a href=#sympybench-a-dynamic-benchmark-for-scientific-reasoning-with-executable-python-codehttpsarxivorgabs251205954v1 aria-label="SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code"><a href=https://arxiv.org/abs/2512.05954v1>SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code</a></a></li><li><a href=#lpd-learnable-prototypes-with-diversity-regularization-for-weakly-supervised-histopathology-segmentationhttpsarxivorgabs251205922v1 aria-label="LPD: Learnable Prototypes with Diversity Regularization for Weakly Supervised Histopathology Segmentation"><a href=https://arxiv.org/abs/2512.05922v1>LPD: Learnable Prototypes with Diversity Regularization for Weakly Supervised Histopathology Segmentation</a></a></li><li><a href=#a-redshift-independent-theoretical-halo-mass-function-validated-with-the-uchuu-simulationshttpsarxivorgabs251205847v1 aria-label="A redshift-independent theoretical halo mass function validated with the Uchuu simulations"><a href=https://arxiv.org/abs/2512.05847v1>A redshift-independent theoretical halo mass function validated with the Uchuu simulations</a></a></li><li><a href=#simpact-simulation-enabled-action-planning-using-vision-language-modelshttpsarxivorgabs251205955v1 aria-label="SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models"><a href=https://arxiv.org/abs/2512.05955v1>SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models</a></a></li><li><a href=#heard-or-halted-gender-interruptions-and-emotional-tone-in-us-supreme-court-oral-argumentshttpsarxivorgabs251205832v1 aria-label="Heard or Halted? Gender, Interruptions, and Emotional Tone in U.S. Supreme Court Oral Arguments"><a href=https://arxiv.org/abs/2512.05832v1>Heard or Halted? Gender, Interruptions, and Emotional Tone in U.S. Supreme Court Oral Arguments</a></a></li><li><a href=#unveiling-affective-polarization-trends-in-parliamentary-proceedingshttpsarxivorgabs251205231v1 aria-label="Unveiling Affective Polarization Trends in Parliamentary Proceedings"><a href=https://arxiv.org/abs/2512.05231v1>Unveiling Affective Polarization Trends in Parliamentary Proceedings</a></a></li><li><a href=#strongly-coupled-quantum-forceshttpsarxivorgabs251205968v1 aria-label="Strongly Coupled Quantum Forces"><a href=https://arxiv.org/abs/2512.05968v1>Strongly Coupled Quantum Forces</a></a></li><li><a href=#entanglement-enhanced-quantum-nano-vibrometryhttpsarxivorgabs251205961v1 aria-label="Entanglement-Enhanced Quantum Nano-Vibrometry"><a href=https://arxiv.org/abs/2512.05961v1>Entanglement-Enhanced Quantum Nano-Vibrometry</a></a></li><li><a href=#categorifying-isomonodromic-deformations-via-lie-groupoids-i-logarithmic-singularitieshttpsarxivorgabs251205966v1 aria-label="Categorifying isomonodromic deformations via Lie groupoids I: Logarithmic singularities"><a href=https://arxiv.org/abs/2512.05966v1>Categorifying isomonodromic deformations via Lie groupoids I: Logarithmic singularities</a></a></li><li><a href=#bootstrapping-fuzzers-for-compilers-of-low-resource-language-dialects-using-language-modelshttpsarxivorgabs251205887v1 aria-label="Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models"><a href=https://arxiv.org/abs/2512.05887v1>Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models</a></a></li><li><a href=#model-selection-with-uncertainty-in-estimating-optimal-dynamic-treatment-regimeshttpsarxivorgabs251205695v1 aria-label="Model selection with uncertainty in estimating optimal dynamic treatment regimes"><a href=https://arxiv.org/abs/2512.05695v1>Model selection with uncertainty in estimating optimal dynamic treatment regimes</a></a></li></ul></li><li><a href=#-psycholinguistics aria-label="üîç psycholinguistics">üîç psycholinguistics</a><ul><li><a href=#evolutionary-system-2-reasoning-an-empirical-proofhttpsarxivorgabs251205760v1 aria-label="Evolutionary System 2 Reasoning: An Empirical Proof"><a href=https://arxiv.org/abs/2512.05760v1>Evolutionary System 2 Reasoning: An Empirical Proof</a></a></li><li><a href=#squeezing-classical-antiferromagnets-into-quantum-spin-liquids-via-global-cavity-fluctuationshttpsarxivorgabs251205630v1 aria-label="Squeezing Classical Antiferromagnets into Quantum Spin Liquids via Global Cavity Fluctuations"><a href=https://arxiv.org/abs/2512.05630v1>Squeezing Classical Antiferromagnets into Quantum Spin Liquids via Global Cavity Fluctuations</a></a></li><li><a href=#trusted-ai-agents-in-the-cloudhttpsarxivorgabs251205951v1 aria-label="Trusted AI Agents in the Cloud"><a href=https://arxiv.org/abs/2512.05951v1>Trusted AI Agents in the Cloud</a></a></li><li><a href=#speech-world-model-causal-state-action-planning-with-explicit-reasoning-for-speechhttpsarxivorgabs251205933v1 aria-label="Speech World Model: Causal State-Action Planning with Explicit Reasoning for Speech"><a href=https://arxiv.org/abs/2512.05933v1>Speech World Model: Causal State-Action Planning with Explicit Reasoning for Speech</a></a></li></ul></li><li><a href=#-llm aria-label="üîç llm">üîç llm</a><ul><li><a href=#group-classification-12-dimensional-linear-equation-of-asian-options-pricinghttpsarxivorgabs251205963v1 aria-label="Group Classification (1+2)-dimensional Linear Equation of Asian Options Pricing"><a href=https://arxiv.org/abs/2512.05963v1>Group Classification (1+2)-dimensional Linear Equation of Asian Options Pricing</a></a></li><li><a href=#transformation-of-orientation-and-rotation-angles-of-synchronous-satellites-application-to-the-galilean-moonshttpsarxivorgabs251205935v1 aria-label="Transformation of orientation and rotation angles of synchronous satellites: Application to the Galilean moons"><a href=https://arxiv.org/abs/2512.05935v1>Transformation of orientation and rotation angles of synchronous satellites: Application to the Galilean moons</a></a></li><li><a href=#consequences-of-kernel-regularity-for-bandit-optimizationhttpsarxivorgabs251205957v1 aria-label="Consequences of Kernel Regularity for Bandit Optimization"><a href=https://arxiv.org/abs/2512.05957v1>Consequences of Kernel Regularity for Bandit Optimization</a></a></li></ul></li><li><a href=#-neuroscience aria-label="üîç neuroscience">üîç neuroscience</a><ul><li><a href=#zoom-in-click-out-unlocking-and-evaluating-the-potential-of-zooming-for-gui-groundinghttpsarxivorgabs251205941v1 aria-label="Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding"><a href=https://arxiv.org/abs/2512.05941v1>Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding</a></a></li><li><a href=#physically-based-simulation-of-automotive-lidarhttpsarxivorgabs251205932v1 aria-label="Physically-Based Simulation of Automotive LiDAR"><a href=https://arxiv.org/abs/2512.05932v1>Physically-Based Simulation of Automotive LiDAR</a></a></li></ul></li><li><a href=#-data_resources aria-label="üîç data_resources">üîç data_resources</a><ul><li><a href=#using-large-language-models-to-create-personalized-networks-from-therapy-sessionshttpsarxivorgabs251205836v1 aria-label="Using Large Language Models to Create Personalized Networks From Therapy Sessions"><a href=https://arxiv.org/abs/2512.05836v1>Using Large Language Models to Create Personalized Networks From Therapy Sessions</a></a></li></ul></li><li><a href=#-emotion_language aria-label="üîç emotion_language">üîç emotion_language</a><ul><li><a href=#measuring-the-effect-of-background-on-classification-and-feature-importance-in-deep-learning-for-av-perceptionhttpsarxivorgabs251205937v1 aria-label="Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception"><a href=https://arxiv.org/abs/2512.05937v1>Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception</a></a></li><li><a href=#minimal-two-band-model-and-experimental-proposals-to-distinguish-pairing-mechanisms-of-the-high-t_c-superconductor-la_3ni_2o_7httpsarxivorgabs251205956v1 aria-label="Minimal two band model and experimental proposals to distinguish pairing mechanisms of the high-T$_c$ superconductor La$_3$Ni$_2$O$_7$"><a href=https://arxiv.org/abs/2512.05956v1>Minimal two band model and experimental proposals to distinguish pairing mechanisms of the high-T$_c$ superconductor La$_3$Ni$_2$O$_7$</a></a></li><li><a href=#world-models-that-know-when-they-dont-know-controllable-video-generation-with-calibrated-uncertaintyhttpsarxivorgabs251205927v1 aria-label="World Models That Know When They Don&rsquo;t Know: Controllable Video Generation with Calibrated Uncertainty"><a href=https://arxiv.org/abs/2512.05927v1>World Models That Know When They Don&rsquo;t Know: Controllable Video Generation with Calibrated Uncertainty</a></a></li><li><a href=#to-err-is-human-systematic-quantification-of-errors-in-published-ai-papers-via-llm-analysishttpsarxivorgabs251205925v1 aria-label="To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis"><a href=https://arxiv.org/abs/2512.05925v1>To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis</a></a></li><li><a href=#impugan-learning-conditional-generative-models-for-robust-data-imputationhttpsarxivorgabs251205950v1 aria-label="Impugan: Learning Conditional Generative Models for Robust Data Imputation"><a href=https://arxiv.org/abs/2512.05950v1>Impugan: Learning Conditional Generative Models for Robust Data Imputation</a></a></li><li><a href=#invariant-price-of-anarchy-a-metric-for-welfarist-traffic-controlhttpsarxivorgabs251205843v1 aria-label="Invariant Price of Anarchy: a Metric for Welfarist Traffic Control"><a href=https://arxiv.org/abs/2512.05843v1>Invariant Price of Anarchy: a Metric for Welfarist Traffic Control</a></a></li><li><a href=#towards-agent-based-model-informed-neural-networkshttpsarxivorgabs251205764v1 aria-label="Towards agent-based-model informed neural networks"><a href=https://arxiv.org/abs/2512.05764v1>Towards agent-based-model informed neural networks</a></a></li></ul></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h1 id=weekly-paper-notes>Weekly Paper Notes<a hidden class=anchor aria-hidden=true href=#weekly-paper-notes>#</a></h1><h2 id=-multilingual>üîç multilingual<a hidden class=anchor aria-hidden=true href=#-multilingual>#</a></h2><h3 id=m4-rag-a-massive-scale-multilingual-multi-cultural-multimodal-raghttpsarxivorgabs251205959v1><a href=https://arxiv.org/abs/2512.05959v1>M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG</a><a hidden class=anchor aria-hidden=true href=#m4-rag-a-massive-scale-multilingual-multi-cultural-multimodal-raghttpsarxivorgabs251205959v1>#</a></h3><p><strong>Authors:</strong> David Anugraha, Patrick Amadeus Irawan, Anshul Singh, En-Shiun Annie Lee, Genta Indra Winata
<strong>Venue:</strong> arXiv (2025)</p><p>Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05959v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05959v1">üìÑ Download PDF</a></p><hr><h3 id=efficient-text-classification-with-conformal-in-context-learninghttpsarxivorgabs251205732v1><a href=https://arxiv.org/abs/2512.05732v1>Efficient Text Classification with Conformal In-Context Learning</a><a hidden class=anchor aria-hidden=true href=#efficient-text-classification-with-conformal-in-context-learninghttpsarxivorgabs251205732v1>#</a></h3><p><strong>Authors:</strong> Ippokratis Pantelidis, Korbinian Randl, Aron Henriksson
<strong>Venue:</strong> arXiv (2025)</p><p>Large Language Models (LLMs) demonstrate strong in-context learning abilities, yet their effectiveness in text classification depends heavily on prompt design and incurs substantial computational cost. Conformal In-Context Learning (CICLe) has been proposed as a resource-efficient framework that integrates a lightweight base classifier with Conformal Prediction to guide LLM prompting by adaptively reducing the set of candidate classes. However, its broader applicability and efficiency benefits beyond a single domain have not yet been systematically explored. In this paper, we present a comprehensive evaluation of CICLe across diverse NLP classification benchmarks. The results show that CICLe consistently improves over its base classifier and outperforms few-shot prompting baselines when the sample size is sufficient for training the base classifier, and performs comparably in low-data regimes. In terms of efficiency, CICLe reduces the number of shots and prompt length by up to 34.45% and 25.16%, respectively, and enables the use of smaller models with competitive performance. CICLe is furthermore particularly advantageous for text classification tasks with high class imbalance. These findings highlight CICLe as a practical and scalable approach for efficient text classification, combining the robustness of traditional classifiers with the adaptability of LLMs, and achieving substantial gains in data and computational efficiency.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05732v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05732v1">üìÑ Download PDF</a></p><hr><h3 id=rrpo-robust-reward-policy-optimization-for-llm-based-emotional-ttshttpsarxivorgabs251204552v1><a href=https://arxiv.org/abs/2512.04552v1>RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS</a><a hidden class=anchor aria-hidden=true href=#rrpo-robust-reward-policy-optimization-for-llm-based-emotional-ttshttpsarxivorgabs251204552v1>#</a></h3><p><strong>Authors:</strong> Cong Wang, Changfeng Gao, Yang Xiang, Zhihao Du, Keyu An, Han Zhao, Qian Chen, Xiangang Li, Yingming Gao, Ya Li
<strong>Venue:</strong> arXiv (2025)</p><p>Differentiable reinforcement learning (RL) frameworks like DiffRO offer a powerful approach for controllable text-to-speech (TTS), but are vulnerable to reward hacking, particularly for nuanced tasks like emotion control. The policy model can exploit a vanilla Reward Model (RM) by generating acoustic artifacts to achieve spurious rewards, but at the cost of degrading perceptual quality. To address this, we propose Robust Reward Policy Optimization (RRPO), a novel framework that employs a hybrid regularization scheme. This scheme develops a robust RM whose reward signal is more reliably aligned with human perception, compelling the policy to abandon detrimental shortcuts and instead learn the complex features of genuine emotions. Our ablation study confirms the enhanced robustness of our RM, as evidenced by its strong cross-lingual generalization. The subjective evaluation demonstrates that this robust RM effectively mitigates reward hacking, leading to significant improvements in both emotional expressiveness and naturalness over all baselines. Demo page: <a href=https://lrwinr.github.io/RRPO-CosyVoice>https://lrwinr.github.io/RRPO-CosyVoice</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.04552v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.04552v1">üìÑ Download PDF</a></p><hr><h3 id=adapting-large-language-models-to-low-resource-tibetan-a-two-stage-continual-and-supervised-fine-tuning-studyhttpsarxivorgabs251203976v1><a href=https://arxiv.org/abs/2512.03976v1>Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study</a><a hidden class=anchor aria-hidden=true href=#adapting-large-language-models-to-low-resource-tibetan-a-two-stage-continual-and-supervised-fine-tuning-studyhttpsarxivorgabs251203976v1>#</a></h3><p><strong>Authors:</strong> Lifeng Chen, Ryan Lai, Tianming Liu
<strong>Venue:</strong> arXiv (2025)</p><p>Adapting large language models (LLMs) to low-resource languages remains a major challenge due to data scarcity and cross-lingual drift. This work presents a two-stage adaptation of Qwen2.5-3B to Tibetan, a morphologically rich and underrepresented language. We employ Continual Pretraining (CPT) to establish Tibetan linguistic grounding, followed by Supervised Fine-Tuning (SFT) for task and translation specialization. Empirical evaluations demonstrate a consistent decrease in perplexity (from 2.98 $\rightarrow$ 1.54) and substantial improvements in Chinese$\rightarrow$Tibetan translation quality (BLEU: 0.046 $\rightarrow$ 0.261; chrF: 2.2 $\rightarrow$ 6.6). Layer-wise analysis across 435 layers in Qwen3-4B reveals that adaptation primarily concentrates on embedding and output heads, with mid&ndash;late MLP projections encoding domain-specific transformations. Our findings suggest that CPT constructs a Tibetan semantic manifold while SFT sharpens task alignment with minimal representational disruption. This study provides the first quantitative exploration of Tibetan adaptation dynamics for LLMs, and offers an open, reproducible framework for extending multilingual foundation models to low-resource settings.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.03976v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.03976v1">üìÑ Download PDF</a></p><hr><h3 id=beyond-data-filtering-knowledge-localization-for-capability-removal-in-llmshttpsarxivorgabs251205648v1><a href=https://arxiv.org/abs/2512.05648v1>Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs</a><a hidden class=anchor aria-hidden=true href=#beyond-data-filtering-knowledge-localization-for-capability-removal-in-llmshttpsarxivorgabs251205648v1>#</a></h3><p><strong>Authors:</strong> Igor Shilov, Alex Cloud, Aryo Pradipta Gema, Jacob Goldman-Wetzler, Nina Panickssery, Henry Sleight, Erik Jones, Cem Anil
<strong>Venue:</strong> arXiv (2025)</p><p>Large Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, even small amounts of mislabeled content could give rise to dangerous capabilities. To address risks associated with mislabeled harmful content, prior work proposed Gradient Routing (Cloud et al., 2024) &ndash; a technique that localizes target knowledge into a dedicated subset of model parameters so they can later be removed. We explore an improved variant of Gradient Routing, which we call Selective GradienT Masking (SGTM), with particular focus on evaluating its robustness to label noise. SGTM zero-masks selected gradients such that target domain examples only update their dedicated parameters. We test SGTM&rsquo;s effectiveness in two applications: removing knowledge of one language from a model trained on a bilingual synthetic dataset, and removing biology knowledge from a model trained on English Wikipedia. In both cases SGTM provides better retain/forget trade-off in the presence of labeling errors compared to both data filtering and a previously proposed instantiation of Gradient Routing. Unlike shallow unlearning approaches that can be quickly undone through fine-tuning, SGTM exhibits strong robustness to adversarial fine-tuning, requiring seven times more fine-tuning steps to reach baseline performance on the forget set compared to a finetuning-based unlearning method (RMU). Our results suggest SGTM provides a promising pretraining-time complement to existing safety mitigations, particularly in settings where label noise is unavoidable.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05648v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05648v1">üìÑ Download PDF</a></p><hr><h3 id=exploiting-spatial-multiplexing-based-on-pixel-antennas-an-antenna-coding-approachhttpsarxivorgabs251205706v1><a href=https://arxiv.org/abs/2512.05706v1>Exploiting Spatial Multiplexing Based on Pixel Antennas: An Antenna Coding Approach</a><a hidden class=anchor aria-hidden=true href=#exploiting-spatial-multiplexing-based-on-pixel-antennas-an-antenna-coding-approachhttpsarxivorgabs251205706v1>#</a></h3><p><strong>Authors:</strong> Zixiang Han, Shanpu Shen, Ross Murch
<strong>Venue:</strong> arXiv (2025)</p><p>An antenna coding approach for exploiting the spatial multiplexing capability of pixel antennas is proposed. This approach can leverage additional degrees of freedom in the beamspace domain to transmit more information streams. Pixel antennas are a general reconfigurable antenna design where a radiating structure with arbitrary shape and size can be discretized into sub-wavelength elements called pixels which are connected by radio frequency switches. By controlling the switch states, the pixel antenna topology can be flexibly adjusted so that the resulting radiation pattern can be reconfigured for beamspace spatial multiplexing. In this work, we introduce the antenna coder and pattern coder for pixel antennas, provide a multiple-input multiple-output (MIMO) communication system model with antenna coding in the beamspace domain, and derive the spectral efficiency. Utilizing the antenna coder, the radiation pattern of the pixel antenna is analyzed and efficient optimization algorithms are provided for antenna coding design. Numerical simulation results show that the proposed technique using pixel antennas can enhance spectral efficiency of 4-by-4 MIMO by up to 12 bits/s/Hz or equivalently reduce the required transmit power by up to 90% when compared to conventional MIMO, demonstrating the effectiveness of the antenna coding technique in spectral efficiency enhancement and its promise for future sixth generation (6G) wireless communication.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05706v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05706v1">üìÑ Download PDF</a></p><hr><h3 id=the-decision-path-to-control-ai-risks-completely-fundamental-control-mechanisms-for-ai-governancehttpsarxivorgabs251204489v1><a href=https://arxiv.org/abs/2512.04489v1>The Decision Path to Control AI Risks Completely: Fundamental Control Mechanisms for AI Governance</a><a hidden class=anchor aria-hidden=true href=#the-decision-path-to-control-ai-risks-completely-fundamental-control-mechanisms-for-ai-governancehttpsarxivorgabs251204489v1>#</a></h3><p><strong>Authors:</strong> Yong Tao
<strong>Venue:</strong> arXiv (2025)</p><p>Artificial intelligence (AI) advances rapidly but achieving complete human control over AI risks remains an unsolved problem, akin to driving the fast AI &ldquo;train&rdquo; without a &ldquo;brake system.&rdquo; By exploring fundamental control mechanisms at key elements of AI decisions, this paper develops a systematic solution to thoroughly control AI risks, providing an architecture for AI governance and legislation with five pillars supported by six control mechanisms, illustrated through a minimum set of AI Mandates (AIMs). Three of the AIMs must be built inside AI systems and three in society to address major areas of AI risks: 1) align AI values with human users; 2) constrain AI decision-actions by societal ethics, laws, and regulations; 3) build in human intervention options for emergencies and shut-off switches for existential threats; 4) limit AI access to resources to reinforce controls inside AI; 5) mitigate spillover risks like job loss from AI. We also highlight the differences in AI governance on physical AI systems versus generative AI. We discuss how to strengthen analog physical safeguards to prevent smarter AI/AGI/ASI from circumventing core safety controls by exploiting AI&rsquo;s intrinsic disconnect from the analog physical world: AI&rsquo;s nature as pure software code run on chips controlled by humans, and the prerequisite that all AI-driven physical actions must be digitized. These findings establish a theoretical foundation for AI governance and legislation as the basic structure of a &ldquo;brake system&rdquo; for AI decisions. If enacted, these controls can rein in AI dangers as completely as humanly possible, removing large chunks of currently wide-open AI risks, substantially reducing overall AI risks to residual human errors.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.04489v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.04489v1">üìÑ Download PDF</a></p><hr><h3 id=structured-reasoning-with-tree-of-thoughts-for-bengali-math-word-problemshttpsarxivorgabs251205580v1><a href=https://arxiv.org/abs/2512.05580v1>Structured Reasoning with Tree-of-Thoughts for Bengali Math Word Problems</a><a hidden class=anchor aria-hidden=true href=#structured-reasoning-with-tree-of-thoughts-for-bengali-math-word-problemshttpsarxivorgabs251205580v1>#</a></h3><p><strong>Authors:</strong> Aurprita Mahmood, Sabrin alam, Neloy kumer Sagor, Md. Abdul Hadi, Md. Sehab Al Islam, Minhajul Islam
<strong>Venue:</strong> arXiv (2025)</p><p>Mathematical Word Problems (MWPs) are among the most challenging tasks in natural language processing because they require both linguistic understanding and multi-step numerical reasoning. While Chain-of-Thought (CoT) prompting has shown promise, its linear structure often propagates errors, limiting overall effectiveness. To address this limitation, we present the a systematic study of Tree-of-Thought (ToT) reasoning for Bengali MWPs using the SOMADHAN dataset. Owing to computational and token-cost constraints, we evaluate a curated set of 100 representative problems across multiple large language models (LLMs), including GPT-OSS and LLaMA variants, under standard prompting, CoT, and ToT strategies. Our results show that CoT improves baseline accuracy from 78% (standard prompting) to 83% on average, while ToT further increases performance by up to 5 percentage points, achieving 88% accuracy with GPT-OSS-120B. These improvements highlight that ToT is particularly effective in medium-to-large-scale models but may offer less advantage for smaller ones. Overall, our findings establish ToT as a robust framework for solving mathematical problems in low-resource languages such as Bengali. More broadly, this study shows that structured reasoning methods like ToT can provide more reliable and globally consistent outcomes than CoT, paving the way for better reasoning strategies in multilingual NLP.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05580v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05580v1">üìÑ Download PDF</a></p><hr><h3 id=enhancing-retrieval-augmented-generation-with-entity-linking-for-educational-platformshttpsarxivorgabs251205967v1><a href=https://arxiv.org/abs/2512.05967v1>Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms</a><a hidden class=anchor aria-hidden=true href=#enhancing-retrieval-augmented-generation-with-entity-linking-for-educational-platformshttpsarxivorgabs251205967v1>#</a></h3><p><strong>Authors:</strong> Francesco Granata, Francesco Poggi, Misael Mongiov√¨
<strong>Venue:</strong> arXiv (2025)</p><p>In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05967v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05967v1">üìÑ Download PDF</a></p><hr><h3 id=training-time-action-conditioning-for-efficient-real-time-chunkinghttpsarxivorgabs251205964v1><a href=https://arxiv.org/abs/2512.05964v1>Training-Time Action Conditioning for Efficient Real-Time Chunking</a><a hidden class=anchor aria-hidden=true href=#training-time-action-conditioning-for-efficient-real-time-chunkinghttpsarxivorgabs251205964v1>#</a></h3><p><strong>Authors:</strong> Kevin Black, Allen Z. Ren, Michael Equi, Sergey Levine
<strong>Venue:</strong> arXiv (2025)</p><p>Real-time chunking (RTC) enables vision-language-action models (VLAs) to generate smooth, reactive robot trajectories by asynchronously predicting action chunks and conditioning on previously committed actions via inference-time inpainting. However, this inpainting method introduces computational overhead that increases inference latency. In this work, we propose a simple alternative: simulating inference delay at training time and conditioning on action prefixes directly, eliminating any inference-time overhead. Our method requires no modifications to the model architecture or robot runtime, and can be implemented with only a few additional lines of code. In simulated experiments, we find that training-time RTC outperforms inference-time RTC at higher inference delays. In real-world experiments on box building and espresso making tasks with the $œÄ_{0.6}$ VLA, we demonstrate that training-time RTC maintains both task performance and speed parity with inference-time RTC while being computationally cheaper. Our results suggest that training-time action conditioning is a practical drop-in replacement for inference-time inpainting in real-time robot control.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05964v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05964v1">üìÑ Download PDF</a></p><hr><h3 id=thermodynamics-of-shear-equilibration-during-magnetic-reconnection-onset-in-mixed-equilibrium-current-sheetshttpsarxivorgabs251205921v1><a href=https://arxiv.org/abs/2512.05921v1>Thermodynamics of Shear Equilibration During Magnetic Reconnection Onset in Mixed-Equilibrium Current Sheets</a><a hidden class=anchor aria-hidden=true href=#thermodynamics-of-shear-equilibration-during-magnetic-reconnection-onset-in-mixed-equilibrium-current-sheetshttpsarxivorgabs251205921v1>#</a></h3><p><strong>Authors:</strong> Dominic Payne, Marc Swisdak, James Drake, Tak Chu Li
<strong>Venue:</strong> arXiv (2025)</p><p>Magnetic shear across the polarity inversion line (PIL) plays an important role in the explosive nature of reconnection onset and in the equilibration of current sheets, acting as a source of free energy that can enhance or inhibit the onset process under certain conditions. In this study, we use a 2D PIC simulation to examine the local interaction between the reconnection guide field and thermodynamic variables during reconnection onset in a region of initially depleted thermal energy and enhanced magnetic energy in a large guide field background. We identify critical stages of the equilibration process, characterize intervals based on whether the pressure evolution is driven by changes in density or temperature, and discuss what these intervals imply about the evolution of local heat and work density. Finally, we examine power densities associated with electromagnetic field time evolution and electromagnetic energy transfer and compare to those related to thermodynamic changes.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05921v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05921v1">üìÑ Download PDF</a></p><hr><h3 id=scail-towards-studio-grade-character-animation-via-in-context-learning-of-3d-consistent-pose-representationshttpsarxivorgabs251205905v1><a href=https://arxiv.org/abs/2512.05905v1>SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations</a><a hidden class=anchor aria-hidden=true href=#scail-towards-studio-grade-character-animation-via-in-context-learning-of-3d-consistent-pose-representationshttpsarxivorgabs251205905v1>#</a></h3><p><strong>Authors:</strong> Wenhao Yan, Sheng Ye, Zhuoyi Yang, Jiayan Teng, ZhenHui Dong, Kairui Wen, Xiaotao Gu, Yong-Jin Liu, Jie Tang
<strong>Venue:</strong> arXiv (2025)</p><p>Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present \textbf{SCAIL} (\textbf{S}tudio-grade \textbf{C}haracter \textbf{A}nimation via \textbf{I}n-context \textbf{L}earning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that \textbf{SCAIL} achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05905v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05905v1">üìÑ Download PDF</a></p><hr><h3 id=editthinker-unlocking-iterative-reasoning-for-any-image-editorhttpsarxivorgabs251205965v1><a href=https://arxiv.org/abs/2512.05965v1>EditThinker: Unlocking Iterative Reasoning for Any Image Editor</a><a hidden class=anchor aria-hidden=true href=#editthinker-unlocking-iterative-reasoning-for-any-image-editorhttpsarxivorgabs251205965v1>#</a></h3><p><strong>Authors:</strong> Hongyu Li, Manyuan Zhang, Dian Zheng, Ziyu Guo, Yimeng Jia, Kaituo Feng, Hao Yu, Yexin Liu, Yan Feng, Peng Pei, Xunliang Cai, Linjiang Huang, Hongsheng Li, Si Liu
<strong>Venue:</strong> arXiv (2025)</p><p>Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to &rsquo;think&rsquo; while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker&rsquo;s thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05965v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05965v1">üìÑ Download PDF</a></p><hr><h2 id=-linguistics>üîç linguistics<a hidden class=anchor aria-hidden=true href=#-linguistics>#</a></h2><h3 id=aqua-net-adaptive-frequency-fusion-and-illumination-aware-network-for-underwater-image-enhancementhttpsarxivorgabs251205960v1><a href=https://arxiv.org/abs/2512.05960v1>AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement</a><a hidden class=anchor aria-hidden=true href=#aqua-net-adaptive-frequency-fusion-and-illumination-aware-network-for-underwater-image-enhancementhttpsarxivorgabs251205960v1>#</a></h3><p><strong>Authors:</strong> Munsif Ali, Najmul Hassan, Lucia Ventura, Davide Di Bari, Simonepietro Canese
<strong>Venue:</strong> arXiv (2025)</p><p>Underwater images often suffer from severe color distortion, low contrast, and a hazy appearance due to wavelength-dependent light absorption and scattering. Simultaneously, existing deep learning models exhibit high computational complexity, which limits their practical deployment for real-time underwater applications. To address these challenges, this paper presents a novel underwater image enhancement model, called Adaptive Frequency Fusion and Illumination Aware Network (AQUA-Net). It integrates a residual encoder decoder with dual auxiliary branches, which operate in the frequency and illumination domains. The frequency fusion encoder enriches spatial representations with frequency cues from the Fourier domain and preserves fine textures and structural details. Inspired by Retinex, the illumination-aware decoder performs adaptive exposure correction through a learned illumination map that separates reflectance from lighting effects. This joint spatial, frequency, and illumination design enables the model to restore color balance, visual contrast, and perceptual realism under diverse underwater conditions. Additionally, we present a high-resolution, real-world underwater video-derived dataset from the Mediterranean Sea, which captures challenging deep-sea conditions with realistic visual degradations to enable robust evaluation and development of deep learning models. Extensive experiments on multiple benchmark datasets show that AQUA-Net performs on par with SOTA in both qualitative and quantitative evaluations while using less number of parameters. Ablation studies further confirm that the frequency and illumination branches provide complementary contributions that improve visibility and color representation. Overall, the proposed model shows strong generalization capability and robustness, and it provides an effective solution for real-world underwater imaging applications.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05960v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05960v1">üìÑ Download PDF</a></p><hr><h3 id=sympybench-a-dynamic-benchmark-for-scientific-reasoning-with-executable-python-codehttpsarxivorgabs251205954v1><a href=https://arxiv.org/abs/2512.05954v1>SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code</a><a hidden class=anchor aria-hidden=true href=#sympybench-a-dynamic-benchmark-for-scientific-reasoning-with-executable-python-codehttpsarxivorgabs251205954v1>#</a></h3><p><strong>Authors:</strong> Shima Imani, Seungwhan Moon, Adel Ahmadyan, Lu Zhang, Kirmani Ahmed, Babak Damavandi
<strong>Venue:</strong> arXiv (2025)</p><p>We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05954v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05954v1">üìÑ Download PDF</a></p><hr><h3 id=lpd-learnable-prototypes-with-diversity-regularization-for-weakly-supervised-histopathology-segmentationhttpsarxivorgabs251205922v1><a href=https://arxiv.org/abs/2512.05922v1>LPD: Learnable Prototypes with Diversity Regularization for Weakly Supervised Histopathology Segmentation</a><a hidden class=anchor aria-hidden=true href=#lpd-learnable-prototypes-with-diversity-regularization-for-weakly-supervised-histopathology-segmentationhttpsarxivorgabs251205922v1>#</a></h3><p><strong>Authors:</strong> Khang Le, Anh Mai Vu, Thi Kim Trang Vo, Ha Thach, Ngoc Bui Lam Quang, Thanh-Huy Nguyen, Minh H. N. Le, Zhu Han, Chandra Mohan, Hien Van Nguyen
<strong>Venue:</strong> arXiv (2025)</p><p>Weakly supervised semantic segmentation (WSSS) in histopathology reduces pixel-level labeling by learning from image-level labels, but it is hindered by inter-class homogeneity, intra-class heterogeneity, and CAM-induced region shrinkage (global pooling-based class activation maps whose activations highlight only the most distinctive areas and miss nearby class regions). Recent works address these challenges by constructing a clustering prototype bank and then refining masks in a separate stage; however, such two-stage pipelines are costly, sensitive to hyperparameters, and decouple prototype discovery from segmentation learning, limiting their effectiveness and efficiency. We propose a cluster-free, one-stage learnable-prototype framework with diversity regularization to enhance morphological intra-class heterogeneity coverage. Our approach achieves state-of-the-art (SOTA) performance on BCSS-WSSS, outperforming prior methods in mIoU and mDice. Qualitative segmentation maps show sharper boundaries and fewer mislabels, and activation heatmaps further reveal that, compared with clustering-based prototypes, our learnable prototypes cover more diverse and complementary regions within each class, providing consistent qualitative evidence for their effectiveness.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05922v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05922v1">üìÑ Download PDF</a></p><hr><h3 id=a-redshift-independent-theoretical-halo-mass-function-validated-with-the-uchuu-simulationshttpsarxivorgabs251205847v1><a href=https://arxiv.org/abs/2512.05847v1>A redshift-independent theoretical halo mass function validated with the Uchuu simulations</a><a hidden class=anchor aria-hidden=true href=#a-redshift-independent-theoretical-halo-mass-function-validated-with-the-uchuu-simulationshttpsarxivorgabs251205847v1>#</a></h3><p><strong>Authors:</strong> Elena Fern√°ndez-Garc√≠a, Juan E. Betancort-Rijo, Francisco Prada, Tomoaki Ishiyama, Anatoly Klypin, Jos√© Ruedas
<strong>Venue:</strong> arXiv (2025)</p><p>We present a new theoretical framework for the halo mass function (HMF) that accurately predicts the abundance of dark matter haloes across an exceptionally wide range in mass and redshift. Building on a generalised Press & Schechter model and triaxial collapse (GPS+), we predict the HMF in terms of the variance of the linear density field, with only a weak explicit dependence on halo mass and no explicit dependence on redshift. The GPS+ model naturally provides the correct normalization and high-mass behaviour without requiring empirical fitting. We calibrate and validate the GPS+ model using the Uchuu N-body simulation suite, which combines large cosmological volume and high mass resolution under Planck cosmology. Using six simulations with up to 300 realizations, we obtain precision HMF measurements spanning halo masses in the range 6.5 &lt; log($M_{\rm 200m}$/[h$^{-1}$ $M_{\odot}$]) &lt;16 over 0 &lt; z &lt; 20, with reduced cosmic variance. Across this full domain, the GPS+ model reproduces the simulated HMF with deviations typically below 10-20%. Comparison with the Sheth-Tormen (ST) model shows similar performance at z &lt; 2, but markedly improved agreement at higher redshifts, where ST can deviate by 70-80% while our model remains within ~20%. Finally, we assess the impact of the halo mass definition: adopting the evolving virial overdensity of Bryan & Norman (1998) worsens agreement at low redshift and high masses, whereas M200m yields a more universal, nearly redshift-independent HMF.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05847v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05847v1">üìÑ Download PDF</a></p><hr><h3 id=simpact-simulation-enabled-action-planning-using-vision-language-modelshttpsarxivorgabs251205955v1><a href=https://arxiv.org/abs/2512.05955v1>SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models</a><a hidden class=anchor aria-hidden=true href=#simpact-simulation-enabled-action-planning-using-vision-language-modelshttpsarxivorgabs251205955v1>#</a></h3><p><strong>Authors:</strong> Haowen Liu, Shaoxiong Yao, Haonan Chen, Jiawei Gao, Jiayuan Mao, Jia-Bin Huang, Yilun Du
<strong>Venue:</strong> arXiv (2025)</p><p>Vision-Language Models (VLMs) exhibit remarkable common-sense and semantic reasoning capabilities. However, they lack a grounded understanding of physical dynamics. This limitation arises from training VLMs on static internet-scale visual-language data that contain no causal interactions or action-conditioned changes. Consequently, it remains challenging to leverage VLMs for fine-grained robotic manipulation tasks that require physical understanding, reasoning, and corresponding action planning. To overcome this, we present SIMPACT, a test-time, SIMulation-enabled ACTion Planning framework that equips VLMs with physical reasoning through simulation-in-the-loop world modeling, without requiring any additional training. From a single RGB-D observation, SIMPACT efficiently constructs physics simulations, enabling the VLM to propose informed actions, observe simulated rollouts, and iteratively refine its reasoning. By integrating language reasoning with physics prediction, our simulation-enabled VLM can understand contact dynamics and action outcomes in a physically grounded way. Our method demonstrates state-of-the-art performance on five challenging, real-world rigid-body and deformable manipulation tasks that require fine-grained physical reasoning, outperforming existing general-purpose robotic manipulation models. Our results demonstrate that embedding physics understanding via efficient simulation into VLM reasoning at test time offers a promising path towards generalizable embodied intelligence. Project webpage can be found at <a href=https://simpact-bot.github.io>https://simpact-bot.github.io</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05955v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05955v1">üìÑ Download PDF</a></p><hr><h3 id=heard-or-halted-gender-interruptions-and-emotional-tone-in-us-supreme-court-oral-argumentshttpsarxivorgabs251205832v1><a href=https://arxiv.org/abs/2512.05832v1>Heard or Halted? Gender, Interruptions, and Emotional Tone in U.S. Supreme Court Oral Arguments</a><a hidden class=anchor aria-hidden=true href=#heard-or-halted-gender-interruptions-and-emotional-tone-in-us-supreme-court-oral-argumentshttpsarxivorgabs251205832v1>#</a></h3><p><strong>Authors:</strong> Yifei Tong
<strong>Venue:</strong> arXiv (2025)</p><p>This study examines how interruptions during U.S. Supreme Court oral arguments shape both the semantic content and emotional tone of advocates&rsquo; speech, with a focus on gendered dynamics in judicial discourse. Using the ConvoKit Supreme Court Corpus (2010-2019), we analyze 12,663 speech chunks from advocate-justice interactions to assess whether interruptions alter the meaning of an advocate&rsquo;s argument and whether interruptions toward female advocates exhibit more negative emotional valence. Semantic shifts are quantified using GloVe-based sentence embeddings, while sentiment is measured through lexicon-based analysis. We find that semantic similarity between pre- and post-interruption speech remains consistently high, suggesting that interruptions do not substantially alter argumentative content. However, interruptions directed at female advocates contain significantly higher levels of negative sentiment. These results deepen empirical understanding of gendered communication in elite institutional settings and demonstrate the value of computational linguistic methods for studying power, discourse, and equity in judicial proceedings.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05832v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05832v1">üìÑ Download PDF</a></p><hr><h3 id=unveiling-affective-polarization-trends-in-parliamentary-proceedingshttpsarxivorgabs251205231v1><a href=https://arxiv.org/abs/2512.05231v1>Unveiling Affective Polarization Trends in Parliamentary Proceedings</a><a hidden class=anchor aria-hidden=true href=#unveiling-affective-polarization-trends-in-parliamentary-proceedingshttpsarxivorgabs251205231v1>#</a></h3><p><strong>Authors:</strong> Gili Goldin, Ella Rabinovich, Shuly Wintner
<strong>Venue:</strong> arXiv (2025)</p><p>Recent years have seen an increase in polarized discourse worldwide, on various platforms. We propose a novel method for quantifying polarization, based on the emotional style of the discourse rather than on differences in ideological stands. Using measures of Valence, Arousal and Dominance, we detect signals of emotional discourse and use them to operationalize the concept of affective polarization. Applying this method to a recently released corpus of proceedings of the Knesset, the Israeli parliament (in Hebrew), we find that the emotional style of members of government differs from that of opposition members; and that the level of affective polarization, as reflected by this style, is significantly increasing with time.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05231v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05231v1">üìÑ Download PDF</a></p><hr><h3 id=strongly-coupled-quantum-forceshttpsarxivorgabs251205968v1><a href=https://arxiv.org/abs/2512.05968v1>Strongly Coupled Quantum Forces</a><a hidden class=anchor aria-hidden=true href=#strongly-coupled-quantum-forceshttpsarxivorgabs251205968v1>#</a></h3><p><strong>Authors:</strong> Yuval Grossman, Chinhsan Sieng, Xun-Jie Xu, Bingrong Yu
<strong>Venue:</strong> arXiv (2025)</p><p>Quantum forces are long-range interactions originating from vacuum fluctuations of mediator fields. Such forces inevitably arise between ordinary matter particles whenever they couple to light mediator species. Conventional computations of quantum forces rely on evaluating one-loop Feynman diagrams of the relevant scattering processes. In this work, we introduce a novel framework to compute quantum forces. Instead of relying on perturbative scattering amplitudes, we directly evaluate the quantum fluctuations of the mediator field by solving its quantized equation of motion with appropriate boundary conditions. This approach remains valid beyond the Born approximation and thus applies to regimes of strong coupling between the mediator and matter fields. In the weak-coupling limit, our results reproduce the known expressions from the Feynman diagram approach. In the strong-coupling regime, the result is modified by a factor that can suppress or enhance the effect. In contrast to classical forces, quantum forces intrinsically violate the superposition principle. Our approach may therefore offer a useful tool for probing non-perturbative effects in the infrared regime.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05968v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05968v1">üìÑ Download PDF</a></p><hr><h3 id=entanglement-enhanced-quantum-nano-vibrometryhttpsarxivorgabs251205961v1><a href=https://arxiv.org/abs/2512.05961v1>Entanglement-Enhanced Quantum Nano-Vibrometry</a><a hidden class=anchor aria-hidden=true href=#entanglement-enhanced-quantum-nano-vibrometryhttpsarxivorgabs251205961v1>#</a></h3><p><strong>Authors:</strong> Colin P. Lualdi, Joshua Rapp, Spencer J. Johnson, Michael Vayninger, Paul G. Kwiat
<strong>Venue:</strong> arXiv (2025)</p><p>The study of dynamic systems at the nanometer scale can benefit from the loss and background resilience offered by quantum two-photon interference. However, fast measurements with the required resolution are difficult to realize. As a solution, we introduce extreme energy entanglement between the photons undergoing interference. Using a flux probing analysis technique, we recover vibrational signals with frequencies as high as 21 kHz. Along with validating nanometer-scale precision and accuracy, we observe a significant quantum advantage when measuring in the presence of loss and background.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05961v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05961v1">üìÑ Download PDF</a></p><hr><h3 id=categorifying-isomonodromic-deformations-via-lie-groupoids-i-logarithmic-singularitieshttpsarxivorgabs251205966v1><a href=https://arxiv.org/abs/2512.05966v1>Categorifying isomonodromic deformations via Lie groupoids I: Logarithmic singularities</a><a hidden class=anchor aria-hidden=true href=#categorifying-isomonodromic-deformations-via-lie-groupoids-i-logarithmic-singularitieshttpsarxivorgabs251205966v1>#</a></h3><p><strong>Authors:</strong> Waleed Qaisar
<strong>Venue:</strong> arXiv (2025)</p><p>We upgrade the classical operation of \textit{isomonodromic deformations} along a path $Œ≥$ to a functor $\mathbb{P}_Œ≥$ between categories of flat connections with logarithmic singularities along a divisor $D$, which itself depends functorially on $Œ≥$, using tools from the theory of Lie groupoids. As applications, (1) we get that isomonodromy gives a map of moduli \textit{stacks} of flat connections with logarithmic singularities, (2) we encode higher homotopical information at level 2, i.e. we get an action of the fundamental 2-groupoid of the base of our family on the categories of logarithmic flat connections on the fibres, and (3) our methods produce a geometric incarnation of the isomonodromy functors as Morita equivalences which are more primary than the isomonodromy functors themselves, and from which they can be formally extracted by passing to representation categories.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05966v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05966v1">üìÑ Download PDF</a></p><hr><h3 id=bootstrapping-fuzzers-for-compilers-of-low-resource-language-dialects-using-language-modelshttpsarxivorgabs251205887v1><a href=https://arxiv.org/abs/2512.05887v1>Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models</a><a hidden class=anchor aria-hidden=true href=#bootstrapping-fuzzers-for-compilers-of-low-resource-language-dialects-using-language-modelshttpsarxivorgabs251205887v1>#</a></h3><p><strong>Authors:</strong> Sairam Vaidya, Marcel B√∂hme, Loris D&rsquo;Antoni
<strong>Venue:</strong> arXiv (2025)</p><p>Modern extensible compiler frameworks-such as MLIR-enable rapid creation of domain-specific language dialects. This flexibility, however, makes correctness harder to ensure as the same extensibility that accelerates development also complicates maintaining the testing infrastructure. Extensible languages require automated test generation that is both dialect-agnostic (works across dialects without manual adaptation) and dialect-effective (targets dialect-specific features to find bugs). Existing approaches typically sacrifice one of these goals by either requiring manually constructed seed corpora for each dialect, or by failing to be effective. We present a dialect-agnostic and dialect-effective grammar-based and coverage-guided fuzzing approach for extensible compilers that combines two key insights from existing work: (i) the grammars of dialects, which already encode the structural and type constraints, can often be extracted automatically from the dialect specification; and (ii) these grammars can be used in combination with pre-trained large language models to automatically generate representative and diverse seed inputs from the full dialect space without requiring any manual input or training data. These seeds can then be used to bootstrap coverage-guided fuzzers. We built this approach into a tool, Germinator. When evaluated on six MLIR projects spanning 91 dialects, Germinator generated seeds improve line coverage by 10-120% over grammar-based baselines. We compare against grammar-based baselines because they are the only class of existing automatic seed generators that can be applied uniformly across MLIR&rsquo;s heterogeneous dialect ecosystem. Germinator discovers 88 previously unknown bugs (40 confirmed), including 23 in dialects with no prior automated test generators, demonstrating effective and controllable testing of low-resource dialects at scale.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05887v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05887v1">üìÑ Download PDF</a></p><hr><h3 id=model-selection-with-uncertainty-in-estimating-optimal-dynamic-treatment-regimeshttpsarxivorgabs251205695v1><a href=https://arxiv.org/abs/2512.05695v1>Model selection with uncertainty in estimating optimal dynamic treatment regimes</a><a hidden class=anchor aria-hidden=true href=#model-selection-with-uncertainty-in-estimating-optimal-dynamic-treatment-regimeshttpsarxivorgabs251205695v1>#</a></h3><p><strong>Authors:</strong> Chunyu Wang, Brian Tom
<strong>Venue:</strong> arXiv (2025)</p><p>Optimal dynamic treatment regimes (DTRs), as a key part of precision medicine, have progressively gained more attention recently. To inform clinical decision making, interpretable and parsimonious models for contrast functions are preferred, raising concerns about undue misspecification. It is therefore important to properly evaluate the performance of candidate interpretable models and select the one that best approximates the unknown contrast function. Moreover, since a DTR usually involves multiple decision points, an inaccurate approximation at a later decision point affects its estimation at an earlier decision point when a backward induction algorithm is applied. This paper aims to perform model selection for contrast functions in the context of learning optimal DTRs from observed data. Note that the relative performance of candidate models may heavily depend on the sample size when, for example, the comparison is made between parametric and tree-based models. Therefore, instead of investigating the limiting behavior of each candidate model and developing methods to select asymptotically the `correct&rsquo; one, we focus on the finite sample performance of each model and attempt to perform model selection under a given sample size. To this end, we adopt the counterfactual cross-validation metric and propose a novel method to estimate the variance of the metric. Supplementing the cross-validation metric with its estimated variance allows us to characterize the uncertainty in model selection under a given sample size and facilitates hypothesis testing associated with a preferred model structure.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05695v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05695v1">üìÑ Download PDF</a></p><hr><h2 id=-psycholinguistics>üîç psycholinguistics<a hidden class=anchor aria-hidden=true href=#-psycholinguistics>#</a></h2><h3 id=evolutionary-system-2-reasoning-an-empirical-proofhttpsarxivorgabs251205760v1><a href=https://arxiv.org/abs/2512.05760v1>Evolutionary System 2 Reasoning: An Empirical Proof</a><a hidden class=anchor aria-hidden=true href=#evolutionary-system-2-reasoning-an-empirical-proofhttpsarxivorgabs251205760v1>#</a></h3><p><strong>Authors:</strong> Zeyuan Ma, Wenqi Huang, Guo-Huan Song, Hongshu Guo, Sijie Ma, Zhiguang Cao, Yue-Jiao Gong
<strong>Venue:</strong> arXiv (2025)</p><p>Machine intelligence marks the ultimate dream of making machines&rsquo; intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at <a href=https://github.com/MetaEvo/ERO>https://github.com/MetaEvo/ERO</a> for reproduction needs.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05760v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05760v1">üìÑ Download PDF</a></p><hr><h3 id=squeezing-classical-antiferromagnets-into-quantum-spin-liquids-via-global-cavity-fluctuationshttpsarxivorgabs251205630v1><a href=https://arxiv.org/abs/2512.05630v1>Squeezing Classical Antiferromagnets into Quantum Spin Liquids via Global Cavity Fluctuations</a><a hidden class=anchor aria-hidden=true href=#squeezing-classical-antiferromagnets-into-quantum-spin-liquids-via-global-cavity-fluctuationshttpsarxivorgabs251205630v1>#</a></h3><p><strong>Authors:</strong> Charlie-Ray Mann, Mark A. Oehlgrien, B≈Ça≈ºej Jaworowski, Giuseppe Calaj√≥, Jamir Marino, Kyung S. Choi, Darrick E. Chang
<strong>Venue:</strong> arXiv (2025)</p><p>Cavity quantum electrodynamics with atomic ensembles is typically associated with collective spin phenomena, such as superradiance and spin squeezing, in which the atoms evolve collectively as a macroscopic spin ($S\sim N/2$) on the Bloch sphere. Surprisingly, we show that the tendency toward a collective spin description need not imply collective spin phenomena; rather, it can be exploited to generate new forms of strongly correlated quantum matter. The key idea is to use uniform cavity-mediated interactions to energetically project the system into the total-spin singlet sector ($S=0$) - a highly entangled subspace where the physics is governed entirely by cavity fluctuations. Focusing on Rydberg atom arrays coupled to a single-mode cavity, we show that global cavity fluctuations can effectively squeeze classical antiferromagnets into quantum spin liquids, characterized by non-local entanglement, fractionalized excitations, and emergent gauge fields. This work suggests that cavity QED can be a surprising resource for inducing strongly correlated phenomena, which could be explored in the new generation of hybrid tweezer-cavity platforms.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05630v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05630v1">üìÑ Download PDF</a></p><hr><h3 id=trusted-ai-agents-in-the-cloudhttpsarxivorgabs251205951v1><a href=https://arxiv.org/abs/2512.05951v1>Trusted AI Agents in the Cloud</a><a hidden class=anchor aria-hidden=true href=#trusted-ai-agents-in-the-cloudhttpsarxivorgabs251205951v1>#</a></h3><p><strong>Authors:</strong> Teofil Bodea, Masanori Misono, Julian Pritzi, Patrick Sabanic, Thore Sommer, Harshavardhan Unnibhavi, David Schall, Nuno Santos, Dimitrios Stavrakakis, Pramod Bhatotia
<strong>Venue:</strong> arXiv (2025)</p><p>AI agents powered by large language models are increasingly deployed as cloud services that autonomously access sensitive data, invoke external tools, and interact with other agents. However, these agents run within a complex multi-party ecosystem, where untrusted components can lead to data leakage, tampering, or unintended behavior. Existing Confidential Virtual Machines (CVMs) provide only per binary protection and offer no guarantees for cross-principal trust, accelerator-level isolation, or supervised agent behavior. We present Omega, a system that enables trusted AI agents by enforcing end-to-end isolation, establishing verifiable trust across all contributing principals, and supervising every external interaction with accountable provenance. Omega builds on Confidential VMs and Confidential GPUs to create a Trusted Agent Platform that hosts many agents within a single CVM using nested isolation. It also provides efficient multi-agent orchestration with cross-principal trust establishment via differential attestation, and a policy specification and enforcement framework that governs data access, tool usage, and inter-agent communication for data protection and regulatory compliance. Implemented on AMD SEV-SNP and NVIDIA H100, Omega fully secures agent state across CVM-GPU, and achieves high performance while enabling high-density, policy-compliant multi-agent deployments at cloud scale.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05951v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05951v1">üìÑ Download PDF</a></p><hr><h3 id=speech-world-model-causal-state-action-planning-with-explicit-reasoning-for-speechhttpsarxivorgabs251205933v1><a href=https://arxiv.org/abs/2512.05933v1>Speech World Model: Causal State-Action Planning with Explicit Reasoning for Speech</a><a hidden class=anchor aria-hidden=true href=#speech-world-model-causal-state-action-planning-with-explicit-reasoning-for-speechhttpsarxivorgabs251205933v1>#</a></h3><p><strong>Authors:</strong> Xuanru Zhou, Jiachen Lian, Henry Hong, Xinyi Yang, Gopala Anumanchipalli
<strong>Venue:</strong> arXiv (2025)</p><p>Current speech-language models (SLMs) typically use a cascade of speech encoder and large language model, treating speech understanding as a single black box. They analyze the content of speech well but reason weakly about other aspects, especially under sparse supervision. Thus, we argue for explicit reasoning over speech states and actions with modular and transparent decisions. Inspired by cognitive science we adopt a modular perspective and a world model view in which the system learns forward dynamics over latent states. We factorize speech understanding into four modules that communicate through a causal graph, establishing a cognitive state search space. Guided by posterior traces from this space, an instruction-tuned language model produces a concise causal analysis and a user-facing response, enabling counterfactual interventions and interpretability under partial supervision. We present the first graph based modular speech model for explicit reasoning and we will open source the model and data to promote the development of advanced speech understanding.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05933v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05933v1">üìÑ Download PDF</a></p><hr><h2 id=-llm>üîç llm<a hidden class=anchor aria-hidden=true href=#-llm>#</a></h2><h3 id=group-classification-12-dimensional-linear-equation-of-asian-options-pricinghttpsarxivorgabs251205963v1><a href=https://arxiv.org/abs/2512.05963v1>Group Classification (1+2)-dimensional Linear Equation of Asian Options Pricing</a><a hidden class=anchor aria-hidden=true href=#group-classification-12-dimensional-linear-equation-of-asian-options-pricinghttpsarxivorgabs251205963v1>#</a></h3><p><strong>Authors:</strong> Stanislav V. Spichak, Valeriy I. Stogniy, Inna M. Kopas
<strong>Venue:</strong> arXiv (2025)</p><p>We consider a class of (1+2)-dimensional linear partial differential of Asian options pricing. Special cases have been used to models of financial mathematics. We carry out group classification of a class equations. In particular, the maximum dimension Lie invariance algebra within the above class is eight-dimensional. It is shown that an equation with such an algebra can be transformed into the linear Kolmogorov equation with the help of the point transformations of variables. Using the operators of invariance algebra symmetry reduction is carried out and invariant exact solutions are constructed for some equations.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05963v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05963v1">üìÑ Download PDF</a></p><hr><h3 id=transformation-of-orientation-and-rotation-angles-of-synchronous-satellites-application-to-the-galilean-moonshttpsarxivorgabs251205935v1><a href=https://arxiv.org/abs/2512.05935v1>Transformation of orientation and rotation angles of synchronous satellites: Application to the Galilean moons</a><a hidden class=anchor aria-hidden=true href=#transformation-of-orientation-and-rotation-angles-of-synchronous-satellites-application-to-the-galilean-moonshttpsarxivorgabs251205935v1>#</a></h3><p><strong>Authors:</strong> Marie Yseboodt, Rose-Marie Baland
<strong>Venue:</strong> arXiv (2025)</p><p>The orientation and rotation of a synchronous satellite can be referred to both its Laplace plane and the ICRF equatorial plane, in terms of Euler angles or spin axis Cartesian coordinates and Earth equatorial coordinates, respectively. We computed second-order analytical expressions to make the transformation between the two systems and applied them to the Galilean satellites (Io, Europa, Ganymede, and Callisto). If one term of the spin axis Cartesian coordinates series is dominant, trigonometric series can be generated for the inertial and orbital obliquities, node longitude and offset with respect to the Cassini plane. Since the transformation does not require any fit of amplitudes and frequencies on numerical series, the physical meaning of the frequencies is preserved from the input series and the amplitudes can be directly related to the geophysical parameters of interest. We provide tables for the coordinates and angles&rsquo; series assuming that the satellites are entirely solid, and considering two different orbital theories. The possible amplitude ranges for the main terms are also examined in the case where a liquid layer is assumed in the interior model. We use our transformation method to propose an updated IAU WG solution which would result in an improvement with respect to zero obliquity models used so far. This method will also be useful for the interpretation of future Earth-based radar observations or JUICE data.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05935v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05935v1">üìÑ Download PDF</a></p><hr><h3 id=consequences-of-kernel-regularity-for-bandit-optimizationhttpsarxivorgabs251205957v1><a href=https://arxiv.org/abs/2512.05957v1>Consequences of Kernel Regularity for Bandit Optimization</a><a hidden class=anchor aria-hidden=true href=#consequences-of-kernel-regularity-for-bandit-optimizationhttpsarxivorgabs251205957v1>#</a></h3><p><strong>Authors:</strong> Madison Lee, Tara Javidi
<strong>Venue:</strong> arXiv (2025)</p><p>In this work we investigate the relationship between kernel regularity and algorithmic performance in the bandit optimization of RKHS functions. While reproducing kernel Hilbert space (RKHS) methods traditionally rely on global kernel regressors, it is also common to use a smoothness-based approach that exploits local approximations. We show that these perspectives are deeply connected through the spectral properties of isotropic kernels. In particular, we characterize the Fourier spectra of the Mat√©rn, square-exponential, rational-quadratic, $Œ≥$-exponential, piecewise-polynomial, and Dirichlet kernels, and show that the decay rate determines asymptotic regret from both viewpoints. For kernelized bandit algorithms, spectral decay yields upper bounds on the maximum information gain, governing worst-case regret, while for smoothness-based methods, the same decay rates establish H√∂lder space embeddings and Besov space norm-equivalences, enabling local continuity analysis. These connections show that kernel-based and locally adaptive algorithms can be analyzed within a unified framework. This allows us to derive explicit regret bounds for each kernel family, obtaining novel results in several cases and providing improved analysis for others. Furthermore, we analyze LP-GP-UCB, an algorithm that combines both approaches, augmenting global Gaussian process surrogates with local polynomial estimators. While the hybrid approach does not uniformly dominate specialized methods, it achieves order-optimality across multiple kernel families.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05957v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05957v1">üìÑ Download PDF</a></p><hr><h2 id=-neuroscience>üîç neuroscience<a hidden class=anchor aria-hidden=true href=#-neuroscience>#</a></h2><h3 id=zoom-in-click-out-unlocking-and-evaluating-the-potential-of-zooming-for-gui-groundinghttpsarxivorgabs251205941v1><a href=https://arxiv.org/abs/2512.05941v1>Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding</a><a hidden class=anchor aria-hidden=true href=#zoom-in-click-out-unlocking-and-evaluating-the-potential-of-zooming-for-gui-groundinghttpsarxivorgabs251205941v1>#</a></h3><p><strong>Authors:</strong> Zhiyuan Jiang, Shenghao Xie, Wenyi Li, Wenqiang Zu, Peihang Li, Jiahao Qiu, Siqi Pei, Lei Ma, Tiejun Huang, Mengdi Wang, Shilong Liu
<strong>Venue:</strong> arXiv (2025)</p><p>Grounding is a fundamental capability for building graphical user interface (GUI) agents. Although existing approaches rely on large-scale bounding box supervision, they still face various challenges, such as cross-platform generalization, complex layout analysis, and fine-grained element localization. In this paper, we investigate zoom as a strong yet underexplored prior for GUI grounding, and propose a training-free method, ZoomClick. By characterizing four key properties of zoom (i.e., pre-zoom, depth, shrink size, minimal crop size), we unlock its full capabilities for dynamic spatial focusing and adaptive context switching. Experiments demonstrate that our method significantly boosts the performance of both general vision-language and specialized GUI grounding models, achieving state-of-the-art results on several mainstream benchmarks; for example, UI-Venus-72B attains a 73.1% success rate on ScreenSpot-Pro. Furthermore, we present GUIZoom-Bench, a benchmark for evaluating model adaptability to zoom, aiming to inspire future research on improving zoom for further training and test-time scaling in GUI grounding tasks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05941v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05941v1">üìÑ Download PDF</a></p><hr><h3 id=physically-based-simulation-of-automotive-lidarhttpsarxivorgabs251205932v1><a href=https://arxiv.org/abs/2512.05932v1>Physically-Based Simulation of Automotive LiDAR</a><a hidden class=anchor aria-hidden=true href=#physically-based-simulation-of-automotive-lidarhttpsarxivorgabs251205932v1>#</a></h3><p><strong>Authors:</strong> L. Dudzik, M. Roschani, A. Sielemann, K. Trampert, J. Ziehn, J. Beyerer, C. Neumann
<strong>Venue:</strong> arXiv (2025)</p><p>We present an analytic model for simulating automotive time-of-flight (ToF) LiDAR that includes blooming, echo pulse width, and ambient light, along with steps to determine model parameters systematically through optical laboratory measurements. The model uses physically based rendering (PBR) in the near-infrared domain. It assumes single-bounce reflections and retroreflections over rasterized rendered images from shading or ray tracing, including light emitted from the sensor as well as stray light from other, non-correlated sources such as sunlight. Beams from the sensor and sensitivity of the receiving diodes are modeled with flexible beam steering patterns and with non-vanishing diameter.
Different (all non-real time) computational approaches can be chosen based on system properties, computing capabilities, and desired output properties.
Model parameters include system-specific properties, namely the physical spread of the LiDAR beam, combined with the sensitivity of the receiving diode; the intensity of the emitted light; the conversion between the intensity of reflected light and the echo pulse width; and scenario parameters such as environment lighting, positioning, and surface properties of the target(s) in the relevant infrared domain. System-specific properties of the model are determined from laboratory measurements of the photometric luminance on different target surfaces aligned with a goniometer at 0.01¬∞ resolution, which marks the best available resolution for measuring the beam pattern.
The approach is calibrated for and tested on two automotive LiDAR systems, the Valeo Scala Gen. 2 and the Blickfeld Cube 1. Both systems differ notably in their properties and available interfaces, but the relevant model parameters could be extracted successfully.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05932v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05932v1">üìÑ Download PDF</a></p><hr><h2 id=-data_resources>üîç data_resources<a hidden class=anchor aria-hidden=true href=#-data_resources>#</a></h2><h3 id=using-large-language-models-to-create-personalized-networks-from-therapy-sessionshttpsarxivorgabs251205836v1><a href=https://arxiv.org/abs/2512.05836v1>Using Large Language Models to Create Personalized Networks From Therapy Sessions</a><a hidden class=anchor aria-hidden=true href=#using-large-language-models-to-create-personalized-networks-from-therapy-sessionshttpsarxivorgabs251205836v1>#</a></h3><p><strong>Authors:</strong> Clarissa W. Ong, Hiba Arnaout, Kate Sheehan, Estella Fox, Eugen Owtscharow, Iryna Gurevych
<strong>Venue:</strong> arXiv (2025)</p><p>Recent advances in psychotherapy have focused on treatment personalization, such as by selecting treatment modules based on personalized networks. However, estimating personalized networks typically requires intensive longitudinal data, which is not always feasible. A solution to facilitate scalability of network-driven treatment personalization is leveraging LLMs. In this study, we present an end-to-end pipeline for automatically generating client networks from 77 therapy transcripts to support case conceptualization and treatment planning. We annotated 3364 psychological processes and their corresponding dimensions in therapy transcripts. Using these data, we applied in-context learning to jointly identify psychological processes and their dimensions. The method achieved high performance even with a few training examples. To organize the processes into networks, we introduced a two-step method that grouped them into clinically meaningful clusters. We then generated explanation-augmented relationships between clusters. Experts found that networks produced by our multi-step approach outperformed those built with direct prompting for clinical utility and interpretability, with up to 90% preferring our approach. In addition, the networks were rated favorably by experts, with scores for clinical relevance, novelty, and usefulness ranging from 72-75%. Our findings provide a proof of concept for using LLMs to create clinically relevant networks from therapy transcripts. Advantages of our approach include bottom-up case conceptualization from client utterances in therapy sessions and identification of latent themes. Networks generated from our pipeline may be used in clinical settings and supervision and training. Future research should examine whether these networks improve treatment outcomes relative to other methods of treatment personalization, including statistically estimated networks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05836v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05836v1">üìÑ Download PDF</a></p><hr><h2 id=-emotion_language>üîç emotion_language<a hidden class=anchor aria-hidden=true href=#-emotion_language>#</a></h2><h3 id=measuring-the-effect-of-background-on-classification-and-feature-importance-in-deep-learning-for-av-perceptionhttpsarxivorgabs251205937v1><a href=https://arxiv.org/abs/2512.05937v1>Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception</a><a hidden class=anchor aria-hidden=true href=#measuring-the-effect-of-background-on-classification-and-feature-importance-in-deep-learning-for-av-perceptionhttpsarxivorgabs251205937v1>#</a></h3><p><strong>Authors:</strong> Anne Sielemann, Valentin Barner, Stefan Wolf, Masoud Roschani, Jens Ziehn, Juergen Beyerer
<strong>Venue:</strong> arXiv (2025)</p><p>Common approaches to explainable AI (XAI) for deep learning focus on analyzing the importance of input features on the classification task in a given model: saliency methods like SHAP and GradCAM are used to measure the impact of spatial regions of the input image on the classification result. Combined with ground truth information about the location of the object in the input image (e.g., a binary mask), it is determined whether object pixels had a high impact on the classification result, or whether the classification focused on background pixels. The former is considered to be a sign of a healthy classifier, whereas the latter is assumed to suggest overfitting on spurious correlations. A major challenge, however, is that these intuitive interpretations are difficult to test quantitatively, and hence the output of such explanations lacks an explanation itself. One particular reason is that correlations in real-world data are difficult to avoid, and whether they are spurious or legitimate is debatable. Synthetic data in turn can facilitate to actively enable or disable correlations where desired but often lack a sufficient quantification of realism and stochastic properties. [&mldr;] Therefore, we systematically generate six synthetic datasets for the task of traffic sign recognition, which differ only in their degree of camera variation and background correlation [&mldr;] to quantify the isolated influence of background correlation, different levels of camera variation, and considered traffic sign shapes on the classification performance, as well as background feature importance. [&mldr;] Results include a quantification of when and how much background features gain importance to support the classification task based on changes in the training domain [&mldr;].
Download: synset.de/datasets/synset-signset-ger/background-effect</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05937v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05937v1">üìÑ Download PDF</a></p><hr><h3 id=minimal-two-band-model-and-experimental-proposals-to-distinguish-pairing-mechanisms-of-the-high-t_c-superconductor-la_3ni_2o_7httpsarxivorgabs251205956v1><a href=https://arxiv.org/abs/2512.05956v1>Minimal two band model and experimental proposals to distinguish pairing mechanisms of the high-T$_c$ superconductor La$_3$Ni$_2$O$_7$</a><a hidden class=anchor aria-hidden=true href=#minimal-two-band-model-and-experimental-proposals-to-distinguish-pairing-mechanisms-of-the-high-t_c-superconductor-la_3ni_2o_7httpsarxivorgabs251205956v1>#</a></h3><p><strong>Authors:</strong> Zheng-Duo Fan, Ashvin Vishwanath
<strong>Venue:</strong> arXiv (2025)</p><p>The discovery of high-T$_c$ superconductivity in La$_3$Ni$_2$O$_7$ has opened the door to a new route to high temperature superconductivity, distinct from that in cuprates and iron-based materials. Yet, despite intense recent activity, we lack experimentally testable protocols for distinguishing between different pairing scenarios. In this Letter, we construct a minimal two-band model that reproduces the Fermi-surface topology observed in recent ARPES measurements and DFT calculations, and we analyze superconductivity arising from two distinct pairing mechanisms. We show that these mechanisms yield sharply different responses to an applied perpendicular electric field. Thus, La$_3$Ni$_2$O$_7$ offers the unique opportunity to cleanly distinguish between different pairing scenarios. Finally, we propose three concrete experimental proposals designed to distinguish these scenarios and thereby identify the pairing mechanism most relevant to the real material.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05956v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05956v1">üìÑ Download PDF</a></p><hr><h3 id=world-models-that-know-when-they-dont-know-controllable-video-generation-with-calibrated-uncertaintyhttpsarxivorgabs251205927v1><a href=https://arxiv.org/abs/2512.05927v1>World Models That Know When They Don&rsquo;t Know: Controllable Video Generation with Calibrated Uncertainty</a><a hidden class=anchor aria-hidden=true href=#world-models-that-know-when-they-dont-know-controllable-video-generation-with-calibrated-uncertaintyhttpsarxivorgabs251205927v1>#</a></h3><p><strong>Authors:</strong> Zhiting Mei, Tenny Yin, Micah Baker, Ola Shorinwa, Anirudha Majumdar
<strong>Venue:</strong> arXiv (2025)</p><p>Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model&rsquo;s uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05927v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05927v1">üìÑ Download PDF</a></p><hr><h3 id=to-err-is-human-systematic-quantification-of-errors-in-published-ai-papers-via-llm-analysishttpsarxivorgabs251205925v1><a href=https://arxiv.org/abs/2512.05925v1>To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis</a><a hidden class=anchor aria-hidden=true href=#to-err-is-human-systematic-quantification-of-errors-in-published-ai-papers-via-llm-analysishttpsarxivorgabs251205925v1>#</a></h3><p><strong>Authors:</strong> Federico Bianchi, Yongchan Kwon, Zachary Izzo, Linjun Zhang, James Zou
<strong>Venue:</strong> arXiv (2025)</p><p>How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05925v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05925v1">üìÑ Download PDF</a></p><hr><h3 id=impugan-learning-conditional-generative-models-for-robust-data-imputationhttpsarxivorgabs251205950v1><a href=https://arxiv.org/abs/2512.05950v1>Impugan: Learning Conditional Generative Models for Robust Data Imputation</a><a hidden class=anchor aria-hidden=true href=#impugan-learning-conditional-generative-models-for-robust-data-imputationhttpsarxivorgabs251205950v1>#</a></h3><p><strong>Authors:</strong> Zalish Mahmud, Anantaa Kotal, Aritran Piplai
<strong>Venue:</strong> arXiv (2025)</p><p>Incomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82% lower Earth Mover&rsquo;s Distance (EMD) and 70% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: github.com/zalishmahmud/impuganBigData2025</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05950v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05950v1">üìÑ Download PDF</a></p><hr><h3 id=invariant-price-of-anarchy-a-metric-for-welfarist-traffic-controlhttpsarxivorgabs251205843v1><a href=https://arxiv.org/abs/2512.05843v1>Invariant Price of Anarchy: a Metric for Welfarist Traffic Control</a><a hidden class=anchor aria-hidden=true href=#invariant-price-of-anarchy-a-metric-for-welfarist-traffic-controlhttpsarxivorgabs251205843v1>#</a></h3><p><strong>Authors:</strong> Ilia Shilov, Mingjia He, Heinrich H. Nax, Emilio Frazzoli, Gioele Zardini, Saverio Bolognani
<strong>Venue:</strong> arXiv (2025)</p><p>The Price of Anarchy (PoA) is a standard metric for quantifying inefficiency in socio-technical systems, widely used to guide policies like traffic tolling. Conventional PoA analysis relies on exact numerical costs. However, in many settings, costs represent agents&rsquo; preferences and may be defined only up to possibly arbitrary scaling and shifting, representing informational and modeling ambiguities. We observe that while such transformations preserve equilibrium and optimal outcomes, they change the PoA value. To resolve this issue, we rely on results from Social Choice Theory and define the Invariant PoA. By connecting admissible transformations to degrees of comparability of agents&rsquo; costs, we derive the specific social welfare functions which ensure that efficiency evaluations do not depend on arbitrary rescalings or translations of individual costs. Case studies on a toy example and the Zurich network demonstrate that identical tolling strategies can lead to substantially different efficiency estimates depending on the assumed comparability. Our framework thus demonstrates that explicit axiomatic foundations are necessary in order to define efficiency metrics and to appropriately guide policy in large-scale infrastructure design robustly and effectively.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05843v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05843v1">üìÑ Download PDF</a></p><hr><h3 id=towards-agent-based-model-informed-neural-networkshttpsarxivorgabs251205764v1><a href=https://arxiv.org/abs/2512.05764v1>Towards agent-based-model informed neural networks</a><a hidden class=anchor aria-hidden=true href=#towards-agent-based-model-informed-neural-networkshttpsarxivorgabs251205764v1>#</a></h3><p><strong>Authors:</strong> Nino Antulov-Fantulin
<strong>Venue:</strong> arXiv (2025)</p><p>In this article, we present a framework for designing neural networks that remain consistent with the underlying principles of agent-based models. We begin by highlighting the limitations of standard neural differential equations in modeling complex systems, where physical invariants (like energy) are often absent but other constraints (like mass conservation, network locality, bounded rationality) must be enforced. To address this, we introduce Agent-Based-Model informed Neural Networks(ABM-NNs), which leverage restricted graph neural networks and hierarchical decomposition to learn interpretable, structure-preserving dynamics. We validate the framework across three case studies of increasing complexity: (i) a generalized Generalized Lotka&ndash;Volterra system, where we recover ground-truth parameters from short trajectories in presence of interventions; (ii) a graph-based SIR contagion model, where our method outperforms state-of-the-art graph learning baselines (GCN, GraphSAGE, Graph Transformer) in out-of-sample forecasting and noise robustness; and (iii) a real-world macroeconomic model of the ten largest economies, where we learn coupled GDP dynamics from empirical data and demonstrate gradient-based counterfactual analysis for policy interventions.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05764v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05764v1">üìÑ Download PDF</a></p><hr></div><footer class=post-footer><ul class=post-tags></ul></footer><div><div class=pagination__title><span class=pagination__title-h style=font-size:20px>Comments</span><hr></div><div id=tcomment></div><script src=https://cdn.staticfile.org/twikoo/1.6.39/twikoo.all.min.js></script><script>twikoo.init({envId:"https://mangodb-theta.vercel.app/",el:"#tcomment",lang:"en-US",path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://garyforreal.me/en/>Gary's House</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>Views: <span id=busuanzi_value_site_pv></span>
</span><span id=busuanzi_container_site_uv>Visitors: <span id=busuanzi_value_site_uv></span></span></div></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>