<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Weekly Paper Notes - 2026-01-11 | Gary's House</title>
<meta name=keywords content><meta name=description content="Weekly Paper Notes
üîç multilingual
Code-Mix Sentiment Analysis on Hinglish Tweets
Authors: Aashi Garg, Aneshya Das, Arshi Arya, Anushka Goyal, Aditi
Venue: arXiv (2026)
The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish&ndash;a hybrid of Hindi and English&ndash;used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments."><meta name=author content="Gary"><link rel=canonical href=https://garyforreal.me/en/posts/paper/paper-2026-01-11-weekly/><meta name=google-site-verification content="XYZabc"><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><link crossorigin=anonymous href=/assets/css/stylesheet.7e5251d8716d933fafcf3df70d7ecd02729661d6d232fc5dd0b8a85ef75e3409.css integrity="sha256-flJR2HFtkz+vzz33DX7NAnKWYdbSMvxd0LioXvdeNAk=" rel="preload stylesheet" as=style><link rel=icon href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=16x16 href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=32x32 href=https://garyforreal.me/img/Q.jpg><link rel=apple-touch-icon href=https://garyforreal.me/Q.jpg><link rel=mask-icon href=https://garyforreal.me/Q.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://garyforreal.me/zh/posts/paper/paper-2026-01-11-weekly/><link rel=alternate hreflang=en href=https://garyforreal.me/en/posts/paper/paper-2026-01-11-weekly/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@200..900&display=swap" rel=stylesheet><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous referrerpolicy=no-referrer></script><meta property="og:title" content="Weekly Paper Notes - 2026-01-11"><meta property="og:description" content="Weekly Paper Notes
üîç multilingual
Code-Mix Sentiment Analysis on Hinglish Tweets
Authors: Aashi Garg, Aneshya Das, Arshi Arya, Anushka Goyal, Aditi
Venue: arXiv (2026)
The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish&ndash;a hybrid of Hindi and English&ndash;used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments."><meta property="og:type" content="article"><meta property="og:url" content="https://garyforreal.me/en/posts/paper/paper-2026-01-11-weekly/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-11T15:25:07+00:00"><meta property="article:modified_time" content="2026-01-11T15:25:07+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Weekly Paper Notes - 2026-01-11"><meta name=twitter:description content="Weekly Paper Notes
üîç multilingual
Code-Mix Sentiment Analysis on Hinglish Tweets
Authors: Aashi Garg, Aneshya Das, Arshi Arya, Anushka Goyal, Aditi
Venue: arXiv (2026)
The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish&ndash;a hybrid of Hindi and English&ndash;used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://garyforreal.me/en/posts/"},{"@type":"ListItem","position":2,"name":"Paper","item":"https://garyforreal.me/en/posts/paper/"},{"@type":"ListItem","position":3,"name":"Weekly Paper Notes - 2026-01-11","item":"https://garyforreal.me/en/posts/paper/paper-2026-01-11-weekly/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Weekly Paper Notes - 2026-01-11","name":"Weekly Paper Notes - 2026-01-11","description":"Weekly Paper Notes üîç multilingual Code-Mix Sentiment Analysis on Hinglish Tweets Authors: Aashi Garg, Aneshya Das, Arshi Arya, Anushka Goyal, Aditi Venue: arXiv (2026)\nThe effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish\u0026ndash;a hybrid of Hindi and English\u0026ndash;used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments.\n","keywords":[],"articleBody":"Weekly Paper Notes üîç multilingual Code-Mix Sentiment Analysis on Hinglish Tweets Authors: Aashi Garg, Aneshya Das, Arshi Arya, Anushka Goyal, Aditi Venue: arXiv (2026)\nThe effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish‚Äìa hybrid of Hindi and English‚Äìused widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments.\nüìÑ Download PDF\nCurricuLLM: Designing Personalized and Workforce-Aligned Cybersecurity Curricula Using Fine-Tuned LLMs Authors: Arthur Nijdam, Harri K√§hk√∂nen, Valtteri Niemi, Paul Stankovski Wagner, Sara Ramezanian Venue: arXiv (2026)\nThe cybersecurity landscape is constantly evolving, driven by increased digitalization and new cybersecurity threats. Cybersecurity programs often fail to equip graduates with skills demanded by the workforce, particularly concerning recent developments in cybersecurity, as curriculum design is costly and labor-intensive. To address this misalignment, we present a novel Large Language Model (LLM)-based framework for automated design and analysis of cybersecurity curricula, called CurricuLLM. Our approach provides three key contributions: (1) automation of personalized curriculum design, (2) a data-driven pipeline aligned with industry demands, and (3) a comprehensive methodology for leveraging fine-tuned LLMs in curriculum development. CurricuLLM utilizes a two-tier approach consisting of PreprocessLM, which standardizes input data, and ClassifyLM, which assigns course content to nine Knowledge Areas in cybersecurity. We systematically evaluated multiple Natural Language Processing (NLP) architectures and fine-tuning strategies, ultimately selecting the Bidirectional Encoder Representations from Transformers (BERT) model as ClassifyLM, fine-tuned on foundational cybersecurity concepts and workforce competencies. We are the first to validate our method with human experts who analyzed real-world cybersecurity curricula and frameworks, motivating that CurricuLLM is an efficient solution to replace labor-intensive curriculum analysis. Moreover, once course content has been classified, it can be integrated with established cybersecurity role-based weights, enabling alignment of the educational program with specific job roles, workforce categories, or general market needs. This lays the foundation for personalized, workforce-aligned cybersecurity curricula that prepare students for the evolving demands in cybersecurity.\nüìÑ Download PDF\nCan AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences Authors: Arkadiusz Modzelewski, Pawe≈Ç Golik, Anna Ko≈Ços, Giovanni Da San Martino Venue: arXiv (2026)\nLarge Language Models (LLMs) can generate highly persuasive text, raising concerns about their misuse for propaganda, manipulation, and other harmful purposes. This leads us to our central question: Is LLM-generated persuasion more difficult to automatically detect than human-written persuasion? To address this, we categorize controllable generation approaches for producing persuasive content with LLMs and introduce Persuaficial, a high-quality multilingual benchmark covering six languages: English, German, Polish, Italian, French and Russian. Using this benchmark, we conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts. We find that although overtly persuasive LLM-generated texts can be easier to detect than human-written ones, subtle LLM-generated persuasion consistently degrades automatic detection performance. Beyond detection performance, we provide the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts, offering insights that may guide the development of more interpretable and robust detection tools.\nüìÑ Download PDF\nScaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform Authors: Suyash Mishra, Qiang Li, Srikanth Patil, Satyanarayan Pati, Baddu Narendra Venue: arXiv (2026)\nVision Language Models (VLMs) have shown strong performance on multimodal reasoning tasks, yet most evaluations focus on short videos and assume unconstrained computational resources. In industrial settings such as pharmaceutical content understanding, practitioners must process long-form videos under strict GPU, latency, and cost constraints, where many existing approaches fail to scale. In this work, we present an industrial GenAI framework that processes over 200,000 PDFs, 25,326 videos across eight formats (e.g., MP4, M4V, etc.), and 888 multilingual audio files in more than 20 languages. Our study makes three contributions: (i) an industrial large-scale architecture for multimodal reasoning in pharmaceutical domains; (ii) empirical analysis of over 40 VLMs on two leading benchmarks (Video-MME and MMBench) and proprietary dataset of 25,326 videos across 14 disease areas; and (iii) four findings relevant to long-form video reasoning: the role of multimodality, attention mechanism trade-offs, temporal reasoning limits, and challenges of video splitting under GPU constraints. Results show 3-8 times efficiency gains with SDPA attention on commodity GPUs, multimodality improving up to 8/12 task domains (especially length-dependent tasks), and clear bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs. Rather than proposing a new ‚ÄúA+B‚Äù model, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, and provide actionable guidance for both researchers and practitioners designing scalable multimodal systems for long-form video understanding in industrial domains.\nüìÑ Download PDF\nLANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal Authors: Dongjun Kim, Jeongho Yoon, Chanjun Park, Heuiseok Lim Venue: arXiv (2026)\nDense retrieval in multilingual settings often searches over mixed-language collections, yet multilingual embeddings encode language identity alongside semantics. This language signal can inflate similarity for same-language pairs and crowd out relevant evidence written in other languages. We propose LANGSAE EDITING, a post-hoc sparse autoencoder trained on pooled embeddings that enables controllable removal of language-identity signal directly in vector space. The method identifies language-associated latent units using cross-language activation statistics, suppresses these units at inference time, and reconstructs embeddings in the original dimensionality, making it compatible with existing vector databases without retraining the base encoder or re-encoding raw text. Experiments across multiple languages show consistent improvements in ranking quality and cross-language coverage, with especially strong gains for script-distinct languages.\nüìÑ Download PDF\nQwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking Authors: Mingxin Li, Yanzhao Zhang, Dingkun Long, Keqin Chen, Sibo Song, Shuai Bai, Zhibo Yang, Pengjun Xie, An Yang, Dayiheng Liu, Jingren Zhou, Junyang Lin Venue: arXiv (2026)\nIn this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in $\\textbf{2B}$ and $\\textbf{8B}$ parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of $\\textbf{77.8}$ on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.\nüìÑ Download PDF\nBanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation Authors: Amit Bin Tariqul, A N M Zahid Hossain Milkan, Sahab-Al-Chowdhury, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan Venue: arXiv (2026)\nAs large language models (LLMs) are increasingly deployed for text generation, watermarking has become essential for authorship attribution, intellectual property protection, and misuse detection. While existing watermarking methods perform well in high-resource languages, their robustness in low-resource languages remains underexplored. This work presents the first systematic evaluation of state-of-the-art text watermarking methods: KGW, Exponential Sampling (EXP), and Waterfall, for Bangla LLM text generation under cross-lingual round-trip translation (RTT) attacks. Under benign conditions, KGW and EXP achieve high detection accuracy (\u003e88%) with negligible perplexity and ROUGE degradation. However, RTT causes detection accuracy to collapse below RTT causes detection accuracy to collapse to 9-13%, indicating a fundamental failure of token-level watermarking. To address this, we propose a layered watermarking strategy that combines embedding-time and post-generation watermarks. Experimental results show that layered watermarking improves post-RTT detection accuracy by 25-35%, achieving 40-50% accuracy, representing a 3$\\times$ to 4$\\times$ relative improvement over single-layer methods, at the cost of controlled semantic degradation. Our findings quantify the robustness-quality trade-off in multilingual watermarking and establish layered watermarking as a practical, training-free solution for low-resource languages such as Bangla. Our code and data will be made public.\nüìÑ Download PDF\nThe Overlooked Role of Graded Relevance Thresholds in Multilingual Dense Retrieval Authors: Tomer Wullach, Ori Shapira, Amir DN Cohen Venue: arXiv (2026)\nDense retrieval models are typically fine-tuned with contrastive learning objectives that require binary relevance judgments, even though relevance is inherently graded. We analyze how graded relevance scores and the threshold used to convert them into binary labels affect multilingual dense retrieval. Using a multilingual dataset with LLM-annotated relevance scores, we examine monolingual, multilingual mixture, and cross-lingual retrieval scenarios. Our findings show that the optimal threshold varies systematically across languages and tasks, often reflecting differences in resource level. A well-chosen threshold can improve effectiveness, reduce the amount of fine-tuning data required, and mitigate annotation noise, whereas a poorly chosen one can degrade performance. We argue that graded relevance is a valuable but underutilized signal for dense retrieval, and that threshold calibration should be treated as a principled component of the fine-tuning pipeline.\nüìÑ Download PDF\nDialect Matters: Cross-Lingual ASR Transfer for Low-Resource Indic Language Varieties Authors: Akriti Dhasmana, Aarohi Srivastava, David Chiang Venue: arXiv (2026)\nWe conduct an empirical study of cross-lingual transfer using spontaneous, noisy, and code-mixed speech across a wide range of Indic dialects and language varieties. Our results indicate that although ASR performance is generally improved with reduced phylogenetic distance between languages, this factor alone does not fully explain performance in dialectal settings. Often, fine-tuning on smaller amounts of dialectal data yields performance comparable to fine-tuning on larger amounts of phylogenetically-related, high-resource standardized languages. We also present a case study on Garhwali, a low-resource Pahari language variety, and evaluate multiple contemporary ASR models. Finally, we analyze transcription errors to examine bias toward pre-training languages, providing additional insight into challenges faced by ASR systems on dialectal and non-standardized speech.\nüìÑ Download PDF\nCSSG: Measuring Code Similarity with Semantic Graphs Authors: Jingwen Xu, Yiyang Lu, Changze Lv, Zisu Huang, Zhengkang Guo, Zhengyuan Wang, Muzhao Tian, Xuanjing Huang, Xiaoqing Zheng Venue: arXiv (2026)\nExisting code similarity metrics, such as BLEU, CodeBLEU, and TSED, largely rely on surface-level string overlap or abstract syntax tree structures, and often fail to capture deeper semantic relationships between programs.We propose CSSG (Code Similarity using Semantic Graphs), a novel metric that leverages program dependence graphs to explicitly model control dependencies and variable interactions, providing a semantics-aware representation of code.Experiments on the CodeContests+ dataset show that CSSG consistently outperforms existing metrics in distinguishing more similar code from less similar code under both monolingual and cross-lingual settings, demonstrating that dependency-aware graph representations offer a more effective alternative to surface-level or syntax-based similarity measures.\nüìÑ Download PDF\nAnalyzing and Improving Cross-lingual Knowledge Transfer for Machine Translation Authors: David Stap Venue: arXiv (2026)\nMultilingual machine translation systems aim to make knowledge accessible across languages, yet learning effective cross-lingual representations remains challenging. These challenges are especially pronounced for low-resource languages, where limited parallel data constrains generalization and transfer. Understanding how multilingual models share knowledge across languages requires examining the interaction between representations, data availability, and training strategies. In this thesis, we study cross-lingual knowledge transfer in neural models and develop methods to improve robustness and generalization in multilingual settings, using machine translation as a central testbed. We analyze how similarity between languages influences transfer, how retrieval and auxiliary supervision can strengthen low-resource translation, and how fine-tuning on parallel data can introduce unintended trade-offs in large language models. We further examine the role of language diversity during training and show that increasing translation coverage improves generalization and reduces off-target behavior. Together, this work highlights how modeling choices and data composition shape multilingual learning and offers insights toward more inclusive and resilient multilingual NLP systems.\nüìÑ Download PDF\nIndexTTS 2.5 Technical Report Authors: Yunpei Li, Xun Zhou, Jinchao Wang, Lu Wang, Yong Wu, Siyi Zhou, Yiquan Zhou, Jingchen Shu Venue: arXiv (2026)\nIn prior work, we introduced IndexTTS 2, a zero-shot neural text-to-speech foundation model comprising two core components: a transformer-based Text-to-Semantic (T2S) module and a non-autoregressive Semantic-to-Mel (S2M) module, which together enable faithful emotion replication and establish the first autoregressive duration-controllable generative paradigm. Building upon this, we present IndexTTS 2.5, which significantly enhances multilingual coverage, inference speed, and overall synthesis quality through four key improvements: 1) Semantic Codec Compression: we reduce the semantic codec frame rate from 50 Hz to 25 Hz, halving sequence length and substantially lowering both training and inference costs; 2) Architectural Upgrade: we replace the U-DiT-based backbone of the S2M module with a more efficient Zipformer-based modeling architecture, achieving notable parameter reduction and faster mel-spectrogram generation; 3) Multilingual Extension: We propose three explicit cross-lingual modeling strategies, boundary-aware alignment, token-level concatenation, and instruction-guided generation, establishing practical design principles for zero-shot multilingual emotional TTS that supports Chinese, English, Japanese, and Spanish, and enables robust emotion transfer even without target-language emotional training data; 4) Reinforcement Learning Optimization: we apply GRPO in post-training of the T2S module, improving pronunciation accuracy and natrualness. Experiments show that IndexTTS 2.5 not only supports broader language coverage but also replicates emotional prosody in unseen languages under the same zero-shot setting. IndexTTS 2.5 achieves a 2.28 times improvement in RTF while maintaining comparable WER and speaker similarity to IndexTTS 2.\nüìÑ Download PDF\nPrior-Informed Zeroth-Order Optimization with Adaptive Direction Alignment for Memory-Efficient LLM Fine-Tuning Authors: Feihu Jin, Shipeng Cen, Ying Tan Venue: arXiv (2026)\nFine-tuning large language models (LLMs) has achieved remarkable success across various NLP tasks, but the substantial memory overhead during backpropagation remains a critical bottleneck, especially as model scales grow. Zeroth-order (ZO) optimization alleviates this issue by estimating gradients through forward passes and Gaussian sampling, avoiding the need for backpropagation. However, conventional ZO methods suffer from high variance in gradient estimation due to their reliance on random perturbations, leading to slow convergence and suboptimal performance. We propose a simple plug-and-play method that incorporates prior-informed perturbations to refine gradient estimation. Our method dynamically computes a guiding vector from Gaussian samples, which directs perturbations toward more informative directions, significantly accelerating convergence compared to standard ZO approaches. We further investigate a greedy perturbation strategy to explore the impact of prior knowledge on gradient estimation. Theoretically, we prove that our gradient estimator achieves stronger alignment with the true gradient direction, enhancing optimization efficiency. Extensive experiments across LLMs of varying scales and architectures demonstrate that our proposed method could seamlessly integrate into existing optimization methods, delivering faster convergence and superior performance. Notably, on the OPT-13B model, our method outperforms traditional ZO optimization across all 11 benchmark tasks and surpasses gradient-based baselines on 9 out of 11 tasks, establishing a robust balance between efficiency and accuracy.\nüìÑ Download PDF\nAligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization Authors: Mizanur Rahman, Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Shafiq Joty, Enamul Hoque Venue: arXiv (2026)\nText-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.\nüìÑ Download PDF\nIdentifying Good and Bad Neurons for Task-Level Controllable LLMs Authors: Wenjie Li, Guansong Pang, Hezhe Qiao, Debin Gao, David Lo Venue: arXiv (2026)\nLarge Language Models have demonstrated remarkable capabilities on multiple-choice question answering benchmarks, but the complex mechanisms underlying their large-scale neurons remain opaque, posing significant challenges for understanding and steering LLMs. While recent studies made progress on identifying responsible neurons for certain abilities, these ability-specific methods are infeasible for task-focused scenarios requiring coordinated use of multiple abilities. Moreover, these approaches focus only on supportive neurons that correlate positively with task completion, while neglecting neurons with other roles-such as inhibitive roles-and misled neuron attribution due to fortuitous behaviors in LLMs (i.e., correctly answer the questions by chance rather than genuine understanding). To address these challenges, we propose NeuronLLM, a novel task-level LLM understanding framework that adopts the biological principle of functional antagonism for LLM neuron identification. The key insight is that task performance is jointly determined by neurons with two opposing roles: good neurons that facilitate task completion and bad neurons that inhibit it. NeuronLLM achieves a holistic modeling of neurons via contrastive learning of good and bad neurons, while leveraging augmented question sets to mitigate the fortuitous behaviors in LLMs. Comprehensive experiments on LLMs of different sizes and families show the superiority of NeuronLLM over existing methods in four NLP tasks, providing new insights into LLM functional organization.\nüìÑ Download PDF\nNon-Homogeneous Markov-Switching Generalized Additive Models for Location, Scale, and Shape Authors: Katharina Ammann, Timo Adam, Jan-Ole Koslik Venue: arXiv (2026)\nWe propose an extension of Markov-switching generalized additive models for location, scale, and shape (MS-GAMLSS) that allows covariates to influence not only the parameters of the state-dependent distributions but also the state transition probabilities. Traditional MS-GAMLSS, which combine distributional regression with hidden Markov models, typically assume time-homogeneous (i.e., constant) transition probabilities, thereby preventing regime shifts from responding to covariate-driven changes. Our approach overcomes this limitation by modeling the transition probabilities as smooth functions of covariates, enabling a flexible, data-driven characterization of covariate-dependent regime dynamics. Estimation is carried out within a penalized likelihood framework, where automatic smoothness selection controls model complexity and guards against overfitting. We evaluate the proposed methodology through simulations and applications to daily Lufthansa stock prices and Spanish energy prices. Our results show that incorporating macroeconomic indicators into the transition probabilities yields additional insights into market dynamics. Data and R code to reproduce the results are available online.\nüìÑ Download PDF\nOLA: Output Language Alignment in Code-Switched LLM Interactions Authors: Juhyun Oh, Haneul Yoo, Faiz Ghifari Haznitrama, Alice Oh Venue: arXiv (2026)\nCode-switching, alternating between languages within a conversation, is natural for multilingual users, yet poses fundamental challenges for large language models (LLMs). When a user code-switches in their prompt to an LLM, they typically do not specify the expected language of the LLM response, and thus LLMs must infer the output language from contextual and pragmatic cues. We find that current LLMs systematically fail to align with this expectation, responding in undesired languages even when cues are clear to humans. We introduce OLA, a benchmark to evaluate LLMs‚Äô Output Language Alignment in code-switched interactions. OLA focuses on Korean‚ÄìEnglish code-switching and spans simple intra-sentential mixing to instruction-content mismatches. Even frontier models frequently misinterpret implicit language expectation, exhibiting a bias toward non-English responses. We further show this bias generalizes beyond Korean to Chinese and Indonesian pairs. Models also show instability through mid-response switching and language intrusions. Chain-of-Thought prompting fails to resolve these errors, indicating weak pragmatic reasoning about output language. However, Code-Switching Aware DPO with minimal data (about 1K examples) substantially reduces misalignment, suggesting these failures stem from insufficient alignment rather than fundamental limitations. Our results highlight the need to align multilingual LLMs with users‚Äô implicit expectations in real-world code-switched interactions.\nüìÑ Download PDF\nThe winds of OBA hypergiants and luminous blue variables: Dynamically-consistent atmosphere models reveal multiple wind regimes Authors: Matheus Bernini-Peron, Andreas A. C. Sander, Gautham N. Sabhahit, Francisco Najarro, Varsha Ramachandran, Jorick S. Vink Venue: arXiv (2026)\nOBA hypergiants (OBAHGs) are evolved massive stars with notable wind features in their optical spectrum. Positioned at the cool edge of the line-driven wind regime, many are candidate luminous blue variables (LBVs) likely near the Eddington Limit. Although brief, this evolutionary stage deeply impacts their surroundings and subsequent evolution. We study the mechanisms behind OBAHG winds and spectra, covering the temperature range of non-eruptive LBVs. Using the PoWR atmosphere code, we compute models with an Eddington parameter Gamma_e ~ 0.4 and moderate turbulent pressure, typical for cool hypergiants, varying the effective temperature from ~12.5 to ~38.0 kK at solar metallicity. Our models show a complex temperature-dependent mass-loss pattern, with regions of higher/lower rates linked to two wind solutions: ‚Äúdense‚Äù and ‚Äúairy.‚Äù Spectra of known OBAHGs and LBVs match models from all solution regions. We find bi-stability jumps ‚Äì with sharp mass-loss increases ‚Äì at temperatures where Fe IV recombines to Fe III (and Fe III to Fe II). ‚ÄúDrops‚Äù in mass-loss also occur when the leading Fe ion changes at wind onset, signaling a switch to airy solutions under insufficient driving opacity. The resulting velocity fields also reflect these different regimes: airy solutions match the empirical terminal velocity vs temperature relation, while dense ones deviate. Turbulent pressure is crucial for wind acceleration at cooler temperatures, especially in airy regimes. We demonstrate that the bi-stability jumps exist in OBAHGs but are part of a broader complex behavior not replicated by current mass-loss recipes. Combining our and other recent results, we suggest that the switch between airy and dense solutions only occurs within a certain proximity to the Eddington Limit. Testing this requires future models with broader parameters and advanced treatments of radiatively-driven turbulence.\nüìÑ Download PDF\nAI-Native 6G Physical Layer with Cross-Module Optimization and Cooperative Control Agents Authors: Xufei Zheng, Han Xiao, Shi Jin, Zhiqin Wang, Wenqiang Tian, Wendong Liu, Jianfei Cao, Jia Shen, Zhihua Shi, Zhi Zhang, Ning Yang Venue: arXiv (2026)\nIn this article, a framework of AI-native cross-module optimized physical layer with cooperative control agents is proposed, which involves optimization across global AI/ML modules of the physical layer with innovative design of multiple enhancement mechanisms and control strategies. Specifically, it achieves simultaneous optimization across global modules of uplink AI/ML-based joint source-channel coding with modulation, and downlink AI/ML-based modulation with precoding and corresponding data detection, reducing traditional inter-module information barriers to facilitate end-to-end optimization toward global objectives. Moreover, multiple enhancement mechanisms are also proposed, including i) an AI/ML-based cross-layer modulation approach with theoretical analysis for downlink transmission that breaks the isolation of inter-layer features to expand the solution space for determining improved constellation, ii) a utility-oriented precoder construction method that shifts the role of the AI/ML-based CSI feedback decoder from recovering the original CSI to directly generating precoding matrices aiming to improve end-to-end performance, and iii) incorporating modulation into AI/ML-based CSI feedback to bypass bit-level bottlenecks that introduce quantization errors, non-differentiable gradients, and limitations in constellation solution spaces. Furthermore, AI/ML based control agents for optimized transmission schemes are proposed that leverage AI/ML to perform model switching according to channel state, thereby enabling integrated control for global throughput optimization. Finally, simulation results demonstrate the superiority of the proposed solutions in terms of BLER and throughput. These extensive simulations employ more practical assumptions that are aligned with the requirements of the 3GPP, which hopefully provides valuable insights for future standardization discussions.\nüìÑ Download PDF\nDynamically consistent analysis of Galactic WN4b stars Authors: Roel R. Lefever, Andreas A. C. Sander, Matheus Bernini-Peron, Gemma Gon√°lez-Tor√†, Wolf-Rainer Hamann, Joris Josiek, Varsha Ramachandran, Elisa C. Sch√∂sser, Helge Todt Venue: arXiv (2026)\nMany Wolf-Rayet (WR) stars have optically thick winds that cloak the hydrostatic layers of the underlying star. In these cases, traditional spectral analysis methods are plagued by degeneracies that make it difficult to constrain parameters such as the stellar radius and the deeper density and velocity structure of the atmosphere. Focussing on the regime of nitrogen-rich WN4-stars with strong emission lines, we employ hydrodynamically-consistent modelling using the PoWR-HD code branch to perform a next generation spectral analysis. The inherent coupling of the stellar and wind parameters enables us to break parameter degeneracies, constrain the wind structure, and get a mass estimate. With this information, we can draw evolutionary implications and test current mass-loss descriptions for WR stars. We selected a sample of six Galactic WN4b stars. Applying updated parallaxes from Gaia DR3 and calculating PoWR-HD models that sufficiently resemble most of their spectral appearance, we obtain new values for the stellar and wind parameters of the WN4b sample. We compare our results to previous studies employing grid models with a beta = 1 velocity structure and cross-check our derived parameters with stellar structure predictions from GENEC and FRANEC evolution tracks. For all six targets, we obtain a narrow range of stellar temperatures T140 kK, in contrast to previous grid-model analyses. We confirm the existence of WRs with luminosities as low as log L/Lsol = 5.0 and M5 Msol. All derived velocity fields include a plateau feature at ~85% of the terminal velocity. Both the distance updates and the switch to dynamically-consistent atmospheres lead to substantial parameter adjustments compared to earlier grid-based studies. A comparison of the derived mass-loss rates favours a different description for the WN4b sample than for WN2 stars analysed with the same methodology.\nüìÑ Download PDF\nAspect Extraction from E-Commerce Product and Service Reviews Authors: Valiant Lance D. Dionela, Fatima Kriselle S. Dy, Robin James M. Hombrebueno, Aaron Rae M. Nicolas, Charibeth K. Cheng, Raphael W. Gonda Venue: arXiv (2026)\nAspect Extraction (AE) is a key task in Aspect-Based Sentiment Analysis (ABSA), yet it remains difficult to apply in low-resource and code-switched contexts like Taglish, a mix of Tagalog and English commonly used in Filipino e-commerce reviews. This paper introduces a comprehensive AE pipeline designed for Taglish, combining rule-based, large language model (LLM)-based, and fine-tuning techniques to address both aspect identification and extraction. A Hierarchical Aspect Framework (HAF) is developed through multi-method topic modeling, along with a dual-mode tagging scheme for explicit and implicit aspects. For aspect identification, four distinct models are evaluated: a Rule-Based system, a Generative LLM (Gemini 2.0 Flash), and two Fine-Tuned Gemma-3 1B models trained on different datasets (Rule-Based vs. LLM-Annotated). Results indicate that the Generative LLM achieved the highest performance across all tasks (Macro F1 0.91), demonstrating superior capability in handling implicit aspects. In contrast, the fine-tuned models exhibited limited performance due to dataset imbalance and architectural capacity constraints. This work contributes a scalable and linguistically adaptive framework for enhancing ABSA in diverse, code-switched environments.\nüìÑ Download PDF\nAn Adaptive Power Division Strategy for Nonlinear Components in Rectification Authors: Zhongqi He, Liping Yan, Changjun Liu Venue: arXiv (2026)\nThis letter presents a novel adaptive power division strategy, which uses two rectifying diodes with nonlinear impedance characteristics that are configured in parallel to function optimally at their individual power levels. Through the strategic adjustment of the input admittance, the conductance of the low-power diode decreases progressively with increasing power, while the conductance of the high-power diode increases correspondingly. This conductance-based power allocation method ensures that the power is rectified consistently by the most appropriate diode, regardless of the power level, and, thus, enables efficient rectification across an extended range. This letter presents a rectifier typology to substantiate the proposed strategy. Experimental results confirm the efficiency of the adaptive power division strategy, with the rectifier showing efficiency in excess of 60% from 5 to 29.5 dBm, giving a power dynamic range of 24.5 dB.\nüìÑ Download PDF\nLaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model Authors: Zhuoyang Liu, Jiaming Liu, Hao Chen, Ziyu Guo, Chengkai Hou, Chenyang Gu, Jiale Yu, Xiangju Mi, Renrui Zhang, Zhengping Che, Jian Tang, Pheng-Ann Heng, Shanghang Zhang Venue: arXiv (2026)\nVision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST$_0$, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST$_0$ adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST$_0$ is trained with heterogeneous operation frequencies, enabling adaptive switching between reasoning and action inference rates during deployment. Across ten simulated and six real-world manipulation tasks, LaST$_0$ improves mean success rates by 8% and 13% over prior VLA methods, respectively, while achieving substantially faster inference. Project website: https://sites.google.com/view/last0\nüìÑ Download PDF\nGREx: Generalized Referring Expression Segmentation, Comprehension, and Generation Authors: Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Yu-Gang Jiang Venue: arXiv (2026)\nReferring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object. Existing datasets and methods commonly support single-target expressions only, i.e., one expression refers to one object, not considering multi-target and no-target expressions. This greatly limits the real applications of REx (RES/REC/REG). This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects. We construct the first large-scale GREx dataset gRefCOCO that contains multi-target, no-target, and single-target expressions and their corresponding images with labeled targets. GREx and gRefCOCO are designed to be backward-compatible with REx, facilitating extensive experiments to study the performance gap of the existing REx methods on GREx tasks. One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies. The proposed ReLA achieves the state-of-the-art results on the both GRES and GREC tasks. The proposed gRefCOCO dataset and method are available at https://henghuiding.github.io/GREx.\nüìÑ Download PDF\nGDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization Authors: Shih-Yang Liu, Xin Dong, Ximing Lu, Shizhe Diao, Peter Belcak, Mingjie Liu, Min-Hung Chen, Hongxu Yin, Yu-Chiang Frank Wang, Kwang-Ting Cheng, Yejin Choi, Jan Kautz, Pavlo Molchanov Venue: arXiv (2026)\nAs language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.\nüìÑ Download PDF\nRoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation Authors: Boyang Wang, Haoran Zhang, Shujie Zhang, Jinkun Hao, Mingda Jia, Qi Lv, Yucheng Mao, Zhaoyang Lyu, Jia Zeng, Xudong Xu, Jiangmiao Pang Venue: arXiv (2026)\nThe diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.\nüìÑ Download PDF\nRobust Reasoning as a Symmetry-Protected Topological Phase Authors: Ilmo Sung Venue: arXiv (2026)\nLarge language models suffer from ‚Äúhallucinations‚Äù-logical inconsistencies induced by semantic noise. We propose that current architectures operate in a ‚ÄúMetric Phase,‚Äù where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braiding, replacing fragile geometric interpolation with robust topological invariants. Empirically, we demonstrate a sharp topological phase transition: while Transformers and RNNs exhibit gapless decay, our Holonomic Network reveals a macroscopic ‚Äúmass gap,‚Äù maintaining invariant fidelity below a critical noise threshold. Furthermore, in a variable-binding task on $S_{10}$ ($3.6 \\times 10^6$ states) representing symbolic manipulation, we demonstrate holonomic generalization: the topological model maintains perfect fidelity extrapolating $100\\times$ beyond training ($L=50 \\to 5000$), consistent with a theoretically indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies indicate this protection emerges strictly from non-Abelian gauge symmetry. This provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold.\nüìÑ Download PDF\nMeasuring and Fostering Peace through Machine Learning and Artificial Intelligence Authors: P. Gilda, P. Dungarwal, A. Thongkham, E. T. Ajayi, S. Choudhary, T. M. Terol, C. Lam, J. P. Araujo, M. McFadyen-Mungalln, L. S. Liebovitch, P. T. Coleman, H. West, K. Sieck, S. Carter Venue: arXiv (2026)\nWe used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.\nüìÑ Download PDF\nGenerate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration Authors: Xingyi He, Adhitya Polavaram, Yunhao Cao, Om Deshmukh, Tianrui Wang, Xiaowei Zhou, Kuan Fang Venue: arXiv (2026)\nFunctional grasping with dexterous robotic hands is a key capability for enabling tool use and complex manipulation, yet progress has been constrained by two persistent bottlenecks: the scarcity of large-scale datasets and the absence of integrated semantic and geometric reasoning in learned models. In this work, we present CorDex, a framework that robustly learns dexterous functional grasps of novel objects from synthetic data generated from just a single human demonstration. At the core of our approach is a correspondence-based data engine that generates diverse, high-quality training data in simulation. Based on the human demonstration, our data engine generates diverse object instances of the same category, transfers the expert grasp to the generated objects through correspondence estimation, and adapts the grasp through optimization. Building on the generated data, we introduce a multimodal prediction network that integrates visual and geometric information. By devising a local-global fusion module and an importance-aware sampling mechanism, we enable robust and computationally efficient prediction of functional dexterous grasps. Through extensive experiments across various object categories, we demonstrate that CorDex generalizes well to unseen object instances and significantly outperforms state-of-the-art baselines.\nüìÑ Download PDF\nLearning Latent Action World Models In The Wild Authors: Quentin Garrido, Tushar Nagarajan, Basile Terver, Nicolas Ballas, Yann LeCun, Michael Rabbat Venue: arXiv (2026)\nAgents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.\nüìÑ Download PDF\nLocal Multimodal Dynamics in Mixed Ionic-Electronic Conductors and Their Fingerprints in Organic Electrochemical Transistor Operation Authors: Shubham Tanwar, Han-Yan Wu, Chi-Yuan Yang, Ruben Millan-Solsona, Simone Fabiano, Adrica Kyndiah, Gabriel Gomila Venue: arXiv (2026)\nMixed ionic-electronic conductors host tightly coupled interactions among mobile ions, electronic charges, and the polymer matrix, giving rise to complex multimodal responses spanning electrical, mechanical, and morphological transformations. These materials underpin organic electrochemical transistors (OECTs), which translate such interactions into low-voltage signal amplification and sensing for applications in bioelectronics, neuromorphic computing, and memory. Despite their central role, OECT current-voltage transfer characteristics are often treated phenomenologically, as both the local multimodal dynamics and their connection to global device response remain unresolved. Here, we reveal that the transfer curve encodes a cascade of spatially localized electrochemical transitions, each associated with distinct changes in conductivity, stiffness, and morphology, fundamentally redefining it as a spatially resolved fingerprint of device‚Äôs internal state. Using automated operando multimodal in-liquid scanning dielectric microscopy, we directly map these dynamics and identify region-specific electrochemical thresholds governing the interplay between source, channel, and drain. We found that the local tip-sample electrostatic force serves as a remarkable mechanistic observable of coupled multimodal dynamics in mixed conductors. A physically grounded model links it to general material, interfacial, and geometric parameters, enabling mechanistic interpretation and predictive insights. Our work provides a new framework for probing and understanding mixed conduction in ion-electron coupled systems.\nüìÑ Download PDF\nQuantum Elastic Network Models and their Application to Graphene Authors: Ioannis Kolotouros, Adithya Sireesh, Stuart Ferguson, Sean Thrasher, Petros Wallden, Julien Michel Venue: arXiv (2026)\nMolecular dynamics simulations are a central computational methodology in materials design for relating atomic composition to mechanical properties. However, simulating materials with atomic-level resolution on a macroscopic scale is infeasible on current classical hardware, even when using the simplest elastic network models (ENMs) that represent molecular vibrations as a network of coupled oscillators. To address this issue, we introduce Quantum Elastic Network Models (QENMs) and utilize the quantum algorithm of Babbush et al. (PRX, 2023), which offers an exponential advantage when simulating systems of coupled oscillators under some specific conditions and assumptions. Here, we demonstrate how our method enables the efficient simulation of planar materials. As an example, we apply our algorithm to the task of simulating a 2D graphene sheet. We analyze the exact complexity for initial-state preparation, Hamiltonian simulation, and measurement of this material, and provide two real-world applications: heat transfer and the out-of-plane rippling effect. We estimate that an atomistic simulation of a graphene sheet on the centimeter scale, classically requiring hundreds of petabytes of memory and prohibitive runtimes, could be encoded and simulated with as few as $\\sim 160$ logical qubits.\nüìÑ Download PDF\nMachine learning for radiative hydrodynamics in astrophysics Authors: Gonzague Radureau Venue: arXiv (2026)\nRadiation hydrodynamics describes the interaction between high-temperature hypersonic plasmas and the radiation they emit or absorb, a coupling that plays a central role in many astrophysical phenomena related to accretion and ejection processes. The HADES code was developed to model such systems by coupling hydrodynamics with M1-gray or M1-multigroup radiative transfer models, which are well suited to optically intermediate media. Despite its accuracy, radiation hydrodynamics simulations remain extremely demanding in terms of computational cost. Two main limitations are responsible for this. First, the M1-multigroup model relies on a closure relation with no analytic expression, requiring expensive numerical evaluations. Second, the Courant-Friedrichs-Lewy condition strongly restricts the time step of the explicit schemes used in HADES. To overcome these difficulties, two complementary Artificial Intelligence based strategies were developed in this thesis. The first approach consists in training a Multi-Layer Perceptron to approximate the M1-multigroup closure relation. This method achieves excellent accuracy while reducing the computational cost by a factor of 3000, making it the most efficient approach currently available for this task. This performance gain enables high-fidelity simulations of radiative shocks, in which radiation directly influences the shock structure. In particular, increasing spectral resolution slows down the shock and enlarges the radiative precursor. The second approach explores the use of Physics-Informed Neural Networks to directly solve the radiation hydrodynamics equations and extrapolate simulations beyond their initial time range. Tests on purely hydrodynamic shocks show accurate handling of discontinuities, but application to radiative shocks remains challenging and requires further investigation.\nüìÑ Download PDF\nRe-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing Authors: Runze He, Yiji Cheng, Tiankai Hang, Zhimin Li, Yu Xu, Zijin Yin, Shiyi Zhang, Wenxun Dai, Penghui Du, Ao Ma, Chunyu Wang, Qinglin Lu, Jizhong Han, Jiao Dai Venue: arXiv (2026)\nIn-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model‚Äôs overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.\nüìÑ Download PDF\nThree-dimensional scene reconstruction using Roman slitless spectra Authors: Tri L. Astraatmadja, Andrew S. Fruchter, Susana E. Deustua, Helen Qu, Masao Sako, Russell E. Ryan, Yannick Copin, Greg Aldering, Rebekah A. Hounsell, David Rubin, Llu√≠s Galbany, Saul Perlmutter, Benjamin M. Rose Venue: arXiv (2026)\nThe Nancy Grace Roman Space Telescope will carry out a wide-field imaging and slitless spectroscopic survey of Type Ia Supernovae to improve our understanding of dark energy. Crucial to this endeavor is obtaining supernova spectra uncontaminated by light from their host galaxies. However, obtaining such spectra is made more difficult by the inherent problem in wide-field slitless spectroscopic surveys: the blending of spectra of close objects. The spectrum of a supernova will blend with the host galaxy, even from regions distant from the supernova on the sky. If not properly removed, this contamination will introduce systematic bias when the supernova spectra are later used to determine intrinsic supernova parameters and to infer the parameters of dark energy. To address this problem we developed an algorithm that makes use of the spectroscopic observations of the host galaxy at all available observatory roll angles to reconstruct a three-dimensional (3d; 2d spatial, 1d spectral) representation of the underlying host galaxy that accurately matches the 2d slitless spectrum of the host galaxy when projected to an arbitrary rotation angle. We call this ``scene reconstruction‚Äô‚Äô. The projection of the reconstructed scene can be subtracted from an observation of a supernova to remove the contamination from the underlying host. Using simulated Roman data, we show that our method has extremely small systematic errors and significantly less random noise than if we subtracted a single perfectly aligned spectrum of the host obtained before or after the supernova was visible.\nüìÑ Download PDF\nEstimating Consensus Ideal Points Using Multi-Source Data Authors: Mellissa Meisels, Melody Huang, Tiffany M. Tang Venue: arXiv (2026)\nIn the advent of big data and machine learning, researchers now have a wealth of congressional candidate ideal point estimates at their disposal for theory testing. Weak relationships raise questions about the extent to which they capture a shared quantity ‚Äì rather than idiosyncratic, domain-specific factors ‚Äì yet different measures are used interchangeably in most substantive analyses. Moreover, questions central to the study of American politics implicate relationships between candidate ideal points and other variables derived from the same data sources, introducing endogeneity. We propose a method, consensus multidimensional scaling (CoMDS), which better aligns with how applied scholars use ideal points in practice. CoMDS captures the shared, stable associations of a set of underlying ideal point estimates and can be interpreted as their common spatial representation. We illustrate the utility of our approach for assessing relationships within domains of existing measures and provide a suite of diagnostic tools to aid in practical usage.\nüìÑ Download PDF\nAn interpretable data-driven approach to optimizing clinical fall risk assessment Authors: Fardin Ganjkhanloo, Emmett Springer, Erik H. Hoyer, Daniel L. Young, Holley Farley, Kimia Ghobadi Venue: arXiv (2026)\nIn this study, we aim to better align fall risk prediction from the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically meaningful measures via a data-driven modelling approach. We conducted a retrospective cohort analysis of 54,209 inpatient admissions from three Johns Hopkins Health System hospitals between March 2022 and October 2023. A total of 20,208 admissions were included as high fall risk encounters, and 13,941 were included as low fall risk encounters. To incorporate clinical knowledge and maintain interpretability, we employed constrained score optimization (CSO) models to reweight the JHFRAT scoring weights, while preserving its additive structure and clinical thresholds. Recalibration refers to adjusting item weights so that the resulting score can order encounters more consistently by the study‚Äôs risk labels, and without changing the tool‚Äôs form factor or deployment workflow. The model demonstrated significant improvements in predictive performance over the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). This performance improvement translates to protecting an additional 35 high-risk patients per week across the Johns Hopkins Health System. The constrained score optimization models performed similarly with and without the EHR variables. Although the benchmark black-box model (XGBoost), improves upon the performance metrics of the knowledge-based constrained logistic regression (AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk labeling. This evidence-based approach provides a robust foundation for health systems to systematically enhance inpatient fall prevention protocols and patient safety using data-driven optimization techniques, contributing to improved risk assessment and resource allocation in healthcare settings.\nüìÑ Download PDF\nInformation-Theoretic Limits on Exact Subgraph Alignment Problem Authors: Chun Hei Michael Shiu, Hei Victor Cheng, Lele Wang Venue: arXiv (2026)\nThe graph alignment problem aims to identify the vertex correspondence between two correlated graphs. Most existing studies focus on the scenario in which the two graphs share the same vertex set. However, in many real-world applications, such as computer vision, social network analysis, and bioinformatics, the task often involves locating a small graph pattern within a larger graph. Existing graph alignment algorithms and analysis cannot directly address these scenarios because they are not designed to identify the specific subset of vertices where the small graph pattern resides within the larger graph. Motivated by this limitation, we introduce the subgraph alignment problem, which seeks to recover both the vertex set and/or the vertex correspondence of a small graph pattern embedded in a larger graph. In the special case where the small graph pattern is an induced subgraph of the larger graph and both the vertex set and correspondence are to be recovered, the problem reduces to the subgraph isomorphism problem, which is NP-complete in the worst case. In this paper, we formally formulate the subgraph alignment problem by proposing the Erdos-Renyi subgraph pair model together with some appropriate recovery criterion. We then establish almost-tight information-theoretic results for the subgraph alignment problem and present some novel approaches for the analysis.\nüìÑ Download PDF\nCoV: Chain-of-View Prompting for Spatial Reasoning Authors: Haoyu Zhao, Akide Liu, Zeyu Zhang, Weijie Wang, Feng Chen, Ruihan Zhu, Gholamreza Haffari, Bohan Zhuang Venue: arXiv (2026)\nEmbodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision‚Äìlanguage models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached. We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56% improvement in LLM-Match, with a maximum gain of +13.62% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51% average improvement, peaking at +3.73% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.\nüìÑ Download PDF\nPlenoptic Video Generation Authors: Xiao Fu, Shitao Tang, Min Shi, Xian Liu, Jinwei Gu, Ming-Yu Liu, Dahua Lin, Chen-Hsuan Lin Venue: arXiv (2026)\nCamera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/\nüìÑ Download PDF\nMultivector Reranking in the Era of Strong First-Stage Retrievers Authors: Silvio Martinico, Franco Maria Nardini, Cosimo Rulli, Rossano Venturini Venue: arXiv (2026)\nLearned multivector representations power modern search systems with strong retrieval effectiveness, but their real-world use is limited by the high cost of exhaustive token-level retrieval. Therefore, most systems adopt a \\emph{gather-and-refine} strategy, where a lightweight gather phase selects candidates for full scoring. However, this approach requires expensive searches over large token-level indexes and often misses the documents that would rank highest under full similarity. In this paper, we reproduce several state-of-the-art multivector retrieval methods on two publicly available datasets, providing a clear picture of the current multivector retrieval field and observing the inefficiency of token-level gathering. Building on top of that, we show that replacing the token-level gather phase with a single-vector document retriever ‚Äì specifically, a learned sparse retriever (LSR) ‚Äì produces a smaller and more semantically coherent candidate set. This recasts the gather-and-refine pipeline into the well-established two-stage retrieval architecture. As retrieval latency decreases, query encoding with two neural encoders becomes the dominant computational bottleneck. To mitigate this, we integrate recent inference-free LSR methods, demonstrating that they preserve the retrieval effectiveness of the dual-encoder pipeline while substantially reducing query encoding time. Finally, we investigate multiple reranking configurations that balance efficiency, memory, and effectiveness, and we introduce two optimization techniques that prune low-quality candidates early. Empirical results show that these techniques improve retrieval efficiency by up to 1.8$\\times$ with no loss in quality. Overall, our two-stage approach achieves over $24\\times$ speedup over the state-of-the-art multivector retrieval systems, while maintaining comparable or superior retrieval quality.\nüìÑ Download PDF\nDivide and Conquer: Cluster and manifold-based interpretation of complex flows Authors: Qihong L. Li-Hu, Guy Y. Cornejo Maceda, Andrea Ianiro, Stefano Discetti Venue: arXiv (2026)\nWe propose a framework for a global description of the dynamics of complex flows via clusterized spatial representations of the flow, isolating and identifying local dynamics, retrieving different Space-Time Cluster-Based Network Models (ST-CNMs). The key enabler is the partitioning of the domain based on a nonlinear manifold learning approach, in which spatial points are clustered based on the similarity of their dynamics, as observed in their compact embedding in manifold coordinates. The method receives as input time-resolved flow fields. From these, the spatial manifold is computed through isometric mapping applied to the vorticity time histories at each spatial location. An unsupervised clustering method, applied in the manifold space, partitions the full flow domain into subdomains. The dynamics of each subdomain are then described with cluster-based modeling. The method is demonstrated on two flow-field datasets obtained with a direct numerical simulation of a fluidic pinball under periodic forcing and with two-dimensional particle image velocimetry measurements of a transitional jet flow. The spatial manifold-based flow partitioning identifies regions with similar dynamics in an automated way. For both cases, ST-CNM identifies local dynamics that are not captured by a global approach. In particular, vortex shedding and vortex pairing dynamics are isolated in the jet flow experiment. The proposed fully automated domain partitioning method will benefit the structural description of controlled flows and unveil the actuation mechanisms at play.\nüìÑ Download PDF\nMulti-Disciplinary Dataset Discovery from Citation-Verified Literature Contexts Authors: Zhiyin Tan, Changxu Duan Venue: arXiv (2026)\nIdentifying suitable datasets for a research question remains challenging because existing dataset search engines rely heavily on metadata quality and keyword overlap, which often fail to capture the semantic intent of scientific investigation. We introduce a literature-driven framework that discovers datasets from citation contexts in scientific papers, enabling retrieval grounded in actual research use rather than metadata availability. Our approach combines large-scale citation-context extraction, schema-guided dataset recognition with Large Language Models, and provenance-preserving entity resolution. We evaluate the system on eight survey-derived computer science queries and find that it achieves substantially higher recall than Google Dataset Search and DataCite Commons, with normalized recall ranging from an average of 47.47% to a highest value of 81.82%. Beyond recovering gold-standard datasets, the method also surfaces additional datasets not documented in the surveys. Expert assessments across five top-level Fields of Science indicate that a substantial portion of the additional datasets are considered high utility, and some are regarded as novel for the specific topics chosen by the experts. These findings establish citation-context mining as an effective and generalizable paradigm for dataset discovery, particularly in settings where datasets lack sufficient or reliable metadata. To support reproducibility and future extensions, we release our code, evaluation datasets, and results on GitHub (https://github.com/Fireblossom/citation-context-dataset-discovery).\nüìÑ Download PDF\nSurveying exogenous species in Saturn with ALMA I. Detecting and Mapping CO Authors: Deborah Bardet, Thierry Fouchet, Thibault Cavali√©, Rapha√´l Moreno, Emmanuel Lellouch, Camille Lefour, Bilal Benmahi, Sandrine Guerlet Venue: arXiv (2026)\nThe origin of carbon monoxide (CO) in Saturn‚Äôs stratosphere remains uncertain, with proposed sources including internal thermochemical production, cometary impacts, and exogenic material from the rings and icy moons (i.e. Enceladus). We aim to constrain the vertical and meridional distribution of stratospheric CO and assess the relative contributions of these potential sources. Here, we analysed high-spectral-resolution ALMA observations of the CO (J=3-2) line obtained on 25 May 2018, sampling Saturn‚Äôs limb from 20¬∞S to 69¬∞N. CO vertical profiles were retrieved using a line-by-line radiative transfer model combined with spectral inversion techniques, testing multiple prior scenarios representative of different source hypotheses. CO is confined to a narrow layer between 0.1 and 1 mbar, with a robust negative vertical gradient and mean abundances of (3.7+/- 0.8) x 10$^{-8}$ at 0.1 mbar and (7.2 +/- 0.9) x 10$^{-8}$ at 1 mbar. The meridional distribution is statistically homogeneous, with a marginal enhancement near 60¬∞ N plausibly related to Enceladus. No significant equatorial enhancement is detected. The absence of a strong equatorial enhancement rules out a long-lived steady source associated with ring infall. The observations are most consistent with a relatively recent ($\\approx$200-year-old or younger) cometary impact whose material has since been horizontally mixed, while any Cassini Grand Finale ring influx was either too recent or inefficient to affect CO abundances at the probed pressure levels.\nüìÑ Download PDF\nüîç linguistics RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes Authors: Yuan-Kang Lee, Kuan-Lin Chen, Chia-Che Chang, Yu-Lun Liu Venue: arXiv (2026)\nNighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/\nüìÑ Download PDF\nPixel-Perfect Visual Geometry Estimation Authors: Gangwei Xu, Haotong Lin, Hongcheng Luo, Haiyang Sun, Bing Wang, Guang Chen, Sida Peng, Hangjun Ye, Xin Yang Venue: arXiv (2026)\nRecovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.\nüìÑ Download PDF\nOptimal Lower Bounds for Online Multicalibration Authors: Natalie Collina, Jiuyao Lu, Georgy Noarov, Aaron Roth Venue: arXiv (2026)\nWe prove tight lower bounds for online multicalibration, establishing an information-theoretic separation from marginal calibration. In the general setting where group functions can depend on both context and the learner‚Äôs predictions, we prove an $Œ©(T^{2/3})$ lower bound on expected multicalibration error using just three disjoint binary groups. This matches the upper bounds of Noarov et al. (2025) up to logarithmic factors and exceeds the $O(T^{2/3-\\varepsilon})$ upper bound for marginal calibration (Dagan et al., 2025), thereby separating the two problems. We then turn to lower bounds for the more difficult case of group functions that may depend on context but not on the learner‚Äôs predictions. In this case, we establish an $\\widetildeŒ©(T^{2/3})$ lower bound for online multicalibration via a $Œò(T)$-sized group family constructed using orthogonal function systems, again matching upper bounds up to logarithmic factors.\nüìÑ Download PDF\nUnveiling the 3D structure of the central molecular zone from stellar kinematics and photometry: The 50 and 20 km/s clouds Authors: Francisco Nogueras-Lara, Ashley T. Barnes, Jonathan D. Henshaw, Karl Fiteni, Yoshiaki Sofue, Rainer Sch√∂del, √Ålvaro Mart√≠nez-Arranz, Mattia C. Sormani, Jairo Armijos-Abenda√±o, Laura Colzi, Izaskun Jim√©nez-Serra, V√≠ctor M. Rivilla, Pablo Garc√≠a, Adam Ginsburg, Yue Hu, Ralf S. Klessen, J. M. Diederik Kruijssen, Volker Tolls, Alex Lazarian, Dani R. Lipman, Steven N. Longmore, Xing Lu, Sergio Mart√≠n, Denise Riquelme-V√°squez, Jaime E. Pineda, √Ålvaro S√°nchez-Monge, Arianna Vasini, Elisabeth A. C. Mills Venue: arXiv (2026)\nThe central molecular zone (CMZ), surrounding the Galactic centre, is the largest reservoir of dense molecular gas in the Galaxy. Despite its relative proximity, the 3D structure of the CMZ remains poorly constrained, primarily due to projection effects. We aim to constrain the line-of-sight location of two molecular clouds in the CMZ ‚Äì the 50 and 20 km/s clouds ‚Äì and to investigate their possible physical connection using stellar kinematics and photometry. This study serves as a pilot for future applications across the full CMZ. We estimated the line-of-sight position of the clouds by analysing stellar kinematics, stellar densities, and stellar populations towards the cloud regions and a control field. We find an absence of westward moving stars in the cloud regions, which indicates that they lie on the near side of the CMZ. This interpretation is supported by the stellar density distributions. The similar behaviour observed in the two clouds, as well as in the region between them (the ridge), suggests that they are located at comparable distances and are physically linked. We also identified an intermediate-age stellar population (2-7 Gyr) in both regions, consistent with that observed on the near side of the CMZ. We estimated the line-of-sight distances at which the clouds and the ridge become kinematically detectable (i.e. where the proper motion component parallel to the Galactic plane differs from that of the control field at the 3 sigma level) by converting their measured proper motions parallel to the Galactic plane using a theoretical model of the stellar distribution. We find that the 50 and 20 km/s clouds are located at $43\\pm8$ pc and $56\\pm11$ pc from Sgr A*, respectively, and that the ridge lies at $56\\pm11$ pc; this supports the idea that the clouds are physically connected through the ridge.\nüìÑ Download PDF\nMesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video Authors: Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, Andrea Vedaldi Venue: arXiv (2026)\nWe propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object‚Äôs complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object‚Äôs overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.\nüìÑ Download PDF\nStability of the Local Ni$^{2+}$ Electronic Structure to $A$-site Disorder in the Pyrochlore Antiferromagnet NaCaNi$_2$F$_7$ Authors: M. F. DiScala, A. de la Torre, J. W. Krizan, J. Wouters, V. Bisogni, J. Pelliciari, R. J. Cava, K. W. Plumb Venue: arXiv (2026)\nNaCaNi$2$F$7$ is a unique example of spin-1 Heisenberg antiferromagnet on the pyrochlore lattice, but the presence of Na$^{1+}$/Ca$^{2+}$ $A$-site disorder complicates the local electronic and magnetic environment of the Ni$^{2+}$ $B$-site. We utilize resonant inelastic X-ray scattering (RIXS) to study the influence of $A$-site disorder on the $B$-site electronic structure of NaCaNi$2$F$7$. Ni L-edge RIXS measurements reveal a Ni$^{2+}$ electronic structure in nearly ideal octahedral coordination, with only a small trigonal compression ($Œ¥$ = -200$;$meV) required to capture all spectral features. Within the $D{3d}$ symmetry of the Ni local environment, we extract an anisotropic $g$-factor of $g{\\parallel} = 2.26$ and $g{\\perp} = 2.27$, and a corresponding paramagnetic moment of $Œº{\\rm{eff}}=3.2;Œº_B$. To simulate disorder, RIXS spectra were calculated with realistic distributions of crystal field parameters; however, these spectra are invariant relative to a disorder-free model, demonstrating the robustness of the Ni$^{2+}$ electronic environment to the $A$-site disorder, within the resolution of our measurement.\nüìÑ Download PDF\nMimicking Phantom Dark Energy with Evolving Dark Matter Mass Authors: Lorenzo La Penna, Alessio Notari, Michele Redi Venue: arXiv (2026)\nWe present a general method to reproduce a given cosmological background through energy exchange between dark energy (DE) and dark matter (DM). This can be simply realized with a standard quintessence scalar field that controls the DM mass. In particular a background with phantom crossing can be effectively realized without introducing ghosts or other pathologies. For example one can reproduce exactly the background that gives the best fit to the recent DESI+CMB+DESY5 data, within the Chevallier-Polarski-Linder (CPL) parametrization of DE. Although the background evolution is identical, the perturbations differ, leading to modified growth of structures. If the DM mass varies at late times, early-time observables are not modified and can reproduce the main predictions of the target model, but late-time observables are affected. We discuss in particular the effects on the matter power spectrum, CMB lensing and ISW effect. When reproducing the best fit CPL background model, this scenario generically predicts $\\mathcal{O}(10%)$ deviations in such observables. However, for suitable choices of parameters, effects on the matter power spectrum can be smaller, motivating a detailed study. In general, energy exchange between DE and DM generates a mismatch between the matter power spectrum and the gravitational potential amplitudes compared to the decoupled case, that can lead to deviations observable in future experiments.\nüìÑ Download PDF\nStochastic Deep Learning: A Probabilistic Framework for Modeling Uncertainty in Structured Temporal Data Authors: James Rice Venue: arXiv (2026)\nI propose a novel framework that integrates stochastic differential equations (SDEs) with deep generative models to improve uncertainty quantification in machine learning applications involving structured and temporal data. This approach, termed Stochastic Latent Differential Inference (SLDI), embeds an It√¥ SDE in the latent space of a variational autoencoder, allowing for flexible, continuous-time modeling of uncertainty while preserving a principled mathematical foundation. The drift and diffusion terms of the SDE are parameterized by neural networks, enabling data-driven inference and generalizing classical time series models to handle irregular sampling and complex dynamic structure. A central theoretical contribution is the co-parameterization of the adjoint state with a dedicated neural network, forming a coupled forward-backward system that captures not only latent evolution but also gradient dynamics. I introduce a pathwise-regularized adjoint loss and analyze variance-reduced gradient flows through the lens of stochastic calculus, offering new tools for improving training stability in deep latent SDEs. My paper unifies and extends variational inference, continuous-time generative modeling, and control-theoretic optimization, providing a rigorous foundation for future developments in stochastic probabilistic machine learning.\nüìÑ Download PDF\nMeasurement of the Higgs boson total decay width using the H $\\to$ WW $\\to$ e$ŒΩŒºŒΩ$ decay channel in proton-proton collisions at $\\sqrt{s}$ = 13 TeV Authors: CMS Collaboration Venue: arXiv (2026)\nThe Higgs boson (H) decay width is determined from the ratio of off- and on-shell production of H $\\to$ WW $\\to$ e$ŒΩŒºŒΩ$ using proton-proton collision data corresponding to an integrated luminosity of 138 fb$^{-1}$ collected at $\\sqrt{s}$ = 13 TeV by the CMS experiment at the LHC. The off-shell signal strength is measured as $Œº_\\text{off-shell}$ = 1.2$^{+0.8}{-0.7}$. The Higgs boson total decay width is $Œì\\text{H}$ = 3.9$^{+2.7}_{-2.2}$ MeV, in agreement with the standard model prediction. The uncertainty in this result represents a factor of three improvement over the previous CMS result in this decay channel.\nüìÑ Download PDF\nA First-principles Study of Weyl Nodal Loop and Multiple Sets of Weyl Points in Trigonal PtBi$_2$ Authors: Lin-Lin Wang Venue: arXiv (2026)\nCoexistence of surface superconductivity and Fermi arcs in trigonal $Œ≥$-PtBi$_2$ has recently attracted attention for possible realization of topological superconductivity. The Fermi arcs on the two different (0001) surface terminations have been associated with the set of Weyl points just above the Fermi energy (E$_F$). Here using first-principles calculations to explore the band crossings over the full Brillouin zone between the nominally highest valence and lowest conduction bands in $Œ≥$-PtBi$_2$, we find a Weyl nodal loop (WNL) and multiple sets of Weyl points (WPs). The main difference between the two reported experimental structural parameters is the magnitude of Bi-layer buckling. While the WNL, bulk gap region and the set of Weyl points just above the E$_F$ are robust, the number and location of the other sets of WPs depend sensitively on the structural parameters with different magnitude of Bi-layer buckling. Besides calculating the 2D Fermi surface with Fermi arcs and quasi-particle interference (QPI) around the E$_F$ in good agreements with ARPES and experimental QPI, we also predict new Fermi arc features at higher energy.\nüìÑ Download PDF\nMultigroup Radiation Diffusion on a Moving Mesh: Implementation in RICH and Application to Tidal Disruption Events Authors: Itamar Giron, Menahem Krief, Nicholas C. Stone, Elad Steinberg Venue: arXiv (2026)\nRadiation-hydrodynamics (RHD) determines the bulk evolution and observable emission in a wide variety of high-energy astrophysical phenomena. Due to their complexity, RHD problems must usually be studied through numerical simulation. We have extended the publicly available RICH code, which previously solved the equations of RHD in the limit of grey flux-limited diffusion (FLD), to operate with a multigroup FLD solver. RICH is a semi-Lagrangian code that solves the equations of RHD on an unstructured moving mesh, and is the first multigroup RHD moving mesh code, making it uniquely applicable to problems with extreme dynamic range and dynamically important radiation forces. We validate our multigroup module against multiple analytic benchmarks, including a novel test of the RHD Doppler term. The computational efficiency of the code is aided by a novel scheme to accelerate convergence in optically thick cells. Finally, we apply multigroup RICH in a pilot study of a stellar tidal disruption event (TDE), using a $10^4 M_\\odot$ intermediate-mass black hole. Our simulations self-consistently produce a bright early-time X-ray flash prior to peak optical/UV light, in qualitative agreement with post-processing of (grey) RICH simulations of supermassive black hole TDEs, as well as X-ray observations of the TDE AT 2022dsb.\nüìÑ Download PDF\nEvaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior Authors: Wajid Nasser Venue: arXiv (2026)\nLLM-as-judge systems promise scalable, consistent evaluation. We find the opposite: judges are consistent, but not with each other; they are consistent with themselves. Across 3,240 evaluations (9 judges x 120 unique video x pack items x 3 independent runs), inter-judge agreement is near-zero (Krippendorff‚Äôs Œ± = 0.042). On two dimensions, judges disagree more than random noise would predict (Œ± \u003c 0). Yet this disagreement isn‚Äôt chaos; it‚Äôs structured. A classifier identifies which judge produced an evaluation with 77.1% accuracy from rubric scores alone, rising to 89.9% with disposition features. Within model families, the signal is even stronger: GPT-4.1 and GPT-5.2 are distinguishable with 99.6% accuracy. We call this the reliability paradox: judges cannot agree on what constitutes quality, yet their disagreement patterns are so stable they function as fingerprints. Each judge implements a distinct, stable theory of quality: an ‚Äúevaluative disposition‚Äù that shapes how it interprets any rubric. We characterize these dispositions along multiple axes: harshness/leniency, dimension emphasis, within-judge stability (ICC), and evidence behavior (receipt validity, semantic linkage via NLI, and shotgun index). The implication is stark: LLM judges are not interchangeable instruments measuring a shared construct. They are distinct measurement devices, each encoding its own implicit theory of quality. Averaging their scores produces a synthetic verdict that corresponds to no judge‚Äôs actual values.\nüìÑ Download PDF\nSemantically Orthogonal Framework for Citation Classification: Disentangling Intent and Content Authors: Changxu Duan, Zhiyin Tan Venue: arXiv (2026)\nUnderstanding the role of citations is essential for research assessment and citation-aware digital libraries. However, existing citation classification frameworks often conflate citation intent (why a work is cited) with cited content type (what part is cited), limiting their effectiveness in auto classification due to a dilemma between fine-grained type distinctions and practical classification reliability. We introduce SOFT, a Semantically Orthogonal Framework with Two dimensions that explicitly separates citation intent from cited content type, drawing inspiration from semantic role theory. We systematically re-annotate the ACL-ARC dataset using SOFT and release a cross-disciplinary test set sampled from ACT2. Evaluation with both zero-shot and fine-tuned Large Language Models demonstrates that SOFT enables higher agreement between human annotators and LLMs, and supports stronger classification performance and robust cross-domain generalization compared to ACL-ARC and SciCite annotation frameworks. These results confirm SOFT‚Äôs value as a clear, reusable annotation standard, improving clarity, consistency, and generalizability for digital libraries and scholarly communication infrastructures. All code and data are publicly available on GitHub https://github.com/zhiyintan/SOFT.\nüìÑ Download PDF\nQNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer Authors: Daniele Lizzio Bosco, Shuteng Wang, Giuseppe Serra, Vladislav Golyanik Venue: arXiv (2026)\nRecently, Quantum Visual Fields (QVFs) have shown promising improvements in model compactness and convergence speed for learning the provided 2D or 3D signals. Meanwhile, novel-view synthesis has seen major advances with Neural Radiance Fields (NeRFs), where models learn a compact representation from 2D images to render 3D scenes, albeit at the cost of larger models and intensive training. In this work, we extend the approach of QVFs by introducing QNeRF, the first hybrid quantum-classical model designed for novel-view synthesis from 2D images. QNeRF leverages parameterised quantum circuits to encode spatial and view-dependent information via quantum superposition and entanglement, resulting in more compact models compared to the classical counterpart. We present two architectural variants. Full QNeRF maximally exploits all quantum amplitudes to enhance representational capabilities. In contrast, Dual-Branch QNeRF introduces a task-informed inductive bias by branching spatial and view-dependent quantum state preparations, drastically reducing the complexity of this operation and ensuring scalability and potential hardware compatibility. Our experiments demonstrate that ‚Äì when trained on images of moderate resolution ‚Äì QNeRF matches or outperforms classical NeRF baselines while using less than half the number of parameters. These results suggest that quantum machine learning can serve as a competitive alternative for continuous signal representation in mid-level tasks in computer vision, such as 3D representation learning from 2D observations.\nüìÑ Download PDF\nMechanics of axis formation in $\\textit{Hydra}$ Authors: Arthur Hernandez, Cuncheng Zhu, Luca Giomi Venue: arXiv (2026)\nThe emergence of a body axis is a fundamental step in the development of multicellular organisms. In simple systems such as $\\textit{Hydra}$, growing evidence suggests that mechanical forces generated by collective cellular activity play a central role in this process. Here, we explore a physical mechanism for axis formation based on the coupling between active stresses and tissue elasticity. We analyse the elastic deformation induced by activity-generated stresses and show that, owing to the spherical topology of the tissue, forces globally condense toward configurations in which both elastic strain and nematic defect localise at opposite poles. These mechanically selected states define either a polar or apolar head-food axis. To characterize the condensed regime, we introduce a compact parametrization of of the active force and flux distributions, enabling analytical predictions and direct comparison with experiments. Using this framework, we calculate experimentally relevant observables, including areal strain, lateral pressure, and normal displacements during muscular contraction, as well as the detailed structure of topological defect complexes in head and foot regions. Together, our results identify a mechanical route by which active tissues can spontaneously break symmetry at the organismal scale, suggesting a general physical principle underlying body-axis specification during morphogenesis.\nüìÑ Download PDF\nRAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection Authors: Zhiwei Liu, Runteng Guo, Baojie Qu, Yuechen Jiang, Min Peng, Qianqian Xie, Sophia Ananiadou Venue: arXiv (2026)\nCross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample‚Äôs semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at https://github.com/lzw108/RAAR.\nüìÑ Download PDF\nDefense Against Indirect Prompt Injection via Tool Result Parsing Authors: Qiang Yu, Xinran Cheng, Chuanyi Liu Venue: arXiv (2026)\nAs LLM agents transition from digital assistants to physical controllers in autonomous systems and robotics, they face an escalating threat from indirect prompt injection. By embedding adversarial instructions into the results of tool calls, attackers can hijack the agent‚Äôs decision-making process to execute unauthorized actions. This vulnerability poses a significant risk as agents gain more direct control over physical environments. Existing defense mechanisms against Indirect Prompt Injection (IPI) generally fall into two categories. The first involves training dedicated detection models; however, this approach entails high computational overhead for both training and inference, and requires frequent updates to keep pace with evolving attack vectors. Alternatively, prompt-based methods leverage the inherent capabilities of LLMs to detect or ignore malicious instructions via prompt engineering. Despite their flexibility, most current prompt-based defenses suffer from high Attack Success Rates (ASR), demonstrating limited robustness against sophisticated injection attacks. In this paper, we propose a novel method that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. Our approach achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. Code is available at GitHub.\nüìÑ Download PDF\nSciFig: Towards Automating Scientific Figure Generation Authors: Siyuan Huang, Yutong Gao, Juyang Bai, Yifan Zhou, Zi Yin, Xinxin Liu, Rama Chellappa, Chun Pong Lau, Sayan Nag, Cheng Peng, Shraman Pramanick Venue: arXiv (2026)\nCreating high-quality figures and visualizations for scientific papers is a time-consuming task that requires both deep domain knowledge and professional design skills. Despite over 2.5 million scientific papers published annually, the figure generation process remains largely manual. We introduce $\\textbf{SciFig}$, an end-to-end AI agent system that generates publication-ready pipeline figures directly from research paper texts. SciFig uses a hierarchical layout generation strategy, which parses research descriptions to identify component relationships, groups related elements into functional modules, and generates inter-module connections to establish visual organization. Furthermore, an iterative chain-of-thought (CoT) feedback mechanism progressively improves layouts through multiple rounds of visual analysis and reasoning. We introduce a rubric-based evaluation framework that analyzes 2,219 real scientific figures to extract evaluation rubrics and automatically generates comprehensive evaluation criteria. SciFig demonstrates remarkable performance: achieving 70.1$%$ overall quality on dataset-level evaluation and 66.2$%$ on paper-specific evaluation, and consistently high scores across metrics such as visual clarity, structural organization, and scientific accuracy. SciFig figure generation pipeline and our evaluation benchmark will be open-sourced.\nüìÑ Download PDF\nDisco-RAG: Discourse-Aware Retrieval-Augmented Generation Authors: Dongqi Liu, Hang Ding, Qiming Feng, Jian Li, Xurong Xie, Zhucun Xue, Chengjie Wang, Jiangning Zhang, Yabiao Wang Venue: arXiv (2026)\nRetrieval-Augmented Generation (RAG) has emerged as an important means of enhancing the performance of large language models (LLMs) in knowledge-intensive tasks. However, most existing RAG strategies treat retrieved passages in a flat and unstructured way, which prevents the model from capturing structural cues and constrains its ability to synthesize knowledge from dispersed evidence across documents. To overcome these limitations, we propose Disco-RAG, a discourse-aware framework that explicitly injects discourse signals into the generation process. Our method constructs intra-chunk discourse trees to capture local hierarchies and builds inter-chunk rhetorical graphs to model cross-passage coherence. These structures are jointly integrated into a planning blueprint that conditions the generation. Experiments on question answering and long-document summarization benchmarks show the efficacy of our approach. Disco-RAG achieves state-of-the-art results on the benchmarks without fine-tuning. These findings underscore the important role of discourse structure in advancing RAG systems.\nüìÑ Download PDF\nLLMberjack: Guided Trimming of Debate Trees for Multi-Party Conversation Creation Authors: Leonardo Bottona, Nicol√≤ Penzo, Bruno Lepri, Marco Guerini, Sara Tonelli Venue: arXiv (2026)\nWe present LLMberjack, a platform for creating multi-party conversations starting from existing debates, originally structured as reply trees. The system offers an interactive interface that visualizes discussion trees and enables users to construct coherent linearized dialogue sequences while preserving participant identity and discourse relations. It integrates optional large language model (LLM) assistance to support automatic editing of the messages and speakers‚Äô descriptions. We demonstrate the platform‚Äôs utility by showing how tree visualization facilitates the creation of coherent, meaningful conversation threads and how LLM support enhances output quality while reducing human effort. The tool is open-source and designed to promote transparent and reproducible workflows to create multi-party conversations, addressing a lack of resources of this type.\nüìÑ Download PDF\nThe Bathtub of European AI Governance: Identifying Technical Sandboxes as the Micro-Foundation of Regulatory Learning Authors: Tom Deckenbrunnen, Alessio Buscemi, Marco Almada, Alfredo Capozucca, German Castignani Venue: arXiv (2026)\nThe EU AI Act adopts a horizontal and adaptive approach to govern AI technologies characterised by rapid development and unpredictable emerging capabilities. To maintain relevance, the Act embeds provisions for regulatory learning. However, these provisions operate within a complex network of actors and mechanisms that lack a clearly defined technical basis for scalable information flow. This paper addresses this gap by establishing a theoretical model of regulatory learning space defined by the AI Act, decomposed into micro, meso, and macro levels. Drawing from this functional perspective of this model, we situate the diverse stakeholders - ranging from the EU Commission at the macro level to AI developers at the micro level - within the transitions of enforcement (macro-micro) and evidence aggregation (micro-macro). We identify AI Technical Sandboxes as the essential engine for evidence generation at the micro level, providing the necessary data to drive scalable learning across all levels of the model. By providing an extensive discussion of the requirements and challenges for AITSes to serve as this micro-level evidence generator, we aim to bridge the gap between legislative commands and technical operationalisation, thereby enabling a structured discourse between technical and legal experts.\nüìÑ Download PDF\nSymbolically regressing dark matter halo profiles using weak lensing Authors: Alicia Mart√≠n, Tariq Yasin, Deaglan J. Bartlett, Harry Desmond, Pedro G. Ferreira Venue: arXiv (2026)\nThe structure of dark matter haloes is often described by radial density profiles motivated by cosmological simulations. These are typically assumed to have a fixed functional form (e.g. NFW), with some free parameters that can be constrained with observations. However, relying on simulations has the disadvantage that the resulting profiles depend on the dark matter model and the baryonic physics implementation, which are highly uncertain. Instead, we present a method to constrain halo density profiles directly from observations. This is done using a symbolic regression algorithm called Exhaustive Symbolic Regression (ESR). ESR searches for the optimal analytic expression to fit data, combining both accuracy and simplicity. We apply ESR to a sample of 149 galaxy clusters from the HSC-XXL survey to identify which functional forms perform best across the entire sample of clusters. We identify density profiles that statistically outperform NFW under a minimum-description-length criterion. Within the radial range probed by the weak-lensing data ($R \\sim 0.3 - 3$ h$^{-1}$ Mpc), the highest-ranked ESR profiles exhibit shallow inner behaviour and a maximum in the density profile. As a practical application, we show how the best-fitting ESR models can be used to obtain enclosed mass estimates. We find masses that are, on average, higher than those derived using NFW, highlighting a source of potential bias when assuming the wrong density profile. These results have important knock-on effects for analyses that utilise clusters, for example cosmological constraints on $œÉ_8$ and $Œ©_m$ from cluster abundance and clustering. Beyond the HSC dataset, the method is readily applicable to any data constraining the dark matter distribution in galaxies and galaxy clusters, such as other weak lensing surveys, galactic rotation curves, or complementary probes.\nüìÑ Download PDF\nBeyond the imbalance: site-resolved dynamics probing resonances in many-body localization Authors: Asmi Haldar, Thibault Scoquart, Fabien Alet, Nicolas Laflorencie Venue: arXiv (2026)\nWe explore the limitations of using imbalance dynamics as a diagnostic tool for many-body localization (MBL) and show that spatial averaging can mask important microscopic features. Focusing on the strongly disordered regime of the random-field XXZ chain, we use state-of-the-art numerical techniques (Krylov time evolution and full diagonalization) to demonstrate that site-resolved spin autocorrelators reveal a rich and complex dynamical behavior that is obscured by the imbalance observable. By analyzing the time evolution and infinite-time limits of these local probes, we reveal resonant structures and rare local instabilities within the MBL phase. These numerical findings are supported by an analytical, few-site toy model that captures the emergence of a multiple-peak structure in local magnetization histograms, which is a hallmark of local resonances. These few-body local effects provide a more detailed understanding of ergodicity-breaking dynamics, and also allow us to explain the finite-size effects of long-time imbalance, and its sensitivity to the initial conditions in quench protocols. Overall, our experimentally testable predictions highlight the necessity of a refined, site-resolved approach to fully understand the complexities of MBL and its connection to rare-region effects.\nüìÑ Download PDF\nVision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering Authors: Shuliang Liu, Songbo Yang, Dong Fang, Sihang Jia, Yuqi Tang, Lingfeng Su, Ruoshui Peng, Yibo Yan, Xin Zou, Xuming Hu Venue: arXiv (2026)\nObject hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.\nüìÑ Download PDF\n$PC^2$: Politically Controversial Content Generation via Jailbreaking Attacks on GPT-based Text-to-Image Models Authors: Wonwoo Choi, Minjae Seo, Minkyoo Song, Hwanjo Heo, Seungwon Shin, Myoungsung You Venue: arXiv (2026)\nThe rapid evolution of text-to-image (T2I) models has enabled high-fidelity visual synthesis on a global scale. However, these advancements have introduced significant security risks, particularly regarding the generation of harmful content. Politically harmful content, such as fabricated depictions of public figures, poses severe threats when weaponized for fake news or propaganda. Despite its criticality, the robustness of current T2I safety filters against such politically motivated adversarial prompting remains underexplored. In response, we propose $PC^2$, the first black-box political jailbreaking framework for T2I models. It exploits a novel vulnerability where safety filters evaluate political sensitivity based on linguistic context. $PC^2$ operates through: (1) Identity-Preserving Descriptive Mapping to obfuscate sensitive keywords into neutral descriptions, and (2) Geopolitically Distal Translation to map these descriptions into fragmented, low-sensitivity languages. This strategy prevents filters from constructing toxic relationships between political entities within prompts, effectively bypassing detection. We construct a benchmark of 240 politically sensitive prompts involving 36 public figures. Evaluation on commercial T2I models, specifically GPT-series, shows that while all original prompts are blocked, $PC^2$ achieves attack success rates of up to 86%.\nüìÑ Download PDF\nWhen and why non-Hermitian eigenvalues miss eigenstates in topological physics Authors: Lucien Jezequel, Lo√Øc Herviou, Jens Bardarson Venue: arXiv (2026)\nNon-Hermitian systems exhibit a fundamental spectral dichotomy absent in Hermitian physics: the eigenvalue spectrum and the eigenstate spectrum can deviate significantly in the thermodynamic limit. We explain how non-Hermitian Hamiltonians can support eigenstates completely undetected by eigenvalues, with the unidirectional Hatano-Nelson model serving as both a minimal realization and universal paradigm for this phenomenon. Through exact analytical solutions, we show that this model contains not only hidden modes but multiple macroscopic hidden exceptional points that appear more generally in all systems with a non-trivial bulk winding. Our framework explains how the apparent bulk-edge correspondence failures in models like the non-Hermitian SSH chain instead reflect the systematic inability of the eigenvalue spectrum to detect certain eigenstates in systems with a skin-effect. These results establish the limitation of the eigenvalue spectrum and suggest how the eigenstate approach can lead to improved characterization of non-Hermitian topology.\nüìÑ Download PDF\nFrom Rays to Projections: Better Inputs for Feed-Forward View Synthesis Authors: Zirui Wu, Zeren Jiang, Martin R. Oswald, Jie Song Venue: arXiv (2026)\nFeed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Pl√ºcker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.\nüìÑ Download PDF\nHorn inequalities on a quiver with an involution Authors: Antoine M√©doc Venue: arXiv (2026)\nDerksen and Weyman described the cone of semi-invariants associated with a quiver. We give an inductive description of this cone, followed by an example of refinement of the inequalities characterising anti-invariant weights in the case of a quiver equipped with an involution.\nüìÑ Download PDF\nEffect of Dispatch Decisions on Small-Signal Stability of Converter-Dominated Power Systems Authors: Maitraya Avadhut Desai, Ognjen Stanojev, Simon Muntwiler, Gabriela Hug Venue: arXiv (2026)\nSmall-signal stability of modern converter-dominated power systems has been the subject of extensive research, particularly from the perspective of device-level control design for grid-forming (GFM) and grid-following (GFL) converters. However, the influence of power flow variables on system stability has received limited attention. Conventional small-signal stability analyses are typically conducted at a specific operating point, emphasizing the selection of control or system design parameters while neglecting the sensitivity of stability characteristics to operating conditions. This paper seeks to bridge this gap by systematically investigating the impact of dispatch decisions on the small-signal stability of converter-based power systems. Our findings are first illustrated on a three-bus system and then validated on the standard IEEE 39-bus test system to demonstrate scalability. Across the test systems, we find that high-voltage capacitive operation of GFL converters limits its active power injection, whereas inductive operation permits higher injections, and it is generally preferable for the GFM converter to supply more active power.\nüìÑ Download PDF\nApproximate equivariance via projection-based regularisation Authors: Torben Berndt, Jan St√ºhmer Venue: arXiv (2026)\nEquivariance is a powerful inductive bias in neural networks, improving generalisation and physical consistency. Recently, however, non-equivariant models have regained attention, due to their better runtime performance and imperfect symmetries that might arise in real-world applications. This has motivated the development of approximately equivariant models that strike a middle ground between respecting symmetries and fitting the data distribution. Existing approaches in this field usually apply sample-based regularisers which depend on data augmentation at training time, incurring a high sample complexity, in particular for continuous groups such as $SO(3)$. This work instead approaches approximate equivariance via a projection-based regulariser which leverages the orthogonal decomposition of linear layers into equivariant and non-equivariant components. In contrast to existing methods, this penalises non-equivariance at an operator level across the full group orbit, rather than point-wise. We present a mathematical framework for computing the non-equivariance penalty exactly and efficiently in both the spatial and spectral domain. In our experiments, our method consistently outperforms prior approximate equivariance approaches in both model performance and efficiency, achieving substantial runtime gains over sample-based regularisers.\nüìÑ Download PDF\nThe Squirrel Parser: A Linear-Time PEG Packrat Parser Capable of Left Recursion and Optimal Error Recovery Authors: Luke A. D. Hutchison Venue: arXiv (2026)\nWe present the squirrel parser, a PEG packrat parser that directly handles all forms of left recursion with optimal error recovery, while maintaining linear time complexity in the length of the input even in the presence of an arbitrary number of errors. Traditional approaches to handling left recursion in a recursive descent parser require grammar rewriting or complex algorithmic extensions. We derive a minimal algorithm from first principles: cycle detection via per-position state tracking and $O(1)$-per-LR-cycle communication from descendant to ancestor recursion frames, and fixed-point search via iterative expansion. For error recovery, we derived a set of four axioms and twelve constraints that must be imposed upon an optimal error recovery design to ensure completeness, correctness, optimality of performance, and intuitiveness of behavior. We utilized a constraint satisfaction mechanism to search the space of all possibilities, arriving at a provably optimal and robust error recovery strategy that maintains perfect performance linearity.\nüìÑ Download PDF\nüîç psycholinguistics Random Models and Guarded Logic Authors: Oskar Fiuk Venue: arXiv (2026)\nBuilding on ideas of Gurevich and Shelah for the G√∂del Class, we present a new probabilistic proof of the finite model property for the Guarded Fragment of First-Order Logic. Our proof is conceptually simple and yields the optimal doubly-exponential upper bound on the size of minimal models. We precisely analyse the obtained bound, up to constant factors in the exponents, and construct sentences that enforce models of tightly matching size. The probabilistic approach adapts naturally to the Triguarded Fragment, an extension of the Guarded Fragment that also subsumes the Two-Variable Fragment. Finally, we derandomise the probabilistic proof by providing an explicit model construction which replaces randomness with deterministic hash functions.\nüìÑ Download PDF\nConcurrent Balanced Augmented Trees Authors: Evan Wrench, Ajay Singh, Younghun Roh, Panagiota Fatourou, Siddhartha Jayanti, Eric Ruppert, Yuanhao Wei Venue: arXiv (2026)\nAugmentation makes search trees tremendously more versatile, allowing them to support efficient aggregation queries, order-statistic queries, and range queries in addition to insertion, deletion, and lookup. In this paper, we present the first lock-free augmented balanced search tree. Our algorithmic ideas build upon a recent augmented unbalanced search tree presented by Fatourou and Ruppert [DISC, 2024]. We implement both data structures, solving some memory reclamation challenges in the process, and provide an experimental performance analysis of them. We also present optimized versions of our balanced tree that use delegation to achieve better scalability and performance (by more than 2x in some workloads). Our experiments show that our augmented balanced tree is 2.2 to 30 times faster than the unbalanced augmented tree, and up to several orders of magnitude faster than unaugmented trees on 120 threads.\nüìÑ Download PDF\nEARL: Energy-Aware Optimization of Liquid State Machines for Pervasive AI Authors: Zain Iqbal, Lorenzo Valerio Venue: arXiv (2026)\nPervasive AI increasingly depends on on-device learning systems that deliver low-latency and energy-efficient computation under strict resource constraints. Liquid State Machines (LSMs) offer a promising approach for low-power temporal processing in pervasive and neuromorphic systems, but their deployment remains challenging due to high hyperparameter sensitivity and the computational cost of traditional optimization methods that ignore energy constraints. This work presents EARL, an energy-aware reinforcement learning framework that integrates Bayesian optimization with an adaptive reinforcement learning based selection policy to jointly optimize accuracy and energy consumption. EARL employs surrogate modeling for global exploration, reinforcement learning for dynamic candidate prioritization, and an early termination mechanism to eliminate redundant evaluations, substantially reducing computational overhead. Experiments on three benchmark datasets demonstrate that EARL achieves 6 to 15 percent higher accuracy, 60 to 80 percent lower energy consumption, and up to an order of magnitude reduction in optimization time compared to leading hyperparameter tuning frameworks. These results highlight the effectiveness of energy-aware adaptive search in improving the efficiency and scalability of LSMs for resource-constrained on-device AI applications.\nüìÑ Download PDF\nSimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning Authors: Yanchang Liang, Xiaowei Zhao Venue: arXiv (2026)\nLarge language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.\nüìÑ Download PDF\nNot All Steps are Informative: On the Linearity of LLMs‚Äô RLVR Training Authors: Tianle Wang, Zhongyuan Wu, Shenghao Jin, Hao Xu, Wei Chen, Ning Miao Venue: arXiv (2026)\nReinforcement learning with verifiable rewards (RLVR) has become a central component of large language model (LLM) post-training. Unlike supervised fine-tuning (SFT), RLVR lets an LLM generate multiple candidate solutions and reinforces those that lead to a verifiably correct final answer. However, in practice, RLVR often requires thousands of training steps to reach strong performance, incurring substantial computation largely attributed to prolonged exploration. In this work, we make a surprising observation: during RLVR, LLMs evolve in a strongly linear manner. Specifically, both model weights and model output log-probabilities exhibit strong linear correlations with RL training steps. This suggests that RLVR predominantly amplifies trends that emerge early in training, rather than continuously discovering new behaviors throughout the entire optimization trajectory. Motivated by this linearity, we investigate whether future model states can be predicted from intermediate checkpoints via extrapolation, avoiding continued expensive training. We show that Weight Extrapolation produces models with performance comparable to standard RL training while requiring significantly less computation. Moreover, Logits Extrapolation consistently outperforms continued RL training on all four benchmarks by extrapolating beyond the step range where RL training remains stable.\nüìÑ Download PDF\nQuantifying the Effect of Test Set Contamination on Generative Evaluations Authors: Rylan Schaeffer, Joshua Kazdan, Baber Abbasi, Ken Ziyu Liu, Brando Miranda, Ahmed Ahmed, Abhay Puri, Niloofar Mireshghallah, Sanmi Koyejo Venue: arXiv (2026)\nAs frontier AI systems are pretrained on web-scale data, test set contamination has become a critical concern for accurately assessing their capabilities. While research has thoroughly investigated the impact of test set contamination on discriminative evaluations like multiple-choice question-answering, comparatively little research has studied the impact of test set contamination on generative evaluations. In this work, we quantitatively assess the effect of test set contamination on generative evaluations through the language model lifecycle. We pretrain language models on mixtures of web data and the MATH benchmark, sweeping model sizes and number of test set replicas contaminating the pretraining corpus; performance improves with contamination and model size. Using scaling laws, we make a surprising discovery: including even a single test set replica enables models to achieve lower loss than the irreducible error of training on the uncontaminated corpus. We then study further training: overtraining with fresh data reduces the effects of contamination, whereas supervised finetuning on the training set can either increase or decrease performance on test data, depending on the amount of pretraining contamination. Finally, at inference, we identify factors that modulate memorization: high sampling temperatures mitigate contamination effects, and longer solutions are exponentially more difficult to memorize than shorter ones, presenting a contrast with discriminative evaluations, where solutions are only a few tokens in length. By characterizing how generation and memorization interact, we highlight a new layer of complexity for trustworthy evaluation of AI systems.\nüìÑ Download PDF\nFairness in Opinion Dynamics Authors: Stanis≈Çaw Stƒôpie≈Ñ, Michalina Janik, Mateusz Nurek, Akrati Saxena, Rados≈Çaw Michalski Venue: arXiv (2026)\nWays in which people‚Äôs opinions change are, without a doubt, subject to a rich tapestry of differing influences. Factors that affect how one arrives at an opinion reflect how they have been shaped by their environment throughout their lives, education, material status, what belief systems are they subscribed to, and what socio-economic minorities are they a part of. This already complex system is further expanded by the ever-changing nature of one‚Äôs social network. It is therefore no surprise that many models have a tendency to perform best for the majority of the population and discriminating those people who are members of various marginalized groups . This bias and the study of how to counter it are subject to a rapidly developing field of Fairness in Social Network Analysis (SNA). The focus of this work is to look into how a state-of-the-art model discriminates certain minority groups and whether it is possible to reliably predict for whom it will perform worse. Moreover, is such prediction possible based solely on one‚Äôs demographic or topological features? To this end, the NetSense dataset, together with a state-of-the-art CoDiNG model for opinion prediction have been employed. Our work explores how three classifier models (Demography-Based, Topology-Based, and Hybrid) perform when assessing for whom this algorithm will provide inaccurate predictions. Finally, through a comprehensive analysis of these experimental results, we identify four key patterns of algorithmic bias. Our findings suggest that no single paradigm provides the best results and that there is a real need for context-aware strategies in fairness-oriented social network analysis. We conclude that a multi-faceted approach, incorporating both individual attributes and network structures, is essential for reducing algorithmic bias and promoting inclusive decision-making.\nüìÑ Download PDF\nProbabilistic Transformers for Joint Modeling of Global Weather Dynamics and Decision-Centric Variables Authors: Paulius Rauba, Viktor Cikojevic, Fran Bartolic, Sam Levang, Ty Dickinson, Chase Dwelle Venue: arXiv (2026)\nWeather forecasts sit upstream of high-stakes decisions in domains such as grid operations, aviation, agriculture, and emergency response. Yet forecast users often face a difficult trade-off. Many decision-relevant targets are functionals of the atmospheric state variables, such as extrema, accumulations, and threshold exceedances, rather than state variables themselves. As a result, users must estimate these targets via post-processing, which can be suboptimal and can introduce structural bias. The core issue is that decisions depend on distributions over these functionals that the model is not trained to learn directly. In this work, we introduce GEM-2, a probabilistic transformer that jointly learns global atmospheric dynamics alongside a suite of variables that users directly act upon. Using this training recipe, we show that a lightweight (~275M params) and computationally efficient (~20-100x training speedup relative to state-of-the-art) transformer trained on the CRPS objective can directly outperform operational numerical weather prediction (NWP) models and be competitive with ML models that rely on expensive multi-step diffusion processes or require bespoke multi-stage fine-tuning strategies. We further demonstrate state-of-the-art economic value metrics under decision-theoretic evaluation, stable convergence to climatology at S2S and seasonal timescales, and a surprising insensitivity to many commonly assumed architectural and training design choices.\nüìÑ Download PDF\nOn flying through the base of a pseudo-streamer Authors: Forrest Mozer, Oleksiy Agapitov, Kyungeun Choi, Andrii Voshchepynets Venue: arXiv (2026)\nNear the 10 solar radius perihelion of Parker Solar Probe orbit 24, a confined region containing an enhanced plasma density of 25,000 particles per cubic centimeter and broadband electrostatic waves was encountered. The solar wind velocity of 200 kilometers per second and ion temperature of 25 eV were significantly reduced as compared to their values in the ambient solar wind. These anomalous plasma conditions were observed on closed magnetic field lines, as determined from observations of the suprathermal electron strahl. Because the polarity of the radial magnetic field did not change sign on the two sides of the crossing and the crossed region contained a double-peaked plasma structure, the spacecraft must have passed through the base of a pseudo-streamer whose structure extended out to 10 solar radii. In the plasma frame, an electric field as large as 400 millivolts per meter was detected during the crossing. The current associated with this electric field was less than one milliampere per square meter, corresponding to a drift velocity less than 2.5 kilometers per second. It also contained a turbulent plasma with density fluctuations divided by density as large as 0.3, suggesting that the resistive term in the generalized ohm‚Äôs law was significant. Also, the density as a function of time had a non-zero slope when the electric field was non-zero, suggesting that the pressure gradient term also mattered. As compared to earlier remote sensing and theoretical results, it is surprising that the plasma in this pseudo-streamer had a remarkably low flow velocity and that the pseudo-streamer base extended out to 10 solar radii.\nüìÑ Download PDF\nListen to the Unexpected: Self-Supervised Surprise Detection for Efficient Viewport Prediction Authors: Arman Nik Khah, Ravi Prakash Venue: arXiv (2026)\nAdaptive streaming of 360-degree video relies on viewport prediction to allocate bandwidth efficiently. Current approaches predominantly use visual saliency or historical gaze patterns, neglecting the role of spatial audio in guiding user attention. This paper presents a self-learning framework for detecting ‚Äúsurprising‚Äù auditory events ‚Äì moments that deviate from learned temporal expectations ‚Äì and demonstrates their utility for viewport prediction. The proposed architecture combines $SE(3)$-equivariant graph neural networks with recurrent temporal modeling, trained via a dual self-supervised objective. A key feature is the natural modeling of temporal attention decay: surprise is high at event onset but diminishes as the listener adapts. Experiments on the AVTrack360 dataset show that integrating audio surprise with visual cues reduces bitrate waste by up to 18% compared to visual-only methods.\nüìÑ Download PDF\nControl of the MoTe$_2$ Fermi Surface by Nb Doping Authors: Andrew P. Weber, I√±igo Robredo, Philipp R√ºssmann, Maxim Ilyn, Arnaud Magrez, Philippe Bugnon, Nan Xu, Vladimir Strocov, J. Hugo Dil, J. Enrique Ortega, Julen Iba√±ez-Azpiroz Venue: arXiv (2026)\nAb initio calculations and angle-resolved photoemission experiments show that the bulk and surface electronic structure of Weyl semimetal candidate MoTe$_2$ changes significantly by tuning the chemical potential by less than 0.4 eV. Calculations show that several Lifshitz transitions can occur among multiple electron and hole Fermi pockets of differing orbital character. Experiments show that 18% Nb-Mo substitution reduces the occupation of bulk and (001) surface bands, effectively producing a chemical potential shift of $\\approx 0.3$ eV. Orbital character and dimensionality of the bulk bands is examined by soft X-ray angle resolved photoemission with control of the excitation light polarization. The band filling at the surface is shown to increase upon deposition of alkali atoms. The results indicate that multiple regimes of electronic properties can be easily accessed in this versatile, layered material.\nüìÑ Download PDF\nFundamental Tradeoffs for ISAC Multiple Access in Finite-Blocklength Regime Authors: Zhentian Zhang, Christos Masouros, Kai-Kit Wong, Jian Dang, Zaichen Zhang, Kaitao Meng, Farshad Rostami Ghadi, Mohammad Javad Ahmadi Venue: arXiv (2026)\nThis paper investigates the fundamental communication‚Äìsensing tradeoffs of uplink dual-functional integrated sensing and communication (ISAC) multiple access under finite blocklength (FBL) constraints. Unlike conventional asymptotic analyses, we explicitly account for the limitations under FBL constraints imposed by short packets and low-latency transmission. By examining the unbiased channel state sensing estimator, we establish a geometric decomposition of the sensing error, indicating that it is jointly determined by the signal-to-noise ratio and the correlation structure of the information codebook. This insight reveals how cross-correlation among active users in the codebook geometry fundamentally constrains dual-functional ISAC performance. Consequently, we derive achievability and converse bounds that characterize the tradeoff between communication code rate and sensing accuracy in the FBL regime, with the converse further bounded by Shannon capacity. Moreover, by treating channel state sensing as a high-level sensing objective, a universal Cram√©r‚ÄìRao bound is derived to link channel estimation accuracy to practical sensing parameters. Examples of parameter sensing are also provided based on 3GPP standard. Numerical results validate the theoretical analysis and demonstrate the impact of blocklength, antenna dimensions, and sensing requirements.\nüìÑ Download PDF\nSuperluminal modes in a quantum field simulator for cosmology from analog Transplanckian physics Authors: Christian F. Schmidt, Stefan Floerchinger Venue: arXiv (2026)\nThe quantum-field-theoretic description for the U(1)-Goldstone boson of a scalar Bose-Einstein condensate with time-dependent contact interactions is developed beyond the acoustic approximation in accordance with Bogoliubov theory. The resulting effective action is mapped to a relativistic quantum field theory on a dispersive (or rainbow) cosmological spacetime which has a superluminal Corley-Jacobson dispersion relation. Time-dependent changes of the s-wave scattering length to quantum-simulate cosmological particle production are accompanied by a time-dependent healing length that can be interpreted as an analog Planck length in the comoving frame. Non-adiabatic transitions acquire a dispersive character, which is thoroughly discussed. The framework is applied to exponentially expanding or power-law contracting $(2+1)$-dimensional spacetimes which are known to produce scale-invariant cosmological power spectra. The sensitivity of these scenarios to the time-dependence of the Bogoliubov dispersion is investigated: We find a violation of scale-invariance via analytically trackable Transplanckian damping effects if the cut-off scale is not well separated from the horizon-crossing scale. In case of the exponential expansion, these damping effects remarkably settle and converge to another scale-invariant plateau in the far ultraviolet regime where non-adiabatic transitions are suppressed by the high dispersion. The developed framework enables quantitative access to more drastic analog cosmological scenarios with improved predictability in the ultraviolet regime that ultimately may lead to the observation of a scale-invariant cosmological power spectrum in the laboratory.\nüìÑ Download PDF\nEnhanced Electron Reflectionat Mott-Insulator Interfaces Authors: Jan Verlage, Peter Kratzer Venue: arXiv (2026)\nThe Klein paradox describes an incoming electron being scattered at a supercritical barrier to create electron-positron pairs, a phenomenon widely discussed in textbooks. While demonstrating this phenomenon experimentally with the fundamental particles remains challenging, condensed matter analogs are more accessible to experimental realization. For spinless quasi-particles, theoretical works show an enhancement of the pair production rate, and analogs of this effect in condensed matter systems have been studied theoretically. Here, we present another condensed matter system, a heterostructure comprised of two materials with strongly and weakly interacting electrons, that allows for constructing analytical solutions using the hierarchy-of-correlations method. The results show enhanced electron reflection related with the production of doublon-holon pairs, as known from the Klein paradox.\nüìÑ Download PDF\nHow Dark is Dark? A Reflectance and Scattering Analysis of Black Materials Authors: Jiri Filip, Radomir Vavra Venue: arXiv (2026)\nBlack materials play a critical role in applications such as image registration, camera calibration, stray light suppression, and visual design. Although many such materials appear similarly dark under diffuse illumination, their reflectance behavior can differ substantially as a function of viewing and lighting geometry. Ultra-black materials achieve exceptional light attenuation but are often constrained by cost and mechanical fragility, motivating the evaluation of more robust and accessible alternatives. In this study, we employ a gonimetric measurement system to capture the isotropic bidirectional reflectance distribution function of a range of black materials, including the ultra-black reference Vantablack, commercially available alternatives such as Musou Black and black velvet, and standard matte black coatings. We analyze their reflectance characteristics in terms of diffuse and specular scattering, as well as total integrated scatter, to quantify angular-dependent reflection. In addition, we compare their perceptual appearance using physically based rendering driven by the measured BRDFs and a psychophysical evaluation of perceived darkness. Together, these analyses provide a comprehensive assessment of black materials that links reflectance properties to visual appearance and perceptual performance, enabling informed material selection for optical applications.\nüìÑ Download PDF\nPreconditioned Multivariate Quantum Solution Extraction Authors: Gumaro Rendon, Stepan Smid Venue: arXiv (2026)\nNumerically solving partial differential equations is a ubiquitous computational task with broad applications in many fields of science. Quantum computers can potentially provide high-degree polynomial speed-ups for solving PDEs, however many algorithms simply end with preparing the quantum state encoding the solution in its amplitudes. Trying to access explicit properties of the solution naively with quantum amplitude estimation can subsequently diminish the potential speed-up. In this work, we present a technique for extracting a smooth positive function encoded in the amplitudes of a quantum state, which achieves the Heisenberg limit scaling. We improve upon previous methods by allowing higher dimensional functions, by significantly reducing the quantum complexity with respect to the number of qubits encoding the function, and by removing the dependency on the minimum of the function using preconditioning. Our technique works by sampling the cumulative distribution of the given function, fitting it with Chebyshev polynomials, and subsequently extracting a representation of the whole encoded function. Finally, we trial our method by carrying out small scale numerical simulations.\nüìÑ Download PDF\nGeometric developmental principles for the emergence of brain-like weighted and directed neuronal networks Authors: Aitor Morales-Gregorio, Anno C. Kurth, Karol√≠na Korvasov√° Venue: arXiv (2026)\nBrain networks exhibit remarkable structural properties, including high local clustering, short path lengths, and heavy-tailed weight and degree distributions. While these features are thought to enable efficient information processing with minimal wiring costs, the fundamental principles that generate such complex network architectures across species remain unclear. Here, we analyse single-neuron resolution connectomes across five species (C. Elegans, Platynereis, Drosophila M., zebrafish and mouse) to investigate the fundamental wiring principles underlying brain network formation. We show that distance-dependent connectivity alone produces small-world networks, but fails to generate heavy-tailed distributions. By incorporating weight-preferential attachment, which arises from spatial clustering of synapses along neurites, we reproduce heavy-tailed weight distributions while maintaining small-world topology. Adding degree-preferential attachment, linked to the extent of dendritic and axonal arborization, enables the generation of heavy-tailed degree distributions. Through systematic parameter exploration, we demonstrate that the combination of distance dependence, weight-preferential attachment, and degree-preferential attachment is sufficient to reproduce all characteristic properties of empirical brain networks. Our results reveal that activity-independent geometric constraints during neural development can account for the conserved architectural principles observed across evolutionarily distant species, suggesting universal mechanisms governing neural circuit assembly.\nüìÑ Download PDF\nH√°n DƒÅn Xu√© B√π (Mimicry) or Qƒ´ng Ch≈´ Y√∫ L√°n (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models Authors: Yueqing Hu, Xinyang Peng, Shuting Peng, Hanqi Wang, Tianhong Wang Venue: arXiv (2026)\nRecent Large Reasoning Models trained via reinforcement learning exhibit a ‚Äúnatural‚Äù alignment with human cognitive costs. However, we show that the prevailing paradigm of reasoning distillation ‚Äì training student models to mimic these traces via Supervised Fine-Tuning (SFT) ‚Äì fails to transmit this cognitive structure. Testing the ‚ÄúH√°n DƒÅn Xu√© B√π‚Äù (Superficial Mimicry) hypothesis across 14 models, we find that distillation induces a ‚ÄúFunctional Alignment Collapse‚Äù: while teacher models mirror human difficulty scaling ($\\bar{r}=0.64$), distilled students significantly degrade this alignment ($\\bar{r}=0.34$), often underperforming their own pre-distillation baselines (‚ÄúNegative Transfer‚Äù). Our analysis suggests that SFT induces a ‚ÄúCargo Cult‚Äù effect, where students ritualistically replicate the linguistic form of reasoning (verbosity) without internalizing the teacher‚Äôs dynamic resource allocation policy. Consequently, reasoning distillation decouples computational cost from cognitive demand, revealing that human-like cognition is an emergent property of active reinforcement, not passive imitation.\nüìÑ Download PDF\nConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning Authors: Minda Hu, Zexuan Qiu, Zenan Xu, Kun Li, Bo Zhou, Irwin King Venue: arXiv (2026)\nRecent breakthroughs in Large Reasoning Models (LRMs) have demonstrated that extensive Chain-of-Thought (CoT) generation is critical for enabling intricate cognitive behaviors, such as self-verification and backtracking, to solve complex tasks. However, this capability often leads to ``overthinking‚Äô‚Äô, where models generate redundant reasoning paths that inflate computational costs without improving accuracy. While Supervised Fine-Tuning (SFT) on reasoning traces is a standard paradigm for the ‚Äòcold start‚Äô phase, applying existing compression techniques to these traces often compromises logical coherence or incurs prohibitive sampling costs. In this paper, we introduce ConMax (Confidence-Maximizing Compression), a novel reinforcement learning framework designed to automatically compress reasoning traces while preserving essential reasoning patterns. ConMax formulates compression as a reward-driven optimization problem, training a policy to prune redundancy by maximizing a weighted combination of answer confidence for predictive fidelity and thinking confidence for reasoning validity through a frozen auxiliary LRM. Extensive experiments across five reasoning datasets demonstrate that ConMax achieves a superior efficiency-performance trade-off. Specifically, it reduces inference length by 43% over strong baselines at the cost of a mere 0.7% dip in accuracy, proving its effectiveness in generating high-quality, efficient training data for LRMs.\nüìÑ Download PDF\nBreaking Robustness Barriers in Cognitive Diagnosis: A One-Shot Neural Architecture Search Perspective Authors: Ziwen Wang, Shangshang Yang, Xiaoshan Yu, Haiping Ma, Xingyi Zhang Venue: arXiv (2026)\nWith the advancement of network technologies, intelligent tutoring systems (ITS) have emerged to deliver increasingly precise and tailored personalized learning services. Cognitive diagnosis (CD) has emerged as a core research task in ITS, aiming to infer learners‚Äô mastery of specific knowledge concepts by modeling the mapping between learning behavior data and knowledge states. However, existing research prioritizes model performance enhancement while neglecting the pervasive noise contamination in observed response data, significantly hindering practical deployment. Furthermore, current cognitive diagnosis models (CDMs) rely heavily on researchers‚Äô domain expertise for structural design, which fails to exhaustively explore architectural possibilities, thus leaving model architectures‚Äô full potential untapped. To address this issue, we propose OSCD, an evolutionary multi-objective One-Shot neural architecture search method for Cognitive Diagnosis, designed to efficiently and robustly improve the model‚Äôs capability in assessing learner proficiency. Specifically, OSCD operates through two distinct stages: training and searching. During the training stage, we construct a search space encompassing diverse architectural combinations and train a weight-sharing supernet represented via the complete binary tree topology, enabling comprehensive exploration of potential architectures beyond manual design priors. In the searching stage, we formulate the optimal architecture search under heterogeneous noise scenarios as a multi-objective optimization problem (MOP), and develop an optimization framework integrating a Pareto-optimal solution search strategy with cross-scenario performance evaluation for resolution. Extensive experiments on real-world educational datasets validate the effectiveness and robustness of the optimal architectures discovered by our OSCD model for CD tasks.\nüìÑ Download PDF\nOnomaCompass: A Texture Exploration Interface that Shuttles between Words and Images Authors: Miki Okamura, Shuhey Koyama, Li Jingjing, Yoichi Ochiai Venue: arXiv (2026)\nHumans can finely perceive material textures, yet articulating such somatic impressions in words is a cognitive bottleneck in design ideation. We present OnomaCompass, a web-based exploration system that links sound-symbolic onomatopoeia and visual texture representations to support early-stage material discovery. Instead of requiring users to craft precise prompts for generative AI, OnomaCompass provides two coordinated latent-space maps‚Äìone for texture images and one for onomatopoeic term‚Äìbuilt from an authored dataset of invented onomatopoeia and corresponding textures generated via Stable Diffusion. Users can navigate both spaces, trigger cross-modal highlighting, curate findings in a gallery, and preview textures applied to objects via an image-editing model. The system also supports video interpolation between selected textures and re-embedding of extracted frames to form an emergent exploration loop. We conducted a within-subjects study with 11 participants comparing OnomaCompass to a prompt-based image-generation workflow using Gemini 2.5 Flash Image (‚ÄúNano Banana‚Äù). OnomaCompass significantly reduced workload (NASA-TLX overall, mental demand, effort, and frustration; p \u003c .05) and increased hedonic user experience (UEQ), while usability (SUS) favored the baseline. Qualitative findings indicate that OnomaCompass helps users externalize vague sensory expectations and promotes serendipitous discovery, but also reveals interaction challenges in spatial navigation. Overall, leveraging sound symbolism as a lightweight cue offers a complementary approach to Kansei-driven material ideation beyond prompt-centric generation.\nüìÑ Download PDF\nMind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis Authors: Mingyue Cheng, Daoyu Wang, Qi Liu, Shuo Yu, Xiaoyu Tao, Yuqian Wang, Chengzhong Chu, Yu Duan, Mingkang Long, Enhong Chen Venue: arXiv (2026)\nSynthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at https://github.com/Melmaphother/Mind2Report.\nüìÑ Download PDF\nüîç llm CAOS: Conformal Aggregation of One-Shot Predictors Authors: Maja Waldron Venue: arXiv (2026)\nOne-shot prediction enables rapid adaptation of pretrained foundation models to new tasks using only one labeled example, but lacks principled uncertainty quantification. While conformal prediction provides finite-sample coverage guarantees, standard split conformal methods are inefficient in the one-shot setting due to data splitting and reliance on a single predictor. We propose Conformal Aggregation of One-Shot Predictors (CAOS), a conformal framework that adaptively aggregates multiple one-shot predictors and uses a leave-one-out calibration scheme to fully exploit scarce labeled data. Despite violating classical exchangeability assumptions, we prove that CAOS achieves valid marginal coverage using a monotonicity-based argument. Experiments on one-shot facial landmarking and RAFT text classification tasks show that CAOS produces substantially smaller prediction sets than split conformal baselines while maintaining reliable coverage.\nüìÑ Download PDF\nHow many-body chaos emerges in the presence of quasiparticles Authors: Sibaram Ruidas, Sthitadhi Roy, Subhro Bhattacharjee, Roderich Moessner Venue: arXiv (2026)\nMany-body chaos is a default property of many-body systems; at the same time, near-integrable behaviour due to weakly interacting quasiparticles is ubiquitous throughout condensed matter at low temperature. There must therefore be a, possibly generic, crossover between these very different regimes. Here, we develop a theory encapsulating the notion of a cascade of lightcones seeded by sequences of scattering of weakly interacting harmonic modes as witnessed by a suitably defined chaos diagnostic (classical decorrelator) that measures the spatiotemporal profile of many-body chaos. Our numerics deals with the concrete case of a classical Heisenberg chain, for either sign of the interaction, at low temperatures where the short-time dynamics are well captured in terms of non-interacting spin waves. To model low-temperature dynamics, we use ensembles of initial states with randomly embedded point defects in an otherwise ordered background, which provides a controlled setting for studying the scattering events. The decorrelator exhibits a short-time integrable regime followed by an intermediate `scarred‚Äô regime of the cascade of lightcones in progress; these then overlap, leading to an avalanche of scattering events which finally yields the standard long-time signature of many-body chaos.\nüìÑ Download PDF\nFluctuation-response relation for a nonequilibrium system with resolved Markovian embedding Authors: R√©mi Goerlich, Antoine Tartar, Yael Roichman, Igor M Sokolov Venue: arXiv (2026)\nFluctuation-response relations must be modified to describe nonequilibrium systems with non-Markovian dynamics. Here, we experimentally demonstrate that such relation is quantitatively recovered when the appropriate Markovian embedding of the dynamics is explicitly resolved. Using a colloidal particle optically trapped in a harmonic potential and driven out of equilibrium by a controlled colored noise, we study the response to a perturbation of the stiffness of the confining potential. While the reduced dynamics violates equilibrium fluctuation-response relations, we show that the dynamical response to the stiffness perturbation is fully determined by steady-state correlations involving the exact conjugate observable in the Markovian embedding.\nüìÑ Download PDF\nBasis Number of Graphs Excluding Minors Authors: Colin Geniet, Ugo Giocanti Venue: arXiv (2026)\nThe basis number of a graph $G$ is the minimum $k$ such that the cycle space of $G$ is generated by a family of cycles using each edge at most $k$ times. A classical result of Mac Lane states that planar graphs are exactly graphs with basis number at most 2, and more generally, graphs embedded on a fixed surface are known to have bounded basis number. Generalising this, we prove that graphs excluding a fixed minor $H$ have bounded basis number. Our proof uses the Graph Minor Structure Theorem, which requires us to understand how basis number behaves in tree-decompositions. In particular, we prove that graphs of treewidth $k$ have basis number bounded by some function of $k$. We handle tree-decompositions using the proof framework developed by Boja≈Ñczyk and Pilipczuk in their proof of Courcelle‚Äôs conjecture. Combining our approach with independent results of Miraftab, Morin and Yuditsky (2025) on basis number and path-decompositions, one can moreover improve our upper bound to a polynomial one: there exists an absolute constant $c\u003e0$ such that every $H$-minor free graph has basis number $O(|H|^c)$.\nüìÑ Download PDF\nHydrodynamic interactions in a binary-mixture colloidal monolayer Authors: M. Chamorro-Burgos, Alvaro Dom√≠nguez Venue: arXiv (2026)\nA colloidal monolayer embedded in the bulk of a fluid experiences a ‚Äúcompressible‚Äù, long-range hydrodynamic interaction which, far from boundaries, leads to a breakdown of Fick‚Äôs law above a well defined length scale, showing up as anomalous collective diffusion. We here extend the model to study the effect of the hydrodynamic interaction on a monolayer formed by two types of particles. The most interesting finding is a new regime, in the limit of very dissimilar kinds of particles, where the effective dynamics of the concentration of ‚Äúbig‚Äù (slow) particles appears to obey Fick‚Äôs law at large scales, but the corresponding collective diffusivity is completely determined, through hydrodynamic coupling, by the diffusivity of the ‚Äúsmall‚Äù (fast) particles.\nüìÑ Download PDF\nMineNPC-Task: Task Suite for Memory-Aware Minecraft Agents Authors: Tamil Sudaravan Mohan Doss, Michael Xu, Sudha Rao, Andrew D. Wilson, Balasaravanan Thoravi Kumaravel Venue: arXiv (2026)\nWe present \\textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \\emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence. As an initial snapshot, we instantiate the framework with GPT-4o and evaluate \\textbf{216} subtasks across \\textbf{8} experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents.\nüìÑ Download PDF\nInternal Representations as Indicators of Hallucinations in Agent Tool Selection Authors: Kait Healy, Bharathi Srinivasan, Visakh Madathil, Jing Wu Venue: arXiv (2026)\nLarge Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit ‚Äôtool bypass‚Äô behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs‚Äô internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.\nüìÑ Download PDF\nCat states and violation of the Bell-CHSH inequality in relativistic Quantum Field Theory Authors: M. S. Guimaraes, I. Roditi, S. P. Sorella Venue: arXiv (2026)\nA cat state localized in the right Rindler wedge is employed to study the violation of the Bell-CHSH inequality in a relativistic scalar free Quantum Field Theory. By means of the bounded Hermitian operator $sign(\\varphi(f))$, where $\\varphi(f)$ stands for the smeared scalar field, it turns out that the Bell-CHSH correlator can be evaluated in closed analytic form in terms of the imaginary error function. Being the superposition of two coherent states, cat states allow for the existence of interference terms which give rise to a violation of the Bell-CHSH inequality. As such, the present setup can be considered as an explicit realization of the results obtained by Summers-Werner.\nüìÑ Download PDF\nüîç neuroscience ECLIPSE: An Evolutionary Computation Library for Instrumentation Prototyping in Scientific Engineering Authors: Max Foreback, Evan Imata, Vincent Ragusa, Jacob Weiler, Christina Shao, Joey Wagner, Katherine G. Skocelas, Jonathan Sy, Aman Hafez, Wolfgang Banzhaf, Amy Conolly, Kyle R. Helson, Rick Marcusen, Charles Ofria, Marcin Pilinski, Rajiv Ramnath, Bryan Reynolds, Anselmo C. Pontes, Emily Dolson, Julie Rolla Venue: arXiv (2026)\nDesigning scientific instrumentation often requires exploring large, highly constrained design spaces using computationally expensive physics simulations. These simulators pose substantial challenges for integrating evolutionary computation (EC) into scientific design workflows. Evolutionary computation typically requires numerous design evaluations, making the integration of slow, low-throughput simulators particularly challenging, as they are optimized for accuracy and ease of use rather than throughput. We present ECLIPSE, an evolutionary computation framework built to interface directly with complex, domain-specific simulation tools while supporting flexible geometric and parametric representations of scientific hardware. ECLIPSE provides a modular architecture consisting of (1) Individuals, which encode hardware designs using domain-aware, physically constrained representations; (2) Evaluators, which prepare simulation inputs, invoke external simulators, and translate the simulator‚Äôs outputs into fitness measures; and (3) Evolvers, which implement EC algorithms suitable for high-cost, limited-throughput environments. We demonstrate the utility of ECLIPSE across several active space-science applications, including evolved 3D antennas and spacecraft geometries optimized for drag reduction in very low Earth orbit. We further discuss the practical challenges encountered when coupling EC with scientific simulation workflows, including interoperability constraints, parallelization limits, and extreme evaluation costs, and outline ongoing efforts to combat these challenges. ECLIPSE enables interdisciplinary teams of physicists, engineers, and EC researchers to collaboratively explore unconventional designs for scientific hardware while leveraging existing domain-specific simulation software.\nüìÑ Download PDF\nDriver-Intention Prediction with Deep Learning: Real-Time Brain-to-Vehicle Communication Authors: Niloufar Alavi, Swati Shah, Rezvan Alamian, Stefan Goetz Venue: arXiv (2026)\nBrain-computer interfaces (BCIs) allow direct communication between the brain and electronics without the need for speech or physical movement. Such interfaces can be particularly beneficial in applications requiring rapid response times, such as driving, where a vehicle‚Äôs advanced driving assistance systems could benefit from immediate understanding of a driver‚Äôs intentions. This study presents a novel method for predicting a driver‚Äôs intention to steer using electroencephalography (EEG) signals through deep learning. A driving simulator created a controlled environment in which participants imagined controlling a vehicle during various driving scenarios, including left and right turns, as well as straight driving. A convolutional neural network (CNN) classified the detected EEG data with minimal pre-processing. Our model achieved an accuracy of 83.7% in distinguishing between the three steering intentions and demonstrated the ability of CNNs to process raw EEG data effectively. The classification accuracy was highest for right-turn segments, which suggests a potential spatial bias in brain activity. This study lays the foundation for more intuitive brain-to-vehicle communication systems.\nüìÑ Download PDF\nOn the effects of protection zone and directed population flux in prey-predator dynamics Authors: Kousuke Kuto, Kazuhiro Oeda Venue: arXiv (2026)\nWe study a spatial predator-prey model in which prey can enter a protection zone (refuge) inaccessible to predators, while predators exhibit directed movement toward prey-rich regions. The directed movement is modeled by a far-sighted population flux motivated by classical movement rules, in contrast to the more commonly analyzed near-sighted chemotaxis-type mechanisms. We first establish local-in-time well-posedness for the corresponding nonstationary problem under Neumann boundary conditions, despite the discontinuity induced by the refuge interface. We then investigate the stationary problem, focusing on how the coexistence states emerge and organize globally in parameter space. In particular, we identify the bifurcation threshold for positive steady states from semitrivial predator-only equilibria, and describe the global continuation of the resulting branches. Our analysis reveals that strong directed movement can induce turning-point structures and multiplicity of coexistence steady states, highlighting a nontrivial interplay between spatial protection and predator movement behavior.\nüìÑ Download PDF\nüîç data_resources ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos Authors: Rustin Soraki, Homanga Bharadhwaj, Ali Farhadi, Roozbeh Mottaghi Venue: arXiv (2026)\nHumans can effortlessly anticipate how objects might move or change through interaction‚Äìimagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences. Unlike conventional world or dynamics models that operate in pixel or latent space, ObjectForesight represents the world explicitly in 3D at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories. To train such a model at scale, we leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2 million plus short clips with pseudo-ground-truth 3D object trajectories. Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes, establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation. objectforesight.github.io\nüìÑ Download PDF\nV-FAT: Benchmarking Visual Fidelity Against Text-bias Authors: Ziteng Wang, Yujie He, Guanliang Li, Siqi Yang, Jiaqi Xiong, Songxiang Liu Venue: arXiv (2026)\nRecent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on standard visual reasoning benchmarks. However, there is growing concern that these models rely excessively on linguistic shortcuts rather than genuine visual grounding, a phenomenon we term Text Bias. In this paper, we investigate the fundamental tension between visual perception and linguistic priors. We decouple the sources of this bias into two dimensions: Internal Corpus Bias, stemming from statistical correlations in pretraining, and External Instruction Bias, arising from the alignment-induced tendency toward sycophancy. To quantify this effect, we introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark comprising 4,026 VQA instances across six semantic domains. V-FAT employs a Three-Level Evaluation Framework that systematically increases the conflict between visual evidence and textual information: (L1) internal bias from atypical images, (L2) external bias from misleading instructions, and (L3) synergistic bias where both coincide. We introduce the Visual Robustness Score (VRS), a metric designed to penalize ‚Äúlucky‚Äù linguistic guesses and reward true visual fidelity. Our evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance.\nüìÑ Download PDF\nDVD: A Robust Method for Detecting Variant Contamination in Large Language Model Evaluation Authors: Renzhao Liang, Jingru Chen, Bo Jia, Bo Deng, Chenggang Xie, Yidong Wang, Ke Jin, Xin Wang, Linfeng Zhang, Cunxiang Wang Venue: arXiv (2026)\nEvaluating large language models (LLMs) is increasingly confounded by \\emph{variant contamination}: the training corpus contains semantically equivalent yet lexically or syntactically altered versions of test items. Unlike verbatim leakage, these paraphrased or structurally transformed variants evade existing detectors based on sampling consistency or perplexity, thereby inflating benchmark scores via memorization rather than genuine reasoning. We formalize this problem and introduce \\textbf{DVD} (\\textbf{D}etection via \\textbf{V}ariance of generation \\textbf{D}istribution), a single-sample detector that models the local output distribution induced by temperature sampling. Our key insight is that contaminated items trigger alternation between a \\emph{memory-adherence} state and a \\emph{perturbation-drift} state, yielding abnormally high variance in the synthetic difficulty of low-probability tokens; uncontaminated items remain in drift with comparatively smooth variance. We construct the first benchmark for variant contamination across two domains Omni-MATH and SuperGPQA by generating and filtering semantically equivalent variants, and simulate contamination via fine-tuning models of different scales and architectures (Qwen2.5 and Llama3.1). Across datasets and models, \\textbf{DVD} consistently outperforms perplexity-based, Min-$k$%++, edit-distance (CDD), and embedding-similarity baselines, while exhibiting strong robustness to hyperparameters. Our results establish variance of the generation distribution as a principled and practical fingerprint for detecting variant contamination in LLM evaluation.\nüìÑ Download PDF\nHigher-Order Knowledge Representations for Agentic Scientific Reasoning Authors: Isabella A. Stewart, Markus J. Buehler Venue: arXiv (2026)\nScientific inquiry requires systems-level reasoning that integrates heterogeneous experimental data, cross-domain knowledge, and mechanistic evidence into coherent explanations. While Large Language Models (LLMs) offer inferential capabilities, they often depend on retrieval-augmented contexts that lack structural depth. Traditional Knowledge Graphs (KGs) attempt to bridge this gap, yet their pairwise constraints fail to capture the irreducible higher-order interactions that govern emergent physical behavior. To address this, we introduce a methodology for constructing hypergraph-based knowledge representations that faithfully encode multi-entity relationships. Applied to a corpus of ~1,100 manuscripts on biocomposite scaffolds, our framework constructs a global hypergraph of 161,172 nodes and 320,201 hyperedges, revealing a scale-free topology (power law exponent ~1.23) organized around highly connected conceptual hubs. This representation prevents the combinatorial explosion typical of pairwise expansions and explicitly preserves the co-occurrence context of scientific formulations. We further demonstrate that equipping agentic systems with hypergraph traversal tools, specifically using node-intersection constraints, enables them to bridge semantically distant concepts. By exploiting these higher-order pathways, the system successfully generates grounded mechanistic hypotheses for novel composite materials, such as linking cerium oxide to PCL scaffolds via chitosan intermediates. This work establishes a ‚Äúteacherless‚Äù agentic reasoning system where hypergraph topology acts as a verifiable guardrail, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods.\nüìÑ Download PDF\nVerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control Authors: Sixiao Zheng, Minghao Yin, Wenbo Hu, Xiaoyu Li, Ying Shan, Yanwei Fu Venue: arXiv (2026)\nVideo world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object‚Äôs path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.\nüìÑ Download PDF\nChain-of-Sanitized-Thoughts: Plugging PII Leakage in CoT of Large Reasoning Models Authors: Arghyadeep Das, Sai Sreenivas Chintha, Rishiraj Girmal, Kinjal Pandey, Sharvi Endait Venue: arXiv (2026)\nLarge Reasoning Models (LRMs) improve performance, reliability, and interpretability by generating explicit chain-of-thought (CoT) reasoning, but this transparency introduces a serious privacy risk: intermediate reasoning often leaks personally identifiable information (PII) even when final answers are sanitized. We study how to induce privacy-first reasoning, where models reason without exposing sensitive information, using deployable interventions rather than post-hoc redaction. We introduce PII-CoT-Bench, a supervised dataset with privacy-aware CoT annotations, and a category-balanced evaluation benchmark covering realistic and adversarial leakage scenarios. Our results reveal a capability-dependent trend: state-of-the-art models benefit most from prompt-based controls, whereas weaker models require fine-tuning to achieve meaningful leakage reduction. Across models and categories, both approaches substantially reduce PII exposure with minimal degradation in utility, demonstrating that private reasoning can be achieved without sacrificing performance. Overall, we show that private CoT reasoning can be achieved with minimal utility loss, providing practical guidance for building privacy-preserving reasoning systems.\nüìÑ Download PDF\nFrom Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs) Authors: Suyash Mishra, Qiang Li, Srikanth Patil, Anubhav Girdhar Venue: arXiv (2026)\nVision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars). Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut \u0026 Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.\nüìÑ Download PDF\nLeveraging Prediction Entropy for Automatic Prompt Weighting in Zero-Shot Audio-Language Classification Authors: Karim El Khoury, Maxime Zanella, Tiffanie Godelaine, Christophe De Vleeschouwer, Benoit Macq Venue: arXiv (2026)\nAudio-language models have recently demonstrated strong zero-shot capabilities by leveraging natural-language supervision to classify audio events without labeled training data. Yet, their performance is highly sensitive to the wording of text prompts, with small variations leading to large fluctuations in accuracy. Prior work has mitigated this issue through prompt learning or prompt ensembling. However, these strategies either require annotated data or fail to account for the fact that some prompts may negatively impact performance. In this work, we present an entropy-guided prompt weighting approach that aims to find a robust combination of prompt contributions to maximize prediction confidence. To this end, we formulate a tailored objective function that minimizes prediction entropy to yield new prompt weights, utilizing low-entropy as a proxy for high confidence. Our approach can be applied to individual samples or a batch of audio samples, requiring no additional labels and incurring negligible computational overhead. Experiments on five audio classification datasets covering environmental, urban, and vocal sounds, demonstrate consistent gains compared to classical prompt ensembling methods in a zero-shot setting, with accuracy improvements 5-times larger across the whole benchmark.\nüìÑ Download PDF\nGenProve: Learning to Generate Text with Fine-Grained Provenance Authors: Jingxuan Wei, Xingyue Wang, Yanghaoyu Liao, Jie Dong, Yuchen Liu, Caijun Jia, Bihui Yu, Junnan Zhu Venue: arXiv (2026)\nLarge language models (LLM) often hallucinate, and while adding citations is a common solution, it is frequently insufficient for accountability as users struggle to verify how a cited source supports a generated claim. Existing methods are typically coarse-grained and fail to distinguish between direct quotes and complex reasoning. In this paper, we introduce Generation-time Fine-grained Provenance, a task where models must generate fluent answers while simultaneously producing structured, sentence-level provenance triples. To enable this, we present ReFInE (Relation-aware Fine-grained Interpretability \u0026 Evidence), a dataset featuring expert verified annotations that distinguish between Quotation, Compression, and Inference. Building on ReFInE, we propose GenProve, a framework that combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO). By optimizing a composite reward for answer fidelity and provenance correctness, GenProve significantly outperforms 14 strong LLMs in joint evaluation. Crucially, our analysis uncovers a reasoning gap where models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting that verifiable reasoning remains a frontier challenge distinct from surface-level citation.\nüìÑ Download PDF\nüîç emotion_language Addressing Known Challenges in Solar Flare Forecasting I: Limb-Flare Prediction with a 4-pi Full-Heliosphere Framework Authors: K. D. Leka, Eric L. Wagner, Lisa Upton, Bibhuti Kumar Jha, Kiran Jain, Sara Petty Venue: arXiv (2026)\nA demonstrated failure mode for operational solar flare forecasting is the inability to forecast flares that occur near, or just beyond, the solar limb. To address this shortcoming, we develop a ‚Äú4pi‚Äù full-heliosphere event forecasting framework and evaluate its statistical classification ability against this specific challenge. A magnetic surface flux transport model is used to generate full-sun maps of the photospheric radial magnetic field from which active regions (ARs) are identified and tracked using a new labeling scheme that is observer-location agnostic and allows for post-facto modifications. Flare-relevant magnetic parameters couple to a ‚Äúvisibility‚Äù index that specifies AR location relative to the visible solar limb and expected flare detection. Flare labels are assigned according to peak Soft X-ray flux, and a statistical classification is performed using nonparametric discriminant analysis. A version where new or emerging ARs on the far (‚Äúinvisible‚Äù side of the Sun are incorporated into the model by way of far-side helioseismology, is also tested. We evaluate the new framework by its performance specifically including the limb areas using Brier Skill Score and ROC Skill Score, finding improvement at the 2-sigma level or less. However, we do find that the number of False Negatives, or ‚Äúmissed‚Äù forecasts decreases, and find strong evidence that the additional information provided by the far-side helioseismology can help predict near- and just-beyond-limb flares, particularly for East-limb events. While individual components of this framework could be improved, we demonstrate that a known failure mode for solar flare forecasting can be mitigated with available resources.\nüìÑ Download PDF\nA Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering Authors: Md. Zahid Hossain, Most. Sharmin Sultana Samu, Md. Rakibul Islam, Md. Siam Ansary Venue: arXiv (2026)\nVisual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.\nüìÑ Download PDF\nPrediction of Magnetic Topological Materials Combining Spin and Magnetic Space Groups Authors: Liangliang Huang, Xiangang Wan, Feng Tang Venue: arXiv (2026)\nThe scarcity of predicted magnetic topological materials (MTMs) by magnetic space group (MSG) hinders further exploration towards realistic device applications. Here, we propose a new scheme combining spin space groups (SSGs)‚Äìapproximate symmetry groups neglecting spin-orbit coupling (SOC)‚Äìand MSGs to diagnose topology in collinear magnetic materials based on symmetry-indicator theory, enabling a systematic classification of the electronic topology across 484 experimentally synthesized collinear magnets from the MAGNDATA database. This new scheme exploits a symmetry-hierarchy due to SOC induced symmetry-breaking, so that nontrivial band topology can be revealed by SSG, that is yet invisible by the conventional MSG-based method, as exemplified by real triple points in ferromagnetic CaCu$_3$Fe$_2$Sb$2$O${12}$, Dirac nodal lines at generic $k$-points in antiferromagnetic FePSe$_3$ and Weyl nodal lines in altermagnetic Sr$_4$Fe$4$O${11}$. Notably, FePSe$_3$ is topologically trivial under MSG but hosts Dirac nodal lines within the SSG framework. Upon including SOC, these nodal lines are gapped and generate a sizable anomalous Hall conductivity. Despite a vanishing bulk net magnetism, FePSe$_3$ can host topologically protected surface states with large non-relativistic band spin-splitting. Moreover, topology in MTMs is tunable by rotating the magnetic moment direction once SOC is included, as exemplified in Sr$_4$Fe$4$O${11}$.The interplay of topology with non-relativistic and SOC-induced control of properties via magnetic moment reorientation in the predicted MTMs is worthy of further studies in future.\nüìÑ Download PDF\nSequential Subspace Noise Injection Prevents Accuracy Collapse in Certified Unlearning Authors: Polina Dolgova, Sebastian U. Stich Venue: arXiv (2026)\nCertified unlearning based on differential privacy offers strong guarantees but remains largely impractical: the noisy fine-tuning approaches proposed so far achieve these guarantees but severely reduce model accuracy. We propose sequential noise scheduling, which distributes the noise budget across orthogonal subspaces of the parameter space, rather than injecting it all at once. This simple modification mitigates the destructive effect of noise while preserving the original certification guarantees. We extend the analysis of noisy fine-tuning to the subspace setting, proving that the same $(\\varepsilon,Œ¥)$ privacy budget is retained. Empirical results on image classification benchmarks show that our approach substantially improves accuracy after unlearning while remaining robust to membership inference attacks. These results show that certified unlearning can achieve both rigorous guarantees and practical utility.\nüìÑ Download PDF\nA Geometric Definition of the Integral and Applications Authors: Joshua Lackman Venue: arXiv (2026)\nThe standard definition of integration of differential forms is based on local coordinates and partitions of unity. This definition is mostly a formality and not used used in explicit computations or approximation schemes. We present a definition of the integral that uses triangulations instead. Our definition is a coordinate-free version of the standard definition of the Riemann integral on $\\mathbb{R}^n$ and we argue that it is the natural definition in the contexts of Lie algebroids, stochastic integration and quantum field theory, where path integrals are defined using lattices. In particular, our definition naturally incorporates the different stochastic integrals, which involve integration over H√∂lder continuous paths. Furthermore, our definition is well-adapted to establishing integral identities from their combinatorial counterparts. Our construction is based on the observation that, in great generality, the things that are integrated are determined by cochains on the pair groupoid. Abstractly, our definition uses the van Est map to lift a differential form to the pair groupoid. Our construction suggests a generalization of the fundamental theorem of calculus which we prove: the singular cohomology and de Rham cohomology cap products of a cocycle with the fundamental class are equal.\nüìÑ Download PDF\nVariable Projection Methods for Solving Regularized Separable Inverse Problems with Applications to Semi-Blind Image Deblurring Authors: Delfina B. Comerso Salzer, Malena I. Espa√±ol, Gabriela Jeronimo Venue: arXiv (2026)\nSeparable nonlinear least squares problems appear in many inverse problems, including semi-blind image deblurring. The variable projection (VarPro) method provides an efficient approach for solving such problems by eliminating linear variables and reducing the problem to a smaller, nonlinear one. In this work, we extend VarPro to solve minimization problems containing a differentiable regularization term on the nonlinear parameters, along with a general-form Tikhonov regularization term on the linear variables. Furthermore, we develop a quasi-Newton method for solving the resulting reduced problem, and provide a local convergence analysis under standard smoothness assumptions, establishing conditions for superlinear or quadratic convergence. For large-scale settings, we introduce an inexact LSQR-based variant and prove its local convergence despite inner-solve and Hessian approximations. Numerical experiments on semi-blind deblurring show that parameter regularization prevents degenerate no-blur solutions and that the proposed methods achieve accurate reconstructions, with the inexact variant offering a favorable accuracy-cost tradeoff consistent with the theory.\nüìÑ Download PDF\nFlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching Authors: Danilo Danese, Angela Lombardi, Matteo Attimonelli, Giuseppe Fasano, Tommaso Di Noia Venue: arXiv (2026)\nBrain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual‚Äôs biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.\nüìÑ Download PDF\nA non-commutative de Branges-Rovnyak model for row contractions Authors: Robert T. W. Martin, Jeet Sampat Venue: arXiv (2026)\nWe extend the de Branges-Rovnyak model for completely non-coisometric (CNC) linear contractions on a Hilbert space to the non-commutative multivariate setting of CNC row contractions. Namely, we show that any CNC contraction from several copies of a Hilbert space into a single copy is unitarily equivalent to the adjoint of the restricted backward right shifts acting on the de Branges-Rovnyak space of a contractive left multiplier between vector-valued ‚Äúfree Hardy spaces‚Äù of square-summable power series in several non-commuting (NC) variables. This contractive, operator-valued left multiplier, the characteristic function of the CNC row contraction, is a complete unitary invariant and it is always column-extreme as a contractive left multiplier. Our construction builds a model reproducing kernel Hilbert space of NC functions using a ‚Äúnon-commutative resolvent‚Äù of the row contraction, $T$, which is the inverse of the monic, affine linear pencil of $T$ in a certain NC unit row-ball of the NC universe of all row tuples of square matrices of all finite sizes.\nüìÑ Download PDF\nOn the Value Function of Convex Bolza Problems Governed by Stochastic Difference Equations Authors: Sebasti√°n √Ålvarez, Julio Deride, Cristopher Hermosilla Venue: arXiv (2026)\nIn this paper we study the value function of Bolza problems governed by stochastic difference equations, with particular emphasis on the convex non-anticipative case. Our goal is to provide some insights on the structure of the subdiferential of the value function. In particular, we establish a connection between the evolution of the subgradients of the value function and a stochastic difference equation of Hamiltonian type. This result can be seen as a transposition of the method of characteristics, introduced by Rockafellar and Wolenski in the 2000s, to the stochastic discrete-time setting. Similarly as done in the literature for the deterministic case, the analysis is based on a duality approach. For this reason we study first a dual representation for the value function in terms of the value function of a dual problem, which is a pseudo Bolza problem. The main difference with the deterministic case is that (due to the non-anticipativity) the symmetry between the Bolza problem and its dual is no longer valid. This in turn implies that ensuring the existence of minimizers for the Bolza problem (which is a key point for establishing the method of characteristics) is not as simple as in the deterministic case, and it should be addressed differently. To complete the exposition, we study the existence of minimizers for a particular class of Bolza problems governed by linear stochastic difference equations, the so-called linear-convex optimal control problems.\nüìÑ Download PDF\nConfidence and Organizations Authors: Andr√©s Espitia Venue: arXiv (2026)\nMiscalibrated beliefs are widely viewed as compromising the quality of employees‚Äô decisions. Why, then, might an organization prefer to hire an individual known to be overconfident? This paper develops a theory of organizational demand for employees‚Äô levels of confidence when private information interacts with conflicts of interest. I study a model in which an employee uses private information to make decisions on behalf of the organization and analyze the belief design problem, namely, how the organization would like the employee to interpret his observations. I show that organizations prefer employees whose actions reflect a constant expected conflict of interest across observations. A well-calibrated employee is optimal if and only if private information does not affect this conflict. When the conflict varies with information, organizations optimally select employees whose confidence distorts their responses to information. Overconfidence is optimal when the organization seeks stronger adjustments to information than a well-calibrated employee would provide.\nüìÑ Download PDF\nVideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice Authors: Shuming Liu, Mingchen Zhuge, Changsheng Zhao, Jun Chen, Lemeng Wu, Zechun Liu, Chenchen Zhu, Zhipeng Cai, Chong Zhou, Haozhe Liu, Ernie Chang, Saksham Suri, Hongyu Xu, Qi Qian, Wei Wen, Balakrishnan Varadarajan, Zhuang Liu, Hu Xu, Florian Bordes, Raghuraman Krishnamoorthi, Bernard Ghanem, Vikas Chandra, Yunyang Xiong Venue: arXiv (2026)\nChain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.\nüìÑ Download PDF\nInside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems Authors: Jihao Zhao, Ding Chen, Zhaoxin Fan, Kerun Xu, Mengting Hu, Bo Tang, Feiyu Xiong, Zhiyu li Venue: arXiv (2026)\nExisting long-term personalized dialogue systems struggle to reconcile unbounded interaction streams with finite context constraints, often succumbing to memory noise accumulation, reasoning degradation, and persona inconsistency. To address these challenges, this paper proposes Inside Out, a framework that utilizes a globally maintained PersonaTree as the carrier of long-term user profiling. By constraining the trunk with an initial schema and updating the branches and leaves, PersonaTree enables controllable growth, achieving memory compression while preserving consistency. Moreover, we train a lightweight MemListener via reinforcement learning with process-based rewards to produce structured, executable, and interpretable {ADD, UPDATE, DELETE, NO_OP} operations, thereby supporting the dynamic evolution of the personalized tree. During response generation, PersonaTree is directly leveraged to enhance outputs in latency-sensitive scenarios; when users require more details, the agentic mode is triggered to introduce details on-demand under the constraints of the PersonaTree. Experiments show that PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. Notably, the small MemListener model achieves memory-operation decision performance comparable to, or even surpassing, powerful reasoning models such as DeepSeek-R1-0528 and Gemini-3-Pro.\nüìÑ Download PDF\nMitigating Simulator Dependence in AI Parameter Inference for the Epoch of Reionization: The Importance of Simulation Diversity Authors: Jasper Solt, Jonathan C. Pober, Stephen H. Bach Venue: arXiv (2026)\nThe 21cm signal of neutral hydrogen contains a wealth of information about the poorly constrained era of cosmological history, the Epoch of Reionization (EoR). Recently, AI models trained on EoR simulations have gained significant attention as a powerful and flexible option for inferring parameters from 21cm observations. However, previous works show that AI models trained on data from one simulator fail to generalize to data from another, raising doubts about AI models‚Äô ability to accurately infer parameters from observation. We develop a new strategy for training AI models on cosmological simulations based on the principle that increasing the diversity of the training dataset improves model robustness by averaging out spurious and contradictory information. We train AI models on data from different combinations of four simulators, then compare the models‚Äô performance when predicting on data from held-out simulators acting as proxies for the real universe. We find that models trained on data from multiple simulators perform better on data from a held-out simulator than models trained on data from a single simulator, indicating that increasing the diversity of the training dataset improves a model‚Äôs ability to generalize. This result suggests that future EoR parameter inference methods can mitigate simulator-specific bias by incorporating multiple simulation approaches into their analyses.\nüìÑ Download PDF\nReducibility of higher-order to pairwise interactions: Social impact models on hypergraphs Authors: Jaume Llabr√©s, Ra√∫l Toral, Maxi San Miguel, Federico V√°zquez Venue: arXiv (2026)\nWe show that a general class of social impact models with higher-order interactions on hypergraphs can be exactly reduced to an equivalent model with pairwise interactions on a weighted projected network. This reduction is made by a mapping that preserves the microscopic probabilities of changing the state of the nodes. As a particular case, we introduce hypergraph-voter models, for which we compute the weights of the projected network both analytically and numerically across several hypergraph ensembles, and we characterize their ordering dynamics through simulations of both higher-order and reduced dynamics. For a linear social impact function (hypergraph-linear voter model) the weights of the projected network are static, allowing us to develop a pair approximation that describes with accuracy the time evolution of macroscopic observables, which turn out to be independent of those weights. The macroscopic dynamics is thus equivalent to that of the standard voter model on the unweighted projected network. For a power-law social impact function (hypergraph-nonlinear voter model) the weights of the projected network depend on the instantaneous system configuration. Nevertheless, the nonlinear voter model on the unweighted projected network still reproduces the main macroscopic trends for well connected hypergraphs.\nüìÑ Download PDF\nWhy Are Some Countries More Politically Fragmented Online Than Others? Authors: Yuan Zhang, Laia Castro, Frank Esser, Alexandre Bovet Venue: arXiv (2026)\nOnline political divisions, such as fragmentation or polarization, are a growing global concern that can foster radicalization and hinder democratic cooperation; however, not all divisions are detrimental, some reflect pluralism and healthy diversity of opinion in a democracy. While prior research has predominantly focused on polarization in the United States, there remains a limited body of research on political divides in multiparty systems, and no universal method for comparing fragmentation across countries. Moreover, cross-country comparison is rare. This study first develops a novel measure of structural political fragmentation built on multi-scale community detection and the effective branching factor. Using a dataset of 18,325 political influencers from Brazil, Spain, and the United States, we assess online fragmentation in their Twitter/X co-following networks. We compare the fragmentation of the three countries, as well as the ideological groups within each. We further investigate factors associated with the level of fragmentation in each country. We find that political fragmentation differs across countries and is asymmetric between ideological groups. Brazil is the most fragmented, with higher fragmentation among the left-wing group, while Spain and the United States exhibit similar overall levels, with the left more fragmented in Spain and the right more fragmented in the United States. Additionally, we find that social identity plays a central role in political fragmentation. A strong alignment between ideological and social identities, with minimal overlap between ideologies, tends to promote greater integration and reduce fragmentation. Our findings provide explanations for cross-national and ideological differences in political fragmentation.\nüìÑ Download PDF\nGraph energy as a measure of community detectability in networks Authors: Lucas B√∂ttcher, Mason A. Porter, Santo Fortunato Venue: arXiv (2026)\nA key challenge in network science is the detection of communities, which are sets of nodes in a network that are densely connected internally but sparsely connected to the rest of the network. A fundamental result in community detection is the existence of a nontrivial threshold for community detectability on sparse graphs that are generated by the planted partition model (PPM). Below this so-called ``detectability limit‚Äô‚Äô, no community-detection method can perform better than random chance. Spectral methods for community detection fail before this detectability limit because the eigenvalues corresponding to the eigenvectors that are relevant for community detection can be absorbed by the bulk of the spectrum. One can bypass the detectability problem by using special matrices, like the non-backtracking matrix, but this requires one to consider higher-dimensional matrices. In this paper, we show that the difference in graph energy between a PPM and an Erd≈ës‚ÄìR√©nyi (ER) network has a distinct transition at the detectability threshold even for the adjacency matrices of the underlying networks. The graph energy is based on the full spectrum of an adjacency matrix, so our result suggests that standard graph matrices still allow one to separate the parameter regions with detectable and undetectable communities.\nüìÑ Download PDF\n","wordCount":"28120","inLanguage":"en","datePublished":"2026-01-11T15:25:07.694214Z","dateModified":"2026-01-11T15:25:07.694214Z","author":{"@type":"Person","name":"Gary"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://garyforreal.me/en/posts/paper/paper-2026-01-11-weekly/"},"publisher":{"@type":"Organization","name":"Gary's House","logo":{"@type":"ImageObject","url":"https://garyforreal.me/img/Q.jpg"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><script src=https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://garyforreal.me/en/ accesskey=h title="Gary's Blog (Alt + H)"><img src=https://garyforreal.me/img/me.jpg alt aria-label=logo height=35>Gary's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://garyforreal.me/zh/ title=‰∏≠Êñá aria-label=‰∏≠Êñá>‰∏≠Êñá</a></li></ul></div></div><ul id=menu><li><a href=https://garyforreal.me/en/search title="üîçSearch (Alt + /)" accesskey=/><span>üîçSearch</span></a></li><li><a href=https://garyforreal.me/en/ title=üè†Homepage><span>üè†Homepage</span></a></li><li><a href=https://garyforreal.me/en/posts title=üìöArticle><span>üìöArticle</span></a></li><li><a href=https://garyforreal.me/en/archives/ title=‚è±Archives><span>‚è±Archives</span></a></li><li><a href=https://garyforreal.me/en/music/ title=üéµmusic><span>üéµmusic</span></a></li><li><a href=https://garyforreal.me/en/about title=üôãüèª‚Äç‚ôÇÔ∏èAbout><span>üôãüèª‚Äç‚ôÇÔ∏èAbout</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://garyforreal.me/en/>Home</a>&nbsp;¬ª&nbsp;<a href=https://garyforreal.me/en/posts/>Posts</a>&nbsp;¬ª&nbsp;<a href=https://garyforreal.me/en/posts/paper/>Paper</a></div><h1 class="post-title entry-hint-parent">Weekly Paper Notes - 2026-01-11</h1><div class=post-meta><span title='2026-01-11 15:25:07.694214 +0000 UTC'>2026-01-11</span>&nbsp;¬∑&nbsp;133 min&nbsp;¬∑&nbsp;133 min&nbsp;¬∑&nbsp;Gary&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://garyforreal.me/zh/posts/paper/paper-2026-01-11-weekly/>‰∏≠Êñá</a></li></ul><div class=meta-item>&nbsp¬∑&nbsp
        <span id=busuanzi_container_page_pv>Êú¨ÊñáÈòÖËØªÈáè<span id=busuanzi_value_page_pv></span>Ê¨°</span></div></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#weekly-paper-notes aria-label="Weekly Paper Notes">Weekly Paper Notes</a><ul><li><a href=#-multilingual aria-label="üîç multilingual">üîç multilingual</a><ul><li><a href=#code-mix-sentiment-analysis-on-hinglish-tweetshttpsarxivorgabs260105091v1 aria-label="Code-Mix Sentiment Analysis on Hinglish Tweets"><a href=https://arxiv.org/abs/2601.05091v1>Code-Mix Sentiment Analysis on Hinglish Tweets</a></a></li><li><a href=#curricullm-designing-personalized-and-workforce-aligned-cybersecurity-curricula-using-fine-tuned-llmshttpsarxivorgabs260104940v1 aria-label="CurricuLLM: Designing Personalized and Workforce-Aligned Cybersecurity Curricula Using Fine-Tuned LLMs"><a href=https://arxiv.org/abs/2601.04940v1>CurricuLLM: Designing Personalized and Workforce-Aligned Cybersecurity Curricula Using Fine-Tuned LLMs</a></a></li><li><a href=#can-ai-generated-persuasion-be-detected-persuaficial-benchmark-and-ai-vs-human-linguistic-differenceshttpsarxivorgabs260104925v1 aria-label="Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences"><a href=https://arxiv.org/abs/2601.04925v1>Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences</a></a></li><li><a href=#scaling-vision-language-models-for-pharmaceutical-long-form-video-reasoning-on-industrial-genai-platformhttpsarxivorgabs260104891v1 aria-label="Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform"><a href=https://arxiv.org/abs/2601.04891v1>Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform</a></a></li><li><a href=#langsae-editing-improving-multilingual-information-retrieval-via-post-hoc-language-identity-removalhttpsarxivorgabs260104768v1 aria-label="LANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal"><a href=https://arxiv.org/abs/2601.04768v1>LANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal</a></a></li><li><a href=#qwen3-vl-embedding-and-qwen3-vl-reranker-a-unified-framework-for-state-of-the-art-multimodal-retrieval-and-rankinghttpsarxivorgabs260104720v1 aria-label="Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"><a href=https://arxiv.org/abs/2601.04720v1>Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking</a></a></li><li><a href=#banglalorica-design-and-evaluation-of-a-robust-watermarking-algorithm-for-large-language-models-in-bangla-text-generationhttpsarxivorgabs260104534v1 aria-label="BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation"><a href=https://arxiv.org/abs/2601.04534v1>BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation</a></a></li><li><a href=#the-overlooked-role-of-graded-relevance-thresholds-in-multilingual-dense-retrievalhttpsarxivorgabs260104395v1 aria-label="The Overlooked Role of Graded Relevance Thresholds in Multilingual Dense Retrieval"><a href=https://arxiv.org/abs/2601.04395v1>The Overlooked Role of Graded Relevance Thresholds in Multilingual Dense Retrieval</a></a></li><li><a href=#dialect-matters-cross-lingual-asr-transfer-for-low-resource-indic-language-varietieshttpsarxivorgabs260104373v1 aria-label="Dialect Matters: Cross-Lingual ASR Transfer for Low-Resource Indic Language Varieties"><a href=https://arxiv.org/abs/2601.04373v1>Dialect Matters: Cross-Lingual ASR Transfer for Low-Resource Indic Language Varieties</a></a></li><li><a href=#cssg-measuring-code-similarity-with-semantic-graphshttpsarxivorgabs260104085v1 aria-label="CSSG: Measuring Code Similarity with Semantic Graphs"><a href=https://arxiv.org/abs/2601.04085v1>CSSG: Measuring Code Similarity with Semantic Graphs</a></a></li><li><a href=#analyzing-and-improving-cross-lingual-knowledge-transfer-for-machine-translationhttpsarxivorgabs260104036v1 aria-label="Analyzing and Improving Cross-lingual Knowledge Transfer for Machine Translation"><a href=https://arxiv.org/abs/2601.04036v1>Analyzing and Improving Cross-lingual Knowledge Transfer for Machine Translation</a></a></li><li><a href=#indextts-25-technical-reporthttpsarxivorgabs260103888v2 aria-label="IndexTTS 2.5 Technical Report"><a href=https://arxiv.org/abs/2601.03888v2>IndexTTS 2.5 Technical Report</a></a></li><li><a href=#prior-informed-zeroth-order-optimization-with-adaptive-direction-alignment-for-memory-efficient-llm-fine-tuninghttpsarxivorgabs260104710v1 aria-label="Prior-Informed Zeroth-Order Optimization with Adaptive Direction Alignment for Memory-Efficient LLM Fine-Tuning"><a href=https://arxiv.org/abs/2601.04710v1>Prior-Informed Zeroth-Order Optimization with Adaptive Direction Alignment for Memory-Efficient LLM Fine-Tuning</a></a></li><li><a href=#aligning-text-code-and-vision-a-multi-objective-reinforcement-learning-framework-for-text-to-visualizationhttpsarxivorgabs260104582v1 aria-label="Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization"><a href=https://arxiv.org/abs/2601.04582v1>Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization</a></a></li><li><a href=#identifying-good-and-bad-neurons-for-task-level-controllable-llmshttpsarxivorgabs260104548v1 aria-label="Identifying Good and Bad Neurons for Task-Level Controllable LLMs"><a href=https://arxiv.org/abs/2601.04548v1>Identifying Good and Bad Neurons for Task-Level Controllable LLMs</a></a></li><li><a href=#non-homogeneous-markov-switching-generalized-additive-models-for-location-scale-and-shapehttpsarxivorgabs260103760v1 aria-label="Non-Homogeneous Markov-Switching Generalized Additive Models for Location, Scale, and Shape"><a href=https://arxiv.org/abs/2601.03760v1>Non-Homogeneous Markov-Switching Generalized Additive Models for Location, Scale, and Shape</a></a></li><li><a href=#ola-output-language-alignment-in-code-switched-llm-interactionshttpsarxivorgabs260103589v1 aria-label="OLA: Output Language Alignment in Code-Switched LLM Interactions"><a href=https://arxiv.org/abs/2601.03589v1>OLA: Output Language Alignment in Code-Switched LLM Interactions</a></a></li><li><a href=#the-winds-of-oba-hypergiants-and-luminous-blue-variables-dynamically-consistent-atmosphere-models-reveal-multiple-wind-regimeshttpsarxivorgabs260103072v1 aria-label="The winds of OBA hypergiants and luminous blue variables: Dynamically-consistent atmosphere models reveal multiple wind regimes"><a href=https://arxiv.org/abs/2601.03072v1>The winds of OBA hypergiants and luminous blue variables: Dynamically-consistent atmosphere models reveal multiple wind regimes</a></a></li><li><a href=#ai-native-6g-physical-layer-with-cross-module-optimization-and-cooperative-control-agentshttpsarxivorgabs260102827v2 aria-label="AI-Native 6G Physical Layer with Cross-Module Optimization and Cooperative Control Agents"><a href=https://arxiv.org/abs/2601.02827v2>AI-Native 6G Physical Layer with Cross-Module Optimization and Cooperative Control Agents</a></a></li><li><a href=#dynamically-consistent-analysis-of-galactic-wn4b-starshttpsarxivorgabs260102498v1 aria-label="Dynamically consistent analysis of Galactic WN4b stars"><a href=https://arxiv.org/abs/2601.02498v1>Dynamically consistent analysis of Galactic WN4b stars</a></a></li><li><a href=#aspect-extraction-from-e-commerce-product-and-service-reviewshttpsarxivorgabs260101827v1 aria-label="Aspect Extraction from E-Commerce Product and Service Reviews"><a href=https://arxiv.org/abs/2601.01827v1>Aspect Extraction from E-Commerce Product and Service Reviews</a></a></li><li><a href=#an-adaptive-power-division-strategy-for-nonlinear-components-in-rectificationhttpsarxivorgabs260104598v1 aria-label="An Adaptive Power Division Strategy for Nonlinear Components in Rectification"><a href=https://arxiv.org/abs/2601.04598v1>An Adaptive Power Division Strategy for Nonlinear Components in Rectification</a></a></li><li><a href=#last_0-latent-spatio-temporal-chain-of-thought-for-robotic-vision-language-action-modelhttpsarxivorgabs260105248v1 aria-label="LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model"><a href=https://arxiv.org/abs/2601.05248v1>LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model</a></a></li><li><a href=#grex-generalized-referring-expression-segmentation-comprehension-and-generationhttpsarxivorgabs260105244v1 aria-label="GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation"><a href=https://arxiv.org/abs/2601.05244v1>GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation</a></a></li><li><a href=#gdpo-group-reward-decoupled-normalization-policy-optimization-for-multi-reward-rl-optimizationhttpsarxivorgabs260105242v1 aria-label="GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization"><a href=https://arxiv.org/abs/2601.05242v1>GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization</a></a></li><li><a href=#robovip-multi-view-video-generation-with-visual-identity-prompting-augments-robot-manipulationhttpsarxivorgabs260105241v1 aria-label="RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation"><a href=https://arxiv.org/abs/2601.05241v1>RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation</a></a></li><li><a href=#robust-reasoning-as-a-symmetry-protected-topological-phasehttpsarxivorgabs260105240v1 aria-label="Robust Reasoning as a Symmetry-Protected Topological Phase"><a href=https://arxiv.org/abs/2601.05240v1>Robust Reasoning as a Symmetry-Protected Topological Phase</a></a></li><li><a href=#measuring-and-fostering-peace-through-machine-learning-and-artificial-intelligencehttpsarxivorgabs260105232v1 aria-label="Measuring and Fostering Peace through Machine Learning and Artificial Intelligence"><a href=https://arxiv.org/abs/2601.05232v1>Measuring and Fostering Peace through Machine Learning and Artificial Intelligence</a></a></li><li><a href=#generate-transfer-adapt-learning-functional-dexterous-grasping-from-a-single-human-demonstrationhttpsarxivorgabs260105243v1 aria-label="Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration"><a href=https://arxiv.org/abs/2601.05243v1>Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration</a></a></li><li><a href=#learning-latent-action-world-models-in-the-wildhttpsarxivorgabs260105230v1 aria-label="Learning Latent Action World Models In The Wild"><a href=https://arxiv.org/abs/2601.05230v1>Learning Latent Action World Models In The Wild</a></a></li><li><a href=#local-multimodal-dynamics-in-mixed-ionic-electronic-conductors-and-their-fingerprints-in-organic-electrochemical-transistor-operationhttpsarxivorgabs260105179v1 aria-label="Local Multimodal Dynamics in Mixed Ionic-Electronic Conductors and Their Fingerprints in Organic Electrochemical Transistor Operation"><a href=https://arxiv.org/abs/2601.05179v1>Local Multimodal Dynamics in Mixed Ionic-Electronic Conductors and Their Fingerprints in Organic Electrochemical Transistor Operation</a></a></li><li><a href=#quantum-elastic-network-models-and-their-application-to-graphenehttpsarxivorgabs260105161v1 aria-label="Quantum Elastic Network Models and their Application to Graphene"><a href=https://arxiv.org/abs/2601.05161v1>Quantum Elastic Network Models and their Application to Graphene</a></a></li><li><a href=#machine-learning-for-radiative-hydrodynamics-in-astrophysicshttpsarxivorgabs260105155v1 aria-label="Machine learning for radiative hydrodynamics in astrophysics"><a href=https://arxiv.org/abs/2601.05155v1>Machine learning for radiative hydrodynamics in astrophysics</a></a></li><li><a href=#re-align-structured-reasoning-guided-alignment-for-in-context-image-generation-and-editinghttpsarxivorgabs260105124v1 aria-label="Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing"><a href=https://arxiv.org/abs/2601.05124v1>Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing</a></a></li><li><a href=#three-dimensional-scene-reconstruction-using-roman-slitless-spectrahttpsarxivorgabs260105233v1 aria-label="Three-dimensional scene reconstruction using Roman slitless spectra"><a href=https://arxiv.org/abs/2601.05233v1>Three-dimensional scene reconstruction using Roman slitless spectra</a></a></li><li><a href=#estimating-consensus-ideal-points-using-multi-source-datahttpsarxivorgabs260105213v1 aria-label="Estimating Consensus Ideal Points Using Multi-Source Data"><a href=https://arxiv.org/abs/2601.05213v1>Estimating Consensus Ideal Points Using Multi-Source Data</a></a></li><li><a href=#an-interpretable-data-driven-approach-to-optimizing-clinical-fall-risk-assessmenthttpsarxivorgabs260105194v1 aria-label="An interpretable data-driven approach to optimizing clinical fall risk assessment"><a href=https://arxiv.org/abs/2601.05194v1>An interpretable data-driven approach to optimizing clinical fall risk assessment</a></a></li><li><a href=#information-theoretic-limits-on-exact-subgraph-alignment-problemhttpsarxivorgabs260105173v1 aria-label="Information-Theoretic Limits on Exact Subgraph Alignment Problem"><a href=https://arxiv.org/abs/2601.05173v1>Information-Theoretic Limits on Exact Subgraph Alignment Problem</a></a></li><li><a href=#cov-chain-of-view-prompting-for-spatial-reasoninghttpsarxivorgabs260105172v1 aria-label="CoV: Chain-of-View Prompting for Spatial Reasoning"><a href=https://arxiv.org/abs/2601.05172v1>CoV: Chain-of-View Prompting for Spatial Reasoning</a></a></li><li><a href=#plenoptic-video-generationhttpsarxivorgabs260105239v1 aria-label="Plenoptic Video Generation"><a href=https://arxiv.org/abs/2601.05239v1>Plenoptic Video Generation</a></a></li><li><a href=#multivector-reranking-in-the-era-of-strong-first-stage-retrievershttpsarxivorgabs260105200v1 aria-label="Multivector Reranking in the Era of Strong First-Stage Retrievers"><a href=https://arxiv.org/abs/2601.05200v1>Multivector Reranking in the Era of Strong First-Stage Retrievers</a></a></li><li><a href=#divide-and-conquer-cluster-and-manifold-based-interpretation-of-complex-flowshttpsarxivorgabs260105117v1 aria-label="Divide and Conquer: Cluster and manifold-based interpretation of complex flows"><a href=https://arxiv.org/abs/2601.05117v1>Divide and Conquer: Cluster and manifold-based interpretation of complex flows</a></a></li><li><a href=#multi-disciplinary-dataset-discovery-from-citation-verified-literature-contextshttpsarxivorgabs260105099v1 aria-label="Multi-Disciplinary Dataset Discovery from Citation-Verified Literature Contexts"><a href=https://arxiv.org/abs/2601.05099v1>Multi-Disciplinary Dataset Discovery from Citation-Verified Literature Contexts</a></a></li><li><a href=#surveying-exogenous-species-in-saturn-with-alma-i-detecting-and-mapping-cohttpsarxivorgabs260105090v1 aria-label="Surveying exogenous species in Saturn with ALMA I. Detecting and Mapping CO"><a href=https://arxiv.org/abs/2601.05090v1>Surveying exogenous species in Saturn with ALMA I. Detecting and Mapping CO</a></a></li></ul></li><li><a href=#-linguistics aria-label="üîç linguistics">üîç linguistics</a><ul><li><a href=#rl-awb-deep-reinforcement-learning-for-auto-white-balance-correction-in-low-light-night-time-sceneshttpsarxivorgabs260105249v1 aria-label="RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes"><a href=https://arxiv.org/abs/2601.05249v1>RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes</a></a></li><li><a href=#pixel-perfect-visual-geometry-estimationhttpsarxivorgabs260105246v1 aria-label="Pixel-Perfect Visual Geometry Estimation"><a href=https://arxiv.org/abs/2601.05246v1>Pixel-Perfect Visual Geometry Estimation</a></a></li><li><a href=#optimal-lower-bounds-for-online-multicalibrationhttpsarxivorgabs260105245v1 aria-label="Optimal Lower Bounds for Online Multicalibration"><a href=https://arxiv.org/abs/2601.05245v1>Optimal Lower Bounds for Online Multicalibration</a></a></li><li><a href=#unveiling-the-3d-structure-of-the-central-molecular-zone-from-stellar-kinematics-and-photometry-the-50-and-20-kms-cloudshttpsarxivorgabs260105252v1 aria-label="Unveiling the 3D structure of the central molecular zone from stellar kinematics and photometry: The 50 and 20 km/s clouds"><a href=https://arxiv.org/abs/2601.05252v1>Unveiling the 3D structure of the central molecular zone from stellar kinematics and photometry: The 50 and 20 km/s clouds</a></a></li><li><a href=#mesh4d-4d-mesh-reconstruction-and-tracking-from-monocular-videohttpsarxivorgabs260105251v1 aria-label="Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video"><a href=https://arxiv.org/abs/2601.05251v1>Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video</a></a></li><li><a href=#stability-of-the-local-ni2-electronic-structure-to-a-site-disorder-in-the-pyrochlore-antiferromagnet-nacani_2f_7httpsarxivorgabs260105236v1 aria-label="Stability of the Local Ni$^{2+}$ Electronic Structure to $A$-site Disorder in the Pyrochlore Antiferromagnet NaCaNi$_2$F$_7$"><a href=https://arxiv.org/abs/2601.05236v1>Stability of the Local Ni$^{2+}$ Electronic Structure to $A$-site Disorder in the Pyrochlore Antiferromagnet NaCaNi$_2$F$_7$</a></a></li><li><a href=#mimicking-phantom-dark-energy-with-evolving-dark-matter-masshttpsarxivorgabs260105235v1 aria-label="Mimicking Phantom Dark Energy with Evolving Dark Matter Mass"><a href=https://arxiv.org/abs/2601.05235v1>Mimicking Phantom Dark Energy with Evolving Dark Matter Mass</a></a></li><li><a href=#stochastic-deep-learning-a-probabilistic-framework-for-modeling-uncertainty-in-structured-temporal-datahttpsarxivorgabs260105227v1 aria-label="Stochastic Deep Learning: A Probabilistic Framework for Modeling Uncertainty in Structured Temporal Data"><a href=https://arxiv.org/abs/2601.05227v1>Stochastic Deep Learning: A Probabilistic Framework for Modeling Uncertainty in Structured Temporal Data</a></a></li><li><a href=#measurement-of-the-higgs-boson-total-decay-width-using-the-h-to-ww-to-e%ce%bd%ce%bc%ce%bd-decay-channel-in-proton-proton-collisions-at-sqrts--13-tevhttpsarxivorgabs260105168v1 aria-label="Measurement of the Higgs boson total decay width using the H $\to$ WW $\to$ e$ŒΩŒºŒΩ$ decay channel in proton-proton collisions at $\sqrt{s}$ = 13 TeV"><a href=https://arxiv.org/abs/2601.05168v1>Measurement of the Higgs boson total decay width using the H $\to$ WW $\to$ e$ŒΩŒºŒΩ$ decay channel in proton-proton collisions at $\sqrt{s}$ = 13 TeV</a></a></li><li><a href=#a-first-principles-study-of-weyl-nodal-loop-and-multiple-sets-of-weyl-points-in-trigonal-ptbi_2httpsarxivorgabs260105123v1 aria-label="A First-principles Study of Weyl Nodal Loop and Multiple Sets of Weyl Points in Trigonal PtBi$_2$"><a href=https://arxiv.org/abs/2601.05123v1>A First-principles Study of Weyl Nodal Loop and Multiple Sets of Weyl Points in Trigonal PtBi$_2$</a></a></li><li><a href=#multigroup-radiation-diffusion-on-a-moving-mesh-implementation-in-rich-and-application-to-tidal-disruption-eventshttpsarxivorgabs260105120v1 aria-label="Multigroup Radiation Diffusion on a Moving Mesh: Implementation in RICH and Application to Tidal Disruption Events"><a href=https://arxiv.org/abs/2601.05120v1>Multigroup Radiation Diffusion on a Moving Mesh: Implementation in RICH and Application to Tidal Disruption Events</a></a></li><li><a href=#evaluative-fingerprints-stable-and-systematic-differences-in-llm-evaluator-behaviorhttpsarxivorgabs260105114v1 aria-label="Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior"><a href=https://arxiv.org/abs/2601.05114v1>Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior</a></a></li><li><a href=#semantically-orthogonal-framework-for-citation-classification-disentangling-intent-and-contenthttpsarxivorgabs260105103v1 aria-label="Semantically Orthogonal Framework for Citation Classification: Disentangling Intent and Content"><a href=https://arxiv.org/abs/2601.05103v1>Semantically Orthogonal Framework for Citation Classification: Disentangling Intent and Content</a></a></li><li><a href=#qnerf-neural-radiance-fields-on-a-simulated-gate-based-quantum-computerhttpsarxivorgabs260105250v1 aria-label="QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer"><a href=https://arxiv.org/abs/2601.05250v1>QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer</a></a></li><li><a href=#mechanics-of-axis-formation-in-textithydrahttpsarxivorgabs260105220v1 aria-label="Mechanics of axis formation in $\textit{Hydra}$"><a href=https://arxiv.org/abs/2601.05220v1>Mechanics of axis formation in $\textit{Hydra}$</a></a></li><li><a href=#raar-retrieval-augmented-agentic-reasoning-for-cross-domain-misinformation-detectionhttpsarxivorgabs260104853v1 aria-label="RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection"><a href=https://arxiv.org/abs/2601.04853v1>RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection</a></a></li><li><a href=#defense-against-indirect-prompt-injection-via-tool-result-parsinghttpsarxivorgabs260104795v1 aria-label="Defense Against Indirect Prompt Injection via Tool Result Parsing"><a href=https://arxiv.org/abs/2601.04795v1>Defense Against Indirect Prompt Injection via Tool Result Parsing</a></a></li><li><a href=#scifig-towards-automating-scientific-figure-generationhttpsarxivorgabs260104390v1 aria-label="SciFig: Towards Automating Scientific Figure Generation"><a href=https://arxiv.org/abs/2601.04390v1>SciFig: Towards Automating Scientific Figure Generation</a></a></li><li><a href=#disco-rag-discourse-aware-retrieval-augmented-generationhttpsarxivorgabs260104377v1 aria-label="Disco-RAG: Discourse-Aware Retrieval-Augmented Generation"><a href=https://arxiv.org/abs/2601.04377v1>Disco-RAG: Discourse-Aware Retrieval-Augmented Generation</a></a></li><li><a href=#llmberjack-guided-trimming-of-debate-trees-for-multi-party-conversation-creationhttpsarxivorgabs260104135v1 aria-label="LLMberjack: Guided Trimming of Debate Trees for Multi-Party Conversation Creation"><a href=https://arxiv.org/abs/2601.04135v1>LLMberjack: Guided Trimming of Debate Trees for Multi-Party Conversation Creation</a></a></li><li><a href=#the-bathtub-of-european-ai-governance-identifying-technical-sandboxes-as-the-micro-foundation-of-regulatory-learninghttpsarxivorgabs260104094v1 aria-label="The Bathtub of European AI Governance: Identifying Technical Sandboxes as the Micro-Foundation of Regulatory Learning"><a href=https://arxiv.org/abs/2601.04094v1>The Bathtub of European AI Governance: Identifying Technical Sandboxes as the Micro-Foundation of Regulatory Learning</a></a></li><li><a href=#symbolically-regressing-dark-matter-halo-profiles-using-weak-lensinghttpsarxivorgabs260105203v1 aria-label="Symbolically regressing dark matter halo profiles using weak lensing"><a href=https://arxiv.org/abs/2601.05203v1>Symbolically regressing dark matter halo profiles using weak lensing</a></a></li><li><a href=#beyond-the-imbalance-site-resolved-dynamics-probing-resonances-in-many-body-localizationhttpsarxivorgabs260105177v1 aria-label="Beyond the imbalance: site-resolved dynamics probing resonances in many-body localization"><a href=https://arxiv.org/abs/2601.05177v1>Beyond the imbalance: site-resolved dynamics probing resonances in many-body localization</a></a></li><li><a href=#vision-language-introspection-mitigating-overconfident-hallucinations-in-mllms-via-interpretable-bi-causal-steeringhttpsarxivorgabs260105159v1 aria-label="Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering"><a href=https://arxiv.org/abs/2601.05159v1>Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering</a></a></li><li><a href=#pc2-politically-controversial-content-generation-via-jailbreaking-attacks-on-gpt-based-text-to-image-modelshttpsarxivorgabs260105150v1 aria-label="$PC^2$: Politically Controversial Content Generation via Jailbreaking Attacks on GPT-based Text-to-Image Models"><a href=https://arxiv.org/abs/2601.05150v1>$PC^2$: Politically Controversial Content Generation via Jailbreaking Attacks on GPT-based Text-to-Image Models</a></a></li><li><a href=#when-and-why-non-hermitian-eigenvalues-miss-eigenstates-in-topological-physicshttpsarxivorgabs260105234v1 aria-label="When and why non-Hermitian eigenvalues miss eigenstates in topological physics"><a href=https://arxiv.org/abs/2601.05234v1>When and why non-Hermitian eigenvalues miss eigenstates in topological physics</a></a></li><li><a href=#from-rays-to-projections-better-inputs-for-feed-forward-view-synthesishttpsarxivorgabs260105116v1 aria-label="From Rays to Projections: Better Inputs for Feed-Forward View Synthesis"><a href=https://arxiv.org/abs/2601.05116v1>From Rays to Projections: Better Inputs for Feed-Forward View Synthesis</a></a></li><li><a href=#horn-inequalities-on-a-quiver-with-an-involutionhttpsarxivorgabs260105089v1 aria-label="Horn inequalities on a quiver with an involution"><a href=https://arxiv.org/abs/2601.05089v1>Horn inequalities on a quiver with an involution</a></a></li><li><a href=#effect-of-dispatch-decisions-on-small-signal-stability-of-converter-dominated-power-systemshttpsarxivorgabs260105070v1 aria-label="Effect of Dispatch Decisions on Small-Signal Stability of Converter-Dominated Power Systems"><a href=https://arxiv.org/abs/2601.05070v1>Effect of Dispatch Decisions on Small-Signal Stability of Converter-Dominated Power Systems</a></a></li><li><a href=#approximate-equivariance-via-projection-based-regularisationhttpsarxivorgabs260105028v1 aria-label="Approximate equivariance via projection-based regularisation"><a href=https://arxiv.org/abs/2601.05028v1>Approximate equivariance via projection-based regularisation</a></a></li><li><a href=#the-squirrel-parser-a-linear-time-peg-packrat-parser-capable-of-left-recursion-and-optimal-error-recoveryhttpsarxivorgabs260105012v1 aria-label="The Squirrel Parser: A Linear-Time PEG Packrat Parser Capable of Left Recursion and Optimal Error Recovery"><a href=https://arxiv.org/abs/2601.05012v1>The Squirrel Parser: A Linear-Time PEG Packrat Parser Capable of Left Recursion and Optimal Error Recovery</a></a></li></ul></li><li><a href=#-psycholinguistics aria-label="üîç psycholinguistics">üîç psycholinguistics</a><ul><li><a href=#random-models-and-guarded-logichttpsarxivorgabs260105247v1 aria-label="Random Models and Guarded Logic"><a href=https://arxiv.org/abs/2601.05247v1>Random Models and Guarded Logic</a></a></li><li><a href=#concurrent-balanced-augmented-treeshttpsarxivorgabs260105225v1 aria-label="Concurrent Balanced Augmented Trees"><a href=https://arxiv.org/abs/2601.05225v1>Concurrent Balanced Augmented Trees</a></a></li><li><a href=#earl-energy-aware-optimization-of-liquid-state-machines-for-pervasive-aihttpsarxivorgabs260105205v1 aria-label="EARL: Energy-Aware Optimization of Liquid State Machines for Pervasive AI"><a href=https://arxiv.org/abs/2601.05205v1>EARL: Energy-Aware Optimization of Liquid State Machines for Pervasive AI</a></a></li><li><a href=#simuagent-an-llm-based-simulink-modeling-assistant-enhanced-with-reinforcement-learninghttpsarxivorgabs260105187v1 aria-label="SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning"><a href=https://arxiv.org/abs/2601.05187v1>SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning</a></a></li><li><a href=#not-all-steps-are-informative-on-the-linearity-of-llms-rlvr-traininghttpsarxivorgabs260104537v1 aria-label="Not All Steps are Informative: On the Linearity of LLMs&rsquo; RLVR Training"><a href=https://arxiv.org/abs/2601.04537v1>Not All Steps are Informative: On the Linearity of LLMs&rsquo; RLVR Training</a></a></li><li><a href=#quantifying-the-effect-of-test-set-contamination-on-generative-evaluationshttpsarxivorgabs260104301v1 aria-label="Quantifying the Effect of Test Set Contamination on Generative Evaluations"><a href=https://arxiv.org/abs/2601.04301v1>Quantifying the Effect of Test Set Contamination on Generative Evaluations</a></a></li><li><a href=#fairness-in-opinion-dynamicshttpsarxivorgabs260103859v1 aria-label="Fairness in Opinion Dynamics"><a href=https://arxiv.org/abs/2601.03859v1>Fairness in Opinion Dynamics</a></a></li><li><a href=#probabilistic-transformers-for-joint-modeling-of-global-weather-dynamics-and-decision-centric-variableshttpsarxivorgabs260103753v1 aria-label="Probabilistic Transformers for Joint Modeling of Global Weather Dynamics and Decision-Centric Variables"><a href=https://arxiv.org/abs/2601.03753v1>Probabilistic Transformers for Joint Modeling of Global Weather Dynamics and Decision-Centric Variables</a></a></li><li><a href=#on-flying-through-the-base-of-a-pseudo-streamerhttpsarxivorgabs260103620v1 aria-label="On flying through the base of a pseudo-streamer"><a href=https://arxiv.org/abs/2601.03620v1>On flying through the base of a pseudo-streamer</a></a></li><li><a href=#listen-to-the-unexpected-self-supervised-surprise-detection-for-efficient-viewport-predictionhttpsarxivorgabs260102629v1 aria-label="Listen to the Unexpected: Self-Supervised Surprise Detection for Efficient Viewport Prediction"><a href=https://arxiv.org/abs/2601.02629v1>Listen to the Unexpected: Self-Supervised Surprise Detection for Efficient Viewport Prediction</a></a></li><li><a href=#control-of-the-mote_2-fermi-surface-by-nb-dopinghttpsarxivorgabs260105197v1 aria-label="Control of the MoTe$_2$ Fermi Surface by Nb Doping"><a href=https://arxiv.org/abs/2601.05197v1>Control of the MoTe$_2$ Fermi Surface by Nb Doping</a></a></li><li><a href=#fundamental-tradeoffs-for-isac-multiple-access-in-finite-blocklength-regimehttpsarxivorgabs260105165v1 aria-label="Fundamental Tradeoffs for ISAC Multiple Access in Finite-Blocklength Regime"><a href=https://arxiv.org/abs/2601.05165v1>Fundamental Tradeoffs for ISAC Multiple Access in Finite-Blocklength Regime</a></a></li><li><a href=#superluminal-modes-in-a-quantum-field-simulator-for-cosmology-from-analog-transplanckian-physicshttpsarxivorgabs260105141v1 aria-label="Superluminal modes in a quantum field simulator for cosmology from analog Transplanckian physics"><a href=https://arxiv.org/abs/2601.05141v1>Superluminal modes in a quantum field simulator for cosmology from analog Transplanckian physics</a></a></li><li><a href=#enhanced-electron-reflectionat-mott-insulator-interfaceshttpsarxivorgabs260105140v1 aria-label="Enhanced Electron Reflectionat Mott-Insulator Interfaces"><a href=https://arxiv.org/abs/2601.05140v1>Enhanced Electron Reflectionat Mott-Insulator Interfaces</a></a></li><li><a href=#how-dark-is-dark-a-reflectance-and-scattering-analysis-of-black-materialshttpsarxivorgabs260105094v1 aria-label="How Dark is Dark? A Reflectance and Scattering Analysis of Black Materials"><a href=https://arxiv.org/abs/2601.05094v1>How Dark is Dark? A Reflectance and Scattering Analysis of Black Materials</a></a></li><li><a href=#preconditioned-multivariate-quantum-solution-extractionhttpsarxivorgabs260105077v1 aria-label="Preconditioned Multivariate Quantum Solution Extraction"><a href=https://arxiv.org/abs/2601.05077v1>Preconditioned Multivariate Quantum Solution Extraction</a></a></li><li><a href=#geometric-developmental-principles-for-the-emergence-of-brain-like-weighted-and-directed-neuronal-networkshttpsarxivorgabs260105021v1 aria-label="Geometric developmental principles for the emergence of brain-like weighted and directed neuronal networks"><a href=https://arxiv.org/abs/2601.05021v1>Geometric developmental principles for the emergence of brain-like weighted and directed neuronal networks</a></a></li><li><a href=#h%c3%a1n-d%c4%81n-xu%c3%a9-b%c3%b9-mimicry-or-q%c4%abng-ch%c5%ab-y%c3%ba-l%c3%a1n-mastery-a-cognitive-perspective-on-reasoning-distillation-in-large-language-modelshttpsarxivorgabs260105019v1 aria-label="H√°n DƒÅn Xu√© B√π (Mimicry) or Qƒ´ng Ch≈´ Y√∫ L√°n (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models"><a href=https://arxiv.org/abs/2601.05019v1>H√°n DƒÅn Xu√© B√π (Mimicry) or Qƒ´ng Ch≈´ Y√∫ L√°n (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models</a></a></li><li><a href=#conmax-confidence-maximizing-compression-for-efficient-chain-of-thought-reasoninghttpsarxivorgabs260104973v1 aria-label="ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning"><a href=https://arxiv.org/abs/2601.04973v1>ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning</a></a></li><li><a href=#breaking-robustness-barriers-in-cognitive-diagnosis-a-one-shot-neural-architecture-search-perspectivehttpsarxivorgabs260104918v1 aria-label="Breaking Robustness Barriers in Cognitive Diagnosis: A One-Shot Neural Architecture Search Perspective"><a href=https://arxiv.org/abs/2601.04918v1>Breaking Robustness Barriers in Cognitive Diagnosis: A One-Shot Neural Architecture Search Perspective</a></a></li><li><a href=#onomacompass-a-texture-exploration-interface-that-shuttles-between-words-and-imageshttpsarxivorgabs260104915v1 aria-label="OnomaCompass: A Texture Exploration Interface that Shuttles between Words and Images"><a href=https://arxiv.org/abs/2601.04915v1>OnomaCompass: A Texture Exploration Interface that Shuttles between Words and Images</a></a></li><li><a href=#mind2report-a-cognitive-deep-research-agent-for-expert-level-commercial-report-synthesishttpsarxivorgabs260104879v1 aria-label="Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis"><a href=https://arxiv.org/abs/2601.04879v1>Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis</a></a></li></ul></li><li><a href=#-llm aria-label="üîç llm">üîç llm</a><ul><li><a href=#caos-conformal-aggregation-of-one-shot-predictorshttpsarxivorgabs260105219v1 aria-label="CAOS: Conformal Aggregation of One-Shot Predictors"><a href=https://arxiv.org/abs/2601.05219v1>CAOS: Conformal Aggregation of One-Shot Predictors</a></a></li><li><a href=#how-many-body-chaos-emerges-in-the-presence-of-quasiparticleshttpsarxivorgabs260105238v1 aria-label="How many-body chaos emerges in the presence of quasiparticles"><a href=https://arxiv.org/abs/2601.05238v1>How many-body chaos emerges in the presence of quasiparticles</a></a></li><li><a href=#fluctuation-response-relation-for-a-nonequilibrium-system-with-resolved-markovian-embeddinghttpsarxivorgabs260105198v1 aria-label="Fluctuation-response relation for a nonequilibrium system with resolved Markovian embedding"><a href=https://arxiv.org/abs/2601.05198v1>Fluctuation-response relation for a nonequilibrium system with resolved Markovian embedding</a></a></li><li><a href=#basis-number-of-graphs-excluding-minorshttpsarxivorgabs260105195v1 aria-label="Basis Number of Graphs Excluding Minors"><a href=https://arxiv.org/abs/2601.05195v1>Basis Number of Graphs Excluding Minors</a></a></li><li><a href=#hydrodynamic-interactions-in-a-binary-mixture-colloidal-monolayerhttpsarxivorgabs260105182v1 aria-label="Hydrodynamic interactions in a binary-mixture colloidal monolayer"><a href=https://arxiv.org/abs/2601.05182v1>Hydrodynamic interactions in a binary-mixture colloidal monolayer</a></a></li><li><a href=#minenpc-task-task-suite-for-memory-aware-minecraft-agentshttpsarxivorgabs260105215v1 aria-label="MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents"><a href=https://arxiv.org/abs/2601.05215v1>MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents</a></a></li><li><a href=#internal-representations-as-indicators-of-hallucinations-in-agent-tool-selectionhttpsarxivorgabs260105214v1 aria-label="Internal Representations as Indicators of Hallucinations in Agent Tool Selection"><a href=https://arxiv.org/abs/2601.05214v1>Internal Representations as Indicators of Hallucinations in Agent Tool Selection</a></a></li><li><a href=#cat-states-and-violation-of-the-bell-chsh-inequality-in-relativistic-quantum-field-theoryhttpsarxivorgabs260105216v1 aria-label="Cat states and violation of the Bell-CHSH inequality in relativistic Quantum Field Theory"><a href=https://arxiv.org/abs/2601.05216v1>Cat states and violation of the Bell-CHSH inequality in relativistic Quantum Field Theory</a></a></li></ul></li><li><a href=#-neuroscience aria-label="üîç neuroscience">üîç neuroscience</a><ul><li><a href=#eclipse-an-evolutionary-computation-library-for-instrumentation-prototyping-in-scientific-engineeringhttpsarxivorgabs260105098v1 aria-label="ECLIPSE: An Evolutionary Computation Library for Instrumentation Prototyping in Scientific Engineering"><a href=https://arxiv.org/abs/2601.05098v1>ECLIPSE: An Evolutionary Computation Library for Instrumentation Prototyping in Scientific Engineering</a></a></li><li><a href=#driver-intention-prediction-with-deep-learning-real-time-brain-to-vehicle-communicationhttpsarxivorgabs260105084v1 aria-label="Driver-Intention Prediction with Deep Learning: Real-Time Brain-to-Vehicle Communication"><a href=https://arxiv.org/abs/2601.05084v1>Driver-Intention Prediction with Deep Learning: Real-Time Brain-to-Vehicle Communication</a></a></li><li><a href=#on-the-effects-of-protection-zone-and-directed-population-flux-in-prey-predator-dynamicshttpsarxivorgabs260105054v1 aria-label="On the effects of protection zone and directed population flux in prey-predator dynamics"><a href=https://arxiv.org/abs/2601.05054v1>On the effects of protection zone and directed population flux in prey-predator dynamics</a></a></li></ul></li><li><a href=#-data_resources aria-label="üîç data_resources">üîç data_resources</a><ul><li><a href=#objectforesight-predicting-future-3d-object-trajectories-from-human-videoshttpsarxivorgabs260105237v1 aria-label="ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos"><a href=https://arxiv.org/abs/2601.05237v1>ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos</a></a></li><li><a href=#v-fat-benchmarking-visual-fidelity-against-text-biashttpsarxivorgabs260104897v1 aria-label="V-FAT: Benchmarking Visual Fidelity Against Text-bias"><a href=https://arxiv.org/abs/2601.04897v1>V-FAT: Benchmarking Visual Fidelity Against Text-bias</a></a></li><li><a href=#dvd-a-robust-method-for-detecting-variant-contamination-in-large-language-model-evaluationhttpsarxivorgabs260104895v1 aria-label="DVD: A Robust Method for Detecting Variant Contamination in Large Language Model Evaluation"><a href=https://arxiv.org/abs/2601.04895v1>DVD: A Robust Method for Detecting Variant Contamination in Large Language Model Evaluation</a></a></li><li><a href=#higher-order-knowledge-representations-for-agentic-scientific-reasoninghttpsarxivorgabs260104878v1 aria-label="Higher-Order Knowledge Representations for Agentic Scientific Reasoning"><a href=https://arxiv.org/abs/2601.04878v1>Higher-Order Knowledge Representations for Agentic Scientific Reasoning</a></a></li><li><a href=#versecrafter-dynamic-realistic-video-world-model-with-4d-geometric-controlhttpsarxivorgabs260105138v1 aria-label="VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control"><a href=https://arxiv.org/abs/2601.05138v1>VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control</a></a></li><li><a href=#chain-of-sanitized-thoughts-plugging-pii-leakage-in-cot-of-large-reasoning-modelshttpsarxivorgabs260105076v1 aria-label="Chain-of-Sanitized-Thoughts: Plugging PII Leakage in CoT of Large Reasoning Models"><a href=https://arxiv.org/abs/2601.05076v1>Chain-of-Sanitized-Thoughts: Plugging PII Leakage in CoT of Large Reasoning Models</a></a></li><li><a href=#from-understanding-to-engagement-personalized-pharmacy-video-clips-via-vision-language-models-vlmshttpsarxivorgabs260105059v1 aria-label="From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)"><a href=https://arxiv.org/abs/2601.05059v1>From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)</a></a></li><li><a href=#leveraging-prediction-entropy-for-automatic-prompt-weighting-in-zero-shot-audio-language-classificationhttpsarxivorgabs260105011v1 aria-label="Leveraging Prediction Entropy for Automatic Prompt Weighting in Zero-Shot Audio-Language Classification"><a href=https://arxiv.org/abs/2601.05011v1>Leveraging Prediction Entropy for Automatic Prompt Weighting in Zero-Shot Audio-Language Classification</a></a></li><li><a href=#genprove-learning-to-generate-text-with-fine-grained-provenancehttpsarxivorgabs260104932v1 aria-label="GenProve: Learning to Generate Text with Fine-Grained Provenance"><a href=https://arxiv.org/abs/2601.04932v1>GenProve: Learning to Generate Text with Fine-Grained Provenance</a></a></li></ul></li><li><a href=#-emotion_language aria-label="üîç emotion_language">üîç emotion_language</a><ul><li><a href=#addressing-known-challenges-in-solar-flare-forecasting-i-limb-flare-prediction-with-a-4-pi-full-heliosphere-frameworkhttpsarxivorgabs260105209v1 aria-label="Addressing Known Challenges in Solar Flare Forecasting I: Limb-Flare Prediction with a 4-pi Full-Heliosphere Framework"><a href=https://arxiv.org/abs/2601.05209v1>Addressing Known Challenges in Solar Flare Forecasting I: Limb-Flare Prediction with a 4-pi Full-Heliosphere Framework</a></a></li><li><a href=#a-lightweight-and-explainable-vision-language-framework-for-crop-disease-visual-question-answeringhttpsarxivorgabs260105143v1 aria-label="A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering"><a href=https://arxiv.org/abs/2601.05143v1>A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering</a></a></li><li><a href=#prediction-of-magnetic-topological-materials-combining-spin-and-magnetic-space-groupshttpsarxivorgabs260105142v1 aria-label="Prediction of Magnetic Topological Materials Combining Spin and Magnetic Space Groups"><a href=https://arxiv.org/abs/2601.05142v1>Prediction of Magnetic Topological Materials Combining Spin and Magnetic Space Groups</a></a></li><li><a href=#sequential-subspace-noise-injection-prevents-accuracy-collapse-in-certified-unlearninghttpsarxivorgabs260105134v1 aria-label="Sequential Subspace Noise Injection Prevents Accuracy Collapse in Certified Unlearning"><a href=https://arxiv.org/abs/2601.05134v1>Sequential Subspace Noise Injection Prevents Accuracy Collapse in Certified Unlearning</a></a></li><li><a href=#a-geometric-definition-of-the-integral-and-applicationshttpsarxivorgabs260105228v1 aria-label="A Geometric Definition of the Integral and Applications"><a href=https://arxiv.org/abs/2601.05228v1>A Geometric Definition of the Integral and Applications</a></a></li><li><a href=#variable-projection-methods-for-solving-regularized-separable-inverse-problems-with-applications-to-semi-blind-image-deblurringhttpsarxivorgabs260105224v1 aria-label="Variable Projection Methods for Solving Regularized Separable Inverse Problems with Applications to Semi-Blind Image Deblurring"><a href=https://arxiv.org/abs/2601.05224v1>Variable Projection Methods for Solving Regularized Separable Inverse Problems with Applications to Semi-Blind Image Deblurring</a></a></li><li><a href=#flowlet-conditional-3d-brain-mri-synthesis-using-wavelet-flow-matchinghttpsarxivorgabs260105212v1 aria-label="FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching"><a href=https://arxiv.org/abs/2601.05212v1>FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching</a></a></li><li><a href=#a-non-commutative-de-branges-rovnyak-model-for-row-contractionshttpsarxivorgabs260105211v1 aria-label="A non-commutative de Branges-Rovnyak model for row contractions"><a href=https://arxiv.org/abs/2601.05211v1>A non-commutative de Branges-Rovnyak model for row contractions</a></a></li><li><a href=#on-the-value-function-of-convex-bolza-problems-governed-by-stochastic-difference-equationshttpsarxivorgabs260105207v1 aria-label="On the Value Function of Convex Bolza Problems Governed by Stochastic Difference Equations"><a href=https://arxiv.org/abs/2601.05207v1>On the Value Function of Convex Bolza Problems Governed by Stochastic Difference Equations</a></a></li><li><a href=#confidence-and-organizationshttpsarxivorgabs260105206v1 aria-label="Confidence and Organizations"><a href=https://arxiv.org/abs/2601.05206v1>Confidence and Organizations</a></a></li><li><a href=#videoauto-r1-video-auto-reasoning-via-thinking-once-answering-twicehttpsarxivorgabs260105175v1 aria-label="VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice"><a href=https://arxiv.org/abs/2601.05175v1>VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice</a></a></li><li><a href=#inside-out-evolving-user-centric-core-memory-trees-for-long-term-personalized-dialogue-systemshttpsarxivorgabs260105171v1 aria-label="Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems"><a href=https://arxiv.org/abs/2601.05171v1>Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems</a></a></li><li><a href=#mitigating-simulator-dependence-in-ai-parameter-inference-for-the-epoch-of-reionization-the-importance-of-simulation-diversityhttpsarxivorgabs260105229v1 aria-label="Mitigating Simulator Dependence in AI Parameter Inference for the Epoch of Reionization: The Importance of Simulation Diversity"><a href=https://arxiv.org/abs/2601.05229v1>Mitigating Simulator Dependence in AI Parameter Inference for the Epoch of Reionization: The Importance of Simulation Diversity</a></a></li><li><a href=#reducibility-of-higher-order-to-pairwise-interactions-social-impact-models-on-hypergraphshttpsarxivorgabs260105169v1 aria-label="Reducibility of higher-order to pairwise interactions: Social impact models on hypergraphs"><a href=https://arxiv.org/abs/2601.05169v1>Reducibility of higher-order to pairwise interactions: Social impact models on hypergraphs</a></a></li><li><a href=#why-are-some-countries-more-politically-fragmented-online-than-othershttpsarxivorgabs260105093v1 aria-label="Why Are Some Countries More Politically Fragmented Online Than Others?"><a href=https://arxiv.org/abs/2601.05093v1>Why Are Some Countries More Politically Fragmented Online Than Others?</a></a></li><li><a href=#graph-energy-as-a-measure-of-community-detectability-in-networkshttpsarxivorgabs260105065v1 aria-label="Graph energy as a measure of community detectability in networks"><a href=https://arxiv.org/abs/2601.05065v1>Graph energy as a measure of community detectability in networks</a></a></li></ul></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h1 id=weekly-paper-notes>Weekly Paper Notes<a hidden class=anchor aria-hidden=true href=#weekly-paper-notes>#</a></h1><h2 id=-multilingual>üîç multilingual<a hidden class=anchor aria-hidden=true href=#-multilingual>#</a></h2><h3 id=code-mix-sentiment-analysis-on-hinglish-tweetshttpsarxivorgabs260105091v1><a href=https://arxiv.org/abs/2601.05091v1>Code-Mix Sentiment Analysis on Hinglish Tweets</a><a hidden class=anchor aria-hidden=true href=#code-mix-sentiment-analysis-on-hinglish-tweetshttpsarxivorgabs260105091v1>#</a></h3><p><strong>Authors:</strong> Aashi Garg, Aneshya Das, Arshi Arya, Anushka Goyal, Aditi
<strong>Venue:</strong> arXiv (2026)</p><p>The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish&ndash;a hybrid of Hindi and English&ndash;used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05091v1">üìÑ Download PDF</a></p><hr><h3 id=curricullm-designing-personalized-and-workforce-aligned-cybersecurity-curricula-using-fine-tuned-llmshttpsarxivorgabs260104940v1><a href=https://arxiv.org/abs/2601.04940v1>CurricuLLM: Designing Personalized and Workforce-Aligned Cybersecurity Curricula Using Fine-Tuned LLMs</a><a hidden class=anchor aria-hidden=true href=#curricullm-designing-personalized-and-workforce-aligned-cybersecurity-curricula-using-fine-tuned-llmshttpsarxivorgabs260104940v1>#</a></h3><p><strong>Authors:</strong> Arthur Nijdam, Harri K√§hk√∂nen, Valtteri Niemi, Paul Stankovski Wagner, Sara Ramezanian
<strong>Venue:</strong> arXiv (2026)</p><p>The cybersecurity landscape is constantly evolving, driven by increased digitalization and new cybersecurity threats. Cybersecurity programs often fail to equip graduates with skills demanded by the workforce, particularly concerning recent developments in cybersecurity, as curriculum design is costly and labor-intensive. To address this misalignment, we present a novel Large Language Model (LLM)-based framework for automated design and analysis of cybersecurity curricula, called CurricuLLM. Our approach provides three key contributions: (1) automation of personalized curriculum design, (2) a data-driven pipeline aligned with industry demands, and (3) a comprehensive methodology for leveraging fine-tuned LLMs in curriculum development.
CurricuLLM utilizes a two-tier approach consisting of PreprocessLM, which standardizes input data, and ClassifyLM, which assigns course content to nine Knowledge Areas in cybersecurity. We systematically evaluated multiple Natural Language Processing (NLP) architectures and fine-tuning strategies, ultimately selecting the Bidirectional Encoder Representations from Transformers (BERT) model as ClassifyLM, fine-tuned on foundational cybersecurity concepts and workforce competencies.
We are the first to validate our method with human experts who analyzed real-world cybersecurity curricula and frameworks, motivating that CurricuLLM is an efficient solution to replace labor-intensive curriculum analysis. Moreover, once course content has been classified, it can be integrated with established cybersecurity role-based weights, enabling alignment of the educational program with specific job roles, workforce categories, or general market needs. This lays the foundation for personalized, workforce-aligned cybersecurity curricula that prepare students for the evolving demands in cybersecurity.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04940v1">üìÑ Download PDF</a></p><hr><h3 id=can-ai-generated-persuasion-be-detected-persuaficial-benchmark-and-ai-vs-human-linguistic-differenceshttpsarxivorgabs260104925v1><a href=https://arxiv.org/abs/2601.04925v1>Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences</a><a hidden class=anchor aria-hidden=true href=#can-ai-generated-persuasion-be-detected-persuaficial-benchmark-and-ai-vs-human-linguistic-differenceshttpsarxivorgabs260104925v1>#</a></h3><p><strong>Authors:</strong> Arkadiusz Modzelewski, Pawe≈Ç Golik, Anna Ko≈Ços, Giovanni Da San Martino
<strong>Venue:</strong> arXiv (2026)</p><p>Large Language Models (LLMs) can generate highly persuasive text, raising concerns about their misuse for propaganda, manipulation, and other harmful purposes. This leads us to our central question: Is LLM-generated persuasion more difficult to automatically detect than human-written persuasion? To address this, we categorize controllable generation approaches for producing persuasive content with LLMs and introduce Persuaficial, a high-quality multilingual benchmark covering six languages: English, German, Polish, Italian, French and Russian. Using this benchmark, we conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts. We find that although overtly persuasive LLM-generated texts can be easier to detect than human-written ones, subtle LLM-generated persuasion consistently degrades automatic detection performance. Beyond detection performance, we provide the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts, offering insights that may guide the development of more interpretable and robust detection tools.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04925v1">üìÑ Download PDF</a></p><hr><h3 id=scaling-vision-language-models-for-pharmaceutical-long-form-video-reasoning-on-industrial-genai-platformhttpsarxivorgabs260104891v1><a href=https://arxiv.org/abs/2601.04891v1>Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform</a><a hidden class=anchor aria-hidden=true href=#scaling-vision-language-models-for-pharmaceutical-long-form-video-reasoning-on-industrial-genai-platformhttpsarxivorgabs260104891v1>#</a></h3><p><strong>Authors:</strong> Suyash Mishra, Qiang Li, Srikanth Patil, Satyanarayan Pati, Baddu Narendra
<strong>Venue:</strong> arXiv (2026)</p><p>Vision Language Models (VLMs) have shown strong performance on multimodal reasoning tasks, yet most evaluations focus on short videos and assume unconstrained computational resources. In industrial settings such as pharmaceutical content understanding, practitioners must process long-form videos under strict GPU, latency, and cost constraints, where many existing approaches fail to scale. In this work, we present an industrial GenAI framework that processes over 200,000 PDFs, 25,326 videos across eight formats (e.g., MP4, M4V, etc.), and 888 multilingual audio files in more than 20 languages. Our study makes three contributions: (i) an industrial large-scale architecture for multimodal reasoning in pharmaceutical domains; (ii) empirical analysis of over 40 VLMs on two leading benchmarks (Video-MME and MMBench) and proprietary dataset of 25,326 videos across 14 disease areas; and (iii) four findings relevant to long-form video reasoning: the role of multimodality, attention mechanism trade-offs, temporal reasoning limits, and challenges of video splitting under GPU constraints. Results show 3-8 times efficiency gains with SDPA attention on commodity GPUs, multimodality improving up to 8/12 task domains (especially length-dependent tasks), and clear bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs. Rather than proposing a new &ldquo;A+B&rdquo; model, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, and provide actionable guidance for both researchers and practitioners designing scalable multimodal systems for long-form video understanding in industrial domains.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04891v1">üìÑ Download PDF</a></p><hr><h3 id=langsae-editing-improving-multilingual-information-retrieval-via-post-hoc-language-identity-removalhttpsarxivorgabs260104768v1><a href=https://arxiv.org/abs/2601.04768v1>LANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal</a><a hidden class=anchor aria-hidden=true href=#langsae-editing-improving-multilingual-information-retrieval-via-post-hoc-language-identity-removalhttpsarxivorgabs260104768v1>#</a></h3><p><strong>Authors:</strong> Dongjun Kim, Jeongho Yoon, Chanjun Park, Heuiseok Lim
<strong>Venue:</strong> arXiv (2026)</p><p>Dense retrieval in multilingual settings often searches over mixed-language collections, yet multilingual embeddings encode language identity alongside semantics. This language signal can inflate similarity for same-language pairs and crowd out relevant evidence written in other languages. We propose LANGSAE EDITING, a post-hoc sparse autoencoder trained on pooled embeddings that enables controllable removal of language-identity signal directly in vector space. The method identifies language-associated latent units using cross-language activation statistics, suppresses these units at inference time, and reconstructs embeddings in the original dimensionality, making it compatible with existing vector databases without retraining the base encoder or re-encoding raw text. Experiments across multiple languages show consistent improvements in ranking quality and cross-language coverage, with especially strong gains for script-distinct languages.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04768v1">üìÑ Download PDF</a></p><hr><h3 id=qwen3-vl-embedding-and-qwen3-vl-reranker-a-unified-framework-for-state-of-the-art-multimodal-retrieval-and-rankinghttpsarxivorgabs260104720v1><a href=https://arxiv.org/abs/2601.04720v1>Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking</a><a hidden class=anchor aria-hidden=true href=#qwen3-vl-embedding-and-qwen3-vl-reranker-a-unified-framework-for-state-of-the-art-multimodal-retrieval-and-rankinghttpsarxivorgabs260104720v1>#</a></h3><p><strong>Authors:</strong> Mingxin Li, Yanzhao Zhang, Dingkun Long, Keqin Chen, Sibo Song, Shuai Bai, Zhibo Yang, Pengjun Xie, An Yang, Dayiheng Liu, Jingren Zhou, Junyang Lin
<strong>Venue:</strong> arXiv (2026)</p><p>In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in $\textbf{2B}$ and $\textbf{8B}$ parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of $\textbf{77.8}$ on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04720v1">üìÑ Download PDF</a></p><hr><h3 id=banglalorica-design-and-evaluation-of-a-robust-watermarking-algorithm-for-large-language-models-in-bangla-text-generationhttpsarxivorgabs260104534v1><a href=https://arxiv.org/abs/2601.04534v1>BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation</a><a hidden class=anchor aria-hidden=true href=#banglalorica-design-and-evaluation-of-a-robust-watermarking-algorithm-for-large-language-models-in-bangla-text-generationhttpsarxivorgabs260104534v1>#</a></h3><p><strong>Authors:</strong> Amit Bin Tariqul, A N M Zahid Hossain Milkan, Sahab-Al-Chowdhury, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan
<strong>Venue:</strong> arXiv (2026)</p><p>As large language models (LLMs) are increasingly deployed for text generation, watermarking has become essential for authorship attribution, intellectual property protection, and misuse detection. While existing watermarking methods perform well in high-resource languages, their robustness in low-resource languages remains underexplored. This work presents the first systematic evaluation of state-of-the-art text watermarking methods: KGW, Exponential Sampling (EXP), and Waterfall, for Bangla LLM text generation under cross-lingual round-trip translation (RTT) attacks. Under benign conditions, KGW and EXP achieve high detection accuracy (>88%) with negligible perplexity and ROUGE degradation. However, RTT causes detection accuracy to collapse below RTT causes detection accuracy to collapse to 9-13%, indicating a fundamental failure of token-level watermarking. To address this, we propose a layered watermarking strategy that combines embedding-time and post-generation watermarks. Experimental results show that layered watermarking improves post-RTT detection accuracy by 25-35%, achieving 40-50% accuracy, representing a 3$\times$ to 4$\times$ relative improvement over single-layer methods, at the cost of controlled semantic degradation. Our findings quantify the robustness-quality trade-off in multilingual watermarking and establish layered watermarking as a practical, training-free solution for low-resource languages such as Bangla. Our code and data will be made public.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04534v1">üìÑ Download PDF</a></p><hr><h3 id=the-overlooked-role-of-graded-relevance-thresholds-in-multilingual-dense-retrievalhttpsarxivorgabs260104395v1><a href=https://arxiv.org/abs/2601.04395v1>The Overlooked Role of Graded Relevance Thresholds in Multilingual Dense Retrieval</a><a hidden class=anchor aria-hidden=true href=#the-overlooked-role-of-graded-relevance-thresholds-in-multilingual-dense-retrievalhttpsarxivorgabs260104395v1>#</a></h3><p><strong>Authors:</strong> Tomer Wullach, Ori Shapira, Amir DN Cohen
<strong>Venue:</strong> arXiv (2026)</p><p>Dense retrieval models are typically fine-tuned with contrastive learning objectives that require binary relevance judgments, even though relevance is inherently graded. We analyze how graded relevance scores and the threshold used to convert them into binary labels affect multilingual dense retrieval. Using a multilingual dataset with LLM-annotated relevance scores, we examine monolingual, multilingual mixture, and cross-lingual retrieval scenarios. Our findings show that the optimal threshold varies systematically across languages and tasks, often reflecting differences in resource level. A well-chosen threshold can improve effectiveness, reduce the amount of fine-tuning data required, and mitigate annotation noise, whereas a poorly chosen one can degrade performance. We argue that graded relevance is a valuable but underutilized signal for dense retrieval, and that threshold calibration should be treated as a principled component of the fine-tuning pipeline.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04395v1">üìÑ Download PDF</a></p><hr><h3 id=dialect-matters-cross-lingual-asr-transfer-for-low-resource-indic-language-varietieshttpsarxivorgabs260104373v1><a href=https://arxiv.org/abs/2601.04373v1>Dialect Matters: Cross-Lingual ASR Transfer for Low-Resource Indic Language Varieties</a><a hidden class=anchor aria-hidden=true href=#dialect-matters-cross-lingual-asr-transfer-for-low-resource-indic-language-varietieshttpsarxivorgabs260104373v1>#</a></h3><p><strong>Authors:</strong> Akriti Dhasmana, Aarohi Srivastava, David Chiang
<strong>Venue:</strong> arXiv (2026)</p><p>We conduct an empirical study of cross-lingual transfer using spontaneous, noisy, and code-mixed speech across a wide range of Indic dialects and language varieties. Our results indicate that although ASR performance is generally improved with reduced phylogenetic distance between languages, this factor alone does not fully explain performance in dialectal settings. Often, fine-tuning on smaller amounts of dialectal data yields performance comparable to fine-tuning on larger amounts of phylogenetically-related, high-resource standardized languages. We also present a case study on Garhwali, a low-resource Pahari language variety, and evaluate multiple contemporary ASR models. Finally, we analyze transcription errors to examine bias toward pre-training languages, providing additional insight into challenges faced by ASR systems on dialectal and non-standardized speech.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04373v1">üìÑ Download PDF</a></p><hr><h3 id=cssg-measuring-code-similarity-with-semantic-graphshttpsarxivorgabs260104085v1><a href=https://arxiv.org/abs/2601.04085v1>CSSG: Measuring Code Similarity with Semantic Graphs</a><a hidden class=anchor aria-hidden=true href=#cssg-measuring-code-similarity-with-semantic-graphshttpsarxivorgabs260104085v1>#</a></h3><p><strong>Authors:</strong> Jingwen Xu, Yiyang Lu, Changze Lv, Zisu Huang, Zhengkang Guo, Zhengyuan Wang, Muzhao Tian, Xuanjing Huang, Xiaoqing Zheng
<strong>Venue:</strong> arXiv (2026)</p><p>Existing code similarity metrics, such as BLEU, CodeBLEU, and TSED, largely rely on surface-level string overlap or abstract syntax tree structures, and often fail to capture deeper semantic relationships between programs.We propose CSSG (Code Similarity using Semantic Graphs), a novel metric that leverages program dependence graphs to explicitly model control dependencies and variable interactions, providing a semantics-aware representation of code.Experiments on the CodeContests+ dataset show that CSSG consistently outperforms existing metrics in distinguishing more similar code from less similar code under both monolingual and cross-lingual settings, demonstrating that dependency-aware graph representations offer a more effective alternative to surface-level or syntax-based similarity measures.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04085v1">üìÑ Download PDF</a></p><hr><h3 id=analyzing-and-improving-cross-lingual-knowledge-transfer-for-machine-translationhttpsarxivorgabs260104036v1><a href=https://arxiv.org/abs/2601.04036v1>Analyzing and Improving Cross-lingual Knowledge Transfer for Machine Translation</a><a hidden class=anchor aria-hidden=true href=#analyzing-and-improving-cross-lingual-knowledge-transfer-for-machine-translationhttpsarxivorgabs260104036v1>#</a></h3><p><strong>Authors:</strong> David Stap
<strong>Venue:</strong> arXiv (2026)</p><p>Multilingual machine translation systems aim to make knowledge accessible across languages, yet learning effective cross-lingual representations remains challenging. These challenges are especially pronounced for low-resource languages, where limited parallel data constrains generalization and transfer. Understanding how multilingual models share knowledge across languages requires examining the interaction between representations, data availability, and training strategies. In this thesis, we study cross-lingual knowledge transfer in neural models and develop methods to improve robustness and generalization in multilingual settings, using machine translation as a central testbed. We analyze how similarity between languages influences transfer, how retrieval and auxiliary supervision can strengthen low-resource translation, and how fine-tuning on parallel data can introduce unintended trade-offs in large language models. We further examine the role of language diversity during training and show that increasing translation coverage improves generalization and reduces off-target behavior. Together, this work highlights how modeling choices and data composition shape multilingual learning and offers insights toward more inclusive and resilient multilingual NLP systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04036v1">üìÑ Download PDF</a></p><hr><h3 id=indextts-25-technical-reporthttpsarxivorgabs260103888v2><a href=https://arxiv.org/abs/2601.03888v2>IndexTTS 2.5 Technical Report</a><a hidden class=anchor aria-hidden=true href=#indextts-25-technical-reporthttpsarxivorgabs260103888v2>#</a></h3><p><strong>Authors:</strong> Yunpei Li, Xun Zhou, Jinchao Wang, Lu Wang, Yong Wu, Siyi Zhou, Yiquan Zhou, Jingchen Shu
<strong>Venue:</strong> arXiv (2026)</p><p>In prior work, we introduced IndexTTS 2, a zero-shot neural text-to-speech foundation model comprising two core components: a transformer-based Text-to-Semantic (T2S) module and a non-autoregressive Semantic-to-Mel (S2M) module, which together enable faithful emotion replication and establish the first autoregressive duration-controllable generative paradigm. Building upon this, we present IndexTTS 2.5, which significantly enhances multilingual coverage, inference speed, and overall synthesis quality through four key improvements: 1) Semantic Codec Compression: we reduce the semantic codec frame rate from 50 Hz to 25 Hz, halving sequence length and substantially lowering both training and inference costs; 2) Architectural Upgrade: we replace the U-DiT-based backbone of the S2M module with a more efficient Zipformer-based modeling architecture, achieving notable parameter reduction and faster mel-spectrogram generation; 3) Multilingual Extension: We propose three explicit cross-lingual modeling strategies, boundary-aware alignment, token-level concatenation, and instruction-guided generation, establishing practical design principles for zero-shot multilingual emotional TTS that supports Chinese, English, Japanese, and Spanish, and enables robust emotion transfer even without target-language emotional training data; 4) Reinforcement Learning Optimization: we apply GRPO in post-training of the T2S module, improving pronunciation accuracy and natrualness. Experiments show that IndexTTS 2.5 not only supports broader language coverage but also replicates emotional prosody in unseen languages under the same zero-shot setting. IndexTTS 2.5 achieves a 2.28 times improvement in RTF while maintaining comparable WER and speaker similarity to IndexTTS 2.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.03888v2">üìÑ Download PDF</a></p><hr><h3 id=prior-informed-zeroth-order-optimization-with-adaptive-direction-alignment-for-memory-efficient-llm-fine-tuninghttpsarxivorgabs260104710v1><a href=https://arxiv.org/abs/2601.04710v1>Prior-Informed Zeroth-Order Optimization with Adaptive Direction Alignment for Memory-Efficient LLM Fine-Tuning</a><a hidden class=anchor aria-hidden=true href=#prior-informed-zeroth-order-optimization-with-adaptive-direction-alignment-for-memory-efficient-llm-fine-tuninghttpsarxivorgabs260104710v1>#</a></h3><p><strong>Authors:</strong> Feihu Jin, Shipeng Cen, Ying Tan
<strong>Venue:</strong> arXiv (2026)</p><p>Fine-tuning large language models (LLMs) has achieved remarkable success across various NLP tasks, but the substantial memory overhead during backpropagation remains a critical bottleneck, especially as model scales grow. Zeroth-order (ZO) optimization alleviates this issue by estimating gradients through forward passes and Gaussian sampling, avoiding the need for backpropagation. However, conventional ZO methods suffer from high variance in gradient estimation due to their reliance on random perturbations, leading to slow convergence and suboptimal performance. We propose a simple plug-and-play method that incorporates prior-informed perturbations to refine gradient estimation. Our method dynamically computes a guiding vector from Gaussian samples, which directs perturbations toward more informative directions, significantly accelerating convergence compared to standard ZO approaches. We further investigate a greedy perturbation strategy to explore the impact of prior knowledge on gradient estimation. Theoretically, we prove that our gradient estimator achieves stronger alignment with the true gradient direction, enhancing optimization efficiency. Extensive experiments across LLMs of varying scales and architectures demonstrate that our proposed method could seamlessly integrate into existing optimization methods, delivering faster convergence and superior performance. Notably, on the OPT-13B model, our method outperforms traditional ZO optimization across all 11 benchmark tasks and surpasses gradient-based baselines on 9 out of 11 tasks, establishing a robust balance between efficiency and accuracy.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04710v1">üìÑ Download PDF</a></p><hr><h3 id=aligning-text-code-and-vision-a-multi-objective-reinforcement-learning-framework-for-text-to-visualizationhttpsarxivorgabs260104582v1><a href=https://arxiv.org/abs/2601.04582v1>Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization</a><a hidden class=anchor aria-hidden=true href=#aligning-text-code-and-vision-a-multi-objective-reinforcement-learning-framework-for-text-to-visualizationhttpsarxivorgabs260104582v1>#</a></h3><p><strong>Authors:</strong> Mizanur Rahman, Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Shafiq Joty, Enamul Hoque
<strong>Venue:</strong> arXiv (2026)</p><p>Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at <a href=https://github.com/vis-nlp/RL-Text2Vis>https://github.com/vis-nlp/RL-Text2Vis</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04582v1">üìÑ Download PDF</a></p><hr><h3 id=identifying-good-and-bad-neurons-for-task-level-controllable-llmshttpsarxivorgabs260104548v1><a href=https://arxiv.org/abs/2601.04548v1>Identifying Good and Bad Neurons for Task-Level Controllable LLMs</a><a hidden class=anchor aria-hidden=true href=#identifying-good-and-bad-neurons-for-task-level-controllable-llmshttpsarxivorgabs260104548v1>#</a></h3><p><strong>Authors:</strong> Wenjie Li, Guansong Pang, Hezhe Qiao, Debin Gao, David Lo
<strong>Venue:</strong> arXiv (2026)</p><p>Large Language Models have demonstrated remarkable capabilities on multiple-choice question answering benchmarks, but the complex mechanisms underlying their large-scale neurons remain opaque, posing significant challenges for understanding and steering LLMs. While recent studies made progress on identifying responsible neurons for certain abilities, these ability-specific methods are infeasible for task-focused scenarios requiring coordinated use of multiple abilities. Moreover, these approaches focus only on supportive neurons that correlate positively with task completion, while neglecting neurons with other roles-such as inhibitive roles-and misled neuron attribution due to fortuitous behaviors in LLMs (i.e., correctly answer the questions by chance rather than genuine understanding). To address these challenges, we propose NeuronLLM, a novel task-level LLM understanding framework that adopts the biological principle of functional antagonism for LLM neuron identification. The key insight is that task performance is jointly determined by neurons with two opposing roles: good neurons that facilitate task completion and bad neurons that inhibit it. NeuronLLM achieves a holistic modeling of neurons via contrastive learning of good and bad neurons, while leveraging augmented question sets to mitigate the fortuitous behaviors in LLMs. Comprehensive experiments on LLMs of different sizes and families show the superiority of NeuronLLM over existing methods in four NLP tasks, providing new insights into LLM functional organization.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04548v1">üìÑ Download PDF</a></p><hr><h3 id=non-homogeneous-markov-switching-generalized-additive-models-for-location-scale-and-shapehttpsarxivorgabs260103760v1><a href=https://arxiv.org/abs/2601.03760v1>Non-Homogeneous Markov-Switching Generalized Additive Models for Location, Scale, and Shape</a><a hidden class=anchor aria-hidden=true href=#non-homogeneous-markov-switching-generalized-additive-models-for-location-scale-and-shapehttpsarxivorgabs260103760v1>#</a></h3><p><strong>Authors:</strong> Katharina Ammann, Timo Adam, Jan-Ole Koslik
<strong>Venue:</strong> arXiv (2026)</p><p>We propose an extension of Markov-switching generalized additive models for location, scale, and shape (MS-GAMLSS) that allows covariates to influence not only the parameters of the state-dependent distributions but also the state transition probabilities. Traditional MS-GAMLSS, which combine distributional regression with hidden Markov models, typically assume time-homogeneous (i.e., constant) transition probabilities, thereby preventing regime shifts from responding to covariate-driven changes. Our approach overcomes this limitation by modeling the transition probabilities as smooth functions of covariates, enabling a flexible, data-driven characterization of covariate-dependent regime dynamics. Estimation is carried out within a penalized likelihood framework, where automatic smoothness selection controls model complexity and guards against overfitting. We evaluate the proposed methodology through simulations and applications to daily Lufthansa stock prices and Spanish energy prices. Our results show that incorporating macroeconomic indicators into the transition probabilities yields additional insights into market dynamics. Data and R code to reproduce the results are available online.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.03760v1">üìÑ Download PDF</a></p><hr><h3 id=ola-output-language-alignment-in-code-switched-llm-interactionshttpsarxivorgabs260103589v1><a href=https://arxiv.org/abs/2601.03589v1>OLA: Output Language Alignment in Code-Switched LLM Interactions</a><a hidden class=anchor aria-hidden=true href=#ola-output-language-alignment-in-code-switched-llm-interactionshttpsarxivorgabs260103589v1>#</a></h3><p><strong>Authors:</strong> Juhyun Oh, Haneul Yoo, Faiz Ghifari Haznitrama, Alice Oh
<strong>Venue:</strong> arXiv (2026)</p><p>Code-switching, alternating between languages within a conversation, is natural for multilingual users, yet poses fundamental challenges for large language models (LLMs). When a user code-switches in their prompt to an LLM, they typically do not specify the expected language of the LLM response, and thus LLMs must infer the output language from contextual and pragmatic cues. We find that current LLMs systematically fail to align with this expectation, responding in undesired languages even when cues are clear to humans. We introduce OLA, a benchmark to evaluate LLMs&rsquo; Output Language Alignment in code-switched interactions. OLA focuses on Korean&ndash;English code-switching and spans simple intra-sentential mixing to instruction-content mismatches. Even frontier models frequently misinterpret implicit language expectation, exhibiting a bias toward non-English responses. We further show this bias generalizes beyond Korean to Chinese and Indonesian pairs. Models also show instability through mid-response switching and language intrusions. Chain-of-Thought prompting fails to resolve these errors, indicating weak pragmatic reasoning about output language. However, Code-Switching Aware DPO with minimal data (about 1K examples) substantially reduces misalignment, suggesting these failures stem from insufficient alignment rather than fundamental limitations. Our results highlight the need to align multilingual LLMs with users&rsquo; implicit expectations in real-world code-switched interactions.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.03589v1">üìÑ Download PDF</a></p><hr><h3 id=the-winds-of-oba-hypergiants-and-luminous-blue-variables-dynamically-consistent-atmosphere-models-reveal-multiple-wind-regimeshttpsarxivorgabs260103072v1><a href=https://arxiv.org/abs/2601.03072v1>The winds of OBA hypergiants and luminous blue variables: Dynamically-consistent atmosphere models reveal multiple wind regimes</a><a hidden class=anchor aria-hidden=true href=#the-winds-of-oba-hypergiants-and-luminous-blue-variables-dynamically-consistent-atmosphere-models-reveal-multiple-wind-regimeshttpsarxivorgabs260103072v1>#</a></h3><p><strong>Authors:</strong> Matheus Bernini-Peron, Andreas A. C. Sander, Gautham N. Sabhahit, Francisco Najarro, Varsha Ramachandran, Jorick S. Vink
<strong>Venue:</strong> arXiv (2026)</p><p>OBA hypergiants (OBAHGs) are evolved massive stars with notable wind features in their optical spectrum. Positioned at the cool edge of the line-driven wind regime, many are candidate luminous blue variables (LBVs) likely near the Eddington Limit. Although brief, this evolutionary stage deeply impacts their surroundings and subsequent evolution.
We study the mechanisms behind OBAHG winds and spectra, covering the temperature range of non-eruptive LBVs. Using the PoWR atmosphere code, we compute models with an Eddington parameter Gamma_e ~ 0.4 and moderate turbulent pressure, typical for cool hypergiants, varying the effective temperature from ~12.5 to ~38.0 kK at solar metallicity.
Our models show a complex temperature-dependent mass-loss pattern, with regions of higher/lower rates linked to two wind solutions: &ldquo;dense&rdquo; and &ldquo;airy.&rdquo; Spectra of known OBAHGs and LBVs match models from all solution regions. We find bi-stability jumps &ndash; with sharp mass-loss increases &ndash; at temperatures where Fe IV recombines to Fe III (and Fe III to Fe II). &ldquo;Drops&rdquo; in mass-loss also occur when the leading Fe ion changes at wind onset, signaling a switch to airy solutions under insufficient driving opacity.
The resulting velocity fields also reflect these different regimes: airy solutions match the empirical terminal velocity vs temperature relation, while dense ones deviate. Turbulent pressure is crucial for wind acceleration at cooler temperatures, especially in airy regimes.
We demonstrate that the bi-stability jumps exist in OBAHGs but are part of a broader complex behavior not replicated by current mass-loss recipes. Combining our and other recent results, we suggest that the switch between airy and dense solutions only occurs within a certain proximity to the Eddington Limit. Testing this requires future models with broader parameters and advanced treatments of radiatively-driven turbulence.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.03072v1">üìÑ Download PDF</a></p><hr><h3 id=ai-native-6g-physical-layer-with-cross-module-optimization-and-cooperative-control-agentshttpsarxivorgabs260102827v2><a href=https://arxiv.org/abs/2601.02827v2>AI-Native 6G Physical Layer with Cross-Module Optimization and Cooperative Control Agents</a><a hidden class=anchor aria-hidden=true href=#ai-native-6g-physical-layer-with-cross-module-optimization-and-cooperative-control-agentshttpsarxivorgabs260102827v2>#</a></h3><p><strong>Authors:</strong> Xufei Zheng, Han Xiao, Shi Jin, Zhiqin Wang, Wenqiang Tian, Wendong Liu, Jianfei Cao, Jia Shen, Zhihua Shi, Zhi Zhang, Ning Yang
<strong>Venue:</strong> arXiv (2026)</p><p>In this article, a framework of AI-native cross-module optimized physical layer with cooperative control agents is proposed, which involves optimization across global AI/ML modules of the physical layer with innovative design of multiple enhancement mechanisms and control strategies. Specifically, it achieves simultaneous optimization across global modules of uplink AI/ML-based joint source-channel coding with modulation, and downlink AI/ML-based modulation with precoding and corresponding data detection, reducing traditional inter-module information barriers to facilitate end-to-end optimization toward global objectives. Moreover, multiple enhancement mechanisms are also proposed, including i) an AI/ML-based cross-layer modulation approach with theoretical analysis for downlink transmission that breaks the isolation of inter-layer features to expand the solution space for determining improved constellation, ii) a utility-oriented precoder construction method that shifts the role of the AI/ML-based CSI feedback decoder from recovering the original CSI to directly generating precoding matrices aiming to improve end-to-end performance, and iii) incorporating modulation into AI/ML-based CSI feedback to bypass bit-level bottlenecks that introduce quantization errors, non-differentiable gradients, and limitations in constellation solution spaces. Furthermore, AI/ML based control agents for optimized transmission schemes are proposed that leverage AI/ML to perform model switching according to channel state, thereby enabling integrated control for global throughput optimization. Finally, simulation results demonstrate the superiority of the proposed solutions in terms of BLER and throughput. These extensive simulations employ more practical assumptions that are aligned with the requirements of the 3GPP, which hopefully provides valuable insights for future standardization discussions.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.02827v2">üìÑ Download PDF</a></p><hr><h3 id=dynamically-consistent-analysis-of-galactic-wn4b-starshttpsarxivorgabs260102498v1><a href=https://arxiv.org/abs/2601.02498v1>Dynamically consistent analysis of Galactic WN4b stars</a><a hidden class=anchor aria-hidden=true href=#dynamically-consistent-analysis-of-galactic-wn4b-starshttpsarxivorgabs260102498v1>#</a></h3><p><strong>Authors:</strong> Roel R. Lefever, Andreas A. C. Sander, Matheus Bernini-Peron, Gemma Gon√°lez-Tor√†, Wolf-Rainer Hamann, Joris Josiek, Varsha Ramachandran, Elisa C. Sch√∂sser, Helge Todt
<strong>Venue:</strong> arXiv (2026)</p><p>Many Wolf-Rayet (WR) stars have optically thick winds that cloak the hydrostatic layers of the underlying star. In these cases, traditional spectral analysis methods are plagued by degeneracies that make it difficult to constrain parameters such as the stellar radius and the deeper density and velocity structure of the atmosphere. Focussing on the regime of nitrogen-rich WN4-stars with strong emission lines, we employ hydrodynamically-consistent modelling using the PoWR-HD code branch to perform a next generation spectral analysis. The inherent coupling of the stellar and wind parameters enables us to break parameter degeneracies, constrain the wind structure, and get a mass estimate. With this information, we can draw evolutionary implications and test current mass-loss descriptions for WR stars. We selected a sample of six Galactic WN4b stars. Applying updated parallaxes from Gaia DR3 and calculating PoWR-HD models that sufficiently resemble most of their spectral appearance, we obtain new values for the stellar and wind parameters of the WN4b sample. We compare our results to previous studies employing grid models with a beta = 1 velocity structure and cross-check our derived parameters with stellar structure predictions from GENEC and FRANEC evolution tracks. For all six targets, we obtain a narrow range of stellar temperatures T<del>140 kK, in contrast to previous grid-model analyses. We confirm the existence of WRs with luminosities as low as log L/Lsol = 5.0 and M</del>5 Msol. All derived velocity fields include a plateau feature at ~85% of the terminal velocity. Both the distance updates and the switch to dynamically-consistent atmospheres lead to substantial parameter adjustments compared to earlier grid-based studies. A comparison of the derived mass-loss rates favours a different description for the WN4b sample than for WN2 stars analysed with the same methodology.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.02498v1">üìÑ Download PDF</a></p><hr><h3 id=aspect-extraction-from-e-commerce-product-and-service-reviewshttpsarxivorgabs260101827v1><a href=https://arxiv.org/abs/2601.01827v1>Aspect Extraction from E-Commerce Product and Service Reviews</a><a hidden class=anchor aria-hidden=true href=#aspect-extraction-from-e-commerce-product-and-service-reviewshttpsarxivorgabs260101827v1>#</a></h3><p><strong>Authors:</strong> Valiant Lance D. Dionela, Fatima Kriselle S. Dy, Robin James M. Hombrebueno, Aaron Rae M. Nicolas, Charibeth K. Cheng, Raphael W. Gonda
<strong>Venue:</strong> arXiv (2026)</p><p>Aspect Extraction (AE) is a key task in Aspect-Based Sentiment Analysis (ABSA), yet it remains difficult to apply in low-resource and code-switched contexts like Taglish, a mix of Tagalog and English commonly used in Filipino e-commerce reviews. This paper introduces a comprehensive AE pipeline designed for Taglish, combining rule-based, large language model (LLM)-based, and fine-tuning techniques to address both aspect identification and extraction. A Hierarchical Aspect Framework (HAF) is developed through multi-method topic modeling, along with a dual-mode tagging scheme for explicit and implicit aspects. For aspect identification, four distinct models are evaluated: a Rule-Based system, a Generative LLM (Gemini 2.0 Flash), and two Fine-Tuned Gemma-3 1B models trained on different datasets (Rule-Based vs. LLM-Annotated). Results indicate that the Generative LLM achieved the highest performance across all tasks (Macro F1 0.91), demonstrating superior capability in handling implicit aspects. In contrast, the fine-tuned models exhibited limited performance due to dataset imbalance and architectural capacity constraints. This work contributes a scalable and linguistically adaptive framework for enhancing ABSA in diverse, code-switched environments.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.01827v1">üìÑ Download PDF</a></p><hr><h3 id=an-adaptive-power-division-strategy-for-nonlinear-components-in-rectificationhttpsarxivorgabs260104598v1><a href=https://arxiv.org/abs/2601.04598v1>An Adaptive Power Division Strategy for Nonlinear Components in Rectification</a><a hidden class=anchor aria-hidden=true href=#an-adaptive-power-division-strategy-for-nonlinear-components-in-rectificationhttpsarxivorgabs260104598v1>#</a></h3><p><strong>Authors:</strong> Zhongqi He, Liping Yan, Changjun Liu
<strong>Venue:</strong> arXiv (2026)</p><p>This letter presents a novel adaptive power division strategy, which uses two rectifying diodes with nonlinear impedance characteristics that are configured in parallel to function optimally at their individual power levels. Through the strategic adjustment of the input admittance, the conductance of the low-power diode decreases progressively with increasing power, while the conductance of the high-power diode increases correspondingly. This conductance-based power allocation method ensures that the power is rectified consistently by the most appropriate diode, regardless of the power level, and, thus, enables efficient rectification across an extended range. This letter presents a rectifier typology to substantiate the proposed strategy. Experimental results confirm the efficiency of the adaptive power division strategy, with the rectifier showing efficiency in excess of 60% from 5 to 29.5 dBm, giving a power dynamic range of 24.5 dB.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04598v1">üìÑ Download PDF</a></p><hr><h3 id=last_0-latent-spatio-temporal-chain-of-thought-for-robotic-vision-language-action-modelhttpsarxivorgabs260105248v1><a href=https://arxiv.org/abs/2601.05248v1>LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model</a><a hidden class=anchor aria-hidden=true href=#last_0-latent-spatio-temporal-chain-of-thought-for-robotic-vision-language-action-modelhttpsarxivorgabs260105248v1>#</a></h3><p><strong>Authors:</strong> Zhuoyang Liu, Jiaming Liu, Hao Chen, Ziyu Guo, Chengkai Hou, Chenyang Gu, Jiale Yu, Xiangju Mi, Renrui Zhang, Zhengping Che, Jian Tang, Pheng-Ann Heng, Shanghang Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST$_0$, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST$_0$ adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST$_0$ is trained with heterogeneous operation frequencies, enabling adaptive switching between reasoning and action inference rates during deployment. Across ten simulated and six real-world manipulation tasks, LaST$_0$ improves mean success rates by 8% and 13% over prior VLA methods, respectively, while achieving substantially faster inference. Project website: <a href=https://sites.google.com/view/last0>https://sites.google.com/view/last0</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05248v1">üìÑ Download PDF</a></p><hr><h3 id=grex-generalized-referring-expression-segmentation-comprehension-and-generationhttpsarxivorgabs260105244v1><a href=https://arxiv.org/abs/2601.05244v1>GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation</a><a hidden class=anchor aria-hidden=true href=#grex-generalized-referring-expression-segmentation-comprehension-and-generationhttpsarxivorgabs260105244v1>#</a></h3><p><strong>Authors:</strong> Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Yu-Gang Jiang
<strong>Venue:</strong> arXiv (2026)</p><p>Referring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object. Existing datasets and methods commonly support single-target expressions only, i.e., one expression refers to one object, not considering multi-target and no-target expressions. This greatly limits the real applications of REx (RES/REC/REG). This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects. We construct the first large-scale GREx dataset gRefCOCO that contains multi-target, no-target, and single-target expressions and their corresponding images with labeled targets. GREx and gRefCOCO are designed to be backward-compatible with REx, facilitating extensive experiments to study the performance gap of the existing REx methods on GREx tasks. One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies. The proposed ReLA achieves the state-of-the-art results on the both GRES and GREC tasks. The proposed gRefCOCO dataset and method are available at <a href=https://henghuiding.github.io/GREx>https://henghuiding.github.io/GREx</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05244v1">üìÑ Download PDF</a></p><hr><h3 id=gdpo-group-reward-decoupled-normalization-policy-optimization-for-multi-reward-rl-optimizationhttpsarxivorgabs260105242v1><a href=https://arxiv.org/abs/2601.05242v1>GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization</a><a hidden class=anchor aria-hidden=true href=#gdpo-group-reward-decoupled-normalization-policy-optimization-for-multi-reward-rl-optimizationhttpsarxivorgabs260105242v1>#</a></h3><p><strong>Authors:</strong> Shih-Yang Liu, Xin Dong, Ximing Lu, Shizhe Diao, Peter Belcak, Mingjie Liu, Min-Hung Chen, Hongxu Yin, Yu-Chiang Frank Wang, Kwang-Ting Cheng, Yejin Choi, Jan Kautz, Pavlo Molchanov
<strong>Venue:</strong> arXiv (2026)</p><p>As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05242v1">üìÑ Download PDF</a></p><hr><h3 id=robovip-multi-view-video-generation-with-visual-identity-prompting-augments-robot-manipulationhttpsarxivorgabs260105241v1><a href=https://arxiv.org/abs/2601.05241v1>RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation</a><a hidden class=anchor aria-hidden=true href=#robovip-multi-view-video-generation-with-visual-identity-prompting-augments-robot-manipulationhttpsarxivorgabs260105241v1>#</a></h3><p><strong>Authors:</strong> Boyang Wang, Haoran Zhang, Shujie Zhang, Jinkun Hao, Mingda Jia, Qi Lv, Yucheng Mao, Zhaoyang Lyu, Jia Zeng, Xudong Xu, Jiangmiao Pang
<strong>Venue:</strong> arXiv (2026)</p><p>The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05241v1">üìÑ Download PDF</a></p><hr><h3 id=robust-reasoning-as-a-symmetry-protected-topological-phasehttpsarxivorgabs260105240v1><a href=https://arxiv.org/abs/2601.05240v1>Robust Reasoning as a Symmetry-Protected Topological Phase</a><a hidden class=anchor aria-hidden=true href=#robust-reasoning-as-a-symmetry-protected-topological-phasehttpsarxivorgabs260105240v1>#</a></h3><p><strong>Authors:</strong> Ilmo Sung
<strong>Venue:</strong> arXiv (2026)</p><p>Large language models suffer from &ldquo;hallucinations&rdquo;-logical inconsistencies induced by semantic noise. We propose that current architectures operate in a &ldquo;Metric Phase,&rdquo; where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braiding, replacing fragile geometric interpolation with robust topological invariants. Empirically, we demonstrate a sharp topological phase transition: while Transformers and RNNs exhibit gapless decay, our Holonomic Network reveals a macroscopic &ldquo;mass gap,&rdquo; maintaining invariant fidelity below a critical noise threshold. Furthermore, in a variable-binding task on $S_{10}$ ($3.6 \times 10^6$ states) representing symbolic manipulation, we demonstrate holonomic generalization: the topological model maintains perfect fidelity extrapolating $100\times$ beyond training ($L=50 \to 5000$), consistent with a theoretically indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies indicate this protection emerges strictly from non-Abelian gauge symmetry. This provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05240v1">üìÑ Download PDF</a></p><hr><h3 id=measuring-and-fostering-peace-through-machine-learning-and-artificial-intelligencehttpsarxivorgabs260105232v1><a href=https://arxiv.org/abs/2601.05232v1>Measuring and Fostering Peace through Machine Learning and Artificial Intelligence</a><a hidden class=anchor aria-hidden=true href=#measuring-and-fostering-peace-through-machine-learning-and-artificial-intelligencehttpsarxivorgabs260105232v1>#</a></h3><p><strong>Authors:</strong> P. Gilda, P. Dungarwal, A. Thongkham, E. T. Ajayi, S. Choudhary, T. M. Terol, C. Lam, J. P. Araujo, M. McFadyen-Mungalln, L. S. Liebovitch, P. T. Coleman, H. West, K. Sieck, S. Carter
<strong>Venue:</strong> arXiv (2026)</p><p>We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05232v1">üìÑ Download PDF</a></p><hr><h3 id=generate-transfer-adapt-learning-functional-dexterous-grasping-from-a-single-human-demonstrationhttpsarxivorgabs260105243v1><a href=https://arxiv.org/abs/2601.05243v1>Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration</a><a hidden class=anchor aria-hidden=true href=#generate-transfer-adapt-learning-functional-dexterous-grasping-from-a-single-human-demonstrationhttpsarxivorgabs260105243v1>#</a></h3><p><strong>Authors:</strong> Xingyi He, Adhitya Polavaram, Yunhao Cao, Om Deshmukh, Tianrui Wang, Xiaowei Zhou, Kuan Fang
<strong>Venue:</strong> arXiv (2026)</p><p>Functional grasping with dexterous robotic hands is a key capability for enabling tool use and complex manipulation, yet progress has been constrained by two persistent bottlenecks: the scarcity of large-scale datasets and the absence of integrated semantic and geometric reasoning in learned models. In this work, we present CorDex, a framework that robustly learns dexterous functional grasps of novel objects from synthetic data generated from just a single human demonstration. At the core of our approach is a correspondence-based data engine that generates diverse, high-quality training data in simulation. Based on the human demonstration, our data engine generates diverse object instances of the same category, transfers the expert grasp to the generated objects through correspondence estimation, and adapts the grasp through optimization. Building on the generated data, we introduce a multimodal prediction network that integrates visual and geometric information. By devising a local-global fusion module and an importance-aware sampling mechanism, we enable robust and computationally efficient prediction of functional dexterous grasps. Through extensive experiments across various object categories, we demonstrate that CorDex generalizes well to unseen object instances and significantly outperforms state-of-the-art baselines.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05243v1">üìÑ Download PDF</a></p><hr><h3 id=learning-latent-action-world-models-in-the-wildhttpsarxivorgabs260105230v1><a href=https://arxiv.org/abs/2601.05230v1>Learning Latent Action World Models In The Wild</a><a hidden class=anchor aria-hidden=true href=#learning-latent-action-world-models-in-the-wildhttpsarxivorgabs260105230v1>#</a></h3><p><strong>Authors:</strong> Quentin Garrido, Tushar Nagarajan, Basile Terver, Nicolas Ballas, Yann LeCun, Michael Rabbat
<strong>Venue:</strong> arXiv (2026)</p><p>Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05230v1">üìÑ Download PDF</a></p><hr><h3 id=local-multimodal-dynamics-in-mixed-ionic-electronic-conductors-and-their-fingerprints-in-organic-electrochemical-transistor-operationhttpsarxivorgabs260105179v1><a href=https://arxiv.org/abs/2601.05179v1>Local Multimodal Dynamics in Mixed Ionic-Electronic Conductors and Their Fingerprints in Organic Electrochemical Transistor Operation</a><a hidden class=anchor aria-hidden=true href=#local-multimodal-dynamics-in-mixed-ionic-electronic-conductors-and-their-fingerprints-in-organic-electrochemical-transistor-operationhttpsarxivorgabs260105179v1>#</a></h3><p><strong>Authors:</strong> Shubham Tanwar, Han-Yan Wu, Chi-Yuan Yang, Ruben Millan-Solsona, Simone Fabiano, Adrica Kyndiah, Gabriel Gomila
<strong>Venue:</strong> arXiv (2026)</p><p>Mixed ionic-electronic conductors host tightly coupled interactions among mobile ions, electronic charges, and the polymer matrix, giving rise to complex multimodal responses spanning electrical, mechanical, and morphological transformations. These materials underpin organic electrochemical transistors (OECTs), which translate such interactions into low-voltage signal amplification and sensing for applications in bioelectronics, neuromorphic computing, and memory. Despite their central role, OECT current-voltage transfer characteristics are often treated phenomenologically, as both the local multimodal dynamics and their connection to global device response remain unresolved. Here, we reveal that the transfer curve encodes a cascade of spatially localized electrochemical transitions, each associated with distinct changes in conductivity, stiffness, and morphology, fundamentally redefining it as a spatially resolved fingerprint of device&rsquo;s internal state. Using automated operando multimodal in-liquid scanning dielectric microscopy, we directly map these dynamics and identify region-specific electrochemical thresholds governing the interplay between source, channel, and drain. We found that the local tip-sample electrostatic force serves as a remarkable mechanistic observable of coupled multimodal dynamics in mixed conductors. A physically grounded model links it to general material, interfacial, and geometric parameters, enabling mechanistic interpretation and predictive insights. Our work provides a new framework for probing and understanding mixed conduction in ion-electron coupled systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05179v1">üìÑ Download PDF</a></p><hr><h3 id=quantum-elastic-network-models-and-their-application-to-graphenehttpsarxivorgabs260105161v1><a href=https://arxiv.org/abs/2601.05161v1>Quantum Elastic Network Models and their Application to Graphene</a><a hidden class=anchor aria-hidden=true href=#quantum-elastic-network-models-and-their-application-to-graphenehttpsarxivorgabs260105161v1>#</a></h3><p><strong>Authors:</strong> Ioannis Kolotouros, Adithya Sireesh, Stuart Ferguson, Sean Thrasher, Petros Wallden, Julien Michel
<strong>Venue:</strong> arXiv (2026)</p><p>Molecular dynamics simulations are a central computational methodology in materials design for relating atomic composition to mechanical properties. However, simulating materials with atomic-level resolution on a macroscopic scale is infeasible on current classical hardware, even when using the simplest elastic network models (ENMs) that represent molecular vibrations as a network of coupled oscillators. To address this issue, we introduce Quantum Elastic Network Models (QENMs) and utilize the quantum algorithm of Babbush et al. (PRX, 2023), which offers an exponential advantage when simulating systems of coupled oscillators under some specific conditions and assumptions. Here, we demonstrate how our method enables the efficient simulation of planar materials. As an example, we apply our algorithm to the task of simulating a 2D graphene sheet. We analyze the exact complexity for initial-state preparation, Hamiltonian simulation, and measurement of this material, and provide two real-world applications: heat transfer and the out-of-plane rippling effect. We estimate that an atomistic simulation of a graphene sheet on the centimeter scale, classically requiring hundreds of petabytes of memory and prohibitive runtimes, could be encoded and simulated with as few as $\sim 160$ logical qubits.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05161v1">üìÑ Download PDF</a></p><hr><h3 id=machine-learning-for-radiative-hydrodynamics-in-astrophysicshttpsarxivorgabs260105155v1><a href=https://arxiv.org/abs/2601.05155v1>Machine learning for radiative hydrodynamics in astrophysics</a><a hidden class=anchor aria-hidden=true href=#machine-learning-for-radiative-hydrodynamics-in-astrophysicshttpsarxivorgabs260105155v1>#</a></h3><p><strong>Authors:</strong> Gonzague Radureau
<strong>Venue:</strong> arXiv (2026)</p><p>Radiation hydrodynamics describes the interaction between high-temperature hypersonic plasmas and the radiation they emit or absorb, a coupling that plays a central role in many astrophysical phenomena related to accretion and ejection processes. The HADES code was developed to model such systems by coupling hydrodynamics with M1-gray or M1-multigroup radiative transfer models, which are well suited to optically intermediate media.
Despite its accuracy, radiation hydrodynamics simulations remain extremely demanding in terms of computational cost. Two main limitations are responsible for this. First, the M1-multigroup model relies on a closure relation with no analytic expression, requiring expensive numerical evaluations. Second, the Courant-Friedrichs-Lewy condition strongly restricts the time step of the explicit schemes used in HADES. To overcome these difficulties, two complementary Artificial Intelligence based strategies were developed in this thesis.
The first approach consists in training a Multi-Layer Perceptron to approximate the M1-multigroup closure relation. This method achieves excellent accuracy while reducing the computational cost by a factor of 3000, making it the most efficient approach currently available for this task. This performance gain enables high-fidelity simulations of radiative shocks, in which radiation directly influences the shock structure. In particular, increasing spectral resolution slows down the shock and enlarges the radiative precursor.
The second approach explores the use of Physics-Informed Neural Networks to directly solve the radiation hydrodynamics equations and extrapolate simulations beyond their initial time range. Tests on purely hydrodynamic shocks show accurate handling of discontinuities, but application to radiative shocks remains challenging and requires further investigation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05155v1">üìÑ Download PDF</a></p><hr><h3 id=re-align-structured-reasoning-guided-alignment-for-in-context-image-generation-and-editinghttpsarxivorgabs260105124v1><a href=https://arxiv.org/abs/2601.05124v1>Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing</a><a hidden class=anchor aria-hidden=true href=#re-align-structured-reasoning-guided-alignment-for-in-context-image-generation-and-editinghttpsarxivorgabs260105124v1>#</a></h3><p><strong>Authors:</strong> Runze He, Yiji Cheng, Tiankai Hang, Zhimin Li, Yu Xu, Zijin Yin, Shiyi Zhang, Wenxun Dai, Penghui Du, Ao Ma, Chunyu Wang, Qinglin Lu, Jizhong Han, Jiao Dai
<strong>Venue:</strong> arXiv (2026)</p><p>In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model&rsquo;s overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05124v1">üìÑ Download PDF</a></p><hr><h3 id=three-dimensional-scene-reconstruction-using-roman-slitless-spectrahttpsarxivorgabs260105233v1><a href=https://arxiv.org/abs/2601.05233v1>Three-dimensional scene reconstruction using Roman slitless spectra</a><a hidden class=anchor aria-hidden=true href=#three-dimensional-scene-reconstruction-using-roman-slitless-spectrahttpsarxivorgabs260105233v1>#</a></h3><p><strong>Authors:</strong> Tri L. Astraatmadja, Andrew S. Fruchter, Susana E. Deustua, Helen Qu, Masao Sako, Russell E. Ryan, Yannick Copin, Greg Aldering, Rebekah A. Hounsell, David Rubin, Llu√≠s Galbany, Saul Perlmutter, Benjamin M. Rose
<strong>Venue:</strong> arXiv (2026)</p><p>The Nancy Grace Roman Space Telescope will carry out a wide-field imaging and slitless spectroscopic survey of Type Ia Supernovae to improve our understanding of dark energy. Crucial to this endeavor is obtaining supernova spectra uncontaminated by light from their host galaxies. However, obtaining such spectra is made more difficult by the inherent problem in wide-field slitless spectroscopic surveys: the blending of spectra of close objects. The spectrum of a supernova will blend with the host galaxy, even from regions distant from the supernova on the sky. If not properly removed, this contamination will introduce systematic bias when the supernova spectra are later used to determine intrinsic supernova parameters and to infer the parameters of dark energy. To address this problem we developed an algorithm that makes use of the spectroscopic observations of the host galaxy at all available observatory roll angles to reconstruct a three-dimensional (3d; 2d spatial, 1d spectral) representation of the underlying host galaxy that accurately matches the 2d slitless spectrum of the host galaxy when projected to an arbitrary rotation angle. We call this ``scene reconstruction&rsquo;&rsquo;. The projection of the reconstructed scene can be subtracted from an observation of a supernova to remove the contamination from the underlying host. Using simulated Roman data, we show that our method has extremely small systematic errors and significantly less random noise than if we subtracted a single perfectly aligned spectrum of the host obtained before or after the supernova was visible.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05233v1">üìÑ Download PDF</a></p><hr><h3 id=estimating-consensus-ideal-points-using-multi-source-datahttpsarxivorgabs260105213v1><a href=https://arxiv.org/abs/2601.05213v1>Estimating Consensus Ideal Points Using Multi-Source Data</a><a hidden class=anchor aria-hidden=true href=#estimating-consensus-ideal-points-using-multi-source-datahttpsarxivorgabs260105213v1>#</a></h3><p><strong>Authors:</strong> Mellissa Meisels, Melody Huang, Tiffany M. Tang
<strong>Venue:</strong> arXiv (2026)</p><p>In the advent of big data and machine learning, researchers now have a wealth of congressional candidate ideal point estimates at their disposal for theory testing. Weak relationships raise questions about the extent to which they capture a shared quantity &ndash; rather than idiosyncratic, domain-specific factors &ndash; yet different measures are used interchangeably in most substantive analyses. Moreover, questions central to the study of American politics implicate relationships between candidate ideal points and other variables derived from the same data sources, introducing endogeneity. We propose a method, consensus multidimensional scaling (CoMDS), which better aligns with how applied scholars use ideal points in practice. CoMDS captures the shared, stable associations of a set of underlying ideal point estimates and can be interpreted as their common spatial representation. We illustrate the utility of our approach for assessing relationships within domains of existing measures and provide a suite of diagnostic tools to aid in practical usage.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05213v1">üìÑ Download PDF</a></p><hr><h3 id=an-interpretable-data-driven-approach-to-optimizing-clinical-fall-risk-assessmenthttpsarxivorgabs260105194v1><a href=https://arxiv.org/abs/2601.05194v1>An interpretable data-driven approach to optimizing clinical fall risk assessment</a><a hidden class=anchor aria-hidden=true href=#an-interpretable-data-driven-approach-to-optimizing-clinical-fall-risk-assessmenthttpsarxivorgabs260105194v1>#</a></h3><p><strong>Authors:</strong> Fardin Ganjkhanloo, Emmett Springer, Erik H. Hoyer, Daniel L. Young, Holley Farley, Kimia Ghobadi
<strong>Venue:</strong> arXiv (2026)</p><p>In this study, we aim to better align fall risk prediction from the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically meaningful measures via a data-driven modelling approach. We conducted a retrospective cohort analysis of 54,209 inpatient admissions from three Johns Hopkins Health System hospitals between March 2022 and October 2023. A total of 20,208 admissions were included as high fall risk encounters, and 13,941 were included as low fall risk encounters. To incorporate clinical knowledge and maintain interpretability, we employed constrained score optimization (CSO) models to reweight the JHFRAT scoring weights, while preserving its additive structure and clinical thresholds. Recalibration refers to adjusting item weights so that the resulting score can order encounters more consistently by the study&rsquo;s risk labels, and without changing the tool&rsquo;s form factor or deployment workflow. The model demonstrated significant improvements in predictive performance over the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). This performance improvement translates to protecting an additional 35 high-risk patients per week across the Johns Hopkins Health System. The constrained score optimization models performed similarly with and without the EHR variables. Although the benchmark black-box model (XGBoost), improves upon the performance metrics of the knowledge-based constrained logistic regression (AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk labeling. This evidence-based approach provides a robust foundation for health systems to systematically enhance inpatient fall prevention protocols and patient safety using data-driven optimization techniques, contributing to improved risk assessment and resource allocation in healthcare settings.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05194v1">üìÑ Download PDF</a></p><hr><h3 id=information-theoretic-limits-on-exact-subgraph-alignment-problemhttpsarxivorgabs260105173v1><a href=https://arxiv.org/abs/2601.05173v1>Information-Theoretic Limits on Exact Subgraph Alignment Problem</a><a hidden class=anchor aria-hidden=true href=#information-theoretic-limits-on-exact-subgraph-alignment-problemhttpsarxivorgabs260105173v1>#</a></h3><p><strong>Authors:</strong> Chun Hei Michael Shiu, Hei Victor Cheng, Lele Wang
<strong>Venue:</strong> arXiv (2026)</p><p>The graph alignment problem aims to identify the vertex correspondence between two correlated graphs. Most existing studies focus on the scenario in which the two graphs share the same vertex set. However, in many real-world applications, such as computer vision, social network analysis, and bioinformatics, the task often involves locating a small graph pattern within a larger graph. Existing graph alignment algorithms and analysis cannot directly address these scenarios because they are not designed to identify the specific subset of vertices where the small graph pattern resides within the larger graph. Motivated by this limitation, we introduce the subgraph alignment problem, which seeks to recover both the vertex set and/or the vertex correspondence of a small graph pattern embedded in a larger graph. In the special case where the small graph pattern is an induced subgraph of the larger graph and both the vertex set and correspondence are to be recovered, the problem reduces to the subgraph isomorphism problem, which is NP-complete in the worst case. In this paper, we formally formulate the subgraph alignment problem by proposing the Erdos-Renyi subgraph pair model together with some appropriate recovery criterion. We then establish almost-tight information-theoretic results for the subgraph alignment problem and present some novel approaches for the analysis.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05173v1">üìÑ Download PDF</a></p><hr><h3 id=cov-chain-of-view-prompting-for-spatial-reasoninghttpsarxivorgabs260105172v1><a href=https://arxiv.org/abs/2601.05172v1>CoV: Chain-of-View Prompting for Spatial Reasoning</a><a hidden class=anchor aria-hidden=true href=#cov-chain-of-view-prompting-for-spatial-reasoninghttpsarxivorgabs260105172v1>#</a></h3><p><strong>Authors:</strong> Haoyu Zhao, Akide Liu, Zeyu Zhang, Weijie Wang, Feng Chen, Ruihan Zhu, Gholamreza Haffari, Bohan Zhuang
<strong>Venue:</strong> arXiv (2026)</p><p>Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision&ndash;language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.
We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56% improvement in LLM-Match, with a maximum gain of +13.62% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51% average improvement, peaking at +3.73% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05172v1">üìÑ Download PDF</a></p><hr><h3 id=plenoptic-video-generationhttpsarxivorgabs260105239v1><a href=https://arxiv.org/abs/2601.05239v1>Plenoptic Video Generation</a><a hidden class=anchor aria-hidden=true href=#plenoptic-video-generationhttpsarxivorgabs260105239v1>#</a></h3><p><strong>Authors:</strong> Xiao Fu, Shitao Tang, Min Shi, Xian Liu, Jinwei Gu, Ming-Yu Liu, Dahua Lin, Chen-Hsuan Lin
<strong>Venue:</strong> arXiv (2026)</p><p>Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: <a href=https://research.nvidia.com/labs/dir/plenopticdreamer/>https://research.nvidia.com/labs/dir/plenopticdreamer/</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05239v1">üìÑ Download PDF</a></p><hr><h3 id=multivector-reranking-in-the-era-of-strong-first-stage-retrievershttpsarxivorgabs260105200v1><a href=https://arxiv.org/abs/2601.05200v1>Multivector Reranking in the Era of Strong First-Stage Retrievers</a><a hidden class=anchor aria-hidden=true href=#multivector-reranking-in-the-era-of-strong-first-stage-retrievershttpsarxivorgabs260105200v1>#</a></h3><p><strong>Authors:</strong> Silvio Martinico, Franco Maria Nardini, Cosimo Rulli, Rossano Venturini
<strong>Venue:</strong> arXiv (2026)</p><p>Learned multivector representations power modern search systems with strong retrieval effectiveness, but their real-world use is limited by the high cost of exhaustive token-level retrieval. Therefore, most systems adopt a \emph{gather-and-refine} strategy, where a lightweight gather phase selects candidates for full scoring. However, this approach requires expensive searches over large token-level indexes and often misses the documents that would rank highest under full similarity. In this paper, we reproduce several state-of-the-art multivector retrieval methods on two publicly available datasets, providing a clear picture of the current multivector retrieval field and observing the inefficiency of token-level gathering. Building on top of that, we show that replacing the token-level gather phase with a single-vector document retriever &ndash; specifically, a learned sparse retriever (LSR) &ndash; produces a smaller and more semantically coherent candidate set. This recasts the gather-and-refine pipeline into the well-established two-stage retrieval architecture. As retrieval latency decreases, query encoding with two neural encoders becomes the dominant computational bottleneck. To mitigate this, we integrate recent inference-free LSR methods, demonstrating that they preserve the retrieval effectiveness of the dual-encoder pipeline while substantially reducing query encoding time. Finally, we investigate multiple reranking configurations that balance efficiency, memory, and effectiveness, and we introduce two optimization techniques that prune low-quality candidates early. Empirical results show that these techniques improve retrieval efficiency by up to 1.8$\times$ with no loss in quality. Overall, our two-stage approach achieves over $24\times$ speedup over the state-of-the-art multivector retrieval systems, while maintaining comparable or superior retrieval quality.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05200v1">üìÑ Download PDF</a></p><hr><h3 id=divide-and-conquer-cluster-and-manifold-based-interpretation-of-complex-flowshttpsarxivorgabs260105117v1><a href=https://arxiv.org/abs/2601.05117v1>Divide and Conquer: Cluster and manifold-based interpretation of complex flows</a><a hidden class=anchor aria-hidden=true href=#divide-and-conquer-cluster-and-manifold-based-interpretation-of-complex-flowshttpsarxivorgabs260105117v1>#</a></h3><p><strong>Authors:</strong> Qihong L. Li-Hu, Guy Y. Cornejo Maceda, Andrea Ianiro, Stefano Discetti
<strong>Venue:</strong> arXiv (2026)</p><p>We propose a framework for a global description of the dynamics of complex flows via clusterized spatial representations of the flow, isolating and identifying local dynamics, retrieving different Space-Time Cluster-Based Network Models (ST-CNMs). The key enabler is the partitioning of the domain based on a nonlinear manifold learning approach, in which spatial points are clustered based on the similarity of their dynamics, as observed in their compact embedding in manifold coordinates. The method receives as input time-resolved flow fields. From these, the spatial manifold is computed through isometric mapping applied to the vorticity time histories at each spatial location. An unsupervised clustering method, applied in the manifold space, partitions the full flow domain into subdomains. The dynamics of each subdomain are then described with cluster-based modeling. The method is demonstrated on two flow-field datasets obtained with a direct numerical simulation of a fluidic pinball under periodic forcing and with two-dimensional particle image velocimetry measurements of a transitional jet flow. The spatial manifold-based flow partitioning identifies regions with similar dynamics in an automated way. For both cases, ST-CNM identifies local dynamics that are not captured by a global approach. In particular, vortex shedding and vortex pairing dynamics are isolated in the jet flow experiment. The proposed fully automated domain partitioning method will benefit the structural description of controlled flows and unveil the actuation mechanisms at play.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05117v1">üìÑ Download PDF</a></p><hr><h3 id=multi-disciplinary-dataset-discovery-from-citation-verified-literature-contextshttpsarxivorgabs260105099v1><a href=https://arxiv.org/abs/2601.05099v1>Multi-Disciplinary Dataset Discovery from Citation-Verified Literature Contexts</a><a hidden class=anchor aria-hidden=true href=#multi-disciplinary-dataset-discovery-from-citation-verified-literature-contextshttpsarxivorgabs260105099v1>#</a></h3><p><strong>Authors:</strong> Zhiyin Tan, Changxu Duan
<strong>Venue:</strong> arXiv (2026)</p><p>Identifying suitable datasets for a research question remains challenging because existing dataset search engines rely heavily on metadata quality and keyword overlap, which often fail to capture the semantic intent of scientific investigation. We introduce a literature-driven framework that discovers datasets from citation contexts in scientific papers, enabling retrieval grounded in actual research use rather than metadata availability. Our approach combines large-scale citation-context extraction, schema-guided dataset recognition with Large Language Models, and provenance-preserving entity resolution. We evaluate the system on eight survey-derived computer science queries and find that it achieves substantially higher recall than Google Dataset Search and DataCite Commons, with normalized recall ranging from an average of 47.47% to a highest value of 81.82%. Beyond recovering gold-standard datasets, the method also surfaces additional datasets not documented in the surveys. Expert assessments across five top-level Fields of Science indicate that a substantial portion of the additional datasets are considered high utility, and some are regarded as novel for the specific topics chosen by the experts. These findings establish citation-context mining as an effective and generalizable paradigm for dataset discovery, particularly in settings where datasets lack sufficient or reliable metadata. To support reproducibility and future extensions, we release our code, evaluation datasets, and results on GitHub (<a href=https://github.com/Fireblossom/citation-context-dataset-discovery%29>https://github.com/Fireblossom/citation-context-dataset-discovery)</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05099v1">üìÑ Download PDF</a></p><hr><h3 id=surveying-exogenous-species-in-saturn-with-alma-i-detecting-and-mapping-cohttpsarxivorgabs260105090v1><a href=https://arxiv.org/abs/2601.05090v1>Surveying exogenous species in Saturn with ALMA I. Detecting and Mapping CO</a><a hidden class=anchor aria-hidden=true href=#surveying-exogenous-species-in-saturn-with-alma-i-detecting-and-mapping-cohttpsarxivorgabs260105090v1>#</a></h3><p><strong>Authors:</strong> Deborah Bardet, Thierry Fouchet, Thibault Cavali√©, Rapha√´l Moreno, Emmanuel Lellouch, Camille Lefour, Bilal Benmahi, Sandrine Guerlet
<strong>Venue:</strong> arXiv (2026)</p><p>The origin of carbon monoxide (CO) in Saturn&rsquo;s stratosphere remains uncertain, with proposed sources including internal thermochemical production, cometary impacts, and exogenic material from the rings and icy moons (i.e. Enceladus). We aim to constrain the vertical and meridional distribution of stratospheric CO and assess the relative contributions of these potential sources. Here, we analysed high-spectral-resolution ALMA observations of the CO (J=3-2) line obtained on 25 May 2018, sampling Saturn&rsquo;s limb from 20¬∞S to 69¬∞N. CO vertical profiles were retrieved using a line-by-line radiative transfer model combined with spectral inversion techniques, testing multiple prior scenarios representative of different source hypotheses. CO is confined to a narrow layer between 0.1 and 1 mbar, with a robust negative vertical gradient and mean abundances of (3.7+/- 0.8) x 10$^{-8}$ at 0.1 mbar and (7.2 +/- 0.9) x 10$^{-8}$ at 1 mbar. The meridional distribution is statistically homogeneous, with a marginal enhancement near 60¬∞ N plausibly related to Enceladus. No significant equatorial enhancement is detected. The absence of a strong equatorial enhancement rules out a long-lived steady source associated with ring infall. The observations are most consistent with a relatively recent ($\approx$200-year-old or younger) cometary impact whose material has since been horizontally mixed, while any Cassini Grand Finale ring influx was either too recent or inefficient to affect CO abundances at the probed pressure levels.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05090v1">üìÑ Download PDF</a></p><hr><h2 id=-linguistics>üîç linguistics<a hidden class=anchor aria-hidden=true href=#-linguistics>#</a></h2><h3 id=rl-awb-deep-reinforcement-learning-for-auto-white-balance-correction-in-low-light-night-time-sceneshttpsarxivorgabs260105249v1><a href=https://arxiv.org/abs/2601.05249v1>RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes</a><a hidden class=anchor aria-hidden=true href=#rl-awb-deep-reinforcement-learning-for-auto-white-balance-correction-in-low-light-night-time-sceneshttpsarxivorgabs260105249v1>#</a></h3><p><strong>Authors:</strong> Yuan-Kang Lee, Kuan-Lin Chen, Chia-Che Chang, Yu-Lun Liu
<strong>Venue:</strong> arXiv (2026)</p><p>Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: <a href=https://ntuneillee.github.io/research/rl-awb/>https://ntuneillee.github.io/research/rl-awb/</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05249v1">üìÑ Download PDF</a></p><hr><h3 id=pixel-perfect-visual-geometry-estimationhttpsarxivorgabs260105246v1><a href=https://arxiv.org/abs/2601.05246v1>Pixel-Perfect Visual Geometry Estimation</a><a hidden class=anchor aria-hidden=true href=#pixel-perfect-visual-geometry-estimationhttpsarxivorgabs260105246v1>#</a></h3><p><strong>Authors:</strong> Gangwei Xu, Haotong Lin, Hongcheng Luo, Haiyang Sun, Bing Wang, Guang Chen, Sida Peng, Hangjun Ye, Xin Yang
<strong>Venue:</strong> arXiv (2026)</p><p>Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05246v1">üìÑ Download PDF</a></p><hr><h3 id=optimal-lower-bounds-for-online-multicalibrationhttpsarxivorgabs260105245v1><a href=https://arxiv.org/abs/2601.05245v1>Optimal Lower Bounds for Online Multicalibration</a><a hidden class=anchor aria-hidden=true href=#optimal-lower-bounds-for-online-multicalibrationhttpsarxivorgabs260105245v1>#</a></h3><p><strong>Authors:</strong> Natalie Collina, Jiuyao Lu, Georgy Noarov, Aaron Roth
<strong>Venue:</strong> arXiv (2026)</p><p>We prove tight lower bounds for online multicalibration, establishing an information-theoretic separation from marginal calibration.
In the general setting where group functions can depend on both context and the learner&rsquo;s predictions, we prove an $Œ©(T^{2/3})$ lower bound on expected multicalibration error using just three disjoint binary groups. This matches the upper bounds of Noarov et al. (2025) up to logarithmic factors and exceeds the $O(T^{2/3-\varepsilon})$ upper bound for marginal calibration (Dagan et al., 2025), thereby separating the two problems.
We then turn to lower bounds for the more difficult case of group functions that may depend on context but not on the learner&rsquo;s predictions. In this case, we establish an $\widetildeŒ©(T^{2/3})$ lower bound for online multicalibration via a $Œò(T)$-sized group family constructed using orthogonal function systems, again matching upper bounds up to logarithmic factors.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05245v1">üìÑ Download PDF</a></p><hr><h3 id=unveiling-the-3d-structure-of-the-central-molecular-zone-from-stellar-kinematics-and-photometry-the-50-and-20-kms-cloudshttpsarxivorgabs260105252v1><a href=https://arxiv.org/abs/2601.05252v1>Unveiling the 3D structure of the central molecular zone from stellar kinematics and photometry: The 50 and 20 km/s clouds</a><a hidden class=anchor aria-hidden=true href=#unveiling-the-3d-structure-of-the-central-molecular-zone-from-stellar-kinematics-and-photometry-the-50-and-20-kms-cloudshttpsarxivorgabs260105252v1>#</a></h3><p><strong>Authors:</strong> Francisco Nogueras-Lara, Ashley T. Barnes, Jonathan D. Henshaw, Karl Fiteni, Yoshiaki Sofue, Rainer Sch√∂del, √Ålvaro Mart√≠nez-Arranz, Mattia C. Sormani, Jairo Armijos-Abenda√±o, Laura Colzi, Izaskun Jim√©nez-Serra, V√≠ctor M. Rivilla, Pablo Garc√≠a, Adam Ginsburg, Yue Hu, Ralf S. Klessen, J. M. Diederik Kruijssen, Volker Tolls, Alex Lazarian, Dani R. Lipman, Steven N. Longmore, Xing Lu, Sergio Mart√≠n, Denise Riquelme-V√°squez, Jaime E. Pineda, √Ålvaro S√°nchez-Monge, Arianna Vasini, Elisabeth A. C. Mills
<strong>Venue:</strong> arXiv (2026)</p><p>The central molecular zone (CMZ), surrounding the Galactic centre, is the largest reservoir of dense molecular gas in the Galaxy. Despite its relative proximity, the 3D structure of the CMZ remains poorly constrained, primarily due to projection effects. We aim to constrain the line-of-sight location of two molecular clouds in the CMZ &ndash; the 50 and 20 km/s clouds &ndash; and to investigate their possible physical connection using stellar kinematics and photometry. This study serves as a pilot for future applications across the full CMZ. We estimated the line-of-sight position of the clouds by analysing stellar kinematics, stellar densities, and stellar populations towards the cloud regions and a control field. We find an absence of westward moving stars in the cloud regions, which indicates that they lie on the near side of the CMZ. This interpretation is supported by the stellar density distributions. The similar behaviour observed in the two clouds, as well as in the region between them (the ridge), suggests that they are located at comparable distances and are physically linked. We also identified an intermediate-age stellar population (2-7 Gyr) in both regions, consistent with that observed on the near side of the CMZ. We estimated the line-of-sight distances at which the clouds and the ridge become kinematically detectable (i.e. where the proper motion component parallel to the Galactic plane differs from that of the control field at the 3 sigma level) by converting their measured proper motions parallel to the Galactic plane using a theoretical model of the stellar distribution. We find that the 50 and 20 km/s clouds are located at $43\pm8$ pc and $56\pm11$ pc from Sgr A*, respectively, and that the ridge lies at $56\pm11$ pc; this supports the idea that the clouds are physically connected through the ridge.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05252v1">üìÑ Download PDF</a></p><hr><h3 id=mesh4d-4d-mesh-reconstruction-and-tracking-from-monocular-videohttpsarxivorgabs260105251v1><a href=https://arxiv.org/abs/2601.05251v1>Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video</a><a hidden class=anchor aria-hidden=true href=#mesh4d-4d-mesh-reconstruction-and-tracking-from-monocular-videohttpsarxivorgabs260105251v1>#</a></h3><p><strong>Authors:</strong> Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, Andrea Vedaldi
<strong>Venue:</strong> arXiv (2026)</p><p>We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object&rsquo;s complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object&rsquo;s overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05251v1">üìÑ Download PDF</a></p><hr><h3 id=stability-of-the-local-ni2-electronic-structure-to-a-site-disorder-in-the-pyrochlore-antiferromagnet-nacani_2f_7httpsarxivorgabs260105236v1><a href=https://arxiv.org/abs/2601.05236v1>Stability of the Local Ni$^{2+}$ Electronic Structure to $A$-site Disorder in the Pyrochlore Antiferromagnet NaCaNi$_2$F$_7$</a><a hidden class=anchor aria-hidden=true href=#stability-of-the-local-ni2-electronic-structure-to-a-site-disorder-in-the-pyrochlore-antiferromagnet-nacani_2f_7httpsarxivorgabs260105236v1>#</a></h3><p><strong>Authors:</strong> M. F. DiScala, A. de la Torre, J. W. Krizan, J. Wouters, V. Bisogni, J. Pelliciari, R. J. Cava, K. W. Plumb
<strong>Venue:</strong> arXiv (2026)</p><p>NaCaNi$<em>2$F$<em>7$ is a unique example of spin-1 Heisenberg antiferromagnet on the pyrochlore lattice, but the presence of Na$^{1+}$/Ca$^{2+}$ $A$-site disorder complicates the local electronic and magnetic environment of the Ni$^{2+}$ $B$-site. We utilize resonant inelastic X-ray scattering (RIXS) to study the influence of $A$-site disorder on the $B$-site electronic structure of NaCaNi$<em>2$F$<em>7$. Ni L-edge RIXS measurements reveal a Ni$^{2+}$ electronic structure in nearly ideal octahedral coordination, with only a small trigonal compression ($Œ¥$ = -200$;$meV) required to capture all spectral features. Within the $D</em>{3d}$ symmetry of the Ni local environment, we extract an anisotropic $g$-factor of $g</em>{\parallel} = 2.26$ and $g</em>{\perp} = 2.27$, and a corresponding paramagnetic moment of $Œº</em>{\rm{eff}}=3.2;Œº_B$. To simulate disorder, RIXS spectra were calculated with realistic distributions of crystal field parameters; however, these spectra are invariant relative to a disorder-free model, demonstrating the robustness of the Ni$^{2+}$ electronic environment to the $A$-site disorder, within the resolution of our measurement.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05236v1">üìÑ Download PDF</a></p><hr><h3 id=mimicking-phantom-dark-energy-with-evolving-dark-matter-masshttpsarxivorgabs260105235v1><a href=https://arxiv.org/abs/2601.05235v1>Mimicking Phantom Dark Energy with Evolving Dark Matter Mass</a><a hidden class=anchor aria-hidden=true href=#mimicking-phantom-dark-energy-with-evolving-dark-matter-masshttpsarxivorgabs260105235v1>#</a></h3><p><strong>Authors:</strong> Lorenzo La Penna, Alessio Notari, Michele Redi
<strong>Venue:</strong> arXiv (2026)</p><p>We present a general method to reproduce a given cosmological background through energy exchange between dark energy (DE) and dark matter (DM). This can be simply realized with a standard quintessence scalar field that controls the DM mass. In particular a background with phantom crossing can be effectively realized without introducing ghosts or other pathologies. For example one can reproduce exactly the background that gives the best fit to the recent DESI+CMB+DESY5 data, within the Chevallier-Polarski-Linder (CPL) parametrization of DE. Although the background evolution is identical, the perturbations differ, leading to modified growth of structures. If the DM mass varies at late times, early-time observables are not modified and can reproduce the main predictions of the target model, but late-time observables are affected. We discuss in particular the effects on the matter power spectrum, CMB lensing and ISW effect. When reproducing the best fit CPL background model, this scenario generically predicts $\mathcal{O}(10%)$ deviations in such observables. However, for suitable choices of parameters, effects on the matter power spectrum can be smaller, motivating a detailed study. In general, energy exchange between DE and DM generates a mismatch between the matter power spectrum and the gravitational potential amplitudes compared to the decoupled case, that can lead to deviations observable in future experiments.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05235v1">üìÑ Download PDF</a></p><hr><h3 id=stochastic-deep-learning-a-probabilistic-framework-for-modeling-uncertainty-in-structured-temporal-datahttpsarxivorgabs260105227v1><a href=https://arxiv.org/abs/2601.05227v1>Stochastic Deep Learning: A Probabilistic Framework for Modeling Uncertainty in Structured Temporal Data</a><a hidden class=anchor aria-hidden=true href=#stochastic-deep-learning-a-probabilistic-framework-for-modeling-uncertainty-in-structured-temporal-datahttpsarxivorgabs260105227v1>#</a></h3><p><strong>Authors:</strong> James Rice
<strong>Venue:</strong> arXiv (2026)</p><p>I propose a novel framework that integrates stochastic differential equations (SDEs) with deep generative models to improve uncertainty quantification in machine learning applications involving structured and temporal data. This approach, termed Stochastic Latent Differential Inference (SLDI), embeds an It√¥ SDE in the latent space of a variational autoencoder, allowing for flexible, continuous-time modeling of uncertainty while preserving a principled mathematical foundation. The drift and diffusion terms of the SDE are parameterized by neural networks, enabling data-driven inference and generalizing classical time series models to handle irregular sampling and complex dynamic structure.
A central theoretical contribution is the co-parameterization of the adjoint state with a dedicated neural network, forming a coupled forward-backward system that captures not only latent evolution but also gradient dynamics. I introduce a pathwise-regularized adjoint loss and analyze variance-reduced gradient flows through the lens of stochastic calculus, offering new tools for improving training stability in deep latent SDEs. My paper unifies and extends variational inference, continuous-time generative modeling, and control-theoretic optimization, providing a rigorous foundation for future developments in stochastic probabilistic machine learning.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05227v1">üìÑ Download PDF</a></p><hr><h3 id=measurement-of-the-higgs-boson-total-decay-width-using-the-h-to-ww-to-eŒΩŒºŒΩ-decay-channel-in-proton-proton-collisions-at-sqrts--13-tevhttpsarxivorgabs260105168v1><a href=https://arxiv.org/abs/2601.05168v1>Measurement of the Higgs boson total decay width using the H $\to$ WW $\to$ e$ŒΩŒºŒΩ$ decay channel in proton-proton collisions at $\sqrt{s}$ = 13 TeV</a><a hidden class=anchor aria-hidden=true href=#measurement-of-the-higgs-boson-total-decay-width-using-the-h-to-ww-to-eŒΩŒºŒΩ-decay-channel-in-proton-proton-collisions-at-sqrts--13-tevhttpsarxivorgabs260105168v1>#</a></h3><p><strong>Authors:</strong> CMS Collaboration
<strong>Venue:</strong> arXiv (2026)</p><p>The Higgs boson (H) decay width is determined from the ratio of off- and on-shell production of H $\to$ WW $\to$ e$ŒΩŒºŒΩ$ using proton-proton collision data corresponding to an integrated luminosity of 138 fb$^{-1}$ collected at $\sqrt{s}$ = 13 TeV by the CMS experiment at the LHC. The off-shell signal strength is measured as $Œº_\text{off-shell}$ = 1.2$^{+0.8}<em>{-0.7}$. The Higgs boson total decay width is $Œì</em>\text{H}$ = 3.9$^{+2.7}_{-2.2}$ MeV, in agreement with the standard model prediction. The uncertainty in this result represents a factor of three improvement over the previous CMS result in this decay channel.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05168v1">üìÑ Download PDF</a></p><hr><h3 id=a-first-principles-study-of-weyl-nodal-loop-and-multiple-sets-of-weyl-points-in-trigonal-ptbi_2httpsarxivorgabs260105123v1><a href=https://arxiv.org/abs/2601.05123v1>A First-principles Study of Weyl Nodal Loop and Multiple Sets of Weyl Points in Trigonal PtBi$_2$</a><a hidden class=anchor aria-hidden=true href=#a-first-principles-study-of-weyl-nodal-loop-and-multiple-sets-of-weyl-points-in-trigonal-ptbi_2httpsarxivorgabs260105123v1>#</a></h3><p><strong>Authors:</strong> Lin-Lin Wang
<strong>Venue:</strong> arXiv (2026)</p><p>Coexistence of surface superconductivity and Fermi arcs in trigonal $Œ≥$-PtBi$_2$ has recently attracted attention for possible realization of topological superconductivity. The Fermi arcs on the two different (0001) surface terminations have been associated with the set of Weyl points just above the Fermi energy (E$_F$). Here using first-principles calculations to explore the band crossings over the full Brillouin zone between the nominally highest valence and lowest conduction bands in $Œ≥$-PtBi$_2$, we find a Weyl nodal loop (WNL) and multiple sets of Weyl points (WPs). The main difference between the two reported experimental structural parameters is the magnitude of Bi-layer buckling. While the WNL, bulk gap region and the set of Weyl points just above the E$_F$ are robust, the number and location of the other sets of WPs depend sensitively on the structural parameters with different magnitude of Bi-layer buckling. Besides calculating the 2D Fermi surface with Fermi arcs and quasi-particle interference (QPI) around the E$_F$ in good agreements with ARPES and experimental QPI, we also predict new Fermi arc features at higher energy.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05123v1">üìÑ Download PDF</a></p><hr><h3 id=multigroup-radiation-diffusion-on-a-moving-mesh-implementation-in-rich-and-application-to-tidal-disruption-eventshttpsarxivorgabs260105120v1><a href=https://arxiv.org/abs/2601.05120v1>Multigroup Radiation Diffusion on a Moving Mesh: Implementation in RICH and Application to Tidal Disruption Events</a><a hidden class=anchor aria-hidden=true href=#multigroup-radiation-diffusion-on-a-moving-mesh-implementation-in-rich-and-application-to-tidal-disruption-eventshttpsarxivorgabs260105120v1>#</a></h3><p><strong>Authors:</strong> Itamar Giron, Menahem Krief, Nicholas C. Stone, Elad Steinberg
<strong>Venue:</strong> arXiv (2026)</p><p>Radiation-hydrodynamics (RHD) determines the bulk evolution and observable emission in a wide variety of high-energy astrophysical phenomena. Due to their complexity, RHD problems must usually be studied through numerical simulation. We have extended the publicly available RICH code, which previously solved the equations of RHD in the limit of grey flux-limited diffusion (FLD), to operate with a multigroup FLD solver. RICH is a semi-Lagrangian code that solves the equations of RHD on an unstructured moving mesh, and is the first multigroup RHD moving mesh code, making it uniquely applicable to problems with extreme dynamic range and dynamically important radiation forces. We validate our multigroup module against multiple analytic benchmarks, including a novel test of the RHD Doppler term. The computational efficiency of the code is aided by a novel scheme to accelerate convergence in optically thick cells. Finally, we apply multigroup RICH in a pilot study of a stellar tidal disruption event (TDE), using a $10^4 M_\odot$ intermediate-mass black hole. Our simulations self-consistently produce a bright early-time X-ray flash prior to peak optical/UV light, in qualitative agreement with post-processing of (grey) RICH simulations of supermassive black hole TDEs, as well as X-ray observations of the TDE AT 2022dsb.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05120v1">üìÑ Download PDF</a></p><hr><h3 id=evaluative-fingerprints-stable-and-systematic-differences-in-llm-evaluator-behaviorhttpsarxivorgabs260105114v1><a href=https://arxiv.org/abs/2601.05114v1>Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior</a><a hidden class=anchor aria-hidden=true href=#evaluative-fingerprints-stable-and-systematic-differences-in-llm-evaluator-behaviorhttpsarxivorgabs260105114v1>#</a></h3><p><strong>Authors:</strong> Wajid Nasser
<strong>Venue:</strong> arXiv (2026)</p><p>LLM-as-judge systems promise scalable, consistent evaluation. We find the opposite: judges are consistent, but not with each other; they are consistent with themselves. Across 3,240 evaluations (9 judges x 120 unique video x pack items x 3 independent runs), inter-judge agreement is near-zero (Krippendorff&rsquo;s Œ± = 0.042). On two dimensions, judges disagree more than random noise would predict (Œ± &lt; 0). Yet this disagreement isn&rsquo;t chaos; it&rsquo;s structured. A classifier identifies which judge produced an evaluation with 77.1% accuracy from rubric scores alone, rising to 89.9% with disposition features. Within model families, the signal is even stronger: GPT-4.1 and GPT-5.2 are distinguishable with 99.6% accuracy. We call this the reliability paradox: judges cannot agree on what constitutes quality, yet their disagreement patterns are so stable they function as fingerprints. Each judge implements a distinct, stable theory of quality: an &ldquo;evaluative disposition&rdquo; that shapes how it interprets any rubric. We characterize these dispositions along multiple axes: harshness/leniency, dimension emphasis, within-judge stability (ICC), and evidence behavior (receipt validity, semantic linkage via NLI, and shotgun index). The implication is stark: LLM judges are not interchangeable instruments measuring a shared construct. They are distinct measurement devices, each encoding its own implicit theory of quality. Averaging their scores produces a synthetic verdict that corresponds to no judge&rsquo;s actual values.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05114v1">üìÑ Download PDF</a></p><hr><h3 id=semantically-orthogonal-framework-for-citation-classification-disentangling-intent-and-contenthttpsarxivorgabs260105103v1><a href=https://arxiv.org/abs/2601.05103v1>Semantically Orthogonal Framework for Citation Classification: Disentangling Intent and Content</a><a hidden class=anchor aria-hidden=true href=#semantically-orthogonal-framework-for-citation-classification-disentangling-intent-and-contenthttpsarxivorgabs260105103v1>#</a></h3><p><strong>Authors:</strong> Changxu Duan, Zhiyin Tan
<strong>Venue:</strong> arXiv (2026)</p><p>Understanding the role of citations is essential for research assessment and citation-aware digital libraries. However, existing citation classification frameworks often conflate citation intent (why a work is cited) with cited content type (what part is cited), limiting their effectiveness in auto classification due to a dilemma between fine-grained type distinctions and practical classification reliability. We introduce SOFT, a Semantically Orthogonal Framework with Two dimensions that explicitly separates citation intent from cited content type, drawing inspiration from semantic role theory. We systematically re-annotate the ACL-ARC dataset using SOFT and release a cross-disciplinary test set sampled from ACT2. Evaluation with both zero-shot and fine-tuned Large Language Models demonstrates that SOFT enables higher agreement between human annotators and LLMs, and supports stronger classification performance and robust cross-domain generalization compared to ACL-ARC and SciCite annotation frameworks. These results confirm SOFT&rsquo;s value as a clear, reusable annotation standard, improving clarity, consistency, and generalizability for digital libraries and scholarly communication infrastructures. All code and data are publicly available on GitHub <a href=https://github.com/zhiyintan/SOFT>https://github.com/zhiyintan/SOFT</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05103v1">üìÑ Download PDF</a></p><hr><h3 id=qnerf-neural-radiance-fields-on-a-simulated-gate-based-quantum-computerhttpsarxivorgabs260105250v1><a href=https://arxiv.org/abs/2601.05250v1>QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer</a><a hidden class=anchor aria-hidden=true href=#qnerf-neural-radiance-fields-on-a-simulated-gate-based-quantum-computerhttpsarxivorgabs260105250v1>#</a></h3><p><strong>Authors:</strong> Daniele Lizzio Bosco, Shuteng Wang, Giuseppe Serra, Vladislav Golyanik
<strong>Venue:</strong> arXiv (2026)</p><p>Recently, Quantum Visual Fields (QVFs) have shown promising improvements in model compactness and convergence speed for learning the provided 2D or 3D signals. Meanwhile, novel-view synthesis has seen major advances with Neural Radiance Fields (NeRFs), where models learn a compact representation from 2D images to render 3D scenes, albeit at the cost of larger models and intensive training. In this work, we extend the approach of QVFs by introducing QNeRF, the first hybrid quantum-classical model designed for novel-view synthesis from 2D images. QNeRF leverages parameterised quantum circuits to encode spatial and view-dependent information via quantum superposition and entanglement, resulting in more compact models compared to the classical counterpart. We present two architectural variants. Full QNeRF maximally exploits all quantum amplitudes to enhance representational capabilities. In contrast, Dual-Branch QNeRF introduces a task-informed inductive bias by branching spatial and view-dependent quantum state preparations, drastically reducing the complexity of this operation and ensuring scalability and potential hardware compatibility. Our experiments demonstrate that &ndash; when trained on images of moderate resolution &ndash; QNeRF matches or outperforms classical NeRF baselines while using less than half the number of parameters. These results suggest that quantum machine learning can serve as a competitive alternative for continuous signal representation in mid-level tasks in computer vision, such as 3D representation learning from 2D observations.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05250v1">üìÑ Download PDF</a></p><hr><h3 id=mechanics-of-axis-formation-in-textithydrahttpsarxivorgabs260105220v1><a href=https://arxiv.org/abs/2601.05220v1>Mechanics of axis formation in $\textit{Hydra}$</a><a hidden class=anchor aria-hidden=true href=#mechanics-of-axis-formation-in-textithydrahttpsarxivorgabs260105220v1>#</a></h3><p><strong>Authors:</strong> Arthur Hernandez, Cuncheng Zhu, Luca Giomi
<strong>Venue:</strong> arXiv (2026)</p><p>The emergence of a body axis is a fundamental step in the development of multicellular organisms. In simple systems such as $\textit{Hydra}$, growing evidence suggests that mechanical forces generated by collective cellular activity play a central role in this process. Here, we explore a physical mechanism for axis formation based on the coupling between active stresses and tissue elasticity. We analyse the elastic deformation induced by activity-generated stresses and show that, owing to the spherical topology of the tissue, forces globally condense toward configurations in which both elastic strain and nematic defect localise at opposite poles. These mechanically selected states define either a polar or apolar head-food axis. To characterize the condensed regime, we introduce a compact parametrization of of the active force and flux distributions, enabling analytical predictions and direct comparison with experiments. Using this framework, we calculate experimentally relevant observables, including areal strain, lateral pressure, and normal displacements during muscular contraction, as well as the detailed structure of topological defect complexes in head and foot regions. Together, our results identify a mechanical route by which active tissues can spontaneously break symmetry at the organismal scale, suggesting a general physical principle underlying body-axis specification during morphogenesis.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05220v1">üìÑ Download PDF</a></p><hr><h3 id=raar-retrieval-augmented-agentic-reasoning-for-cross-domain-misinformation-detectionhttpsarxivorgabs260104853v1><a href=https://arxiv.org/abs/2601.04853v1>RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection</a><a hidden class=anchor aria-hidden=true href=#raar-retrieval-augmented-agentic-reasoning-for-cross-domain-misinformation-detectionhttpsarxivorgabs260104853v1>#</a></h3><p><strong>Authors:</strong> Zhiwei Liu, Runteng Guo, Baojie Qu, Yuechen Jiang, Min Peng, Qianqian Xie, Sophia Ananiadou
<strong>Venue:</strong> arXiv (2026)</p><p>Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample&rsquo;s semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at <a href=https://github.com/lzw108/RAAR>https://github.com/lzw108/RAAR</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04853v1">üìÑ Download PDF</a></p><hr><h3 id=defense-against-indirect-prompt-injection-via-tool-result-parsinghttpsarxivorgabs260104795v1><a href=https://arxiv.org/abs/2601.04795v1>Defense Against Indirect Prompt Injection via Tool Result Parsing</a><a hidden class=anchor aria-hidden=true href=#defense-against-indirect-prompt-injection-via-tool-result-parsinghttpsarxivorgabs260104795v1>#</a></h3><p><strong>Authors:</strong> Qiang Yu, Xinran Cheng, Chuanyi Liu
<strong>Venue:</strong> arXiv (2026)</p><p>As LLM agents transition from digital assistants to physical controllers in autonomous systems and robotics, they face an escalating threat from indirect prompt injection. By embedding adversarial instructions into the results of tool calls, attackers can hijack the agent&rsquo;s decision-making process to execute unauthorized actions. This vulnerability poses a significant risk as agents gain more direct control over physical environments. Existing defense mechanisms against Indirect Prompt Injection (IPI) generally fall into two categories. The first involves training dedicated detection models; however, this approach entails high computational overhead for both training and inference, and requires frequent updates to keep pace with evolving attack vectors. Alternatively, prompt-based methods leverage the inherent capabilities of LLMs to detect or ignore malicious instructions via prompt engineering. Despite their flexibility, most current prompt-based defenses suffer from high Attack Success Rates (ASR), demonstrating limited robustness against sophisticated injection attacks. In this paper, we propose a novel method that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. Our approach achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. Code is available at GitHub.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04795v1">üìÑ Download PDF</a></p><hr><h3 id=scifig-towards-automating-scientific-figure-generationhttpsarxivorgabs260104390v1><a href=https://arxiv.org/abs/2601.04390v1>SciFig: Towards Automating Scientific Figure Generation</a><a hidden class=anchor aria-hidden=true href=#scifig-towards-automating-scientific-figure-generationhttpsarxivorgabs260104390v1>#</a></h3><p><strong>Authors:</strong> Siyuan Huang, Yutong Gao, Juyang Bai, Yifan Zhou, Zi Yin, Xinxin Liu, Rama Chellappa, Chun Pong Lau, Sayan Nag, Cheng Peng, Shraman Pramanick
<strong>Venue:</strong> arXiv (2026)</p><p>Creating high-quality figures and visualizations for scientific papers is a time-consuming task that requires both deep domain knowledge and professional design skills. Despite over 2.5 million scientific papers published annually, the figure generation process remains largely manual. We introduce $\textbf{SciFig}$, an end-to-end AI agent system that generates publication-ready pipeline figures directly from research paper texts. SciFig uses a hierarchical layout generation strategy, which parses research descriptions to identify component relationships, groups related elements into functional modules, and generates inter-module connections to establish visual organization. Furthermore, an iterative chain-of-thought (CoT) feedback mechanism progressively improves layouts through multiple rounds of visual analysis and reasoning. We introduce a rubric-based evaluation framework that analyzes 2,219 real scientific figures to extract evaluation rubrics and automatically generates comprehensive evaluation criteria. SciFig demonstrates remarkable performance: achieving 70.1$%$ overall quality on dataset-level evaluation and 66.2$%$ on paper-specific evaluation, and consistently high scores across metrics such as visual clarity, structural organization, and scientific accuracy. SciFig figure generation pipeline and our evaluation benchmark will be open-sourced.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04390v1">üìÑ Download PDF</a></p><hr><h3 id=disco-rag-discourse-aware-retrieval-augmented-generationhttpsarxivorgabs260104377v1><a href=https://arxiv.org/abs/2601.04377v1>Disco-RAG: Discourse-Aware Retrieval-Augmented Generation</a><a hidden class=anchor aria-hidden=true href=#disco-rag-discourse-aware-retrieval-augmented-generationhttpsarxivorgabs260104377v1>#</a></h3><p><strong>Authors:</strong> Dongqi Liu, Hang Ding, Qiming Feng, Jian Li, Xurong Xie, Zhucun Xue, Chengjie Wang, Jiangning Zhang, Yabiao Wang
<strong>Venue:</strong> arXiv (2026)</p><p>Retrieval-Augmented Generation (RAG) has emerged as an important means of enhancing the performance of large language models (LLMs) in knowledge-intensive tasks. However, most existing RAG strategies treat retrieved passages in a flat and unstructured way, which prevents the model from capturing structural cues and constrains its ability to synthesize knowledge from dispersed evidence across documents. To overcome these limitations, we propose Disco-RAG, a discourse-aware framework that explicitly injects discourse signals into the generation process. Our method constructs intra-chunk discourse trees to capture local hierarchies and builds inter-chunk rhetorical graphs to model cross-passage coherence. These structures are jointly integrated into a planning blueprint that conditions the generation. Experiments on question answering and long-document summarization benchmarks show the efficacy of our approach. Disco-RAG achieves state-of-the-art results on the benchmarks without fine-tuning. These findings underscore the important role of discourse structure in advancing RAG systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04377v1">üìÑ Download PDF</a></p><hr><h3 id=llmberjack-guided-trimming-of-debate-trees-for-multi-party-conversation-creationhttpsarxivorgabs260104135v1><a href=https://arxiv.org/abs/2601.04135v1>LLMberjack: Guided Trimming of Debate Trees for Multi-Party Conversation Creation</a><a hidden class=anchor aria-hidden=true href=#llmberjack-guided-trimming-of-debate-trees-for-multi-party-conversation-creationhttpsarxivorgabs260104135v1>#</a></h3><p><strong>Authors:</strong> Leonardo Bottona, Nicol√≤ Penzo, Bruno Lepri, Marco Guerini, Sara Tonelli
<strong>Venue:</strong> arXiv (2026)</p><p>We present LLMberjack, a platform for creating multi-party conversations starting from existing debates, originally structured as reply trees. The system offers an interactive interface that visualizes discussion trees and enables users to construct coherent linearized dialogue sequences while preserving participant identity and discourse relations. It integrates optional large language model (LLM) assistance to support automatic editing of the messages and speakers&rsquo; descriptions. We demonstrate the platform&rsquo;s utility by showing how tree visualization facilitates the creation of coherent, meaningful conversation threads and how LLM support enhances output quality while reducing human effort. The tool is open-source and designed to promote transparent and reproducible workflows to create multi-party conversations, addressing a lack of resources of this type.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04135v1">üìÑ Download PDF</a></p><hr><h3 id=the-bathtub-of-european-ai-governance-identifying-technical-sandboxes-as-the-micro-foundation-of-regulatory-learninghttpsarxivorgabs260104094v1><a href=https://arxiv.org/abs/2601.04094v1>The Bathtub of European AI Governance: Identifying Technical Sandboxes as the Micro-Foundation of Regulatory Learning</a><a hidden class=anchor aria-hidden=true href=#the-bathtub-of-european-ai-governance-identifying-technical-sandboxes-as-the-micro-foundation-of-regulatory-learninghttpsarxivorgabs260104094v1>#</a></h3><p><strong>Authors:</strong> Tom Deckenbrunnen, Alessio Buscemi, Marco Almada, Alfredo Capozucca, German Castignani
<strong>Venue:</strong> arXiv (2026)</p><p>The EU AI Act adopts a horizontal and adaptive approach to govern AI technologies characterised by rapid development and unpredictable emerging capabilities. To maintain relevance, the Act embeds provisions for regulatory learning. However, these provisions operate within a complex network of actors and mechanisms that lack a clearly defined technical basis for scalable information flow. This paper addresses this gap by establishing a theoretical model of regulatory learning space defined by the AI Act, decomposed into micro, meso, and macro levels. Drawing from this functional perspective of this model, we situate the diverse stakeholders - ranging from the EU Commission at the macro level to AI developers at the micro level - within the transitions of enforcement (macro-micro) and evidence aggregation (micro-macro). We identify AI Technical Sandboxes as the essential engine for evidence generation at the micro level, providing the necessary data to drive scalable learning across all levels of the model. By providing an extensive discussion of the requirements and challenges for AITSes to serve as this micro-level evidence generator, we aim to bridge the gap between legislative commands and technical operationalisation, thereby enabling a structured discourse between technical and legal experts.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04094v1">üìÑ Download PDF</a></p><hr><h3 id=symbolically-regressing-dark-matter-halo-profiles-using-weak-lensinghttpsarxivorgabs260105203v1><a href=https://arxiv.org/abs/2601.05203v1>Symbolically regressing dark matter halo profiles using weak lensing</a><a hidden class=anchor aria-hidden=true href=#symbolically-regressing-dark-matter-halo-profiles-using-weak-lensinghttpsarxivorgabs260105203v1>#</a></h3><p><strong>Authors:</strong> Alicia Mart√≠n, Tariq Yasin, Deaglan J. Bartlett, Harry Desmond, Pedro G. Ferreira
<strong>Venue:</strong> arXiv (2026)</p><p>The structure of dark matter haloes is often described by radial density profiles motivated by cosmological simulations. These are typically assumed to have a fixed functional form (e.g. NFW), with some free parameters that can be constrained with observations. However, relying on simulations has the disadvantage that the resulting profiles depend on the dark matter model and the baryonic physics implementation, which are highly uncertain. Instead, we present a method to constrain halo density profiles directly from observations. This is done using a symbolic regression algorithm called Exhaustive Symbolic Regression (ESR). ESR searches for the optimal analytic expression to fit data, combining both accuracy and simplicity. We apply ESR to a sample of 149 galaxy clusters from the HSC-XXL survey to identify which functional forms perform best across the entire sample of clusters. We identify density profiles that statistically outperform NFW under a minimum-description-length criterion. Within the radial range probed by the weak-lensing data ($R \sim 0.3 - 3$ h$^{-1}$ Mpc), the highest-ranked ESR profiles exhibit shallow inner behaviour and a maximum in the density profile. As a practical application, we show how the best-fitting ESR models can be used to obtain enclosed mass estimates. We find masses that are, on average, higher than those derived using NFW, highlighting a source of potential bias when assuming the wrong density profile. These results have important knock-on effects for analyses that utilise clusters, for example cosmological constraints on $œÉ_8$ and $Œ©_m$ from cluster abundance and clustering. Beyond the HSC dataset, the method is readily applicable to any data constraining the dark matter distribution in galaxies and galaxy clusters, such as other weak lensing surveys, galactic rotation curves, or complementary probes.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05203v1">üìÑ Download PDF</a></p><hr><h3 id=beyond-the-imbalance-site-resolved-dynamics-probing-resonances-in-many-body-localizationhttpsarxivorgabs260105177v1><a href=https://arxiv.org/abs/2601.05177v1>Beyond the imbalance: site-resolved dynamics probing resonances in many-body localization</a><a hidden class=anchor aria-hidden=true href=#beyond-the-imbalance-site-resolved-dynamics-probing-resonances-in-many-body-localizationhttpsarxivorgabs260105177v1>#</a></h3><p><strong>Authors:</strong> Asmi Haldar, Thibault Scoquart, Fabien Alet, Nicolas Laflorencie
<strong>Venue:</strong> arXiv (2026)</p><p>We explore the limitations of using imbalance dynamics as a diagnostic tool for many-body localization (MBL) and show that spatial averaging can mask important microscopic features. Focusing on the strongly disordered regime of the random-field XXZ chain, we use state-of-the-art numerical techniques (Krylov time evolution and full diagonalization) to demonstrate that site-resolved spin autocorrelators reveal a rich and complex dynamical behavior that is obscured by the imbalance observable. By analyzing the time evolution and infinite-time limits of these local probes, we reveal resonant structures and rare local instabilities within the MBL phase. These numerical findings are supported by an analytical, few-site toy model that captures the emergence of a multiple-peak structure in local magnetization histograms, which is a hallmark of local resonances. These few-body local effects provide a more detailed understanding of ergodicity-breaking dynamics, and also allow us to explain the finite-size effects of long-time imbalance, and its sensitivity to the initial conditions in quench protocols. Overall, our experimentally testable predictions highlight the necessity of a refined, site-resolved approach to fully understand the complexities of MBL and its connection to rare-region effects.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05177v1">üìÑ Download PDF</a></p><hr><h3 id=vision-language-introspection-mitigating-overconfident-hallucinations-in-mllms-via-interpretable-bi-causal-steeringhttpsarxivorgabs260105159v1><a href=https://arxiv.org/abs/2601.05159v1>Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering</a><a hidden class=anchor aria-hidden=true href=#vision-language-introspection-mitigating-overconfident-hallucinations-in-mllms-via-interpretable-bi-causal-steeringhttpsarxivorgabs260105159v1>#</a></h3><p><strong>Authors:</strong> Shuliang Liu, Songbo Yang, Dong Fang, Sihang Jia, Yuqi Tang, Lingfeng Su, Ruoshui Peng, Yibo Yan, Xin Zou, Xuming Hu
<strong>Venue:</strong> arXiv (2026)</p><p>Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05159v1">üìÑ Download PDF</a></p><hr><h3 id=pc2-politically-controversial-content-generation-via-jailbreaking-attacks-on-gpt-based-text-to-image-modelshttpsarxivorgabs260105150v1><a href=https://arxiv.org/abs/2601.05150v1>$PC^2$: Politically Controversial Content Generation via Jailbreaking Attacks on GPT-based Text-to-Image Models</a><a hidden class=anchor aria-hidden=true href=#pc2-politically-controversial-content-generation-via-jailbreaking-attacks-on-gpt-based-text-to-image-modelshttpsarxivorgabs260105150v1>#</a></h3><p><strong>Authors:</strong> Wonwoo Choi, Minjae Seo, Minkyoo Song, Hwanjo Heo, Seungwon Shin, Myoungsung You
<strong>Venue:</strong> arXiv (2026)</p><p>The rapid evolution of text-to-image (T2I) models has enabled high-fidelity visual synthesis on a global scale. However, these advancements have introduced significant security risks, particularly regarding the generation of harmful content. Politically harmful content, such as fabricated depictions of public figures, poses severe threats when weaponized for fake news or propaganda. Despite its criticality, the robustness of current T2I safety filters against such politically motivated adversarial prompting remains underexplored. In response, we propose $PC^2$, the first black-box political jailbreaking framework for T2I models. It exploits a novel vulnerability where safety filters evaluate political sensitivity based on linguistic context. $PC^2$ operates through: (1) Identity-Preserving Descriptive Mapping to obfuscate sensitive keywords into neutral descriptions, and (2) Geopolitically Distal Translation to map these descriptions into fragmented, low-sensitivity languages. This strategy prevents filters from constructing toxic relationships between political entities within prompts, effectively bypassing detection. We construct a benchmark of 240 politically sensitive prompts involving 36 public figures. Evaluation on commercial T2I models, specifically GPT-series, shows that while all original prompts are blocked, $PC^2$ achieves attack success rates of up to 86%.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05150v1">üìÑ Download PDF</a></p><hr><h3 id=when-and-why-non-hermitian-eigenvalues-miss-eigenstates-in-topological-physicshttpsarxivorgabs260105234v1><a href=https://arxiv.org/abs/2601.05234v1>When and why non-Hermitian eigenvalues miss eigenstates in topological physics</a><a hidden class=anchor aria-hidden=true href=#when-and-why-non-hermitian-eigenvalues-miss-eigenstates-in-topological-physicshttpsarxivorgabs260105234v1>#</a></h3><p><strong>Authors:</strong> Lucien Jezequel, Lo√Øc Herviou, Jens Bardarson
<strong>Venue:</strong> arXiv (2026)</p><p>Non-Hermitian systems exhibit a fundamental spectral dichotomy absent in Hermitian physics: the eigenvalue spectrum and the eigenstate spectrum can deviate significantly in the thermodynamic limit. We explain how non-Hermitian Hamiltonians can support eigenstates completely undetected by eigenvalues, with the unidirectional Hatano-Nelson model serving as both a minimal realization and universal paradigm for this phenomenon. Through exact analytical solutions, we show that this model contains not only hidden modes but multiple macroscopic hidden exceptional points that appear more generally in all systems with a non-trivial bulk winding. Our framework explains how the apparent bulk-edge correspondence failures in models like the non-Hermitian SSH chain instead reflect the systematic inability of the eigenvalue spectrum to detect certain eigenstates in systems with a skin-effect. These results establish the limitation of the eigenvalue spectrum and suggest how the eigenstate approach can lead to improved characterization of non-Hermitian topology.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05234v1">üìÑ Download PDF</a></p><hr><h3 id=from-rays-to-projections-better-inputs-for-feed-forward-view-synthesishttpsarxivorgabs260105116v1><a href=https://arxiv.org/abs/2601.05116v1>From Rays to Projections: Better Inputs for Feed-Forward View Synthesis</a><a hidden class=anchor aria-hidden=true href=#from-rays-to-projections-better-inputs-for-feed-forward-view-synthesishttpsarxivorgabs260105116v1>#</a></h3><p><strong>Authors:</strong> Zirui Wu, Zeren Jiang, Martin R. Oswald, Jie Song
<strong>Venue:</strong> arXiv (2026)</p><p>Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Pl√ºcker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05116v1">üìÑ Download PDF</a></p><hr><h3 id=horn-inequalities-on-a-quiver-with-an-involutionhttpsarxivorgabs260105089v1><a href=https://arxiv.org/abs/2601.05089v1>Horn inequalities on a quiver with an involution</a><a hidden class=anchor aria-hidden=true href=#horn-inequalities-on-a-quiver-with-an-involutionhttpsarxivorgabs260105089v1>#</a></h3><p><strong>Authors:</strong> Antoine M√©doc
<strong>Venue:</strong> arXiv (2026)</p><p>Derksen and Weyman described the cone of semi-invariants associated with a quiver. We give an inductive description of this cone, followed by an example of refinement of the inequalities characterising anti-invariant weights in the case of a quiver equipped with an involution.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05089v1">üìÑ Download PDF</a></p><hr><h3 id=effect-of-dispatch-decisions-on-small-signal-stability-of-converter-dominated-power-systemshttpsarxivorgabs260105070v1><a href=https://arxiv.org/abs/2601.05070v1>Effect of Dispatch Decisions on Small-Signal Stability of Converter-Dominated Power Systems</a><a hidden class=anchor aria-hidden=true href=#effect-of-dispatch-decisions-on-small-signal-stability-of-converter-dominated-power-systemshttpsarxivorgabs260105070v1>#</a></h3><p><strong>Authors:</strong> Maitraya Avadhut Desai, Ognjen Stanojev, Simon Muntwiler, Gabriela Hug
<strong>Venue:</strong> arXiv (2026)</p><p>Small-signal stability of modern converter-dominated power systems has been the subject of extensive research, particularly from the perspective of device-level control design for grid-forming (GFM) and grid-following (GFL) converters. However, the influence of power flow variables on system stability has received limited attention. Conventional small-signal stability analyses are typically conducted at a specific operating point, emphasizing the selection of control or system design parameters while neglecting the sensitivity of stability characteristics to operating conditions. This paper seeks to bridge this gap by systematically investigating the impact of dispatch decisions on the small-signal stability of converter-based power systems. Our findings are first illustrated on a three-bus system and then validated on the standard IEEE 39-bus test system to demonstrate scalability. Across the test systems, we find that high-voltage capacitive operation of GFL converters limits its active power injection, whereas inductive operation permits higher injections, and it is generally preferable for the GFM converter to supply more active power.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05070v1">üìÑ Download PDF</a></p><hr><h3 id=approximate-equivariance-via-projection-based-regularisationhttpsarxivorgabs260105028v1><a href=https://arxiv.org/abs/2601.05028v1>Approximate equivariance via projection-based regularisation</a><a hidden class=anchor aria-hidden=true href=#approximate-equivariance-via-projection-based-regularisationhttpsarxivorgabs260105028v1>#</a></h3><p><strong>Authors:</strong> Torben Berndt, Jan St√ºhmer
<strong>Venue:</strong> arXiv (2026)</p><p>Equivariance is a powerful inductive bias in neural networks, improving generalisation and physical consistency. Recently, however, non-equivariant models have regained attention, due to their better runtime performance and imperfect symmetries that might arise in real-world applications. This has motivated the development of approximately equivariant models that strike a middle ground between respecting symmetries and fitting the data distribution. Existing approaches in this field usually apply sample-based regularisers which depend on data augmentation at training time, incurring a high sample complexity, in particular for continuous groups such as $SO(3)$. This work instead approaches approximate equivariance via a projection-based regulariser which leverages the orthogonal decomposition of linear layers into equivariant and non-equivariant components. In contrast to existing methods, this penalises non-equivariance at an operator level across the full group orbit, rather than point-wise. We present a mathematical framework for computing the non-equivariance penalty exactly and efficiently in both the spatial and spectral domain. In our experiments, our method consistently outperforms prior approximate equivariance approaches in both model performance and efficiency, achieving substantial runtime gains over sample-based regularisers.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05028v1">üìÑ Download PDF</a></p><hr><h3 id=the-squirrel-parser-a-linear-time-peg-packrat-parser-capable-of-left-recursion-and-optimal-error-recoveryhttpsarxivorgabs260105012v1><a href=https://arxiv.org/abs/2601.05012v1>The Squirrel Parser: A Linear-Time PEG Packrat Parser Capable of Left Recursion and Optimal Error Recovery</a><a hidden class=anchor aria-hidden=true href=#the-squirrel-parser-a-linear-time-peg-packrat-parser-capable-of-left-recursion-and-optimal-error-recoveryhttpsarxivorgabs260105012v1>#</a></h3><p><strong>Authors:</strong> Luke A. D. Hutchison
<strong>Venue:</strong> arXiv (2026)</p><p>We present the squirrel parser, a PEG packrat parser that directly handles all forms of left recursion with optimal error recovery, while maintaining linear time complexity in the length of the input even in the presence of an arbitrary number of errors. Traditional approaches to handling left recursion in a recursive descent parser require grammar rewriting or complex algorithmic extensions. We derive a minimal algorithm from first principles: cycle detection via per-position state tracking and $O(1)$-per-LR-cycle communication from descendant to ancestor recursion frames, and fixed-point search via iterative expansion. For error recovery, we derived a set of four axioms and twelve constraints that must be imposed upon an optimal error recovery design to ensure completeness, correctness, optimality of performance, and intuitiveness of behavior. We utilized a constraint satisfaction mechanism to search the space of all possibilities, arriving at a provably optimal and robust error recovery strategy that maintains perfect performance linearity.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05012v1">üìÑ Download PDF</a></p><hr><h2 id=-psycholinguistics>üîç psycholinguistics<a hidden class=anchor aria-hidden=true href=#-psycholinguistics>#</a></h2><h3 id=random-models-and-guarded-logichttpsarxivorgabs260105247v1><a href=https://arxiv.org/abs/2601.05247v1>Random Models and Guarded Logic</a><a hidden class=anchor aria-hidden=true href=#random-models-and-guarded-logichttpsarxivorgabs260105247v1>#</a></h3><p><strong>Authors:</strong> Oskar Fiuk
<strong>Venue:</strong> arXiv (2026)</p><p>Building on ideas of Gurevich and Shelah for the G√∂del Class, we present a new probabilistic proof of the finite model property for the Guarded Fragment of First-Order Logic. Our proof is conceptually simple and yields the optimal doubly-exponential upper bound on the size of minimal models. We precisely analyse the obtained bound, up to constant factors in the exponents, and construct sentences that enforce models of tightly matching size. The probabilistic approach adapts naturally to the Triguarded Fragment, an extension of the Guarded Fragment that also subsumes the Two-Variable Fragment. Finally, we derandomise the probabilistic proof by providing an explicit model construction which replaces randomness with deterministic hash functions.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05247v1">üìÑ Download PDF</a></p><hr><h3 id=concurrent-balanced-augmented-treeshttpsarxivorgabs260105225v1><a href=https://arxiv.org/abs/2601.05225v1>Concurrent Balanced Augmented Trees</a><a hidden class=anchor aria-hidden=true href=#concurrent-balanced-augmented-treeshttpsarxivorgabs260105225v1>#</a></h3><p><strong>Authors:</strong> Evan Wrench, Ajay Singh, Younghun Roh, Panagiota Fatourou, Siddhartha Jayanti, Eric Ruppert, Yuanhao Wei
<strong>Venue:</strong> arXiv (2026)</p><p>Augmentation makes search trees tremendously more versatile, allowing them to support efficient aggregation queries, order-statistic queries, and range queries in addition to insertion, deletion, and lookup. In this paper, we present the first lock-free augmented balanced search tree. Our algorithmic ideas build upon a recent augmented unbalanced search tree presented by Fatourou and Ruppert [DISC, 2024]. We implement both data structures, solving some memory reclamation challenges in the process, and provide an experimental performance analysis of them. We also present optimized versions of our balanced tree that use delegation to achieve better scalability and performance (by more than 2x in some workloads). Our experiments show that our augmented balanced tree is 2.2 to 30 times faster than the unbalanced augmented tree, and up to several orders of magnitude faster than unaugmented trees on 120 threads.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05225v1">üìÑ Download PDF</a></p><hr><h3 id=earl-energy-aware-optimization-of-liquid-state-machines-for-pervasive-aihttpsarxivorgabs260105205v1><a href=https://arxiv.org/abs/2601.05205v1>EARL: Energy-Aware Optimization of Liquid State Machines for Pervasive AI</a><a hidden class=anchor aria-hidden=true href=#earl-energy-aware-optimization-of-liquid-state-machines-for-pervasive-aihttpsarxivorgabs260105205v1>#</a></h3><p><strong>Authors:</strong> Zain Iqbal, Lorenzo Valerio
<strong>Venue:</strong> arXiv (2026)</p><p>Pervasive AI increasingly depends on on-device learning systems that deliver low-latency and energy-efficient computation under strict resource constraints. Liquid State Machines (LSMs) offer a promising approach for low-power temporal processing in pervasive and neuromorphic systems, but their deployment remains challenging due to high hyperparameter sensitivity and the computational cost of traditional optimization methods that ignore energy constraints. This work presents EARL, an energy-aware reinforcement learning framework that integrates Bayesian optimization with an adaptive reinforcement learning based selection policy to jointly optimize accuracy and energy consumption. EARL employs surrogate modeling for global exploration, reinforcement learning for dynamic candidate prioritization, and an early termination mechanism to eliminate redundant evaluations, substantially reducing computational overhead. Experiments on three benchmark datasets demonstrate that EARL achieves 6 to 15 percent higher accuracy, 60 to 80 percent lower energy consumption, and up to an order of magnitude reduction in optimization time compared to leading hyperparameter tuning frameworks. These results highlight the effectiveness of energy-aware adaptive search in improving the efficiency and scalability of LSMs for resource-constrained on-device AI applications.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05205v1">üìÑ Download PDF</a></p><hr><h3 id=simuagent-an-llm-based-simulink-modeling-assistant-enhanced-with-reinforcement-learninghttpsarxivorgabs260105187v1><a href=https://arxiv.org/abs/2601.05187v1>SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning</a><a hidden class=anchor aria-hidden=true href=#simuagent-an-llm-based-simulink-modeling-assistant-enhanced-with-reinforcement-learninghttpsarxivorgabs260105187v1>#</a></h3><p><strong>Authors:</strong> Yanchang Liang, Xiaowei Zhao
<strong>Venue:</strong> arXiv (2026)</p><p>Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05187v1">üìÑ Download PDF</a></p><hr><h3 id=not-all-steps-are-informative-on-the-linearity-of-llms-rlvr-traininghttpsarxivorgabs260104537v1><a href=https://arxiv.org/abs/2601.04537v1>Not All Steps are Informative: On the Linearity of LLMs&rsquo; RLVR Training</a><a hidden class=anchor aria-hidden=true href=#not-all-steps-are-informative-on-the-linearity-of-llms-rlvr-traininghttpsarxivorgabs260104537v1>#</a></h3><p><strong>Authors:</strong> Tianle Wang, Zhongyuan Wu, Shenghao Jin, Hao Xu, Wei Chen, Ning Miao
<strong>Venue:</strong> arXiv (2026)</p><p>Reinforcement learning with verifiable rewards (RLVR) has become a central component of large language model (LLM) post-training. Unlike supervised fine-tuning (SFT), RLVR lets an LLM generate multiple candidate solutions and reinforces those that lead to a verifiably correct final answer. However, in practice, RLVR often requires thousands of training steps to reach strong performance, incurring substantial computation largely attributed to prolonged exploration. In this work, we make a surprising observation: during RLVR, LLMs evolve in a strongly linear manner. Specifically, both model weights and model output log-probabilities exhibit strong linear correlations with RL training steps. This suggests that RLVR predominantly amplifies trends that emerge early in training, rather than continuously discovering new behaviors throughout the entire optimization trajectory. Motivated by this linearity, we investigate whether future model states can be predicted from intermediate checkpoints via extrapolation, avoiding continued expensive training. We show that Weight Extrapolation produces models with performance comparable to standard RL training while requiring significantly less computation. Moreover, Logits Extrapolation consistently outperforms continued RL training on all four benchmarks by extrapolating beyond the step range where RL training remains stable.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04537v1">üìÑ Download PDF</a></p><hr><h3 id=quantifying-the-effect-of-test-set-contamination-on-generative-evaluationshttpsarxivorgabs260104301v1><a href=https://arxiv.org/abs/2601.04301v1>Quantifying the Effect of Test Set Contamination on Generative Evaluations</a><a hidden class=anchor aria-hidden=true href=#quantifying-the-effect-of-test-set-contamination-on-generative-evaluationshttpsarxivorgabs260104301v1>#</a></h3><p><strong>Authors:</strong> Rylan Schaeffer, Joshua Kazdan, Baber Abbasi, Ken Ziyu Liu, Brando Miranda, Ahmed Ahmed, Abhay Puri, Niloofar Mireshghallah, Sanmi Koyejo
<strong>Venue:</strong> arXiv (2026)</p><p>As frontier AI systems are pretrained on web-scale data, test set contamination has become a critical concern for accurately assessing their capabilities. While research has thoroughly investigated the impact of test set contamination on discriminative evaluations like multiple-choice question-answering, comparatively little research has studied the impact of test set contamination on generative evaluations. In this work, we quantitatively assess the effect of test set contamination on generative evaluations through the language model lifecycle. We pretrain language models on mixtures of web data and the MATH benchmark, sweeping model sizes and number of test set replicas contaminating the pretraining corpus; performance improves with contamination and model size. Using scaling laws, we make a surprising discovery: including even a single test set replica enables models to achieve lower loss than the irreducible error of training on the uncontaminated corpus. We then study further training: overtraining with fresh data reduces the effects of contamination, whereas supervised finetuning on the training set can either increase or decrease performance on test data, depending on the amount of pretraining contamination. Finally, at inference, we identify factors that modulate memorization: high sampling temperatures mitigate contamination effects, and longer solutions are exponentially more difficult to memorize than shorter ones, presenting a contrast with discriminative evaluations, where solutions are only a few tokens in length. By characterizing how generation and memorization interact, we highlight a new layer of complexity for trustworthy evaluation of AI systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04301v1">üìÑ Download PDF</a></p><hr><h3 id=fairness-in-opinion-dynamicshttpsarxivorgabs260103859v1><a href=https://arxiv.org/abs/2601.03859v1>Fairness in Opinion Dynamics</a><a hidden class=anchor aria-hidden=true href=#fairness-in-opinion-dynamicshttpsarxivorgabs260103859v1>#</a></h3><p><strong>Authors:</strong> Stanis≈Çaw Stƒôpie≈Ñ, Michalina Janik, Mateusz Nurek, Akrati Saxena, Rados≈Çaw Michalski
<strong>Venue:</strong> arXiv (2026)</p><p>Ways in which people&rsquo;s opinions change are, without a doubt, subject to a rich tapestry of differing influences. Factors that affect how one arrives at an opinion reflect how they have been shaped by their environment throughout their lives, education, material status, what belief systems are they subscribed to, and what socio-economic minorities are they a part of. This already complex system is further expanded by the ever-changing nature of one&rsquo;s social network. It is therefore no surprise that many models have a tendency to perform best for the majority of the population and discriminating those people who are members of various marginalized groups . This bias and the study of how to counter it are subject to a rapidly developing field of Fairness in Social Network Analysis (SNA). The focus of this work is to look into how a state-of-the-art model discriminates certain minority groups and whether it is possible to reliably predict for whom it will perform worse. Moreover, is such prediction possible based solely on one&rsquo;s demographic or topological features? To this end, the NetSense dataset, together with a state-of-the-art CoDiNG model for opinion prediction have been employed. Our work explores how three classifier models (Demography-Based, Topology-Based, and Hybrid) perform when assessing for whom this algorithm will provide inaccurate predictions. Finally, through a comprehensive analysis of these experimental results, we identify four key patterns of algorithmic bias. Our findings suggest that no single paradigm provides the best results and that there is a real need for context-aware strategies in fairness-oriented social network analysis. We conclude that a multi-faceted approach, incorporating both individual attributes and network structures, is essential for reducing algorithmic bias and promoting inclusive decision-making.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.03859v1">üìÑ Download PDF</a></p><hr><h3 id=probabilistic-transformers-for-joint-modeling-of-global-weather-dynamics-and-decision-centric-variableshttpsarxivorgabs260103753v1><a href=https://arxiv.org/abs/2601.03753v1>Probabilistic Transformers for Joint Modeling of Global Weather Dynamics and Decision-Centric Variables</a><a hidden class=anchor aria-hidden=true href=#probabilistic-transformers-for-joint-modeling-of-global-weather-dynamics-and-decision-centric-variableshttpsarxivorgabs260103753v1>#</a></h3><p><strong>Authors:</strong> Paulius Rauba, Viktor Cikojevic, Fran Bartolic, Sam Levang, Ty Dickinson, Chase Dwelle
<strong>Venue:</strong> arXiv (2026)</p><p>Weather forecasts sit upstream of high-stakes decisions in domains such as grid operations, aviation, agriculture, and emergency response. Yet forecast users often face a difficult trade-off. Many decision-relevant targets are functionals of the atmospheric state variables, such as extrema, accumulations, and threshold exceedances, rather than state variables themselves. As a result, users must estimate these targets via post-processing, which can be suboptimal and can introduce structural bias. The core issue is that decisions depend on distributions over these functionals that the model is not trained to learn directly.
In this work, we introduce GEM-2, a probabilistic transformer that jointly learns global atmospheric dynamics alongside a suite of variables that users directly act upon. Using this training recipe, we show that a lightweight (~275M params) and computationally efficient (~20-100x training speedup relative to state-of-the-art) transformer trained on the CRPS objective can directly outperform operational numerical weather prediction (NWP) models and be competitive with ML models that rely on expensive multi-step diffusion processes or require bespoke multi-stage fine-tuning strategies. We further demonstrate state-of-the-art economic value metrics under decision-theoretic evaluation, stable convergence to climatology at S2S and seasonal timescales, and a surprising insensitivity to many commonly assumed architectural and training design choices.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.03753v1">üìÑ Download PDF</a></p><hr><h3 id=on-flying-through-the-base-of-a-pseudo-streamerhttpsarxivorgabs260103620v1><a href=https://arxiv.org/abs/2601.03620v1>On flying through the base of a pseudo-streamer</a><a hidden class=anchor aria-hidden=true href=#on-flying-through-the-base-of-a-pseudo-streamerhttpsarxivorgabs260103620v1>#</a></h3><p><strong>Authors:</strong> Forrest Mozer, Oleksiy Agapitov, Kyungeun Choi, Andrii Voshchepynets
<strong>Venue:</strong> arXiv (2026)</p><p>Near the 10 solar radius perihelion of Parker Solar Probe orbit 24, a confined region containing an enhanced plasma density of 25,000 particles per cubic centimeter and broadband electrostatic waves was encountered. The solar wind velocity of 200 kilometers per second and ion temperature of 25 eV were significantly reduced as compared to their values in the ambient solar wind. These anomalous plasma conditions were observed on closed magnetic field lines, as determined from observations of the suprathermal electron strahl. Because the polarity of the radial magnetic field did not change sign on the two sides of the crossing and the crossed region contained a double-peaked plasma structure, the spacecraft must have passed through the base of a pseudo-streamer whose structure extended out to 10 solar radii. In the plasma frame, an electric field as large as 400 millivolts per meter was detected during the crossing. The current associated with this electric field was less than one milliampere per square meter, corresponding to a drift velocity less than 2.5 kilometers per second. It also contained a turbulent plasma with density fluctuations divided by density as large as 0.3, suggesting that the resistive term in the generalized ohm&rsquo;s law was significant. Also, the density as a function of time had a non-zero slope when the electric field was non-zero, suggesting that the pressure gradient term also mattered. As compared to earlier remote sensing and theoretical results, it is surprising that the plasma in this pseudo-streamer had a remarkably low flow velocity and that the pseudo-streamer base extended out to 10 solar radii.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.03620v1">üìÑ Download PDF</a></p><hr><h3 id=listen-to-the-unexpected-self-supervised-surprise-detection-for-efficient-viewport-predictionhttpsarxivorgabs260102629v1><a href=https://arxiv.org/abs/2601.02629v1>Listen to the Unexpected: Self-Supervised Surprise Detection for Efficient Viewport Prediction</a><a hidden class=anchor aria-hidden=true href=#listen-to-the-unexpected-self-supervised-surprise-detection-for-efficient-viewport-predictionhttpsarxivorgabs260102629v1>#</a></h3><p><strong>Authors:</strong> Arman Nik Khah, Ravi Prakash
<strong>Venue:</strong> arXiv (2026)</p><p>Adaptive streaming of 360-degree video relies on viewport prediction to allocate bandwidth efficiently. Current approaches predominantly use visual saliency or historical gaze patterns, neglecting the role of spatial audio in guiding user attention. This paper presents a self-learning framework for detecting &ldquo;surprising&rdquo; auditory events &ndash; moments that deviate from learned temporal expectations &ndash; and demonstrates their utility for viewport prediction. The proposed architecture combines $SE(3)$-equivariant graph neural networks with recurrent temporal modeling, trained via a dual self-supervised objective. A key feature is the natural modeling of temporal attention decay: surprise is high at event onset but diminishes as the listener adapts. Experiments on the AVTrack360 dataset show that integrating audio surprise with visual cues reduces bitrate waste by up to 18% compared to visual-only methods.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.02629v1">üìÑ Download PDF</a></p><hr><h3 id=control-of-the-mote_2-fermi-surface-by-nb-dopinghttpsarxivorgabs260105197v1><a href=https://arxiv.org/abs/2601.05197v1>Control of the MoTe$_2$ Fermi Surface by Nb Doping</a><a hidden class=anchor aria-hidden=true href=#control-of-the-mote_2-fermi-surface-by-nb-dopinghttpsarxivorgabs260105197v1>#</a></h3><p><strong>Authors:</strong> Andrew P. Weber, I√±igo Robredo, Philipp R√ºssmann, Maxim Ilyn, Arnaud Magrez, Philippe Bugnon, Nan Xu, Vladimir Strocov, J. Hugo Dil, J. Enrique Ortega, Julen Iba√±ez-Azpiroz
<strong>Venue:</strong> arXiv (2026)</p><p>Ab initio calculations and angle-resolved photoemission experiments show that the bulk and surface electronic structure of Weyl semimetal candidate MoTe$_2$ changes significantly by tuning the chemical potential by less than 0.4 eV. Calculations show that several Lifshitz transitions can occur among multiple electron and hole Fermi pockets of differing orbital character. Experiments show that 18% Nb-Mo substitution reduces the occupation of bulk and (001) surface bands, effectively producing a chemical potential shift of $\approx 0.3$ eV. Orbital character and dimensionality of the bulk bands is examined by soft X-ray angle resolved photoemission with control of the excitation light polarization. The band filling at the surface is shown to increase upon deposition of alkali atoms. The results indicate that multiple regimes of electronic properties can be easily accessed in this versatile, layered material.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05197v1">üìÑ Download PDF</a></p><hr><h3 id=fundamental-tradeoffs-for-isac-multiple-access-in-finite-blocklength-regimehttpsarxivorgabs260105165v1><a href=https://arxiv.org/abs/2601.05165v1>Fundamental Tradeoffs for ISAC Multiple Access in Finite-Blocklength Regime</a><a hidden class=anchor aria-hidden=true href=#fundamental-tradeoffs-for-isac-multiple-access-in-finite-blocklength-regimehttpsarxivorgabs260105165v1>#</a></h3><p><strong>Authors:</strong> Zhentian Zhang, Christos Masouros, Kai-Kit Wong, Jian Dang, Zaichen Zhang, Kaitao Meng, Farshad Rostami Ghadi, Mohammad Javad Ahmadi
<strong>Venue:</strong> arXiv (2026)</p><p>This paper investigates the fundamental communication&ndash;sensing tradeoffs of uplink dual-functional integrated sensing and communication (ISAC) multiple access under finite blocklength (FBL) constraints. Unlike conventional asymptotic analyses, we explicitly account for the limitations under FBL constraints imposed by short packets and low-latency transmission. By examining the unbiased channel state sensing estimator, we establish a geometric decomposition of the sensing error, indicating that it is jointly determined by the signal-to-noise ratio and the correlation structure of the information codebook. This insight reveals how cross-correlation among active users in the codebook geometry fundamentally constrains dual-functional ISAC performance. Consequently, we derive achievability and converse bounds that characterize the tradeoff between communication code rate and sensing accuracy in the FBL regime, with the converse further bounded by Shannon capacity. Moreover, by treating channel state sensing as a high-level sensing objective, a universal Cram√©r&ndash;Rao bound is derived to link channel estimation accuracy to practical sensing parameters. Examples of parameter sensing are also provided based on 3GPP standard. Numerical results validate the theoretical analysis and demonstrate the impact of blocklength, antenna dimensions, and sensing requirements.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05165v1">üìÑ Download PDF</a></p><hr><h3 id=superluminal-modes-in-a-quantum-field-simulator-for-cosmology-from-analog-transplanckian-physicshttpsarxivorgabs260105141v1><a href=https://arxiv.org/abs/2601.05141v1>Superluminal modes in a quantum field simulator for cosmology from analog Transplanckian physics</a><a hidden class=anchor aria-hidden=true href=#superluminal-modes-in-a-quantum-field-simulator-for-cosmology-from-analog-transplanckian-physicshttpsarxivorgabs260105141v1>#</a></h3><p><strong>Authors:</strong> Christian F. Schmidt, Stefan Floerchinger
<strong>Venue:</strong> arXiv (2026)</p><p>The quantum-field-theoretic description for the U(1)-Goldstone boson of a scalar Bose-Einstein condensate with time-dependent contact interactions is developed beyond the acoustic approximation in accordance with Bogoliubov theory. The resulting effective action is mapped to a relativistic quantum field theory on a dispersive (or rainbow) cosmological spacetime which has a superluminal Corley-Jacobson dispersion relation. Time-dependent changes of the s-wave scattering length to quantum-simulate cosmological particle production are accompanied by a time-dependent healing length that can be interpreted as an analog Planck length in the comoving frame. Non-adiabatic transitions acquire a dispersive character, which is thoroughly discussed. The framework is applied to exponentially expanding or power-law contracting $(2+1)$-dimensional spacetimes which are known to produce scale-invariant cosmological power spectra. The sensitivity of these scenarios to the time-dependence of the Bogoliubov dispersion is investigated: We find a violation of scale-invariance via analytically trackable Transplanckian damping effects if the cut-off scale is not well separated from the horizon-crossing scale. In case of the exponential expansion, these damping effects remarkably settle and converge to another scale-invariant plateau in the far ultraviolet regime where non-adiabatic transitions are suppressed by the high dispersion. The developed framework enables quantitative access to more drastic analog cosmological scenarios with improved predictability in the ultraviolet regime that ultimately may lead to the observation of a scale-invariant cosmological power spectrum in the laboratory.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05141v1">üìÑ Download PDF</a></p><hr><h3 id=enhanced-electron-reflectionat-mott-insulator-interfaceshttpsarxivorgabs260105140v1><a href=https://arxiv.org/abs/2601.05140v1>Enhanced Electron Reflectionat Mott-Insulator Interfaces</a><a hidden class=anchor aria-hidden=true href=#enhanced-electron-reflectionat-mott-insulator-interfaceshttpsarxivorgabs260105140v1>#</a></h3><p><strong>Authors:</strong> Jan Verlage, Peter Kratzer
<strong>Venue:</strong> arXiv (2026)</p><p>The Klein paradox describes an incoming electron being scattered at a supercritical barrier to create electron-positron pairs, a phenomenon widely discussed in textbooks. While demonstrating this phenomenon experimentally with the fundamental particles remains challenging, condensed matter analogs are more accessible to experimental realization. For spinless quasi-particles, theoretical works show an enhancement of the pair production rate, and analogs of this effect in condensed matter systems have been studied theoretically. Here, we present another condensed matter system, a heterostructure comprised of two materials with strongly and weakly interacting electrons, that allows for constructing analytical solutions using the hierarchy-of-correlations method. The results show enhanced electron reflection related with the production of doublon-holon pairs, as known from the Klein paradox.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05140v1">üìÑ Download PDF</a></p><hr><h3 id=how-dark-is-dark-a-reflectance-and-scattering-analysis-of-black-materialshttpsarxivorgabs260105094v1><a href=https://arxiv.org/abs/2601.05094v1>How Dark is Dark? A Reflectance and Scattering Analysis of Black Materials</a><a hidden class=anchor aria-hidden=true href=#how-dark-is-dark-a-reflectance-and-scattering-analysis-of-black-materialshttpsarxivorgabs260105094v1>#</a></h3><p><strong>Authors:</strong> Jiri Filip, Radomir Vavra
<strong>Venue:</strong> arXiv (2026)</p><p>Black materials play a critical role in applications such as image registration, camera calibration, stray light suppression, and visual design. Although many such materials appear similarly dark under diffuse illumination, their reflectance behavior can differ substantially as a function of viewing and lighting geometry. Ultra-black materials achieve exceptional light attenuation but are often constrained by cost and mechanical fragility, motivating the evaluation of more robust and accessible alternatives. In this study, we employ a gonimetric measurement system to capture the isotropic bidirectional reflectance distribution function of a range of black materials, including the ultra-black reference Vantablack, commercially available alternatives such as Musou Black and black velvet, and standard matte black coatings. We analyze their reflectance characteristics in terms of diffuse and specular scattering, as well as total integrated scatter, to quantify angular-dependent reflection. In addition, we compare their perceptual appearance using physically based rendering driven by the measured BRDFs and a psychophysical evaluation of perceived darkness. Together, these analyses provide a comprehensive assessment of black materials that links reflectance properties to visual appearance and perceptual performance, enabling informed material selection for optical applications.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05094v1">üìÑ Download PDF</a></p><hr><h3 id=preconditioned-multivariate-quantum-solution-extractionhttpsarxivorgabs260105077v1><a href=https://arxiv.org/abs/2601.05077v1>Preconditioned Multivariate Quantum Solution Extraction</a><a hidden class=anchor aria-hidden=true href=#preconditioned-multivariate-quantum-solution-extractionhttpsarxivorgabs260105077v1>#</a></h3><p><strong>Authors:</strong> Gumaro Rendon, Stepan Smid
<strong>Venue:</strong> arXiv (2026)</p><p>Numerically solving partial differential equations is a ubiquitous computational task with broad applications in many fields of science. Quantum computers can potentially provide high-degree polynomial speed-ups for solving PDEs, however many algorithms simply end with preparing the quantum state encoding the solution in its amplitudes. Trying to access explicit properties of the solution naively with quantum amplitude estimation can subsequently diminish the potential speed-up. In this work, we present a technique for extracting a smooth positive function encoded in the amplitudes of a quantum state, which achieves the Heisenberg limit scaling. We improve upon previous methods by allowing higher dimensional functions, by significantly reducing the quantum complexity with respect to the number of qubits encoding the function, and by removing the dependency on the minimum of the function using preconditioning. Our technique works by sampling the cumulative distribution of the given function, fitting it with Chebyshev polynomials, and subsequently extracting a representation of the whole encoded function. Finally, we trial our method by carrying out small scale numerical simulations.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05077v1">üìÑ Download PDF</a></p><hr><h3 id=geometric-developmental-principles-for-the-emergence-of-brain-like-weighted-and-directed-neuronal-networkshttpsarxivorgabs260105021v1><a href=https://arxiv.org/abs/2601.05021v1>Geometric developmental principles for the emergence of brain-like weighted and directed neuronal networks</a><a hidden class=anchor aria-hidden=true href=#geometric-developmental-principles-for-the-emergence-of-brain-like-weighted-and-directed-neuronal-networkshttpsarxivorgabs260105021v1>#</a></h3><p><strong>Authors:</strong> Aitor Morales-Gregorio, Anno C. Kurth, Karol√≠na Korvasov√°
<strong>Venue:</strong> arXiv (2026)</p><p>Brain networks exhibit remarkable structural properties, including high local clustering, short path lengths, and heavy-tailed weight and degree distributions. While these features are thought to enable efficient information processing with minimal wiring costs, the fundamental principles that generate such complex network architectures across species remain unclear. Here, we analyse single-neuron resolution connectomes across five species (C. Elegans, Platynereis, Drosophila M., zebrafish and mouse) to investigate the fundamental wiring principles underlying brain network formation. We show that distance-dependent connectivity alone produces small-world networks, but fails to generate heavy-tailed distributions. By incorporating weight-preferential attachment, which arises from spatial clustering of synapses along neurites, we reproduce heavy-tailed weight distributions while maintaining small-world topology. Adding degree-preferential attachment, linked to the extent of dendritic and axonal arborization, enables the generation of heavy-tailed degree distributions. Through systematic parameter exploration, we demonstrate that the combination of distance dependence, weight-preferential attachment, and degree-preferential attachment is sufficient to reproduce all characteristic properties of empirical brain networks. Our results reveal that activity-independent geometric constraints during neural development can account for the conserved architectural principles observed across evolutionarily distant species, suggesting universal mechanisms governing neural circuit assembly.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05021v1">üìÑ Download PDF</a></p><hr><h3 id=h√°n-dƒÅn-xu√©-b√π-mimicry-or-qƒ´ng-ch≈´-y√∫-l√°n-mastery-a-cognitive-perspective-on-reasoning-distillation-in-large-language-modelshttpsarxivorgabs260105019v1><a href=https://arxiv.org/abs/2601.05019v1>H√°n DƒÅn Xu√© B√π (Mimicry) or Qƒ´ng Ch≈´ Y√∫ L√°n (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models</a><a hidden class=anchor aria-hidden=true href=#h√°n-dƒÅn-xu√©-b√π-mimicry-or-qƒ´ng-ch≈´-y√∫-l√°n-mastery-a-cognitive-perspective-on-reasoning-distillation-in-large-language-modelshttpsarxivorgabs260105019v1>#</a></h3><p><strong>Authors:</strong> Yueqing Hu, Xinyang Peng, Shuting Peng, Hanqi Wang, Tianhong Wang
<strong>Venue:</strong> arXiv (2026)</p><p>Recent Large Reasoning Models trained via reinforcement learning exhibit a &ldquo;natural&rdquo; alignment with human cognitive costs. However, we show that the prevailing paradigm of reasoning distillation &ndash; training student models to mimic these traces via Supervised Fine-Tuning (SFT) &ndash; fails to transmit this cognitive structure. Testing the &ldquo;H√°n DƒÅn Xu√© B√π&rdquo; (Superficial Mimicry) hypothesis across 14 models, we find that distillation induces a &ldquo;Functional Alignment Collapse&rdquo;: while teacher models mirror human difficulty scaling ($\bar{r}=0.64$), distilled students significantly degrade this alignment ($\bar{r}=0.34$), often underperforming their own pre-distillation baselines (&ldquo;Negative Transfer&rdquo;). Our analysis suggests that SFT induces a &ldquo;Cargo Cult&rdquo; effect, where students ritualistically replicate the linguistic form of reasoning (verbosity) without internalizing the teacher&rsquo;s dynamic resource allocation policy. Consequently, reasoning distillation decouples computational cost from cognitive demand, revealing that human-like cognition is an emergent property of active reinforcement, not passive imitation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05019v1">üìÑ Download PDF</a></p><hr><h3 id=conmax-confidence-maximizing-compression-for-efficient-chain-of-thought-reasoninghttpsarxivorgabs260104973v1><a href=https://arxiv.org/abs/2601.04973v1>ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning</a><a hidden class=anchor aria-hidden=true href=#conmax-confidence-maximizing-compression-for-efficient-chain-of-thought-reasoninghttpsarxivorgabs260104973v1>#</a></h3><p><strong>Authors:</strong> Minda Hu, Zexuan Qiu, Zenan Xu, Kun Li, Bo Zhou, Irwin King
<strong>Venue:</strong> arXiv (2026)</p><p>Recent breakthroughs in Large Reasoning Models (LRMs) have demonstrated that extensive Chain-of-Thought (CoT) generation is critical for enabling intricate cognitive behaviors, such as self-verification and backtracking, to solve complex tasks. However, this capability often leads to ``overthinking&rsquo;&rsquo;, where models generate redundant reasoning paths that inflate computational costs without improving accuracy. While Supervised Fine-Tuning (SFT) on reasoning traces is a standard paradigm for the &lsquo;cold start&rsquo; phase, applying existing compression techniques to these traces often compromises logical coherence or incurs prohibitive sampling costs. In this paper, we introduce ConMax (Confidence-Maximizing Compression), a novel reinforcement learning framework designed to automatically compress reasoning traces while preserving essential reasoning patterns. ConMax formulates compression as a reward-driven optimization problem, training a policy to prune redundancy by maximizing a weighted combination of answer confidence for predictive fidelity and thinking confidence for reasoning validity through a frozen auxiliary LRM. Extensive experiments across five reasoning datasets demonstrate that ConMax achieves a superior efficiency-performance trade-off. Specifically, it reduces inference length by 43% over strong baselines at the cost of a mere 0.7% dip in accuracy, proving its effectiveness in generating high-quality, efficient training data for LRMs.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04973v1">üìÑ Download PDF</a></p><hr><h3 id=breaking-robustness-barriers-in-cognitive-diagnosis-a-one-shot-neural-architecture-search-perspectivehttpsarxivorgabs260104918v1><a href=https://arxiv.org/abs/2601.04918v1>Breaking Robustness Barriers in Cognitive Diagnosis: A One-Shot Neural Architecture Search Perspective</a><a hidden class=anchor aria-hidden=true href=#breaking-robustness-barriers-in-cognitive-diagnosis-a-one-shot-neural-architecture-search-perspectivehttpsarxivorgabs260104918v1>#</a></h3><p><strong>Authors:</strong> Ziwen Wang, Shangshang Yang, Xiaoshan Yu, Haiping Ma, Xingyi Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>With the advancement of network technologies, intelligent tutoring systems (ITS) have emerged to deliver increasingly precise and tailored personalized learning services. Cognitive diagnosis (CD) has emerged as a core research task in ITS, aiming to infer learners&rsquo; mastery of specific knowledge concepts by modeling the mapping between learning behavior data and knowledge states. However, existing research prioritizes model performance enhancement while neglecting the pervasive noise contamination in observed response data, significantly hindering practical deployment. Furthermore, current cognitive diagnosis models (CDMs) rely heavily on researchers&rsquo; domain expertise for structural design, which fails to exhaustively explore architectural possibilities, thus leaving model architectures&rsquo; full potential untapped. To address this issue, we propose OSCD, an evolutionary multi-objective One-Shot neural architecture search method for Cognitive Diagnosis, designed to efficiently and robustly improve the model&rsquo;s capability in assessing learner proficiency. Specifically, OSCD operates through two distinct stages: training and searching. During the training stage, we construct a search space encompassing diverse architectural combinations and train a weight-sharing supernet represented via the complete binary tree topology, enabling comprehensive exploration of potential architectures beyond manual design priors. In the searching stage, we formulate the optimal architecture search under heterogeneous noise scenarios as a multi-objective optimization problem (MOP), and develop an optimization framework integrating a Pareto-optimal solution search strategy with cross-scenario performance evaluation for resolution. Extensive experiments on real-world educational datasets validate the effectiveness and robustness of the optimal architectures discovered by our OSCD model for CD tasks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04918v1">üìÑ Download PDF</a></p><hr><h3 id=onomacompass-a-texture-exploration-interface-that-shuttles-between-words-and-imageshttpsarxivorgabs260104915v1><a href=https://arxiv.org/abs/2601.04915v1>OnomaCompass: A Texture Exploration Interface that Shuttles between Words and Images</a><a hidden class=anchor aria-hidden=true href=#onomacompass-a-texture-exploration-interface-that-shuttles-between-words-and-imageshttpsarxivorgabs260104915v1>#</a></h3><p><strong>Authors:</strong> Miki Okamura, Shuhey Koyama, Li Jingjing, Yoichi Ochiai
<strong>Venue:</strong> arXiv (2026)</p><p>Humans can finely perceive material textures, yet articulating such somatic impressions in words is a cognitive bottleneck in design ideation. We present OnomaCompass, a web-based exploration system that links sound-symbolic onomatopoeia and visual texture representations to support early-stage material discovery. Instead of requiring users to craft precise prompts for generative AI, OnomaCompass provides two coordinated latent-space maps&ndash;one for texture images and one for onomatopoeic term&ndash;built from an authored dataset of invented onomatopoeia and corresponding textures generated via Stable Diffusion. Users can navigate both spaces, trigger cross-modal highlighting, curate findings in a gallery, and preview textures applied to objects via an image-editing model. The system also supports video interpolation between selected textures and re-embedding of extracted frames to form an emergent exploration loop. We conducted a within-subjects study with 11 participants comparing OnomaCompass to a prompt-based image-generation workflow using Gemini 2.5 Flash Image (&ldquo;Nano Banana&rdquo;). OnomaCompass significantly reduced workload (NASA-TLX overall, mental demand, effort, and frustration; p &lt; .05) and increased hedonic user experience (UEQ), while usability (SUS) favored the baseline. Qualitative findings indicate that OnomaCompass helps users externalize vague sensory expectations and promotes serendipitous discovery, but also reveals interaction challenges in spatial navigation. Overall, leveraging sound symbolism as a lightweight cue offers a complementary approach to Kansei-driven material ideation beyond prompt-centric generation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04915v1">üìÑ Download PDF</a></p><hr><h3 id=mind2report-a-cognitive-deep-research-agent-for-expert-level-commercial-report-synthesishttpsarxivorgabs260104879v1><a href=https://arxiv.org/abs/2601.04879v1>Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis</a><a hidden class=anchor aria-hidden=true href=#mind2report-a-cognitive-deep-research-agent-for-expert-level-commercial-report-synthesishttpsarxivorgabs260104879v1>#</a></h3><p><strong>Authors:</strong> Mingyue Cheng, Daoyu Wang, Qi Liu, Shuo Yu, Xiaoyu Tao, Yuqian Wang, Chengzhong Chu, Yu Duan, Mingkang Long, Enhong Chen
<strong>Venue:</strong> arXiv (2026)</p><p>Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at <a href=https://github.com/Melmaphother/Mind2Report>https://github.com/Melmaphother/Mind2Report</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04879v1">üìÑ Download PDF</a></p><hr><h2 id=-llm>üîç llm<a hidden class=anchor aria-hidden=true href=#-llm>#</a></h2><h3 id=caos-conformal-aggregation-of-one-shot-predictorshttpsarxivorgabs260105219v1><a href=https://arxiv.org/abs/2601.05219v1>CAOS: Conformal Aggregation of One-Shot Predictors</a><a hidden class=anchor aria-hidden=true href=#caos-conformal-aggregation-of-one-shot-predictorshttpsarxivorgabs260105219v1>#</a></h3><p><strong>Authors:</strong> Maja Waldron
<strong>Venue:</strong> arXiv (2026)</p><p>One-shot prediction enables rapid adaptation of pretrained foundation models to new tasks using only one labeled example, but lacks principled uncertainty quantification. While conformal prediction provides finite-sample coverage guarantees, standard split conformal methods are inefficient in the one-shot setting due to data splitting and reliance on a single predictor. We propose Conformal Aggregation of One-Shot Predictors (CAOS), a conformal framework that adaptively aggregates multiple one-shot predictors and uses a leave-one-out calibration scheme to fully exploit scarce labeled data. Despite violating classical exchangeability assumptions, we prove that CAOS achieves valid marginal coverage using a monotonicity-based argument. Experiments on one-shot facial landmarking and RAFT text classification tasks show that CAOS produces substantially smaller prediction sets than split conformal baselines while maintaining reliable coverage.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05219v1">üìÑ Download PDF</a></p><hr><h3 id=how-many-body-chaos-emerges-in-the-presence-of-quasiparticleshttpsarxivorgabs260105238v1><a href=https://arxiv.org/abs/2601.05238v1>How many-body chaos emerges in the presence of quasiparticles</a><a hidden class=anchor aria-hidden=true href=#how-many-body-chaos-emerges-in-the-presence-of-quasiparticleshttpsarxivorgabs260105238v1>#</a></h3><p><strong>Authors:</strong> Sibaram Ruidas, Sthitadhi Roy, Subhro Bhattacharjee, Roderich Moessner
<strong>Venue:</strong> arXiv (2026)</p><p>Many-body chaos is a default property of many-body systems; at the same time, near-integrable behaviour due to weakly interacting quasiparticles is ubiquitous throughout condensed matter at low temperature. There must therefore be a, possibly generic, crossover between these very different regimes. Here, we develop a theory encapsulating the notion of a cascade of lightcones seeded by sequences of scattering of weakly interacting harmonic modes as witnessed by a suitably defined chaos diagnostic (classical decorrelator) that measures the spatiotemporal profile of many-body chaos. Our numerics deals with the concrete case of a classical Heisenberg chain, for either sign of the interaction, at low temperatures where the short-time dynamics are well captured in terms of non-interacting spin waves. To model low-temperature dynamics, we use ensembles of initial states with randomly embedded point defects in an otherwise ordered background, which provides a controlled setting for studying the scattering events. The decorrelator exhibits a short-time integrable regime followed by an intermediate `scarred&rsquo; regime of the cascade of lightcones in progress; these then overlap, leading to an avalanche of scattering events which finally yields the standard long-time signature of many-body chaos.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05238v1">üìÑ Download PDF</a></p><hr><h3 id=fluctuation-response-relation-for-a-nonequilibrium-system-with-resolved-markovian-embeddinghttpsarxivorgabs260105198v1><a href=https://arxiv.org/abs/2601.05198v1>Fluctuation-response relation for a nonequilibrium system with resolved Markovian embedding</a><a hidden class=anchor aria-hidden=true href=#fluctuation-response-relation-for-a-nonequilibrium-system-with-resolved-markovian-embeddinghttpsarxivorgabs260105198v1>#</a></h3><p><strong>Authors:</strong> R√©mi Goerlich, Antoine Tartar, Yael Roichman, Igor M Sokolov
<strong>Venue:</strong> arXiv (2026)</p><p>Fluctuation-response relations must be modified to describe nonequilibrium systems with non-Markovian dynamics. Here, we experimentally demonstrate that such relation is quantitatively recovered when the appropriate Markovian embedding of the dynamics is explicitly resolved. Using a colloidal particle optically trapped in a harmonic potential and driven out of equilibrium by a controlled colored noise, we study the response to a perturbation of the stiffness of the confining potential. While the reduced dynamics violates equilibrium fluctuation-response relations, we show that the dynamical response to the stiffness perturbation is fully determined by steady-state correlations involving the exact conjugate observable in the Markovian embedding.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05198v1">üìÑ Download PDF</a></p><hr><h3 id=basis-number-of-graphs-excluding-minorshttpsarxivorgabs260105195v1><a href=https://arxiv.org/abs/2601.05195v1>Basis Number of Graphs Excluding Minors</a><a hidden class=anchor aria-hidden=true href=#basis-number-of-graphs-excluding-minorshttpsarxivorgabs260105195v1>#</a></h3><p><strong>Authors:</strong> Colin Geniet, Ugo Giocanti
<strong>Venue:</strong> arXiv (2026)</p><p>The basis number of a graph $G$ is the minimum $k$ such that the cycle space of $G$ is generated by a family of cycles using each edge at most $k$ times. A classical result of Mac Lane states that planar graphs are exactly graphs with basis number at most 2, and more generally, graphs embedded on a fixed surface are known to have bounded basis number. Generalising this, we prove that graphs excluding a fixed minor $H$ have bounded basis number.
Our proof uses the Graph Minor Structure Theorem, which requires us to understand how basis number behaves in tree-decompositions. In particular, we prove that graphs of treewidth $k$ have basis number bounded by some function of $k$. We handle tree-decompositions using the proof framework developed by Boja≈Ñczyk and Pilipczuk in their proof of Courcelle&rsquo;s conjecture.
Combining our approach with independent results of Miraftab, Morin and Yuditsky (2025) on basis number and path-decompositions, one can moreover improve our upper bound to a polynomial one: there exists an absolute constant $c>0$ such that every $H$-minor free graph has basis number $O(|H|^c)$.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05195v1">üìÑ Download PDF</a></p><hr><h3 id=hydrodynamic-interactions-in-a-binary-mixture-colloidal-monolayerhttpsarxivorgabs260105182v1><a href=https://arxiv.org/abs/2601.05182v1>Hydrodynamic interactions in a binary-mixture colloidal monolayer</a><a hidden class=anchor aria-hidden=true href=#hydrodynamic-interactions-in-a-binary-mixture-colloidal-monolayerhttpsarxivorgabs260105182v1>#</a></h3><p><strong>Authors:</strong> M. Chamorro-Burgos, Alvaro Dom√≠nguez
<strong>Venue:</strong> arXiv (2026)</p><p>A colloidal monolayer embedded in the bulk of a fluid experiences a &ldquo;compressible&rdquo;, long-range hydrodynamic interaction which, far from boundaries, leads to a breakdown of Fick&rsquo;s law above a well defined length scale, showing up as anomalous collective diffusion. We here extend the model to study the effect of the hydrodynamic interaction on a monolayer formed by two types of particles. The most interesting finding is a new regime, in the limit of very dissimilar kinds of particles, where the effective dynamics of the concentration of &ldquo;big&rdquo; (slow) particles appears to obey Fick&rsquo;s law at large scales, but the corresponding collective diffusivity is completely determined, through hydrodynamic coupling, by the diffusivity of the &ldquo;small&rdquo; (fast) particles.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05182v1">üìÑ Download PDF</a></p><hr><h3 id=minenpc-task-task-suite-for-memory-aware-minecraft-agentshttpsarxivorgabs260105215v1><a href=https://arxiv.org/abs/2601.05215v1>MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents</a><a hidden class=anchor aria-hidden=true href=#minenpc-task-task-suite-for-memory-aware-minecraft-agentshttpsarxivorgabs260105215v1>#</a></h3><p><strong>Authors:</strong> Tamil Sudaravan Mohan Doss, Michael Xu, Sudha Rao, Andrew D. Wilson, Balasaravanan Thoravi Kumaravel
<strong>Venue:</strong> arXiv (2026)</p><p>We present \textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence.
As an initial snapshot, we instantiate the framework with GPT-4o and evaluate \textbf{216} subtasks across \textbf{8} experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05215v1">üìÑ Download PDF</a></p><hr><h3 id=internal-representations-as-indicators-of-hallucinations-in-agent-tool-selectionhttpsarxivorgabs260105214v1><a href=https://arxiv.org/abs/2601.05214v1>Internal Representations as Indicators of Hallucinations in Agent Tool Selection</a><a hidden class=anchor aria-hidden=true href=#internal-representations-as-indicators-of-hallucinations-in-agent-tool-selectionhttpsarxivorgabs260105214v1>#</a></h3><p><strong>Authors:</strong> Kait Healy, Bharathi Srinivasan, Visakh Madathil, Jing Wu
<strong>Venue:</strong> arXiv (2026)</p><p>Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit &rsquo;tool bypass&rsquo; behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs&rsquo; internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05214v1">üìÑ Download PDF</a></p><hr><h3 id=cat-states-and-violation-of-the-bell-chsh-inequality-in-relativistic-quantum-field-theoryhttpsarxivorgabs260105216v1><a href=https://arxiv.org/abs/2601.05216v1>Cat states and violation of the Bell-CHSH inequality in relativistic Quantum Field Theory</a><a hidden class=anchor aria-hidden=true href=#cat-states-and-violation-of-the-bell-chsh-inequality-in-relativistic-quantum-field-theoryhttpsarxivorgabs260105216v1>#</a></h3><p><strong>Authors:</strong> M. S. Guimaraes, I. Roditi, S. P. Sorella
<strong>Venue:</strong> arXiv (2026)</p><p>A cat state localized in the right Rindler wedge is employed to study the violation of the Bell-CHSH inequality in a relativistic scalar free Quantum Field Theory. By means of the bounded Hermitian operator $sign(\varphi(f))$, where $\varphi(f)$ stands for the smeared scalar field, it turns out that the Bell-CHSH correlator can be evaluated in closed analytic form in terms of the imaginary error function. Being the superposition of two coherent states, cat states allow for the existence of interference terms which give rise to a violation of the Bell-CHSH inequality. As such, the present setup can be considered as an explicit realization of the results obtained by Summers-Werner.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05216v1">üìÑ Download PDF</a></p><hr><h2 id=-neuroscience>üîç neuroscience<a hidden class=anchor aria-hidden=true href=#-neuroscience>#</a></h2><h3 id=eclipse-an-evolutionary-computation-library-for-instrumentation-prototyping-in-scientific-engineeringhttpsarxivorgabs260105098v1><a href=https://arxiv.org/abs/2601.05098v1>ECLIPSE: An Evolutionary Computation Library for Instrumentation Prototyping in Scientific Engineering</a><a hidden class=anchor aria-hidden=true href=#eclipse-an-evolutionary-computation-library-for-instrumentation-prototyping-in-scientific-engineeringhttpsarxivorgabs260105098v1>#</a></h3><p><strong>Authors:</strong> Max Foreback, Evan Imata, Vincent Ragusa, Jacob Weiler, Christina Shao, Joey Wagner, Katherine G. Skocelas, Jonathan Sy, Aman Hafez, Wolfgang Banzhaf, Amy Conolly, Kyle R. Helson, Rick Marcusen, Charles Ofria, Marcin Pilinski, Rajiv Ramnath, Bryan Reynolds, Anselmo C. Pontes, Emily Dolson, Julie Rolla
<strong>Venue:</strong> arXiv (2026)</p><p>Designing scientific instrumentation often requires exploring large, highly constrained design spaces using computationally expensive physics simulations. These simulators pose substantial challenges for integrating evolutionary computation (EC) into scientific design workflows. Evolutionary computation typically requires numerous design evaluations, making the integration of slow, low-throughput simulators particularly challenging, as they are optimized for accuracy and ease of use rather than throughput. We present ECLIPSE, an evolutionary computation framework built to interface directly with complex, domain-specific simulation tools while supporting flexible geometric and parametric representations of scientific hardware. ECLIPSE provides a modular architecture consisting of (1) Individuals, which encode hardware designs using domain-aware, physically constrained representations; (2) Evaluators, which prepare simulation inputs, invoke external simulators, and translate the simulator&rsquo;s outputs into fitness measures; and (3) Evolvers, which implement EC algorithms suitable for high-cost, limited-throughput environments. We demonstrate the utility of ECLIPSE across several active space-science applications, including evolved 3D antennas and spacecraft geometries optimized for drag reduction in very low Earth orbit. We further discuss the practical challenges encountered when coupling EC with scientific simulation workflows, including interoperability constraints, parallelization limits, and extreme evaluation costs, and outline ongoing efforts to combat these challenges. ECLIPSE enables interdisciplinary teams of physicists, engineers, and EC researchers to collaboratively explore unconventional designs for scientific hardware while leveraging existing domain-specific simulation software.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05098v1">üìÑ Download PDF</a></p><hr><h3 id=driver-intention-prediction-with-deep-learning-real-time-brain-to-vehicle-communicationhttpsarxivorgabs260105084v1><a href=https://arxiv.org/abs/2601.05084v1>Driver-Intention Prediction with Deep Learning: Real-Time Brain-to-Vehicle Communication</a><a hidden class=anchor aria-hidden=true href=#driver-intention-prediction-with-deep-learning-real-time-brain-to-vehicle-communicationhttpsarxivorgabs260105084v1>#</a></h3><p><strong>Authors:</strong> Niloufar Alavi, Swati Shah, Rezvan Alamian, Stefan Goetz
<strong>Venue:</strong> arXiv (2026)</p><p>Brain-computer interfaces (BCIs) allow direct communication between the brain and electronics without the need for speech or physical movement. Such interfaces can be particularly beneficial in applications requiring rapid response times, such as driving, where a vehicle&rsquo;s advanced driving assistance systems could benefit from immediate understanding of a driver&rsquo;s intentions. This study presents a novel method for predicting a driver&rsquo;s intention to steer using electroencephalography (EEG) signals through deep learning. A driving simulator created a controlled environment in which participants imagined controlling a vehicle during various driving scenarios, including left and right turns, as well as straight driving. A convolutional neural network (CNN) classified the detected EEG data with minimal pre-processing. Our model achieved an accuracy of 83.7% in distinguishing between the three steering intentions and demonstrated the ability of CNNs to process raw EEG data effectively. The classification accuracy was highest for right-turn segments, which suggests a potential spatial bias in brain activity. This study lays the foundation for more intuitive brain-to-vehicle communication systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05084v1">üìÑ Download PDF</a></p><hr><h3 id=on-the-effects-of-protection-zone-and-directed-population-flux-in-prey-predator-dynamicshttpsarxivorgabs260105054v1><a href=https://arxiv.org/abs/2601.05054v1>On the effects of protection zone and directed population flux in prey-predator dynamics</a><a hidden class=anchor aria-hidden=true href=#on-the-effects-of-protection-zone-and-directed-population-flux-in-prey-predator-dynamicshttpsarxivorgabs260105054v1>#</a></h3><p><strong>Authors:</strong> Kousuke Kuto, Kazuhiro Oeda
<strong>Venue:</strong> arXiv (2026)</p><p>We study a spatial predator-prey model in which prey can enter a protection zone (refuge) inaccessible to predators, while predators exhibit directed movement toward prey-rich regions. The directed movement is modeled by a far-sighted population flux motivated by classical movement rules, in contrast to the more commonly analyzed near-sighted chemotaxis-type mechanisms. We first establish local-in-time well-posedness for the corresponding nonstationary problem under Neumann boundary conditions, despite the discontinuity induced by the refuge interface. We then investigate the stationary problem, focusing on how the coexistence states emerge and organize globally in parameter space. In particular, we identify the bifurcation threshold for positive steady states from semitrivial predator-only equilibria, and describe the global continuation of the resulting branches. Our analysis reveals that strong directed movement can induce turning-point structures and multiplicity of coexistence steady states, highlighting a nontrivial interplay between spatial protection and predator movement behavior.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05054v1">üìÑ Download PDF</a></p><hr><h2 id=-data_resources>üîç data_resources<a hidden class=anchor aria-hidden=true href=#-data_resources>#</a></h2><h3 id=objectforesight-predicting-future-3d-object-trajectories-from-human-videoshttpsarxivorgabs260105237v1><a href=https://arxiv.org/abs/2601.05237v1>ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos</a><a hidden class=anchor aria-hidden=true href=#objectforesight-predicting-future-3d-object-trajectories-from-human-videoshttpsarxivorgabs260105237v1>#</a></h3><p><strong>Authors:</strong> Rustin Soraki, Homanga Bharadhwaj, Ali Farhadi, Roozbeh Mottaghi
<strong>Venue:</strong> arXiv (2026)</p><p>Humans can effortlessly anticipate how objects might move or change through interaction&ndash;imagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences. Unlike conventional world or dynamics models that operate in pixel or latent space, ObjectForesight represents the world explicitly in 3D at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories. To train such a model at scale, we leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2 million plus short clips with pseudo-ground-truth 3D object trajectories. Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes, establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation. objectforesight.github.io</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05237v1">üìÑ Download PDF</a></p><hr><h3 id=v-fat-benchmarking-visual-fidelity-against-text-biashttpsarxivorgabs260104897v1><a href=https://arxiv.org/abs/2601.04897v1>V-FAT: Benchmarking Visual Fidelity Against Text-bias</a><a hidden class=anchor aria-hidden=true href=#v-fat-benchmarking-visual-fidelity-against-text-biashttpsarxivorgabs260104897v1>#</a></h3><p><strong>Authors:</strong> Ziteng Wang, Yujie He, Guanliang Li, Siqi Yang, Jiaqi Xiong, Songxiang Liu
<strong>Venue:</strong> arXiv (2026)</p><p>Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on standard visual reasoning benchmarks. However, there is growing concern that these models rely excessively on linguistic shortcuts rather than genuine visual grounding, a phenomenon we term Text Bias. In this paper, we investigate the fundamental tension between visual perception and linguistic priors. We decouple the sources of this bias into two dimensions: Internal Corpus Bias, stemming from statistical correlations in pretraining, and External Instruction Bias, arising from the alignment-induced tendency toward sycophancy. To quantify this effect, we introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark comprising 4,026 VQA instances across six semantic domains. V-FAT employs a Three-Level Evaluation Framework that systematically increases the conflict between visual evidence and textual information: (L1) internal bias from atypical images, (L2) external bias from misleading instructions, and (L3) synergistic bias where both coincide. We introduce the Visual Robustness Score (VRS), a metric designed to penalize &ldquo;lucky&rdquo; linguistic guesses and reward true visual fidelity. Our evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04897v1">üìÑ Download PDF</a></p><hr><h3 id=dvd-a-robust-method-for-detecting-variant-contamination-in-large-language-model-evaluationhttpsarxivorgabs260104895v1><a href=https://arxiv.org/abs/2601.04895v1>DVD: A Robust Method for Detecting Variant Contamination in Large Language Model Evaluation</a><a hidden class=anchor aria-hidden=true href=#dvd-a-robust-method-for-detecting-variant-contamination-in-large-language-model-evaluationhttpsarxivorgabs260104895v1>#</a></h3><p><strong>Authors:</strong> Renzhao Liang, Jingru Chen, Bo Jia, Bo Deng, Chenggang Xie, Yidong Wang, Ke Jin, Xin Wang, Linfeng Zhang, Cunxiang Wang
<strong>Venue:</strong> arXiv (2026)</p><p>Evaluating large language models (LLMs) is increasingly confounded by \emph{variant contamination}: the training corpus contains semantically equivalent yet lexically or syntactically altered versions of test items. Unlike verbatim leakage, these paraphrased or structurally transformed variants evade existing detectors based on sampling consistency or perplexity, thereby inflating benchmark scores via memorization rather than genuine reasoning. We formalize this problem and introduce \textbf{DVD} (\textbf{D}etection via \textbf{V}ariance of generation \textbf{D}istribution), a single-sample detector that models the local output distribution induced by temperature sampling. Our key insight is that contaminated items trigger alternation between a \emph{memory-adherence} state and a \emph{perturbation-drift} state, yielding abnormally high variance in the synthetic difficulty of low-probability tokens; uncontaminated items remain in drift with comparatively smooth variance. We construct the first benchmark for variant contamination across two domains Omni-MATH and SuperGPQA by generating and filtering semantically equivalent variants, and simulate contamination via fine-tuning models of different scales and architectures (Qwen2.5 and Llama3.1). Across datasets and models, \textbf{DVD} consistently outperforms perplexity-based, Min-$k$%++, edit-distance (CDD), and embedding-similarity baselines, while exhibiting strong robustness to hyperparameters. Our results establish variance of the generation distribution as a principled and practical fingerprint for detecting variant contamination in LLM evaluation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04895v1">üìÑ Download PDF</a></p><hr><h3 id=higher-order-knowledge-representations-for-agentic-scientific-reasoninghttpsarxivorgabs260104878v1><a href=https://arxiv.org/abs/2601.04878v1>Higher-Order Knowledge Representations for Agentic Scientific Reasoning</a><a hidden class=anchor aria-hidden=true href=#higher-order-knowledge-representations-for-agentic-scientific-reasoninghttpsarxivorgabs260104878v1>#</a></h3><p><strong>Authors:</strong> Isabella A. Stewart, Markus J. Buehler
<strong>Venue:</strong> arXiv (2026)</p><p>Scientific inquiry requires systems-level reasoning that integrates heterogeneous experimental data, cross-domain knowledge, and mechanistic evidence into coherent explanations. While Large Language Models (LLMs) offer inferential capabilities, they often depend on retrieval-augmented contexts that lack structural depth. Traditional Knowledge Graphs (KGs) attempt to bridge this gap, yet their pairwise constraints fail to capture the irreducible higher-order interactions that govern emergent physical behavior. To address this, we introduce a methodology for constructing hypergraph-based knowledge representations that faithfully encode multi-entity relationships. Applied to a corpus of ~1,100 manuscripts on biocomposite scaffolds, our framework constructs a global hypergraph of 161,172 nodes and 320,201 hyperedges, revealing a scale-free topology (power law exponent ~1.23) organized around highly connected conceptual hubs. This representation prevents the combinatorial explosion typical of pairwise expansions and explicitly preserves the co-occurrence context of scientific formulations. We further demonstrate that equipping agentic systems with hypergraph traversal tools, specifically using node-intersection constraints, enables them to bridge semantically distant concepts. By exploiting these higher-order pathways, the system successfully generates grounded mechanistic hypotheses for novel composite materials, such as linking cerium oxide to PCL scaffolds via chitosan intermediates. This work establishes a &ldquo;teacherless&rdquo; agentic reasoning system where hypergraph topology acts as a verifiable guardrail, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04878v1">üìÑ Download PDF</a></p><hr><h3 id=versecrafter-dynamic-realistic-video-world-model-with-4d-geometric-controlhttpsarxivorgabs260105138v1><a href=https://arxiv.org/abs/2601.05138v1>VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control</a><a hidden class=anchor aria-hidden=true href=#versecrafter-dynamic-realistic-video-world-model-with-4d-geometric-controlhttpsarxivorgabs260105138v1>#</a></h3><p><strong>Authors:</strong> Sixiao Zheng, Minghao Yin, Wenbo Hu, Xiaoyu Li, Ying Shan, Yanwei Fu
<strong>Venue:</strong> arXiv (2026)</p><p>Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object&rsquo;s path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05138v1">üìÑ Download PDF</a></p><hr><h3 id=chain-of-sanitized-thoughts-plugging-pii-leakage-in-cot-of-large-reasoning-modelshttpsarxivorgabs260105076v1><a href=https://arxiv.org/abs/2601.05076v1>Chain-of-Sanitized-Thoughts: Plugging PII Leakage in CoT of Large Reasoning Models</a><a hidden class=anchor aria-hidden=true href=#chain-of-sanitized-thoughts-plugging-pii-leakage-in-cot-of-large-reasoning-modelshttpsarxivorgabs260105076v1>#</a></h3><p><strong>Authors:</strong> Arghyadeep Das, Sai Sreenivas Chintha, Rishiraj Girmal, Kinjal Pandey, Sharvi Endait
<strong>Venue:</strong> arXiv (2026)</p><p>Large Reasoning Models (LRMs) improve performance, reliability, and interpretability by generating explicit chain-of-thought (CoT) reasoning, but this transparency introduces a serious privacy risk: intermediate reasoning often leaks personally identifiable information (PII) even when final answers are sanitized. We study how to induce privacy-first reasoning, where models reason without exposing sensitive information, using deployable interventions rather than post-hoc redaction. We introduce PII-CoT-Bench, a supervised dataset with privacy-aware CoT annotations, and a category-balanced evaluation benchmark covering realistic and adversarial leakage scenarios. Our results reveal a capability-dependent trend: state-of-the-art models benefit most from prompt-based controls, whereas weaker models require fine-tuning to achieve meaningful leakage reduction. Across models and categories, both approaches substantially reduce PII exposure with minimal degradation in utility, demonstrating that private reasoning can be achieved without sacrificing performance. Overall, we show that private CoT reasoning can be achieved with minimal utility loss, providing practical guidance for building privacy-preserving reasoning systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05076v1">üìÑ Download PDF</a></p><hr><h3 id=from-understanding-to-engagement-personalized-pharmacy-video-clips-via-vision-language-models-vlmshttpsarxivorgabs260105059v1><a href=https://arxiv.org/abs/2601.05059v1>From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)</a><a hidden class=anchor aria-hidden=true href=#from-understanding-to-engagement-personalized-pharmacy-video-clips-via-vision-language-models-vlmshttpsarxivorgabs260105059v1>#</a></h3><p><strong>Authors:</strong> Suyash Mishra, Qiang Li, Srikanth Patil, Anubhav Girdhar
<strong>Venue:</strong> arXiv (2026)</p><p>Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars).
Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut & Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05059v1">üìÑ Download PDF</a></p><hr><h3 id=leveraging-prediction-entropy-for-automatic-prompt-weighting-in-zero-shot-audio-language-classificationhttpsarxivorgabs260105011v1><a href=https://arxiv.org/abs/2601.05011v1>Leveraging Prediction Entropy for Automatic Prompt Weighting in Zero-Shot Audio-Language Classification</a><a hidden class=anchor aria-hidden=true href=#leveraging-prediction-entropy-for-automatic-prompt-weighting-in-zero-shot-audio-language-classificationhttpsarxivorgabs260105011v1>#</a></h3><p><strong>Authors:</strong> Karim El Khoury, Maxime Zanella, Tiffanie Godelaine, Christophe De Vleeschouwer, Benoit Macq
<strong>Venue:</strong> arXiv (2026)</p><p>Audio-language models have recently demonstrated strong zero-shot capabilities by leveraging natural-language supervision to classify audio events without labeled training data. Yet, their performance is highly sensitive to the wording of text prompts, with small variations leading to large fluctuations in accuracy. Prior work has mitigated this issue through prompt learning or prompt ensembling. However, these strategies either require annotated data or fail to account for the fact that some prompts may negatively impact performance. In this work, we present an entropy-guided prompt weighting approach that aims to find a robust combination of prompt contributions to maximize prediction confidence. To this end, we formulate a tailored objective function that minimizes prediction entropy to yield new prompt weights, utilizing low-entropy as a proxy for high confidence. Our approach can be applied to individual samples or a batch of audio samples, requiring no additional labels and incurring negligible computational overhead. Experiments on five audio classification datasets covering environmental, urban, and vocal sounds, demonstrate consistent gains compared to classical prompt ensembling methods in a zero-shot setting, with accuracy improvements 5-times larger across the whole benchmark.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05011v1">üìÑ Download PDF</a></p><hr><h3 id=genprove-learning-to-generate-text-with-fine-grained-provenancehttpsarxivorgabs260104932v1><a href=https://arxiv.org/abs/2601.04932v1>GenProve: Learning to Generate Text with Fine-Grained Provenance</a><a hidden class=anchor aria-hidden=true href=#genprove-learning-to-generate-text-with-fine-grained-provenancehttpsarxivorgabs260104932v1>#</a></h3><p><strong>Authors:</strong> Jingxuan Wei, Xingyue Wang, Yanghaoyu Liao, Jie Dong, Yuchen Liu, Caijun Jia, Bihui Yu, Junnan Zhu
<strong>Venue:</strong> arXiv (2026)</p><p>Large language models (LLM) often hallucinate, and while adding citations is a common solution, it is frequently insufficient for accountability as users struggle to verify how a cited source supports a generated claim. Existing methods are typically coarse-grained and fail to distinguish between direct quotes and complex reasoning. In this paper, we introduce Generation-time Fine-grained Provenance, a task where models must generate fluent answers while simultaneously producing structured, sentence-level provenance triples. To enable this, we present ReFInE (Relation-aware Fine-grained Interpretability & Evidence), a dataset featuring expert verified annotations that distinguish between Quotation, Compression, and Inference. Building on ReFInE, we propose GenProve, a framework that combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO). By optimizing a composite reward for answer fidelity and provenance correctness, GenProve significantly outperforms 14 strong LLMs in joint evaluation. Crucially, our analysis uncovers a reasoning gap where models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting that verifiable reasoning remains a frontier challenge distinct from surface-level citation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.04932v1">üìÑ Download PDF</a></p><hr><h2 id=-emotion_language>üîç emotion_language<a hidden class=anchor aria-hidden=true href=#-emotion_language>#</a></h2><h3 id=addressing-known-challenges-in-solar-flare-forecasting-i-limb-flare-prediction-with-a-4-pi-full-heliosphere-frameworkhttpsarxivorgabs260105209v1><a href=https://arxiv.org/abs/2601.05209v1>Addressing Known Challenges in Solar Flare Forecasting I: Limb-Flare Prediction with a 4-pi Full-Heliosphere Framework</a><a hidden class=anchor aria-hidden=true href=#addressing-known-challenges-in-solar-flare-forecasting-i-limb-flare-prediction-with-a-4-pi-full-heliosphere-frameworkhttpsarxivorgabs260105209v1>#</a></h3><p><strong>Authors:</strong> K. D. Leka, Eric L. Wagner, Lisa Upton, Bibhuti Kumar Jha, Kiran Jain, Sara Petty
<strong>Venue:</strong> arXiv (2026)</p><p>A demonstrated failure mode for operational solar flare forecasting is the inability to forecast flares that occur near, or just beyond, the solar limb. To address this shortcoming, we develop a &ldquo;4pi&rdquo; full-heliosphere event forecasting framework and evaluate its statistical classification ability against this specific challenge. A magnetic surface flux transport model is used to generate full-sun maps of the photospheric radial magnetic field from which active regions (ARs) are identified and tracked using a new labeling scheme that is observer-location agnostic and allows for post-facto modifications. Flare-relevant magnetic parameters couple to a &ldquo;visibility&rdquo; index that specifies AR location relative to the visible solar limb and expected flare detection. Flare labels are assigned according to peak Soft X-ray flux, and a statistical classification is performed using nonparametric discriminant analysis. A version where new or emerging ARs on the far (&ldquo;invisible&rdquo; side of the Sun are incorporated into the model by way of far-side helioseismology, is also tested. We evaluate the new framework by its performance specifically including the limb areas using Brier Skill Score and ROC Skill Score, finding improvement at the 2-sigma level or less. However, we do find that the number of False Negatives, or &ldquo;missed&rdquo; forecasts decreases, and find strong evidence that the additional information provided by the far-side helioseismology can help predict near- and just-beyond-limb flares, particularly for East-limb events. While individual components of this framework could be improved, we demonstrate that a known failure mode for solar flare forecasting can be mitigated with available resources.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05209v1">üìÑ Download PDF</a></p><hr><h3 id=a-lightweight-and-explainable-vision-language-framework-for-crop-disease-visual-question-answeringhttpsarxivorgabs260105143v1><a href=https://arxiv.org/abs/2601.05143v1>A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering</a><a hidden class=anchor aria-hidden=true href=#a-lightweight-and-explainable-vision-language-framework-for-crop-disease-visual-question-answeringhttpsarxivorgabs260105143v1>#</a></h3><p><strong>Authors:</strong> Md. Zahid Hossain, Most. Sharmin Sultana Samu, Md. Rakibul Islam, Md. Siam Ansary
<strong>Venue:</strong> arXiv (2026)</p><p>Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05143v1">üìÑ Download PDF</a></p><hr><h3 id=prediction-of-magnetic-topological-materials-combining-spin-and-magnetic-space-groupshttpsarxivorgabs260105142v1><a href=https://arxiv.org/abs/2601.05142v1>Prediction of Magnetic Topological Materials Combining Spin and Magnetic Space Groups</a><a hidden class=anchor aria-hidden=true href=#prediction-of-magnetic-topological-materials-combining-spin-and-magnetic-space-groupshttpsarxivorgabs260105142v1>#</a></h3><p><strong>Authors:</strong> Liangliang Huang, Xiangang Wan, Feng Tang
<strong>Venue:</strong> arXiv (2026)</p><p>The scarcity of predicted magnetic topological materials (MTMs) by magnetic space group (MSG) hinders further exploration towards realistic device applications. Here, we propose a new scheme combining spin space groups (SSGs)&ndash;approximate symmetry groups neglecting spin-orbit coupling (SOC)&ndash;and MSGs to diagnose topology in collinear magnetic materials based on symmetry-indicator theory, enabling a systematic classification of the electronic topology across 484 experimentally synthesized collinear magnets from the MAGNDATA database. This new scheme exploits a symmetry-hierarchy due to SOC induced symmetry-breaking, so that nontrivial band topology can be revealed by SSG, that is yet invisible by the conventional MSG-based method, as exemplified by real triple points in ferromagnetic CaCu$_3$Fe$_2$Sb$<em>2$O$</em>{12}$, Dirac nodal lines at generic $k$-points in antiferromagnetic FePSe$_3$ and Weyl nodal lines in altermagnetic Sr$_4$Fe$<em>4$O$</em>{11}$. Notably, FePSe$_3$ is topologically trivial under MSG but hosts Dirac nodal lines within the SSG framework. Upon including SOC, these nodal lines are gapped and generate a sizable anomalous Hall conductivity. Despite a vanishing bulk net magnetism, FePSe$_3$ can host topologically protected surface states with large non-relativistic band spin-splitting. Moreover, topology in MTMs is tunable by rotating the magnetic moment direction once SOC is included, as exemplified in Sr$_4$Fe$<em>4$O$</em>{11}$.The interplay of topology with non-relativistic and SOC-induced control of properties via magnetic moment reorientation in the predicted MTMs is worthy of further studies in future.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05142v1">üìÑ Download PDF</a></p><hr><h3 id=sequential-subspace-noise-injection-prevents-accuracy-collapse-in-certified-unlearninghttpsarxivorgabs260105134v1><a href=https://arxiv.org/abs/2601.05134v1>Sequential Subspace Noise Injection Prevents Accuracy Collapse in Certified Unlearning</a><a hidden class=anchor aria-hidden=true href=#sequential-subspace-noise-injection-prevents-accuracy-collapse-in-certified-unlearninghttpsarxivorgabs260105134v1>#</a></h3><p><strong>Authors:</strong> Polina Dolgova, Sebastian U. Stich
<strong>Venue:</strong> arXiv (2026)</p><p>Certified unlearning based on differential privacy offers strong guarantees but remains largely impractical: the noisy fine-tuning approaches proposed so far achieve these guarantees but severely reduce model accuracy. We propose sequential noise scheduling, which distributes the noise budget across orthogonal subspaces of the parameter space, rather than injecting it all at once. This simple modification mitigates the destructive effect of noise while preserving the original certification guarantees. We extend the analysis of noisy fine-tuning to the subspace setting, proving that the same $(\varepsilon,Œ¥)$ privacy budget is retained. Empirical results on image classification benchmarks show that our approach substantially improves accuracy after unlearning while remaining robust to membership inference attacks. These results show that certified unlearning can achieve both rigorous guarantees and practical utility.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05134v1">üìÑ Download PDF</a></p><hr><h3 id=a-geometric-definition-of-the-integral-and-applicationshttpsarxivorgabs260105228v1><a href=https://arxiv.org/abs/2601.05228v1>A Geometric Definition of the Integral and Applications</a><a hidden class=anchor aria-hidden=true href=#a-geometric-definition-of-the-integral-and-applicationshttpsarxivorgabs260105228v1>#</a></h3><p><strong>Authors:</strong> Joshua Lackman
<strong>Venue:</strong> arXiv (2026)</p><p>The standard definition of integration of differential forms is based on local coordinates and partitions of unity. This definition is mostly a formality and not used used in explicit computations or approximation schemes. We present a definition of the integral that uses triangulations instead. Our definition is a coordinate-free version of the standard definition of the Riemann integral on $\mathbb{R}^n$ and we argue that it is the natural definition in the contexts of Lie algebroids, stochastic integration and quantum field theory, where path integrals are defined using lattices. In particular, our definition naturally incorporates the different stochastic integrals, which involve integration over H√∂lder continuous paths. Furthermore, our definition is well-adapted to establishing integral identities from their combinatorial counterparts. Our construction is based on the observation that, in great generality, the things that are integrated are determined by cochains on the pair groupoid. Abstractly, our definition uses the van Est map to lift a differential form to the pair groupoid. Our construction suggests a generalization of the fundamental theorem of calculus which we prove: the singular cohomology and de Rham cohomology cap products of a cocycle with the fundamental class are equal.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05228v1">üìÑ Download PDF</a></p><hr><h3 id=variable-projection-methods-for-solving-regularized-separable-inverse-problems-with-applications-to-semi-blind-image-deblurringhttpsarxivorgabs260105224v1><a href=https://arxiv.org/abs/2601.05224v1>Variable Projection Methods for Solving Regularized Separable Inverse Problems with Applications to Semi-Blind Image Deblurring</a><a hidden class=anchor aria-hidden=true href=#variable-projection-methods-for-solving-regularized-separable-inverse-problems-with-applications-to-semi-blind-image-deblurringhttpsarxivorgabs260105224v1>#</a></h3><p><strong>Authors:</strong> Delfina B. Comerso Salzer, Malena I. Espa√±ol, Gabriela Jeronimo
<strong>Venue:</strong> arXiv (2026)</p><p>Separable nonlinear least squares problems appear in many inverse problems, including semi-blind image deblurring. The variable projection (VarPro) method provides an efficient approach for solving such problems by eliminating linear variables and reducing the problem to a smaller, nonlinear one. In this work, we extend VarPro to solve minimization problems containing a differentiable regularization term on the nonlinear parameters, along with a general-form Tikhonov regularization term on the linear variables. Furthermore, we develop a quasi-Newton method for solving the resulting reduced problem, and provide a local convergence analysis under standard smoothness assumptions, establishing conditions for superlinear or quadratic convergence. For large-scale settings, we introduce an inexact LSQR-based variant and prove its local convergence despite inner-solve and Hessian approximations. Numerical experiments on semi-blind deblurring show that parameter regularization prevents degenerate no-blur solutions and that the proposed methods achieve accurate reconstructions, with the inexact variant offering a favorable accuracy-cost tradeoff consistent with the theory.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05224v1">üìÑ Download PDF</a></p><hr><h3 id=flowlet-conditional-3d-brain-mri-synthesis-using-wavelet-flow-matchinghttpsarxivorgabs260105212v1><a href=https://arxiv.org/abs/2601.05212v1>FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching</a><a hidden class=anchor aria-hidden=true href=#flowlet-conditional-3d-brain-mri-synthesis-using-wavelet-flow-matchinghttpsarxivorgabs260105212v1>#</a></h3><p><strong>Authors:</strong> Danilo Danese, Angela Lombardi, Matteo Attimonelli, Giuseppe Fasano, Tommaso Di Noia
<strong>Venue:</strong> arXiv (2026)</p><p>Brain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual&rsquo;s biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05212v1">üìÑ Download PDF</a></p><hr><h3 id=a-non-commutative-de-branges-rovnyak-model-for-row-contractionshttpsarxivorgabs260105211v1><a href=https://arxiv.org/abs/2601.05211v1>A non-commutative de Branges-Rovnyak model for row contractions</a><a hidden class=anchor aria-hidden=true href=#a-non-commutative-de-branges-rovnyak-model-for-row-contractionshttpsarxivorgabs260105211v1>#</a></h3><p><strong>Authors:</strong> Robert T. W. Martin, Jeet Sampat
<strong>Venue:</strong> arXiv (2026)</p><p>We extend the de Branges-Rovnyak model for completely non-coisometric (CNC) linear contractions on a Hilbert space to the non-commutative multivariate setting of CNC row contractions. Namely, we show that any CNC contraction from several copies of a Hilbert space into a single copy is unitarily equivalent to the adjoint of the restricted backward right shifts acting on the de Branges-Rovnyak space of a contractive left multiplier between vector-valued &ldquo;free Hardy spaces&rdquo; of square-summable power series in several non-commuting (NC) variables. This contractive, operator-valued left multiplier, the characteristic function of the CNC row contraction, is a complete unitary invariant and it is always column-extreme as a contractive left multiplier.
Our construction builds a model reproducing kernel Hilbert space of NC functions using a &ldquo;non-commutative resolvent&rdquo; of the row contraction, $T$, which is the inverse of the monic, affine linear pencil of $T$ in a certain NC unit row-ball of the NC universe of all row tuples of square matrices of all finite sizes.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05211v1">üìÑ Download PDF</a></p><hr><h3 id=on-the-value-function-of-convex-bolza-problems-governed-by-stochastic-difference-equationshttpsarxivorgabs260105207v1><a href=https://arxiv.org/abs/2601.05207v1>On the Value Function of Convex Bolza Problems Governed by Stochastic Difference Equations</a><a hidden class=anchor aria-hidden=true href=#on-the-value-function-of-convex-bolza-problems-governed-by-stochastic-difference-equationshttpsarxivorgabs260105207v1>#</a></h3><p><strong>Authors:</strong> Sebasti√°n √Ålvarez, Julio Deride, Cristopher Hermosilla
<strong>Venue:</strong> arXiv (2026)</p><p>In this paper we study the value function of Bolza problems governed by stochastic difference equations, with particular emphasis on the convex non-anticipative case. Our goal is to provide some insights on the structure of the subdiferential of the value function. In particular, we establish a connection between the evolution of the subgradients of the value function and a stochastic difference equation of Hamiltonian type. This result can be seen as a transposition of the method of characteristics, introduced by Rockafellar and Wolenski in the 2000s, to the stochastic discrete-time setting. Similarly as done in the literature for the deterministic case, the analysis is based on a duality approach. For this reason we study first a dual representation for the value function in terms of the value function of a dual problem, which is a pseudo Bolza problem. The main difference with the deterministic case is that (due to the non-anticipativity) the symmetry between the Bolza problem and its dual is no longer valid. This in turn implies that ensuring the existence of minimizers for the Bolza problem (which is a key point for establishing the method of characteristics) is not as simple as in the deterministic case, and it should be addressed differently. To complete the exposition, we study the existence of minimizers for a particular class of Bolza problems governed by linear stochastic difference equations, the so-called linear-convex optimal control problems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05207v1">üìÑ Download PDF</a></p><hr><h3 id=confidence-and-organizationshttpsarxivorgabs260105206v1><a href=https://arxiv.org/abs/2601.05206v1>Confidence and Organizations</a><a hidden class=anchor aria-hidden=true href=#confidence-and-organizationshttpsarxivorgabs260105206v1>#</a></h3><p><strong>Authors:</strong> Andr√©s Espitia
<strong>Venue:</strong> arXiv (2026)</p><p>Miscalibrated beliefs are widely viewed as compromising the quality of employees&rsquo; decisions. Why, then, might an organization prefer to hire an individual known to be overconfident? This paper develops a theory of organizational demand for employees&rsquo; levels of confidence when private information interacts with conflicts of interest. I study a model in which an employee uses private information to make decisions on behalf of the organization and analyze the belief design problem, namely, how the organization would like the employee to interpret his observations. I show that organizations prefer employees whose actions reflect a constant expected conflict of interest across observations. A well-calibrated employee is optimal if and only if private information does not affect this conflict. When the conflict varies with information, organizations optimally select employees whose confidence distorts their responses to information. Overconfidence is optimal when the organization seeks stronger adjustments to information than a well-calibrated employee would provide.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05206v1">üìÑ Download PDF</a></p><hr><h3 id=videoauto-r1-video-auto-reasoning-via-thinking-once-answering-twicehttpsarxivorgabs260105175v1><a href=https://arxiv.org/abs/2601.05175v1>VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice</a><a hidden class=anchor aria-hidden=true href=#videoauto-r1-video-auto-reasoning-via-thinking-once-answering-twicehttpsarxivorgabs260105175v1>#</a></h3><p><strong>Authors:</strong> Shuming Liu, Mingchen Zhuge, Changsheng Zhao, Jun Chen, Lemeng Wu, Zechun Liu, Chenchen Zhu, Zhipeng Cai, Chong Zhou, Haozhe Liu, Ernie Chang, Saksham Suri, Hongyu Xu, Qi Qian, Wei Wen, Balakrishnan Varadarajan, Zhuang Liu, Hu Xu, Florian Bordes, Raghuraman Krishnamoorthi, Bernard Ghanem, Vikas Chandra, Yunyang Xiong
<strong>Venue:</strong> arXiv (2026)</p><p>Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05175v1">üìÑ Download PDF</a></p><hr><h3 id=inside-out-evolving-user-centric-core-memory-trees-for-long-term-personalized-dialogue-systemshttpsarxivorgabs260105171v1><a href=https://arxiv.org/abs/2601.05171v1>Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems</a><a hidden class=anchor aria-hidden=true href=#inside-out-evolving-user-centric-core-memory-trees-for-long-term-personalized-dialogue-systemshttpsarxivorgabs260105171v1>#</a></h3><p><strong>Authors:</strong> Jihao Zhao, Ding Chen, Zhaoxin Fan, Kerun Xu, Mengting Hu, Bo Tang, Feiyu Xiong, Zhiyu li
<strong>Venue:</strong> arXiv (2026)</p><p>Existing long-term personalized dialogue systems struggle to reconcile unbounded interaction streams with finite context constraints, often succumbing to memory noise accumulation, reasoning degradation, and persona inconsistency. To address these challenges, this paper proposes Inside Out, a framework that utilizes a globally maintained PersonaTree as the carrier of long-term user profiling. By constraining the trunk with an initial schema and updating the branches and leaves, PersonaTree enables controllable growth, achieving memory compression while preserving consistency. Moreover, we train a lightweight MemListener via reinforcement learning with process-based rewards to produce structured, executable, and interpretable {ADD, UPDATE, DELETE, NO_OP} operations, thereby supporting the dynamic evolution of the personalized tree. During response generation, PersonaTree is directly leveraged to enhance outputs in latency-sensitive scenarios; when users require more details, the agentic mode is triggered to introduce details on-demand under the constraints of the PersonaTree. Experiments show that PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. Notably, the small MemListener model achieves memory-operation decision performance comparable to, or even surpassing, powerful reasoning models such as DeepSeek-R1-0528 and Gemini-3-Pro.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05171v1">üìÑ Download PDF</a></p><hr><h3 id=mitigating-simulator-dependence-in-ai-parameter-inference-for-the-epoch-of-reionization-the-importance-of-simulation-diversityhttpsarxivorgabs260105229v1><a href=https://arxiv.org/abs/2601.05229v1>Mitigating Simulator Dependence in AI Parameter Inference for the Epoch of Reionization: The Importance of Simulation Diversity</a><a hidden class=anchor aria-hidden=true href=#mitigating-simulator-dependence-in-ai-parameter-inference-for-the-epoch-of-reionization-the-importance-of-simulation-diversityhttpsarxivorgabs260105229v1>#</a></h3><p><strong>Authors:</strong> Jasper Solt, Jonathan C. Pober, Stephen H. Bach
<strong>Venue:</strong> arXiv (2026)</p><p>The 21cm signal of neutral hydrogen contains a wealth of information about the poorly constrained era of cosmological history, the Epoch of Reionization (EoR). Recently, AI models trained on EoR simulations have gained significant attention as a powerful and flexible option for inferring parameters from 21cm observations. However, previous works show that AI models trained on data from one simulator fail to generalize to data from another, raising doubts about AI models&rsquo; ability to accurately infer parameters from observation. We develop a new strategy for training AI models on cosmological simulations based on the principle that increasing the diversity of the training dataset improves model robustness by averaging out spurious and contradictory information. We train AI models on data from different combinations of four simulators, then compare the models&rsquo; performance when predicting on data from held-out simulators acting as proxies for the real universe. We find that models trained on data from multiple simulators perform better on data from a held-out simulator than models trained on data from a single simulator, indicating that increasing the diversity of the training dataset improves a model&rsquo;s ability to generalize. This result suggests that future EoR parameter inference methods can mitigate simulator-specific bias by incorporating multiple simulation approaches into their analyses.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05229v1">üìÑ Download PDF</a></p><hr><h3 id=reducibility-of-higher-order-to-pairwise-interactions-social-impact-models-on-hypergraphshttpsarxivorgabs260105169v1><a href=https://arxiv.org/abs/2601.05169v1>Reducibility of higher-order to pairwise interactions: Social impact models on hypergraphs</a><a hidden class=anchor aria-hidden=true href=#reducibility-of-higher-order-to-pairwise-interactions-social-impact-models-on-hypergraphshttpsarxivorgabs260105169v1>#</a></h3><p><strong>Authors:</strong> Jaume Llabr√©s, Ra√∫l Toral, Maxi San Miguel, Federico V√°zquez
<strong>Venue:</strong> arXiv (2026)</p><p>We show that a general class of social impact models with higher-order interactions on hypergraphs can be exactly reduced to an equivalent model with pairwise interactions on a weighted projected network. This reduction is made by a mapping that preserves the microscopic probabilities of changing the state of the nodes. As a particular case, we introduce hypergraph-voter models, for which we compute the weights of the projected network both analytically and numerically across several hypergraph ensembles, and we characterize their ordering dynamics through simulations of both higher-order and reduced dynamics. For a linear social impact function (hypergraph-linear voter model) the weights of the projected network are static, allowing us to develop a pair approximation that describes with accuracy the time evolution of macroscopic observables, which turn out to be independent of those weights. The macroscopic dynamics is thus equivalent to that of the standard voter model on the unweighted projected network. For a power-law social impact function (hypergraph-nonlinear voter model) the weights of the projected network depend on the instantaneous system configuration. Nevertheless, the nonlinear voter model on the unweighted projected network still reproduces the main macroscopic trends for well connected hypergraphs.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05169v1">üìÑ Download PDF</a></p><hr><h3 id=why-are-some-countries-more-politically-fragmented-online-than-othershttpsarxivorgabs260105093v1><a href=https://arxiv.org/abs/2601.05093v1>Why Are Some Countries More Politically Fragmented Online Than Others?</a><a hidden class=anchor aria-hidden=true href=#why-are-some-countries-more-politically-fragmented-online-than-othershttpsarxivorgabs260105093v1>#</a></h3><p><strong>Authors:</strong> Yuan Zhang, Laia Castro, Frank Esser, Alexandre Bovet
<strong>Venue:</strong> arXiv (2026)</p><p>Online political divisions, such as fragmentation or polarization, are a growing global concern that can foster radicalization and hinder democratic cooperation; however, not all divisions are detrimental, some reflect pluralism and healthy diversity of opinion in a democracy. While prior research has predominantly focused on polarization in the United States, there remains a limited body of research on political divides in multiparty systems, and no universal method for comparing fragmentation across countries. Moreover, cross-country comparison is rare. This study first develops a novel measure of structural political fragmentation built on multi-scale community detection and the effective branching factor. Using a dataset of 18,325 political influencers from Brazil, Spain, and the United States, we assess online fragmentation in their Twitter/X co-following networks. We compare the fragmentation of the three countries, as well as the ideological groups within each. We further investigate factors associated with the level of fragmentation in each country. We find that political fragmentation differs across countries and is asymmetric between ideological groups. Brazil is the most fragmented, with higher fragmentation among the left-wing group, while Spain and the United States exhibit similar overall levels, with the left more fragmented in Spain and the right more fragmented in the United States. Additionally, we find that social identity plays a central role in political fragmentation. A strong alignment between ideological and social identities, with minimal overlap between ideologies, tends to promote greater integration and reduce fragmentation. Our findings provide explanations for cross-national and ideological differences in political fragmentation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05093v1">üìÑ Download PDF</a></p><hr><h3 id=graph-energy-as-a-measure-of-community-detectability-in-networkshttpsarxivorgabs260105065v1><a href=https://arxiv.org/abs/2601.05065v1>Graph energy as a measure of community detectability in networks</a><a hidden class=anchor aria-hidden=true href=#graph-energy-as-a-measure-of-community-detectability-in-networkshttpsarxivorgabs260105065v1>#</a></h3><p><strong>Authors:</strong> Lucas B√∂ttcher, Mason A. Porter, Santo Fortunato
<strong>Venue:</strong> arXiv (2026)</p><p>A key challenge in network science is the detection of communities, which are sets of nodes in a network that are densely connected internally but sparsely connected to the rest of the network. A fundamental result in community detection is the existence of a nontrivial threshold for community detectability on sparse graphs that are generated by the planted partition model (PPM). Below this so-called ``detectability limit&rsquo;&rsquo;, no community-detection method can perform better than random chance. Spectral methods for community detection fail before this detectability limit because the eigenvalues corresponding to the eigenvectors that are relevant for community detection can be absorbed by the bulk of the spectrum. One can bypass the detectability problem by using special matrices, like the non-backtracking matrix, but this requires one to consider higher-dimensional matrices. In this paper, we show that the difference in graph energy between a PPM and an Erd≈ës&ndash;R√©nyi (ER) network has a distinct transition at the detectability threshold even for the adjacency matrices of the underlying networks. The graph energy is based on the full spectrum of an adjacency matrix, so our result suggests that standard graph matrices still allow one to separate the parameter regions with detectable and undetectable communities.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2601.05065v1">üìÑ Download PDF</a></p><hr></div><footer class=post-footer><ul class=post-tags></ul></footer><div><div class=pagination__title><span class=pagination__title-h style=font-size:20px>Comments</span><hr></div><div id=tcomment></div><script src=https://cdn.staticfile.org/twikoo/1.6.39/twikoo.all.min.js></script><script>twikoo.init({envId:"https://mangodb-theta.vercel.app/",el:"#tcomment",lang:"en-US",path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://garyforreal.me/en/>Gary's House</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>Views: <span id=busuanzi_value_site_pv></span>
</span><span id=busuanzi_container_site_uv>Visitors: <span id=busuanzi_value_site_uv></span></span></div></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>