<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>ËÆ∫Êñá on Gary&#39;s House</title>
    <link>https://garyforreal.me/zh/posts/paper/</link>
    <description>Recent content in ËÆ∫Êñá on Gary&#39;s House</description>
    <generator>Hugo -- 0.141.0</generator>
    <language>zh</language>
    <lastBuildDate>Sun, 25 Jan 2026 15:25:20 +0000</lastBuildDate>
    <atom:link href="https://garyforreal.me/zh/posts/paper/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Weekly Paper Notes - 2026-01-25</title>
      <link>https://garyforreal.me/zh/posts/paper/paper-2026-01-25-weekly/</link>
      <pubDate>Sun, 25 Jan 2026 15:25:20 +0000</pubDate>
      <guid>https://garyforreal.me/zh/posts/paper/paper-2026-01-25-weekly/</guid>
      <description>&lt;h1 id=&#34;weekly-paper-notes&#34;&gt;Weekly Paper Notes&lt;/h1&gt;
&lt;h2 id=&#34;-multilingual&#34;&gt;üîç multilingual&lt;/h2&gt;
&lt;h3 id=&#34;improving-training-efficiency-and-reducing-maintenance-costs-via-language-specific-model-merginghttpsarxivorgabs260116127v1&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2601.16127v1&#34;&gt;Improving Training Efficiency and Reducing Maintenance Costs via Language Specific Model Merging&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Alphaeus Dmonte, Vidhi Gupta, Daniel J Perry, Mark Arehart
&lt;strong&gt;Venue:&lt;/strong&gt; arXiv (2026)&lt;/p&gt;
&lt;p&gt;Fine-tuning a task-specific multilingual large language model (LLM) involves training the model on a multilingual dataset with examples in all the required languages. Updating one or more supported languages with additional data or adding support for a new language involves retraining the model, which can be computationally inefficient and creates a severe maintenance bottleneck. Recent research on merging multilingual multitask models has shown promise in terms of improved quality, but its computational and maintenance efficiency remains unstudied. In this work, we provide the first focused analysis of this merging strategy from an efficiency perspective, evaluating it across three independent tasks. We demonstrate significant efficiency gains while maintaining parity in terms of quality: this merging approach reduces the initial training time by up to 50%. We also demonstrate that updating an individual language and re-merging as part of model maintenance reduces training costs by more than 60%, compared to re-training the full multilingual model. We show this on both public and proprietary industry datasets confirming that the approach works well for industrial use cases in addition to academic settings already studied in previous work.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Weekly Paper Notes - 2026-01-18</title>
      <link>https://garyforreal.me/zh/posts/paper/paper-2026-01-18-weekly/</link>
      <pubDate>Sun, 18 Jan 2026 15:24:18 +0000</pubDate>
      <guid>https://garyforreal.me/zh/posts/paper/paper-2026-01-18-weekly/</guid>
      <description>&lt;h1 id=&#34;weekly-paper-notes&#34;&gt;Weekly Paper Notes&lt;/h1&gt;
&lt;h2 id=&#34;-multilingual&#34;&gt;üîç multilingual&lt;/h2&gt;
&lt;h3 id=&#34;curve-a-benchmark-for-cultural-and-multilingual-long-video-reasoninghttpsarxivorgabs260110649v1&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2601.10649v1&#34;&gt;CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Darshan Singh, Arsha Nagrani, Kawshik Manikantan, Harman Singh, Dinesh Tewari, Tobias Weyand, Cordelia Schmid, Anelia Angelova, Shachi Dave
&lt;strong&gt;Venue:&lt;/strong&gt; arXiv (2026)&lt;/p&gt;
&lt;p&gt;Recent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE&amp;rsquo;s reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under &lt;a href=&#34;https://github.com/google-deepmind/neptune?tab=readme-ov-file&#34;&gt;https://github.com/google-deepmind/neptune?tab=readme-ov-file&lt;/a&gt;#minerva-cultural&lt;/p&gt;</description>
    </item>
    <item>
      <title>Weekly Paper Notes - 2026-01-11</title>
      <link>https://garyforreal.me/zh/posts/paper/paper-2026-01-11-weekly/</link>
      <pubDate>Sun, 11 Jan 2026 15:25:07 +0000</pubDate>
      <guid>https://garyforreal.me/zh/posts/paper/paper-2026-01-11-weekly/</guid>
      <description>&lt;h1 id=&#34;weekly-paper-notes&#34;&gt;Weekly Paper Notes&lt;/h1&gt;
&lt;h2 id=&#34;-multilingual&#34;&gt;üîç multilingual&lt;/h2&gt;
&lt;h3 id=&#34;code-mix-sentiment-analysis-on-hinglish-tweetshttpsarxivorgabs260105091v1&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2601.05091v1&#34;&gt;Code-Mix Sentiment Analysis on Hinglish Tweets&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aashi Garg, Aneshya Das, Arshi Arya, Anushka Goyal, Aditi
&lt;strong&gt;Venue:&lt;/strong&gt; arXiv (2026)&lt;/p&gt;
&lt;p&gt;The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish&amp;ndash;a hybrid of Hindi and English&amp;ndash;used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Weekly Paper Notes - 2026-01-04</title>
      <link>https://garyforreal.me/zh/posts/paper/paper-2026-01-04-weekly/</link>
      <pubDate>Sun, 04 Jan 2026 15:23:50 +0000</pubDate>
      <guid>https://garyforreal.me/zh/posts/paper/paper-2026-01-04-weekly/</guid>
      <description>&lt;h1 id=&#34;weekly-paper-notes&#34;&gt;Weekly Paper Notes&lt;/h1&gt;
&lt;h2 id=&#34;-multilingual&#34;&gt;üîç multilingual&lt;/h2&gt;
&lt;h3 id=&#34;big-ai-is-accelerating-the-metacrisis-what-can-we-dohttpsarxivorgabs251224863v1&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2512.24863v1&#34;&gt;Big AI is accelerating the metacrisis: What can we do?&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Steven Bird
&lt;strong&gt;Venue:&lt;/strong&gt; arXiv (2025)&lt;/p&gt;
&lt;p&gt;The world is in the grip of ecological, meaning, and language crises which are converging into a metacrisis. Big AI is accelerating them all. Language engineers are playing a central role, persisting with a scalability story that is failing humanity, supplying critical talent to plutocrats and kleptocrats, and creating new technologies as if the whole endeavour was value-free. We urgently need to explore alternatives, applying our collective intelligence to design a life-affirming future for NLP that is centered on human flourishing on a living planet.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Weekly Paper Notes - 2025-12-28</title>
      <link>https://garyforreal.me/zh/posts/paper/paper-2025-12-28-weekly/</link>
      <pubDate>Sun, 28 Dec 2025 15:24:24 +0000</pubDate>
      <guid>https://garyforreal.me/zh/posts/paper/paper-2025-12-28-weekly/</guid>
      <description>&lt;h1 id=&#34;weekly-paper-notes&#34;&gt;Weekly Paper Notes&lt;/h1&gt;
&lt;h2 id=&#34;-multilingual&#34;&gt;üîç multilingual&lt;/h2&gt;
&lt;h3 id=&#34;model-merging-via-multi-teacher-knowledge-distillationhttpsarxivorgabs251221288v1&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2512.21288v1&#34;&gt;Model Merging via Multi-Teacher Knowledge Distillation&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Seyed Arshan Dalili, Mehrdad Mahdavi
&lt;strong&gt;Venue:&lt;/strong&gt; arXiv (2025)&lt;/p&gt;
&lt;p&gt;Model merging has emerged as a lightweight alternative to joint multi-task learning (MTL), yet the generalization properties of merged models remain largely unexplored. Establishing such theoretical guarantees is non-trivial, as the merging process typically forbids access to the original training data and involves combining fine-tuned models trained on fundamentally heterogeneous data distributions. Without a principled understanding of these dynamics, current methods often rely on heuristics to approximate the optimal combination of parameters. This dependence is most critical in coefficient scaling, the weighting factors that modulate the magnitude of each fine-tuned model&amp;rsquo;s contribution to the shared parameter. However, without a principled objective to guide their selection, these methods lead to brittle performance and are highly sensitive to scaling initialization. We address this gap by (i) establishing a novel flatness-aware PAC-Bayes generalization bound specifically for the model merging setting. This analysis introduces a &amp;ldquo;cross-task heterogeneity&amp;rdquo; term that formally captures the mismatch between diverse fine-tuned model priors and the target multi-task distributions. Guided by this theoretical insight, (ii) we frame model merging as multi-teacher knowledge distillation on scarce, unlabeled data. We formally demonstrate that minimizing the student-teacher Kullback-Leibler divergence directly tightens the upper bound on the merged model&amp;rsquo;s excess risk. Guided by the flatness-aware bound derived, (iii) we operationalize this objective via SAMerging, a method that employs Sharpness-Aware Minimization (SAM) to find flat minima. Empirically, SAMerging establishes a new state of the art across vision and NLP benchmarks, achieving remarkable performance. The code is available at &lt;a href=&#34;https://github.com/arshandalili/SAMerging&#34;&gt;https://github.com/arshandalili/SAMerging&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Weekly Paper Notes - 2025-12-21</title>
      <link>https://garyforreal.me/zh/posts/paper/paper-2025-12-21-weekly/</link>
      <pubDate>Sun, 21 Dec 2025 15:23:23 +0000</pubDate>
      <guid>https://garyforreal.me/zh/posts/paper/paper-2025-12-21-weekly/</guid>
      <description>&lt;h1 id=&#34;weekly-paper-notes&#34;&gt;Weekly Paper Notes&lt;/h1&gt;
&lt;h2 id=&#34;-multilingual&#34;&gt;üîç multilingual&lt;/h2&gt;
&lt;h3 id=&#34;from-essence-to-defense-adaptive-semantic-aware-watermarking-for-embedding-as-a-service-copyright-protectionhttpsarxivorgabs251216439v1&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2512.16439v1&#34;&gt;From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Li, Yubing Ren, Yanan Cao, Yingjie Li, Fang Fang, Xuebin Wang
&lt;strong&gt;Venue:&lt;/strong&gt; arXiv (2025)&lt;/p&gt;
&lt;p&gt;Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Weekly Paper Notes - 2025-12-14</title>
      <link>https://garyforreal.me/zh/posts/paper/paper-2025-12-14-weekly/</link>
      <pubDate>Sun, 14 Dec 2025 15:23:14 +0000</pubDate>
      <guid>https://garyforreal.me/zh/posts/paper/paper-2025-12-14-weekly/</guid>
      <description>&lt;h1 id=&#34;weekly-paper-notes&#34;&gt;Weekly Paper Notes&lt;/h1&gt;
&lt;h2 id=&#34;-multilingual&#34;&gt;üîç multilingual&lt;/h2&gt;
&lt;h3 id=&#34;grow-up-and-merge-scaling-strategies-for-efficient-language-adaptationhttpsarxivorgabs251210772v1&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2512.10772v1&#34;&gt;Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kevin Glocker, K√§triin Kukk, Romina Oji, Marcel Bollmann, Marco Kuhlmann, Jenny Kunz
&lt;strong&gt;Venue:&lt;/strong&gt; arXiv (2025)&lt;/p&gt;
&lt;p&gt;Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model&amp;rsquo;s capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
