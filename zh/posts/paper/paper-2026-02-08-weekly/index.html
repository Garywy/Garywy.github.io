<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Weekly Paper Notes - 2026-02-08 | Gary's House</title>
<meta name=keywords content><meta name=description content="Weekly Paper Notes
üîç multilingual
Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training
Authors: Junxiao Liu, Zhijun Wang, Yixiao Li, Zhejian Lai, Liqian Huang, Xin Huang, Xue Han, Junlan Feng, Shujian Huang
Venue: arXiv (2026)
Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200."><meta name=author content="Gary"><link rel=canonical href=https://garyforreal.me/zh/posts/paper/paper-2026-02-08-weekly/><meta name=google-site-verification content="XYZabc"><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><link crossorigin=anonymous href=/assets/css/stylesheet.7e5251d8716d933fafcf3df70d7ecd02729661d6d232fc5dd0b8a85ef75e3409.css integrity="sha256-flJR2HFtkz+vzz33DX7NAnKWYdbSMvxd0LioXvdeNAk=" rel="preload stylesheet" as=style><link rel=icon href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=16x16 href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=32x32 href=https://garyforreal.me/img/Q.jpg><link rel=apple-touch-icon href=https://garyforreal.me/Q.jpg><link rel=mask-icon href=https://garyforreal.me/Q.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://garyforreal.me/zh/posts/paper/paper-2026-02-08-weekly/><link rel=alternate hreflang=en href=https://garyforreal.me/en/posts/paper/paper-2026-02-08-weekly/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@200..900&display=swap" rel=stylesheet><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous referrerpolicy=no-referrer></script><meta property="og:title" content="Weekly Paper Notes - 2026-02-08"><meta property="og:description" content="Weekly Paper Notes
üîç multilingual
Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training
Authors: Junxiao Liu, Zhijun Wang, Yixiao Li, Zhejian Lai, Liqian Huang, Xin Huang, Xue Han, Junlan Feng, Shujian Huang
Venue: arXiv (2026)
Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200."><meta property="og:type" content="article"><meta property="og:url" content="https://garyforreal.me/zh/posts/paper/paper-2026-02-08-weekly/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-08T15:31:07+00:00"><meta property="article:modified_time" content="2026-02-08T15:31:07+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Weekly Paper Notes - 2026-02-08"><meta name=twitter:description content="Weekly Paper Notes
üîç multilingual
Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training
Authors: Junxiao Liu, Zhijun Wang, Yixiao Li, Zhejian Lai, Liqian Huang, Xin Huang, Xue Han, Junlan Feng, Shujian Huang
Venue: arXiv (2026)
Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Â∏ñÂ≠ê","item":"https://garyforreal.me/zh/posts/"},{"@type":"ListItem","position":2,"name":"ËÆ∫Êñá","item":"https://garyforreal.me/zh/posts/paper/"},{"@type":"ListItem","position":3,"name":"Weekly Paper Notes - 2026-02-08","item":"https://garyforreal.me/zh/posts/paper/paper-2026-02-08-weekly/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Weekly Paper Notes - 2026-02-08","name":"Weekly Paper Notes - 2026-02-08","description":"Weekly Paper Notes üîç multilingual Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training Authors: Junxiao Liu, Zhijun Wang, Yixiao Li, Zhejian Lai, Liqian Huang, Xin Huang, Xue Han, Junlan Feng, Shujian Huang Venue: arXiv (2026)\nLong reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.\n","keywords":[],"articleBody":"Weekly Paper Notes üîç multilingual Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training Authors: Junxiao Liu, Zhijun Wang, Yixiao Li, Zhejian Lai, Liqian Huang, Xin Huang, Xue Han, Junlan Feng, Shujian Huang Venue: arXiv (2026)\nLong reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.\nüìÑ Download PDF\nPolyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions Authors: L√©o Labat, Etienne Ollion, Fran√ßois Yvon Venue: arXiv (2026)\nMultiple-Choice Questions (MCQs) are often used to assess knowledge, reasoning abilities, and even values encoded in large language models (LLMs). While the effect of multilingualism has been studied on LLM factual recall, this paper seeks to investigate the less explored question of language-induced variation in value-laden MCQ responses. Are multilingual LLMs consistent in their responses across languages, i.e. behave like theoretical polyglots, or do they answer value-laden MCQs depending on the language of the question, like a multitude of monolingual models expressing different values through a single model? We release a new corpus, the Multilingual European Value Survey (MEVS), which, unlike prior work relying on machine translation or ad hoc prompts, solely comprises human-translated survey questions aligned in 8 European languages. We administer a subset of those questions to over thirty multilingual LLMs of various sizes, manufacturers and alignment-fine-tuning status under comprehensive, controlled prompt variations including answer order, symbol type, and tail character. Our results show that while larger, instruction-tuned models display higher overall consistency, the robustness of their responses varies greatly across questions, with certain MCQs eliciting total agreement within and across models while others leave LLM answers split. Language-specific behavior seems to arise in all consistent, instruction-fine-tuned models, but only on certain questions, warranting a further study of the selective effect of preference fine-tuning.\nüìÑ Download PDF\nDr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations Authors: Wei Liu, Jiawei Xu, Yingru Li, Longtao Zheng, Tianjian Li, Qian Liu, Junxian He Venue: arXiv (2026)\nHigh-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.\nüìÑ Download PDF\nEuroLLM-22B: Technical Report Authors: Miguel Moura Ramos, Duarte M. Alves, Hippolyte Gisserot-Boukhlef, Jo√£o Alves, Pedro Henrique Martins, Patrick Fernandes, Jos√© Pombal, Nuno M. Guerreiro, Ricardo Rei, Nicolas Boizard, Amin Farajian, Mateusz Klimaszewski, Jos√© G. C. de Souza, Barry Haddow, Fran√ßois Yvon, Pierre Colombo, Alexandra Birch, Andr√© F. T. Martins Venue: arXiv (2026)\nThis report presents EuroLLM-22B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-22B‚Äôs development, including tokenizer design, architectural specifications, data filtering, and training procedures. Across a broad set of multilingual benchmarks, EuroLLM-22B demonstrates strong performance in reasoning, instruction following, and translation, achieving results competitive with models of comparable size. To support future research, we release our base and instruction-tuned models, our multilingual web pretraining data and updated EuroBlocks instruction datasets, as well as our pre-training and evaluation codebases.\nüìÑ Download PDF\nWhispers of the Butterfly: A Research-through-Design Exploration of In-Situ Conversational AI Guidance in Large-Scale Outdoor MR Exhibitions Authors: Dongyijie Primo Pan, Shuyue Li, Yawei Zhao, Junkun Long, Hao Li, Pan Hui Venue: arXiv (2026)\nLarge-scale outdoor mixed reality (MR) art exhibitions distribute curated virtual works across open public spaces, but interpretation rarely scales without turning exploration into a scripted tour. Through Research-through-Design, we created Dream-Butterfly, an in-situ conversational AI docent embodied as a small non-human companion that visitors summon for multilingual, exhibition-grounded explanations. We deployed Dream-Butterfly in a large-scale outdoor MR exhibition at a public university campus in southern China, and conducted an in-the-wild between-subject study (N=24) comparing a primarily human-led tour with an AI-led tour while keeping staff for safety in both conditions. Combining questionnaires and semi-structured interviews, we characterize how shifting the primary explanation channel reshapes explanation access, perceived responsiveness, immersion, and workload, and how visitors negotiate responsibility handoffs among staff, the AI guide, and themselves. We distill transferable design implications for configuring mixed human-AI guiding roles and embodying conversational agents in mobile, safety-constrained outdoor MR exhibitions.\nüìÑ Download PDF\nFiMI: A Domain-Specific Language Model for Indian Finance Ecosystem Authors: Aboli Kathar, Aman Kumar, Anusha Kamath, Araveeti Srujan, Ashish Sharma, Chandra Bhushan, Dilip Asbe, Divya Sorate, Duddu Prasanth Kumar, Evan Acharya, Harsh Sharma, Hrithik Kadam, Kanishk Singla, Keyur Doshi, Kiran Praveen, Kolisetty Krishna SK, Krishanu Adhikary, Lokesh MPT, Mayurdeep Sonowal, Nadeem Shaikh, Navya Prakash, Nimit Kothari, Nitin Kukreja, Prashant Devadiga, Rakesh Paul, Ratanjeet Pratap Chauhan, Raunak Kalani, Raviraj Joshi, Shamanth MH, Shantanu Pandey, Shubham Soni, Siddharth Dixit, Smriti Jopat, Sunil Patel, Suraj Singh, Suvradip Paul, Tulasi Pilla, Utkarsh Vaidya, Vineeth Nambiar, Vishal Kanvaty, Yatharth Dedhia Venue: arXiv (2026)\nWe present FiMI (Finance Model for India), a domain-specialized financial language model developed for Indian digital payment systems. We develop two model variants: FiMI Base and FiMI Instruct. FiMI adapts the Mistral Small 24B architecture through a multi-stage training pipeline, beginning with continuous pre-training on 68 Billion tokens of curated financial, multilingual (English, Hindi, Hinglish), and synthetic data. This is followed by instruction fine-tuning and domain-specific supervised fine-tuning focused on multi-turn, tool-driven conversations that model real-world workflows, such as transaction disputes and mandate lifecycle management. Evaluations reveal that FiMI Base achieves a 20% improvement over the Mistral Small 24B Base model on finance reasoning benchmark, while FiMI Instruct outperforms the Mistral Small 24B Instruct model by 87% on domain-specific tool-calling. Moreover, FiMI achieves these significant domain gains while maintaining comparable performance to models of similar size on general benchmarks.\nüìÑ Download PDF\nBhashaSetu: Cross-Lingual Knowledge Transfer from High-Resource to Extreme Low-Resource Languages Authors: Subhadip Maji, Arnab Bhattacharya Venue: arXiv (2026)\nDespite remarkable advances in natural language processing, developing effective systems for low-resource languages remains a formidable challenge, with performances typically lagging far behind high-resource counterparts due to data scarcity and insufficient linguistic resources. Cross-lingual knowledge transfer has emerged as a promising approach to address this challenge by leveraging resources from high-resource languages. In this paper, we investigate methods for transferring linguistic knowledge from high-resource languages to low-resource languages, where the number of labeled training instances is in hundreds. We focus on sentence-level and word-level tasks. We introduce a novel method, GETR (Graph-Enhanced Token Representation) for cross-lingual knowledge transfer along with two adopted baselines (a) augmentation in hidden layers and (b) token embedding transfer through token translation. Experimental results demonstrate that our GNN-based approach significantly outperforms existing multilingual and cross-lingual baseline methods, achieving 13 percentage point improvements on truly low-resource languages (Mizo, Khasi) for POS tagging, and 20 and 27 percentage point improvements in macro-F1 on simulated low-resource languages (Marathi, Bangla, Malayalam) across sentiment classification and NER tasks respectively. We also present a detailed analysis of the transfer mechanisms and identify key factors that contribute to successful knowledge transfer in this linguistic context.\nüìÑ Download PDF\nCross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks Authors: Chaimae Abouzahir, Congbo Ma, Nizar Habash, Farah E. Shamout Venue: arXiv (2026)\nIn recent years, Large Language Models (LLMs) have become widely used in medical applications, such as clinical decision support, medical education, and medical question answering. Yet, these models are often English-centric, limiting their robustness and reliability for linguistically diverse communities. Recent work has highlighted discrepancies in performance in low-resource languages for various medical tasks, but the underlying causes remain poorly understood. In this study, we conduct a cross-lingual empirical analysis of LLM performance on Arabic and English medical question and answering. Our findings reveal a persistent language-driven performance gap that intensifies with increasing task complexity. Tokenization analysis exposes structural fragmentation in Arabic medical text, while reliability analysis suggests that model-reported confidence and explanations exhibit limited correlation with correctness. Together, these findings underscore the need for language-aware design and evaluation strategies in LLMs for medical tasks.\nüìÑ Download PDF\nMultilingual Extraction and Recognition of Implicit Discourse Relations in Speech and Text Authors: Ahmed Ruby, Christian Hardmeier, Sara Stymne Venue: arXiv (2026)\nImplicit discourse relation classification is a challenging task, as it requires inferring meaning from context. While contextual cues can be distributed across modalities and vary across languages, they are not always captured by text alone. To address this, we introduce an automatic method for distantly related and unrelated language pairs to construct a multilingual and multimodal dataset for implicit discourse relations in English, French, and Spanish. For classification, we propose a multimodal approach that integrates textual and acoustic information through Qwen2-Audio, allowing joint modeling of text and audio for implicit discourse relation classification across languages. We find that while text-based models outperform audio-based models, integrating both modalities can enhance performance, and cross-lingual transfer can provide substantial improvements for low-resource languages.\nüìÑ Download PDF\n‚ÄúBe My Cheese?‚Äù: Cultural Nuance Benchmarking for Machine Translation in Multilingual LLMs Authors: Madison Van Doren, Casey Ford, Jennifer Barajas, Cory Holland Venue: arXiv (2026)\nWe present a large-scale human evaluation benchmark for assessing cultural localisation in machine translation produced by state-of-the-art multilingual large language models (LLMs). Existing MT benchmarks emphasise token-level and grammatical accuracy, but of ten overlook pragmatic and culturally grounded competencies required for real-world localisation. Building on a pilot study of 87 translations across 20 languages, we evaluate 7 multilingual LLMs across 15 target languages with 5 native-speaker raters per language. Raters scored both full-text translations and segment-level instances of culturally nuanced language (idioms, puns, holidays, and culturally embedded concepts) on an ordinal 0-3 quality scale; segment ratings additionally included an NA option for untranslated segments. Across full-text evaluations, mean overall quality is modest (1.68/3): GPT-5 (2.10/3), Claude Sonnet 3.7 (1.97/3), and Mistral Medium 3.1 (1.84/3) form the strongest tier with fewer catastrophic failures. Segment-level results show sharp category effects: holidays (2.20/3) and cultural concepts (2.19/3) translate substantially better than idioms (1.65/3) and puns (1.45/3), and idioms are most likely to be left untranslated. These findings demonstrate a persistent gap between grammatical adequacy and cultural resonance. To our knowledge, this is the first multilingual, human-annotated benchmark focused explicitly on cultural nuance in translation and localisation, highlighting the need for culturally informed training data, improved cross-lingual pragmatics, and evaluation paradigms that better reflect real-world communicative competence.\nüìÑ Download PDF\nTokenization and Morphological Fidelity in Uralic NLP: A Cross-Lingual Evaluation Authors: Nuo Xu, Ahrii Kim Venue: arXiv (2026)\nSubword tokenization critically affects Natural Language Processing (NLP) performance, yet its behavior in morphologically rich and low-resource language families remains under-explored. This study systematically compares three subword paradigms ‚Äì Byte Pair Encoding (BPE), Overlap BPE (OBPE), and Unigram Language Model ‚Äì across six Uralic languages with varying resource availability and typological diversity. Using part-of-speech (POS) tagging as a controlled downstream task, we show that OBPE consistently achieves stronger morphological alignment and higher tagging accuracy than conventional methods, particularly within the Latin-script group. These gains arise from reduced fragmentation in open-class categories and a better balance across the frequency spectrum. Transfer efficacy further depends on the downstream tagging architecture, interacting with both training volume and genealogical proximity. Taken together, these findings highlight that morphology-sensitive tokenization is not merely a preprocessing choice but a decisive factor in enabling effective cross-lingual transfer for agglutinative, low-resource languages.\nüìÑ Download PDF\nBagging-Based Model Merging for Robust General Text Embeddings Authors: Hengran Zhang, Keping Bi, Jiafeng Guo, Jiaming Zhang, Wenbo Yang, Daiting Shi, Xueqi Cheng Venue: arXiv (2026)\nGeneral-purpose text embedding models underpin a wide range of NLP and information retrieval applications, and are typically trained on large-scale multi-task corpora to encourage broad generalization. However, it remains unclear how different multi-task training strategies compare in practice, and how to efficiently adapt embedding models as new domains and data types continually emerge. In this work, we present a systematic study of multi-task training for text embeddings from two perspectives: data scheduling and model merging. We compare batch-level shuffling, sequential training variants, two-stage training, and multiple merging granularities, and find that simple batch-level shuffling consistently yields the strongest overall performance, suggesting that task conflicts are limited and training datasets are largely complementary. Despite its effectiveness, batch-level shuffling exhibits two practical limitations: suboptimal out-of-domain (OOD) generalization and poor suitability for incremental learning due to expensive full retraining. To address these issues, we propose Bagging-based rObust mOdel Merging (\\modelname), which trains multiple embedding models on sampled subsets and merges them into a single model, improving robustness while retaining single-model inference efficiency. Moreover, \\modelname naturally supports efficient incremental updates by training lightweight update models on new data with a small historical subset and merging them into the existing model. Experiments across diverse embedding benchmarks demonstrate that \\modelname consistently improves both in-domain and OOD performance over full-corpus batch-level shuffling, while substantially reducing training cost in incremental learning settings.\nüìÑ Download PDF\nEvaluating the impact of word embeddings on similarity scoring in practical information retrieval Authors: Niall McCarroll, Kevin Curran, Eugene McNamee, Angela Clist, Andrew Brammer Venue: arXiv (2026)\nSearch behaviour is characterised using synonymy and polysemy as users often want to search information based on meaning. Semantic representation strategies represent a move towards richer associative connections that can adequately capture this complex usage of language. Vector Space Modelling (VSM) and neural word embeddings play a crucial role in modern machine learning and Natural Language Processing (NLP) pipelines. Embeddings use distributional semantics to represent words, sentences, paragraphs or entire documents as vectors in high dimensional spaces. This can be leveraged by Information Retrieval (IR) systems to exploit the semantic relatedness between queries and answers. This paper evaluates an alternative approach to measuring query statement similarity that moves away from the common similarity measure of centroids of neural word embeddings. Motivated by the Word Movers Distance (WMD) model, similarity is evaluated using the distance between individual words of queries and statements. Results from ranked query and response statements demonstrate significant gains in accuracy using the combined approach of similarity ranking through WMD with the word embedding techniques. The top performing WMD + GloVe combination outperforms all other state-of-the-art retrieval models including Doc2Vec and the baseline LSA model. Along with the significant gains in performance of similarity ranking through WMD, we conclude that the use of pre-trained word embeddings, trained on vast amounts of data, result in domain agnostic language processing solutions that are portable to diverse business use-cases.\nüìÑ Download PDF\nMedErrBench: A Fine-Grained Multilingual Benchmark for Medical Error Detection and Correction with Clinical Expert Annotations Authors: Congbo Ma, Yichun Zhang, Yousef Al-Jazzazi, Ahamed Foisal, Laasya Sharma, Yousra Sadqi, Khaled Saleh, Jihad Mallat, Farah E. Shamout Venue: arXiv (2026)\nInaccuracies in existing or generated clinical text may lead to serious adverse consequences, especially if it is a misdiagnosis or incorrect treatment suggestion. With Large Language Models (LLMs) increasingly being used across diverse healthcare applications, comprehensive evaluation through dedicated benchmarks is crucial. However, such datasets remain scarce, especially across diverse languages and contexts. In this paper, we introduce MedErrBench, the first multilingual benchmark for error detection, localization, and correction, developed under the guidance of experienced clinicians. Based on an expanded taxonomy of ten common error types, MedErrBench covers English, Arabic and Chinese, with natural clinical cases annotated and reviewed by domain experts. We assessed the performance of a range of general-purpose, language-specific, and medical-domain language models across all three tasks. Our results reveal notable performance gaps, particularly in non-English settings, highlighting the need for clinically grounded, language-aware systems. By making MedErrBench and our evaluation protocols publicly-available, we aim to advance multilingual clinical NLP to promote safer and more equitable AI-based healthcare globally. The dataset is available in the supplementary material. An anonymized version of the dataset is available at: https://github.com/congboma/MedErrBench.\nüìÑ Download PDF\nCASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models Authors: Rui Jia, Ruiyi Lan, Fengrui Liu, Zhongxiang Dai, Bo Jiang, Jing Shao, Jingyuan Chen, Guandong Xu, Fei Wu, Min Zhang Venue: arXiv (2026)\nLarge language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and psychological, thereby posing potential safety risks to vulnerable groups. Existing safety evaluations primarily rely on context-independent metrics such as factual accuracy, bias, or toxicity, which fail to capture the divergent harms that the same response might cause across different student attributes. To address this gap, we propose the concept of Student-Tailored Personalized Safety and construct CASTLE based on educational theories. This benchmark covers 15 educational safety risks and 14 student attributes, comprising 92,908 bilingual scenarios. We further design three evaluation metrics: Risk Sensitivity, measuring the model ability to detect risks; Emotional Empathy, evaluating the model capacity to recognize student states; and Student Alignment, assessing the match between model responses and student attributes. Experiments on 18 SOTA LLMs demonstrate that CASTLE poses a significant challenge: all models scored below an average safety rating of 2.3 out of 5, indicating substantial deficiencies in personalized safety assurance.\nüìÑ Download PDF\nReporting and Reviewing LLM-Integrated Systems in HCI: Challenges and Considerations Authors: Karla Felix Navarro, Eugene Syriani, Ian Arawjo Venue: arXiv (2026)\nWhat should HCI scholars consider when reporting and reviewing papers that involve LLM-integrated systems? We interview 18 authors of LLM-integrated system papers on their authoring and reviewing experiences. We find that norms of trust-building between authors and reviewers appear to be eroded by the uncertainty of LLM behavior and hyperbolic rhetoric surrounding AI. Authors perceive that reviewers apply uniquely skeptical and inconsistent standards towards papers that report LLM-integrated systems, and mitigate mistrust by adding technical evaluations, justifying usage, and de-emphasizing LLM presence. Authors‚Äô views challenge blanket directives to report all prompts and use open models, arguing that prompt reporting is context-dependent and justifying proprietary model usage despite ethical concerns. Finally, some tensions in peer review appear to stem from clashes between the norms and values of HCI and ML/NLP communities, particularly around what constitutes a contribution and an appropriate level of technical rigor. Based on our findings and additional feedback from six expert HCI researchers, we present a set of guidelines and considerations for authors, reviewers, and HCI communities around reporting and reviewing papers that involve LLM-integrated systems.\nüìÑ Download PDF\nOn Dual Connectivity in 6G Leo Constellations Authors: Achilles Machumilane, Alberto Gotta Venue: arXiv (2026)\nDual connectivity (DC) has garnered significant attention in 5G evolution, allowing for enhancing throughput and reliability by leveraging the channel conditions of two paths. However, when the paths exhibit different delays, such as in terrestrial and non-terrestrial integrated networks with multi-orbit topologies or in networks characterized by frequent topology changes, like Low Earth Orbit (LEO) satellite constellations with different elevation angles, traffic delivery may experience packet reordering or triggering congestion control mechanisms. Additionally, real-time traffic may experience packet drops if their arrival exceeds a play-out threshold. Different techniques have been proposed to address these issues, such as packet duplication, packet switching, and network coding for traffic scheduling in DC. However, if not accurately designed, these techniques can lead to resource waste, encoding/decoding delays, and computational overhead, undermining DC‚Äôs intended benefits. This paper provides a mathematical framework for calculating the average end-to-end packet loss in case of a loss process modeled with a Discrete Markov Chain - typical of a wireless channel - when combining packet duplication and packet switching or when network coding is employed in DC. Such metrics help derive optimal policies with full knowledge of the underlying loss process to be compared to empirical models learned through Machine Learning algorithms.\nüìÑ Download PDF\nHydrodynamic simulations of expanded warm dense foil heated by pulsed-power Authors: Luc Revello, Laurent Videau, Fr√©d√©ric Zucchini, Mathurin Lagr√©e, Christophe Blancard, Benjamin Jodar Venue: arXiv (2026)\nWarm Dense Matter lies at the frontier between condensed matter and plasma, and plays a central role in various fields ranging from planetary science to inertial confinement fusion. Improving our understanding of this regime requires experimental data that can be directly compared with theoretical and numerical models over a broad range of conditions. In this work, a pulsed-power experiment is described in which thin metallic foils, confined within a sapphire cell, are Joule-heated to achieve the expanded warm dense matter regime. Designing such an experiment is challenging, as it requires simultaneously predicting the electrical response of the pulsed-power driver and the hydrodynamic evolution of the heated material. To tackle this challenge, a modeling framework has been developed that couples an electrical description of the pulsed-power system, including the driver, the switching stages and the load with a one-dimensional hydrodynamic code. This coupling allows the electrical energy deposition and the load thermodynamic evolution to be consistently linked through the material electrical conductivity. This approach takes advantage of the simplicity of a 1D geometry while retaining the essential physics and allowing to reproduce various measurements with good accuracy, such as expansion velocity, current and voltage. This numerical approach therefore constitutes a robust and efficient method for designing and optimizing future Warm Dense Matter experiments using pulsed-power facilities.\nüìÑ Download PDF\ndziribot: rag based intelligent conversational agent for algerian arabic dialect Authors: El Batoul Bechiri, Dihia Lanasri Venue: arXiv (2026)\nThe rapid digitalization of customer service has intensified the demand for conversational agents capable of providing accurate and natural interactions. In the Algerian context, this is complicated by the linguistic complexity of Darja, a dialect characterized by non-standardized orthography, extensive code-switching with French, and the simultaneous use of Arabic and Latin (Arabizi) scripts. This paper introduces DziriBOT, a hybrid intelligent conversational agent specifically engineered to overcome these challenges. We propose a multi-layered architecture that integrates specialized Natural Language Understanding (NLU) with Retrieval-Augmented Generation (RAG), allowing for both structured service flows and dynamic, knowledge-intensive responses grounded in curated enterprise documentation. To address the low-resource nature of Darja, we systematically evaluate three distinct approaches: a sparse-feature Rasa pipeline, classical machine learning baselines, and transformer-based fine-tuning. Our experimental results demonstrate that the fine-tuned DziriBERT model achieves state-of-the-art performance. These results significantly outperform traditional baselines, particularly in handling orthographic noise and rare intents. Ultimately, DziriBOT provides a robust, scalable solution that bridges the gap between formal language models and the linguistic realities of Algerian users, offering a blueprint for dialect-aware automation in the regional market.\nüìÑ Download PDF\nAdaptive Quantum-Safe Cryptography for 6G Vehicular Networks via Context-Aware Optimization Authors: Poushali Sengupta, Mayank Raikwar, Sabita Maharjan, Frank Eliassen, Yan Zhang Venue: arXiv (2026)\nPowerful quantum computers in the future may be able to break the security used for communication between vehicles and other devices (Vehicle-to-Everything, or V2X). New security methods called post-quantum cryptography can help protect these systems, but they often require more computing power and can slow down communication, posing a challenge for fast 6G vehicle networks. In this paper, we propose an adaptive post-quantum cryptography (PQC) framework that predicts short-term mobility and channel variations and dynamically selects suitable lattice-, code-, or hash-based PQC configurations using a predictive multi-objective evolutionary algorithm (APMOEA) to meet vehicular latency and security constraints.However, frequent cryptographic reconfiguration in dynamic vehicular environments introduces new attack surfaces during algorithm transitions. A secure monotonic-upgrade protocol prevents downgrade, replay, and desynchronization attacks during transitions. Theoretical results show decision stability under bounded prediction error, latency boundedness under mobility drift, and correctness under small forecast noise. These results demonstrate a practical path toward quantum-safe cryptography in future 6G vehicular networks. Through extensive experiments based on realistic mobility (LuST), weather (ERA5), and NR-V2X channel traces, we show that the proposed framework reduces end-to-end latency by up to 27%, lowers communication overhead by up to 65%, and effectively stabilizes cryptographic switching behavior using reinforcement learning. Moreover, under the evaluated adversarial scenarios, the monotonic-upgrade protocol successfully prevents downgrade, replay, and desynchronization attacks.\nüìÑ Download PDF\nE-Globe: Scalable $Œµ$-Global Verification of Neural Networks via Tight Upper Bounds and Pattern-Aware Branching Authors: Wenting Li, Saif R. Kazi, Russell Bent, Duo Zhou, Huan Zhang Venue: arXiv (2026)\nNeural networks achieve strong empirical performance, but robustness concerns still hinder deployment in safety-critical applications. Formal verification provides robustness guarantees, but current methods face a scalability-completeness trade-off. We propose a hybrid verifier in a branch-and-bound (BaB) framework that efficiently tightens both upper and lower bounds until an $Œµ-$global optimum is reached or early stop is triggered. The key is an exact nonlinear program with complementarity constraints (NLP-CC) for upper bounding that preserves the ReLU input-output graph, so any feasible solution yields a valid counterexample and enables rapid pruning of unsafe subproblems. We further accelerate verification with (i) warm-started NLP solves requiring minimal constraint-matrix updates and (ii) pattern-aligned strong branching that prioritizes splits most effective at tightening relaxations. We also provide conditions under which NLP-CC upper bounds are tight. Experiments on MNIST and CIFAR-10 show markedly tighter upper bounds than PGD across perturbation radii spanning up to three orders of magnitude, fast per-node solves in practice, and substantial end-to-end speedups over MIP-based verification, amplified by warm-starting, GPU batching, and pattern-aligned branching.\nüìÑ Download PDF\nShared LoRA Subspaces for almost Strict Continual Learning Authors: Prakhar Kaushik, Ankit Vaidya, Shravan Chaudhari, Rama Chellappa, Alan Yuille Venue: arXiv (2026)\nAdapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.\nüìÑ Download PDF\nPredicting Camera Pose from Perspective Descriptions for Spatial Reasoning Authors: Xuejun Zhang, Aditi Tiwari, Zhenhailong Wang, Heng Ji Venue: arXiv (2026)\nMulti-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20¬∞ and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.\nüìÑ Download PDF\nDyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching Authors: Yuxing Lu, Yucheng Hu, Xukai Zhao, Jiuxin Cao Venue: arXiv (2026)\nMulti-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager‚Äôs round goal, each agent outputs lightweight natural-language query (need) and \\key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.\nüìÑ Download PDF\nSwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs Authors: Jintao Tong, Shilin Yan, Hongwei Xue, Xiaojun Tang, Kunyu Shi, Guannan Zhang, Ruixuan Li, Yixiong Zou Venue: arXiv (2026)\nMultimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as ‚Äúvisual thoughts‚Äù into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.\nüìÑ Download PDF\nCommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction Authors: Xiaopan Zhang, Zejin Wang, Zhixu Li, Jianpeng Yao, Jiachen Li Venue: arXiv (2026)\nTo complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.\nüìÑ Download PDF\nThinking with Geometry: Active Geometry Integration for Spatial Reasoning Authors: Haoyuan Li, Qihang Cao, Tao Tang, Kun Xiang, Zihan Guo, Jianhua Han, Hang Xu, Xiaodan Liang Venue: arXiv (2026)\nRecent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.\nüìÑ Download PDF\nCorrectness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering Authors: Miranda Muqing Miao, Young-Min Cho, Lyle Ungar Venue: arXiv (2026)\nLarge language models (LLMs) exhibit persistent miscalibration, especially after instruction tuning and preference alignment. Modified training objectives can improve calibration, but retraining is expensive. Inference-time steering offers a lightweight alternative, yet most existing methods optimize proxies for correctness rather than correctness itself. We introduce CORAL (Correctness-Optimized Residual Activation Lens), a regularized inference-time steering method that captures distributed correctness signals from model internal activations using weight-decay MLP probes. We evaluate CORAL across three 7B-parameter models and find that it consistently improves accuracy by 10% and expected calibration error (ECE) by 50% on average. We additionally demonstrate that these gains transfer without retraining to the complete published test sets of four held-out benchmarks (ARC-Challenge, HellaSwag, Math-MC, OpenBookQA), averaging 14% accuracy improvements and 49% ECE improvements. Our results support the hypothesis that distributed information in model internals can be extracted using regularized probes when individual neurons are insufficient. CORAL thus provides a compute-efficient, transferable, and calibration-aware approach to improve MCQA performance during inference.\nüìÑ Download PDF\nModeling integrated frequency shifters and beam splitters Authors: Manuel H. Mu√±oz-Arias, Kevin J. Randles, Nils T. Otterstrom, Paul S. Davids, Michael Gehl, Mohan Sarovar Venue: arXiv (2026)\nPhotonic quantum computing is a strong contender in the race to fault-tolerance. Recent proposals using qubits encoded in frequency modes promise a large reduction in hardware footprint, and have garnered much attention. In this encoding, linear optics, i.e., beam splitters and phase shifters, is necessarily not energy-conserving, and is costly to implement. In this work, we present designs of frequency-mode beam splitters based on modulated arrays of coupled resonators. We develop a methodology to construct their effective transfer matrices based on the SLH formalism for quantum input-output networks. Our methodology is flexible and highly composable, allowing us to define $N$-mode beam splitters either natively based on arrays of $N$-resonators of arbitrary connectivity or as networks of interconnected $l$-mode beam splitters, with $l","wordCount":"22771","inLanguage":"zh","datePublished":"2026-02-08T15:31:07.463236Z","dateModified":"2026-02-08T15:31:07.463236Z","author":{"@type":"Person","name":"Gary"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://garyforreal.me/zh/posts/paper/paper-2026-02-08-weekly/"},"publisher":{"@type":"Organization","name":"Gary's House","logo":{"@type":"ImageObject","url":"https://garyforreal.me/img/Q.jpg"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><script src=https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://garyforreal.me/zh/ accesskey=h title="Gary's Blog (Alt + H)"><img src=https://garyforreal.me/img/me.jpg alt aria-label=logo height=35>Gary's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://garyforreal.me/en/ title=English aria-label=English>English</a></li></ul></div></div><ul id=menu><li><a href=https://garyforreal.me/zh/search title="üîçÊêúÁ¥¢ (Alt + /)" accesskey=/><span>üîçÊêúÁ¥¢</span></a></li><li><a href=https://garyforreal.me/zh/ title=üè†‰∏ªÈ°µ><span>üè†‰∏ªÈ°µ</span></a></li><li><a href=https://garyforreal.me/zh/posts/ title=üìöÊñáÁ´†><span>üìöÊñáÁ´†</span></a></li><li><a href=https://garyforreal.me/zh/archives/ title=‚è±Â≠òÊ°£><span>‚è±Â≠òÊ°£</span></a></li><li><a href=https://garyforreal.me/zh/music/ title=üéµÈü≥‰πê><span>üéµÈü≥‰πê</span></a></li><li><a href=https://garyforreal.me/zh/about title=üôãüèª‚Äç‚ôÇÔ∏èÂÖ≥‰∫é><span>üôãüèª‚Äç‚ôÇÔ∏èÂÖ≥‰∫é</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://garyforreal.me/zh/>‰∏ªÈ°µ</a>&nbsp;¬ª&nbsp;<a href=https://garyforreal.me/zh/posts/>Â∏ñÂ≠ê</a>&nbsp;¬ª&nbsp;<a href=https://garyforreal.me/zh/posts/paper/>ËÆ∫Êñá</a></div><h1 class="post-title entry-hint-parent">Weekly Paper Notes - 2026-02-08</h1><div class=post-meta><span title='2026-02-08 15:31:07.463236 +0000 UTC'>2026-02-08</span>&nbsp;¬∑&nbsp;107 ÂàÜÈíü&nbsp;¬∑&nbsp;107 ÂàÜÈíü&nbsp;¬∑&nbsp;Gary&nbsp;|&nbsp;ËØ≠Ë®Ä:<ul class=i18n_list><li><a href=https://garyforreal.me/en/posts/paper/paper-2026-02-08-weekly/>English</a></li></ul><div class=meta-item>&nbsp¬∑&nbsp
        <span id=busuanzi_container_page_pv>Êú¨ÊñáÈòÖËØªÈáè<span id=busuanzi_value_page_pv></span>Ê¨°</span></div></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>ÁõÆÂΩï</span></summary><div class=inner><ul><li><a href=#weekly-paper-notes aria-label="Weekly Paper Notes">Weekly Paper Notes</a><ul><li><a href=#-multilingual aria-label="üîç multilingual">üîç multilingual</a><ul><li><a href=#self-improving-multilingual-long-reasoning-via-translation-reasoning-integrated-traininghttpsarxivorgabs260205940v1 aria-label="Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training"><a href=https://arxiv.org/abs/2602.05940v1>Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training</a></a></li><li><a href=#polyglots-or-multitudes-multilingual-llm-answers-to-value-laden-multiple-choice-questionshttpsarxivorgabs260205932v1 aria-label="Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions"><a href=https://arxiv.org/abs/2602.05932v1>Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions</a></a></li><li><a href=#dr-kernel-reinforcement-learning-done-right-for-triton-kernel-generationshttpsarxivorgabs260205885v1 aria-label="Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations"><a href=https://arxiv.org/abs/2602.05885v1>Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations</a></a></li><li><a href=#eurollm-22b-technical-reporthttpsarxivorgabs260205879v1 aria-label="EuroLLM-22B: Technical Report"><a href=https://arxiv.org/abs/2602.05879v1>EuroLLM-22B: Technical Report</a></a></li><li><a href=#whispers-of-the-butterfly-a-research-through-design-exploration-of-in-situ-conversational-ai-guidance-in-large-scale-outdoor-mr-exhibitionshttpsarxivorgabs260205826v1 aria-label="Whispers of the Butterfly: A Research-through-Design Exploration of In-Situ Conversational AI Guidance in Large-Scale Outdoor MR Exhibitions"><a href=https://arxiv.org/abs/2602.05826v1>Whispers of the Butterfly: A Research-through-Design Exploration of In-Situ Conversational AI Guidance in Large-Scale Outdoor MR Exhibitions</a></a></li><li><a href=#fimi-a-domain-specific-language-model-for-indian-finance-ecosystemhttpsarxivorgabs260205794v1 aria-label="FiMI: A Domain-Specific Language Model for Indian Finance Ecosystem"><a href=https://arxiv.org/abs/2602.05794v1>FiMI: A Domain-Specific Language Model for Indian Finance Ecosystem</a></a></li><li><a href=#bhashasetu-cross-lingual-knowledge-transfer-from-high-resource-to-extreme-low-resource-languageshttpsarxivorgabs260205599v1 aria-label="BhashaSetu: Cross-Lingual Knowledge Transfer from High-Resource to Extreme Low-Resource Languages"><a href=https://arxiv.org/abs/2602.05599v1>BhashaSetu: Cross-Lingual Knowledge Transfer from High-Resource to Extreme Low-Resource Languages</a></a></li><li><a href=#cross-lingual-empirical-evaluation-of-large-language-models-for-arabic-medical-taskshttpsarxivorgabs260205374v1 aria-label="Cross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks"><a href=https://arxiv.org/abs/2602.05374v1>Cross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks</a></a></li><li><a href=#multilingual-extraction-and-recognition-of-implicit-discourse-relations-in-speech-and-texthttpsarxivorgabs260205107v1 aria-label="Multilingual Extraction and Recognition of Implicit Discourse Relations in Speech and Text"><a href=https://arxiv.org/abs/2602.05107v1>Multilingual Extraction and Recognition of Implicit Discourse Relations in Speech and Text</a></a></li><li><a href=#be-my-cheese-cultural-nuance-benchmarking-for-machine-translation-in-multilingual-llmshttpsarxivorgabs260204729v1 aria-label="&ldquo;Be My Cheese?&rdquo;: Cultural Nuance Benchmarking for Machine Translation in Multilingual LLMs"><a href=https://arxiv.org/abs/2602.04729v1>&ldquo;Be My Cheese?&rdquo;: Cultural Nuance Benchmarking for Machine Translation in Multilingual LLMs</a></a></li><li><a href=#tokenization-and-morphological-fidelity-in-uralic-nlp-a-cross-lingual-evaluationhttpsarxivorgabs260204241v1 aria-label="Tokenization and Morphological Fidelity in Uralic NLP: A Cross-Lingual Evaluation"><a href=https://arxiv.org/abs/2602.04241v1>Tokenization and Morphological Fidelity in Uralic NLP: A Cross-Lingual Evaluation</a></a></li><li><a href=#bagging-based-model-merging-for-robust-general-text-embeddingshttpsarxivorgabs260205787v1 aria-label="Bagging-Based Model Merging for Robust General Text Embeddings"><a href=https://arxiv.org/abs/2602.05787v1>Bagging-Based Model Merging for Robust General Text Embeddings</a></a></li><li><a href=#evaluating-the-impact-of-word-embeddings-on-similarity-scoring-in-practical-information-retrievalhttpsarxivorgabs260205734v1 aria-label="Evaluating the impact of word embeddings on similarity scoring in practical information retrieval"><a href=https://arxiv.org/abs/2602.05734v1>Evaluating the impact of word embeddings on similarity scoring in practical information retrieval</a></a></li><li><a href=#mederrbench-a-fine-grained-multilingual-benchmark-for-medical-error-detection-and-correction-with-clinical-expert-annotationshttpsarxivorgabs260205692v1 aria-label="MedErrBench: A Fine-Grained Multilingual Benchmark for Medical Error Detection and Correction with Clinical Expert Annotations"><a href=https://arxiv.org/abs/2602.05692v1>MedErrBench: A Fine-Grained Multilingual Benchmark for Medical Error Detection and Correction with Clinical Expert Annotations</a></a></li><li><a href=#castle-a-comprehensive-benchmark-for-evaluating-student-tailored-personalized-safety-in-large-language-modelshttpsarxivorgabs260205633v1 aria-label="CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models"><a href=https://arxiv.org/abs/2602.05633v1>CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models</a></a></li><li><a href=#reporting-and-reviewing-llm-integrated-systems-in-hci-challenges-and-considerationshttpsarxivorgabs260205128v1 aria-label="Reporting and Reviewing LLM-Integrated Systems in HCI: Challenges and Considerations"><a href=https://arxiv.org/abs/2602.05128v1>Reporting and Reviewing LLM-Integrated Systems in HCI: Challenges and Considerations</a></a></li><li><a href=#on-dual-connectivity-in-6g-leo-constellationshttpsarxivorgabs260204825v1 aria-label="On Dual Connectivity in 6G Leo Constellations"><a href=https://arxiv.org/abs/2602.04825v1>On Dual Connectivity in 6G Leo Constellations</a></a></li><li><a href=#hydrodynamic-simulations-of-expanded-warm-dense-foil-heated-by-pulsed-powerhttpsarxivorgabs260203458v2 aria-label="Hydrodynamic simulations of expanded warm dense foil heated by pulsed-power"><a href=https://arxiv.org/abs/2602.03458v2>Hydrodynamic simulations of expanded warm dense foil heated by pulsed-power</a></a></li><li><a href=#dziribot-rag-based-intelligent-conversational-agent-for-algerian-arabic-dialecthttpsarxivorgabs260202270v1 aria-label="dziribot: rag based intelligent conversational agent for algerian arabic dialect"><a href=https://arxiv.org/abs/2602.02270v1>dziribot: rag based intelligent conversational agent for algerian arabic dialect</a></a></li><li><a href=#adaptive-quantum-safe-cryptography-for-6g-vehicular-networks-via-context-aware-optimizationhttpsarxivorgabs260201342v1 aria-label="Adaptive Quantum-Safe Cryptography for 6G Vehicular Networks via Context-Aware Optimization"><a href=https://arxiv.org/abs/2602.01342v1>Adaptive Quantum-Safe Cryptography for 6G Vehicular Networks via Context-Aware Optimization</a></a></li><li><a href=#e-globe-scalable-%ce%b5-global-verification-of-neural-networks-via-tight-upper-bounds-and-pattern-aware-branchinghttpsarxivorgabs260205068v1 aria-label="E-Globe: Scalable $Œµ$-Global Verification of Neural Networks via Tight Upper Bounds and Pattern-Aware Branching"><a href=https://arxiv.org/abs/2602.05068v1>E-Globe: Scalable $Œµ$-Global Verification of Neural Networks via Tight Upper Bounds and Pattern-Aware Branching</a></a></li><li><a href=#shared-lora-subspaces-for-almost-strict-continual-learninghttpsarxivorgabs260206043v1 aria-label="Shared LoRA Subspaces for almost Strict Continual Learning"><a href=https://arxiv.org/abs/2602.06043v1>Shared LoRA Subspaces for almost Strict Continual Learning</a></a></li><li><a href=#predicting-camera-pose-from-perspective-descriptions-for-spatial-reasoninghttpsarxivorgabs260206041v1 aria-label="Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning"><a href=https://arxiv.org/abs/2602.06041v1>Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning</a></a></li><li><a href=#dytopo-dynamic-topology-routing-for-multi-agent-reasoning-via-semantic-matchinghttpsarxivorgabs260206039v1 aria-label="DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching"><a href=https://arxiv.org/abs/2602.06039v1>DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching</a></a></li><li><a href=#swimbird-eliciting-switchable-reasoning-mode-in-hybrid-autoregressive-mllmshttpsarxivorgabs260206040v1 aria-label="SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs"><a href=https://arxiv.org/abs/2602.06040v1>SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs</a></a></li><li><a href=#commcp-efficient-multi-agent-coordination-via-llm-based-communication-with-conformal-predictionhttpsarxivorgabs260206038v1 aria-label="CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction"><a href=https://arxiv.org/abs/2602.06038v1>CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction</a></a></li><li><a href=#thinking-with-geometry-active-geometry-integration-for-spatial-reasoninghttpsarxivorgabs260206037v1 aria-label="Thinking with Geometry: Active Geometry Integration for Spatial Reasoning"><a href=https://arxiv.org/abs/2602.06037v1>Thinking with Geometry: Active Geometry Integration for Spatial Reasoning</a></a></li><li><a href=#correctness-optimized-residual-activation-lens-coral-transferrable-and-calibration-aware-inference-time-steeringhttpsarxivorgabs260206022v1 aria-label="Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering"><a href=https://arxiv.org/abs/2602.06022v1>Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering</a></a></li><li><a href=#modeling-integrated-frequency-shifters-and-beam-splittershttpsarxivorgabs260206003v1 aria-label="Modeling integrated frequency shifters and beam splitters"><a href=https://arxiv.org/abs/2602.06003v1>Modeling integrated frequency shifters and beam splitters</a></a></li><li><a href=#giant-bubbles-of-fisher-zeros-in-the-quantum-xy-chainhttpsarxivorgabs260205899v1 aria-label="Giant bubbles of Fisher zeros in the quantum XY chain"><a href=https://arxiv.org/abs/2602.05899v1>Giant bubbles of Fisher zeros in the quantum XY chain</a></a></li><li><a href=#ui-mem-self-evolving-experience-memory-for-online-reinforcement-learning-in-mobile-gui-agentshttpsarxivorgabs260205832v1 aria-label="UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents"><a href=https://arxiv.org/abs/2602.05832v1>UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents</a></a></li><li><a href=#v-retrver-evidence-driven-agentic-reasoning-for-universal-multimodal-retrievalhttpsarxivorgabs260206034v1 aria-label="V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval"><a href=https://arxiv.org/abs/2602.06034v1>V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval</a></a></li><li><a href=#physicsagentabm-physics-guided-generative-agent-based-modelinghttpsarxivorgabs260206030v1 aria-label="PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling"><a href=https://arxiv.org/abs/2602.06030v1>PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling</a></a></li><li><a href=#curiosity-is-knowledge-self-consistent-learning-and-no-regret-optimization-with-active-inferencehttpsarxivorgabs260206029v1 aria-label="Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference"><a href=https://arxiv.org/abs/2602.06029v1>Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference</a></a></li><li><a href=#diffusion-models-generalization-can-be-characterized-by-inductive-biases-toward-a-data-dependent-ridge-manifoldhttpsarxivorgabs260206021v1 aria-label="Diffusion Model&rsquo;s Generalization Can Be Characterized by Inductive Biases toward a Data-Dependent Ridge Manifold"><a href=https://arxiv.org/abs/2602.06021v1>Diffusion Model&rsquo;s Generalization Can Be Characterized by Inductive Biases toward a Data-Dependent Ridge Manifold</a></a></li><li><a href=#mambavf-state-space-model-for-efficient-video-fusionhttpsarxivorgabs260206017v1 aria-label="MambaVF: State Space Model for Efficient Video Fusion"><a href=https://arxiv.org/abs/2602.06017v1>MambaVF: State Space Model for Efficient Video Fusion</a></a></li><li><a href=#sage-benchmarking-and-improving-retrieval-for-deep-research-agentshttpsarxivorgabs260205975v1 aria-label="SAGE: Benchmarking and Improving Retrieval for Deep Research Agents"><a href=https://arxiv.org/abs/2602.05975v1>SAGE: Benchmarking and Improving Retrieval for Deep Research Agents</a></a></li><li><a href=#characterizing-human-semantic-navigation-in-concept-production-as-trajectories-in-embedding-spacehttpsarxivorgabs260205971v1 aria-label="Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space"><a href=https://arxiv.org/abs/2602.05971v1>Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space</a></a></li><li><a href=#agentictagger-structured-item-representation-for-recommendation-with-llm-agentshttpsarxivorgabs260205945v1 aria-label="AgenticTagger: Structured Item Representation for Recommendation with LLM Agents"><a href=https://arxiv.org/abs/2602.05945v1>AgenticTagger: Structured Item Representation for Recommendation with LLM Agents</a></a></li></ul></li><li><a href=#-linguistics aria-label="üîç linguistics">üîç linguistics</a><ul><li><a href=#pseudo-invertible-neural-networkshttpsarxivorgabs260206042v1 aria-label="Pseudo-Invertible Neural Networks"><a href=https://arxiv.org/abs/2602.06042v1>Pseudo-Invertible Neural Networks</a></a></li><li><a href=#context-forcing-consistent-autoregressive-video-generation-with-long-contexthttpsarxivorgabs260206028v1 aria-label="Context Forcing: Consistent Autoregressive Video Generation with Long Context"><a href=https://arxiv.org/abs/2602.06028v1>Context Forcing: Consistent Autoregressive Video Generation with Long Context</a></a></li><li><a href=#act-dr6planck-impact-on-inflation-with-non-zero-vacuum-expectation-value-and-the-post-inflationary-behaviorhttpsarxivorgabs260206027v1 aria-label="ACT DR6+Planck impact on inflation with non-zero vacuum expectation value and the post-inflationary behavior"><a href=https://arxiv.org/abs/2602.06027v1>ACT DR6+Planck impact on inflation with non-zero vacuum expectation value and the post-inflationary behavior</a></a></li><li><a href=#learning-query-aware-budget-tier-routing-for-runtime-agent-memoryhttpsarxivorgabs260206025v1 aria-label="Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory"><a href=https://arxiv.org/abs/2602.06025v1>Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory</a></a></li><li><a href=#mechanisms-of-ai-protein-folding-in-esmfoldhttpsarxivorgabs260206020v1 aria-label="Mechanisms of AI Protein Folding in ESMFold"><a href=https://arxiv.org/abs/2602.06020v1>Mechanisms of AI Protein Folding in ESMFold</a></a></li><li><a href=#convex-unions-and-completions-from-simplicial-pseudomanifoldshttpsarxivorgabs260206016v1 aria-label="Convex unions and completions from simplicial pseudomanifolds"><a href=https://arxiv.org/abs/2602.06016v1>Convex unions and completions from simplicial pseudomanifolds</a></a></li><li><a href=#agenticpay-a-multi-agent-llm-negotiation-system-for-buyer-seller-transactionshttpsarxivorgabs260206008v1 aria-label="AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions"><a href=https://arxiv.org/abs/2602.06008v1>AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions</a></a></li><li><a href=#quantum-noise-scaling-in-continuously-operating-multiparameter-sensorshttpsarxivorgabs260205991v1 aria-label="Quantum noise scaling in continuously operating multiparameter sensors"><a href=https://arxiv.org/abs/2602.05991v1>Quantum noise scaling in continuously operating multiparameter sensors</a></a></li><li><a href=#geometry-and-dynamical-morphology-of-growing-bacterial-colonieshttpsarxivorgabs260205958v1 aria-label="Geometry and dynamical morphology of growing bacterial colonies"><a href=https://arxiv.org/abs/2602.05958v1>Geometry and dynamical morphology of growing bacterial colonies</a></a></li><li><a href=#a-powheg-generator-for-di-jet-production-in-polarized-proton-proton-collisionshttpsarxivorgabs260205949v1 aria-label="A POWHEG generator for di-jet production in polarized proton-proton collisions"><a href=https://arxiv.org/abs/2602.05949v1>A POWHEG generator for di-jet production in polarized proton-proton collisions</a></a></li><li><a href=#interprior-scaling-generative-control-for-physics-based-human-object-interactionshttpsarxivorgabs260206035v1 aria-label="InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions"><a href=https://arxiv.org/abs/2602.06035v1>InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions</a></a></li><li><a href=#can-vision-language-models-learn-intuitive-physics-from-interactionhttpsarxivorgabs260206033v1 aria-label="Can vision language models learn intuitive physics from interaction?"><a href=https://arxiv.org/abs/2602.06033v1>Can vision language models learn intuitive physics from interaction?</a></a></li><li><a href=#splat-and-distill-augmenting-teachers-with-feed-forward-3d-reconstruction-for-3d-aware-distillationhttpsarxivorgabs260206032v1 aria-label="Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation"><a href=https://arxiv.org/abs/2602.06032v1>Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation</a></a></li><li><a href=#a-systematic-evaluation-of-large-language-models-for-ptsd-severity-estimation-the-role-of-contextual-knowledge-and-modeling-strategieshttpsarxivorgabs260206015v1 aria-label="A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies"><a href=https://arxiv.org/abs/2602.06015v1>A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies</a></a></li><li><a href=#lattices-from-pointed-building-sets-generalized-ornamentation-latticeshttpsarxivorgabs260206004v1 aria-label="Lattices from Pointed Building Sets: Generalized Ornamentation Lattices"><a href=https://arxiv.org/abs/2602.06004v1>Lattices from Pointed Building Sets: Generalized Ornamentation Lattices</a></a></li><li><a href=#clouding-the-mirror-stealthy-prompt-injection-attacks-targeting-llm-based-phishing-detectionhttpsarxivorgabs260205484v1 aria-label="Clouding the Mirror: Stealthy Prompt Injection Attacks Targeting LLM-based Phishing Detection"><a href=https://arxiv.org/abs/2602.05484v1>Clouding the Mirror: Stealthy Prompt Injection Attacks Targeting LLM-based Phishing Detection</a></a></li><li><a href=#beyond-length-context-aware-expansion-and-independence-as-developmentally-sensitive-evaluation-in-child-utteranceshttpsarxivorgabs260205392v1 aria-label="Beyond Length: Context-Aware Expansion and Independence as Developmentally Sensitive Evaluation in Child Utterances"><a href=https://arxiv.org/abs/2602.05392v1>Beyond Length: Context-Aware Expansion and Independence as Developmentally Sensitive Evaluation in Child Utterances</a></a></li><li><a href=#dolphin-v2-universal-document-parsing-via-scalable-anchor-promptinghttpsarxivorgabs260205384v1 aria-label="Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting"><a href=https://arxiv.org/abs/2602.05384v1>Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting</a></a></li><li><a href=#deepread-document-structure-aware-reasoning-to-enhance-agentic-searchhttpsarxivorgabs260205014v1 aria-label="DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search"><a href=https://arxiv.org/abs/2602.05014v1>DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search</a></a></li><li><a href=#exploiting-contextual-information-to-improve-stance-detection-in-informal-political-discourse-with-llmshttpsarxivorgabs260204750v1 aria-label="Exploiting contextual information to improve stance detection in informal political discourse with LLMs"><a href=https://arxiv.org/abs/2602.04750v1>Exploiting contextual information to improve stance detection in informal political discourse with LLMs</a></a></li><li><a href=#rise-video-can-video-generators-decode-implicit-world-ruleshttpsarxivorgabs260205986v1 aria-label="RISE-Video: Can Video Generators Decode Implicit World Rules?"><a href=https://arxiv.org/abs/2602.05986v1>RISE-Video: Can Video Generators Decode Implicit World Rules?</a></a></li><li><a href=#does-cosmology-require-hermiticity-in-quantum-mechanicshttpsarxivorgabs260205973v1 aria-label="Does Cosmology require Hermiticity in Quantum Mechanics?"><a href=https://arxiv.org/abs/2602.05973v1>Does Cosmology require Hermiticity in Quantum Mechanics?</a></a></li><li><a href=#transformers-are-born-biased-structural-inductive-biases-at-random-initialization-and-their-practical-consequenceshttpsarxivorgabs260205927v1 aria-label="Transformers Are Born Biased: Structural Inductive Biases at Random Initialization and Their Practical Consequences"><a href=https://arxiv.org/abs/2602.05927v1>Transformers Are Born Biased: Structural Inductive Biases at Random Initialization and Their Practical Consequences</a></a></li><li><a href=#on-particle-dynamics-in-steady-axial-rotor-flowshttpsarxivorgabs260205872v1 aria-label="On particle dynamics in steady axial rotor flows"><a href=https://arxiv.org/abs/2602.05872v1>On particle dynamics in steady axial rotor flows</a></a></li><li><a href=#odysseyarena-benchmarking-large-language-models-for-long-horizon-active-and-inductive-interactionshttpsarxivorgabs260205843v1 aria-label="OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions"><a href=https://arxiv.org/abs/2602.05843v1>OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions</a></a></li><li><a href=#mining-generalizable-activation-functionshttpsarxivorgabs260205688v1 aria-label="Mining Generalizable Activation Functions"><a href=https://arxiv.org/abs/2602.05688v1>Mining Generalizable Activation Functions</a></a></li><li><a href=#generative-ontology-when-structured-knowledge-learns-to-createhttpsarxivorgabs260205636v1 aria-label="Generative Ontology: When Structured Knowledge Learns to Create"><a href=https://arxiv.org/abs/2602.05636v1>Generative Ontology: When Structured Knowledge Learns to Create</a></a></li></ul></li><li><a href=#-psycholinguistics aria-label="üîç psycholinguistics">üîç psycholinguistics</a><ul><li><a href=#chunky-post-training-data-driven-failures-of-generalizationhttpsarxivorgabs260205910v1 aria-label="Chunky Post-Training: Data Driven Failures of Generalization"><a href=https://arxiv.org/abs/2602.05910v1>Chunky Post-Training: Data Driven Failures of Generalization</a></a></li><li><a href=#visualizing-the-loss-landscapes-of-physics-informed-neural-networkshttpsarxivorgabs260205849v1 aria-label="Visualizing the loss landscapes of physics-informed neural networks"><a href=https://arxiv.org/abs/2602.05849v1>Visualizing the loss landscapes of physics-informed neural networks</a></a></li><li><a href=#hidden-simplicity-in-ads-spinning-mellin-amplitudes-via-scaffoldinghttpsarxivorgabs260205568v1 aria-label="Hidden simplicity in AdS spinning Mellin amplitudes via scaffolding"><a href=https://arxiv.org/abs/2602.05568v1>Hidden simplicity in AdS spinning Mellin amplitudes via scaffolding</a></a></li><li><a href=#data-centric-interpretability-for-llm-based-multi-agent-reinforcement-learninghttpsarxivorgabs260205183v1 aria-label="Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning"><a href=https://arxiv.org/abs/2602.05183v1>Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning</a></a></li><li><a href=#a-surprising-discrepancy-in-the-regularity-of-conjugacies-between-generalized-interval-exchange-transformations-and-their-inverses-at-freezinghttpsarxivorgabs260204993v1 aria-label="A surprising discrepancy in the regularity of conjugacies between generalized interval exchange transformations and their inverses at freezing"><a href=https://arxiv.org/abs/2602.04993v1>A surprising discrepancy in the regularity of conjugacies between generalized interval exchange transformations and their inverses at freezing</a></a></li><li><a href=#artificial-intelligence-as-strange-intelligence-against-linear-models-of-intelligencehttpsarxivorgabs260204986v1 aria-label="Artificial Intelligence as Strange Intelligence: Against Linear Models of Intelligence"><a href=https://arxiv.org/abs/2602.04986v1>Artificial Intelligence as Strange Intelligence: Against Linear Models of Intelligence</a></a></li><li><a href=#dflash-block-diffusion-for-flash-speculative-decodinghttpsarxivorgabs260206036v1 aria-label="DFlash: Block Diffusion for Flash Speculative Decoding"><a href=https://arxiv.org/abs/2602.06036v1>DFlash: Block Diffusion for Flash Speculative Decoding</a></a></li><li><a href=#learning-to-share-selective-memory-for-efficient-parallel-agentic-systemshttpsarxivorgabs260205965v1 aria-label="Learning to Share: Selective Memory for Efficient Parallel Agentic Systems"><a href=https://arxiv.org/abs/2602.05965v1>Learning to Share: Selective Memory for Efficient Parallel Agentic Systems</a></a></li><li><a href=#photonic-neuromorphic-processing-with-coupled-spiking-silicon-microringshttpsarxivorgabs260205918v1 aria-label="Photonic neuromorphic processing with coupled spiking silicon microrings"><a href=https://arxiv.org/abs/2602.05918v1>Photonic neuromorphic processing with coupled spiking silicon microrings</a></a></li><li><a href=#regularized-calibration-with-successive-rounding-for-post-training-quantizationhttpsarxivorgabs260205902v1 aria-label="Regularized Calibration with Successive Rounding for Post-Training Quantization"><a href=https://arxiv.org/abs/2602.05902v1>Regularized Calibration with Successive Rounding for Post-Training Quantization</a></a></li><li><a href=#fhaim-fully-homomorphic-aim-for-private-synthetic-data-generationhttpsarxivorgabs260205838v1 aria-label="FHAIM: Fully Homomorphic AIM For Private Synthetic Data Generation"><a href=https://arxiv.org/abs/2602.05838v1>FHAIM: Fully Homomorphic AIM For Private Synthetic Data Generation</a></a></li><li><a href=#large-data-acquisition-and-analytics-at-synchrotron-radiation-facilitieshttpsarxivorgabs260205837v1 aria-label="Large Data Acquisition and Analytics at Synchrotron Radiation Facilities"><a href=https://arxiv.org/abs/2602.05837v1>Large Data Acquisition and Analytics at Synchrotron Radiation Facilities</a></a></li><li><a href=#sparcd-a-spectral-graph-framework-for-revealing-differential-functional-connectivity-in-fmri-datahttpsarxivorgabs260205807v1 aria-label="SpARCD: A Spectral Graph Framework for Revealing Differential Functional Connectivity in fMRI Data"><a href=https://arxiv.org/abs/2602.05807v1>SpARCD: A Spectral Graph Framework for Revealing Differential Functional Connectivity in fMRI Data</a></a></li><li><a href=#from-vision-to-decision-neuromorphic-control-for-autonomous-navigation-and-trackinghttpsarxivorgabs260205683v1 aria-label="From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking"><a href=https://arxiv.org/abs/2602.05683v1>From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking</a></a></li><li><a href=#a-hybrid-data-driven-algorithm-for-real-time-friction-force-estimation-in-hydraulic-cylindershttpsarxivorgabs260205967v1 aria-label="A Hybrid Data-Driven Algorithm for Real-Time Friction Force Estimation in Hydraulic Cylinders"><a href=https://arxiv.org/abs/2602.05967v1>A Hybrid Data-Driven Algorithm for Real-Time Friction Force Estimation in Hydraulic Cylinders</a></a></li><li><a href=#prompting-destiny-negotiating-socialization-and-growth-in-an-llm-mediated-speculative-gameworldhttpsarxivorgabs260205864v1 aria-label="Prompting Destiny: Negotiating Socialization and Growth in an LLM-Mediated Speculative Gameworld"><a href=https://arxiv.org/abs/2602.05864v1>Prompting Destiny: Negotiating Socialization and Growth in an LLM-Mediated Speculative Gameworld</a></a></li></ul></li><li><a href=#-llm aria-label="üîç llm">üîç llm</a><ul><li><a href=#multi-token-prediction-via-self-distillationhttpsarxivorgabs260206019v1 aria-label="Multi-Token Prediction via Self-Distillation"><a href=https://arxiv.org/abs/2602.06019v1>Multi-Token Prediction via Self-Distillation</a></a></li><li><a href=#genarena-how-can-we-achieve-human-aligned-evaluation-for-visual-generation-taskshttpsarxivorgabs260206013v1 aria-label="GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?"><a href=https://arxiv.org/abs/2602.06013v1>GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?</a></a></li><li><a href=#orthogonal-self-attentionhttpsarxivorgabs260205996v1 aria-label="Orthogonal Self-Attention"><a href=https://arxiv.org/abs/2602.05996v1>Orthogonal Self-Attention</a></a></li><li><a href=#ap-ood-attention-pooling-for-out-of-distribution-detectionhttpsarxivorgabs260206031v1 aria-label="AP-OOD: Attention Pooling for Out-of-Distribution Detection"><a href=https://arxiv.org/abs/2602.06031v1>AP-OOD: Attention Pooling for Out-of-Distribution Detection</a></a></li><li><a href=#breaking-symmetry-bottlenecks-in-gnn-readoutshttpsarxivorgabs260205950v1 aria-label="Breaking Symmetry Bottlenecks in GNN Readouts"><a href=https://arxiv.org/abs/2602.05950v1>Breaking Symmetry Bottlenecks in GNN Readouts</a></a></li></ul></li><li><a href=#-neuroscience aria-label="üîç neuroscience">üîç neuroscience</a><ul><li><a href=#visrefiner-learning-from-visual-differences-for-screenshot-to-code-generationhttpsarxivorgabs260205998v1 aria-label="VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation"><a href=https://arxiv.org/abs/2602.05998v1>VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation</a></a></li><li><a href=#the-effects-of-non-ideal-mixing-in-planetary-magma-oceans-and-atmosphereshttpsarxivorgabs260205917v1 aria-label="The Effects of Non-ideal Mixing in Planetary Magma Oceans and Atmospheres"><a href=https://arxiv.org/abs/2602.05917v1>The Effects of Non-ideal Mixing in Planetary Magma Oceans and Atmospheres</a></a></li><li><a href=#darwin-dynamic-agentically-rewriting-self-improving-networkhttpsarxivorgabs260205848v1 aria-label="DARWIN: Dynamic Agentically Rewriting Self-Improving Network"><a href=https://arxiv.org/abs/2602.05848v1>DARWIN: Dynamic Agentically Rewriting Self-Improving Network</a></a></li><li><a href=#where-does-warm-up-come-from-adaptive-scheduling-for-norm-constrained-optimizershttpsarxivorgabs260205813v1 aria-label="Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers"><a href=https://arxiv.org/abs/2602.05813v1>Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers</a></a></li><li><a href=#assessing-the-sensitivity-of-niobium--and-tantalum-based-superconducting-qubits-to-infrared-radiationhttpsarxivorgabs260205806v1 aria-label="Assessing the Sensitivity of Niobium- and Tantalum-Based Superconducting Qubits to Infrared Radiation"><a href=https://arxiv.org/abs/2602.05806v1>Assessing the Sensitivity of Niobium- and Tantalum-Based Superconducting Qubits to Infrared Radiation</a></a></li><li><a href=#a-perturbation-correction-method-based-on-local-randomized-neural-networks-for-quasi-linear-interface-problemshttpsarxivorgabs260205800v1 aria-label="A Perturbation-Correction Method Based on Local Randomized Neural Networks for Quasi-Linear Interface Problems"><a href=https://arxiv.org/abs/2602.05800v1>A Perturbation-Correction Method Based on Local Randomized Neural Networks for Quasi-Linear Interface Problems</a></a></li></ul></li><li><a href=#-data_resources aria-label="üîç data_resources">üîç data_resources</a><ul><li><a href=#supply-vs-demand-in-community-based-fact-checking-on-social-mediahttpsarxivorgabs260206005v1 aria-label="Supply vs. Demand in Community-Based Fact-Checking on Social Media"><a href=https://arxiv.org/abs/2602.06005v1>Supply vs. Demand in Community-Based Fact-Checking on Social Media</a></a></li><li><a href=#speech-emotion-recognition-leveraging-openais-whisper-representations-and-attentive-pooling-methodshttpsarxivorgabs260206000v1 aria-label="Speech Emotion Recognition Leveraging OpenAI&rsquo;s Whisper Representations and Attentive Pooling Methods"><a href=https://arxiv.org/abs/2602.06000v1>Speech Emotion Recognition Leveraging OpenAI&rsquo;s Whisper Representations and Attentive Pooling Methods</a></a></li><li><a href=#persistent-human-feedback-llms-and-static-analyzers-for-secure-code-generation-and-vulnerability-detectionhttpsarxivorgabs260205868v1 aria-label="Persistent Human Feedback, LLMs, and Static Analyzers for Secure Code Generation and Vulnerability Detection"><a href=https://arxiv.org/abs/2602.05868v1>Persistent Human Feedback, LLMs, and Static Analyzers for Secure Code Generation and Vulnerability Detection</a></a></li><li><a href=#contextbench-a-benchmark-for-context-retrieval-in-coding-agentshttpsarxivorgabs260205892v1 aria-label="ContextBench: A Benchmark for Context Retrieval in Coding Agents"><a href=https://arxiv.org/abs/2602.05892v1>ContextBench: A Benchmark for Context Retrieval in Coding Agents</a></a></li></ul></li><li><a href=#-emotion_language aria-label="üîç emotion_language">üîç emotion_language</a><ul><li><a href=#dimensionality-reduction-on-riemannian-manifolds-in-data-analysishttpsarxivorgabs260205936v1 aria-label="Dimensionality Reduction on Riemannian Manifolds in Data Analysis"><a href=https://arxiv.org/abs/2602.05936v1>Dimensionality Reduction on Riemannian Manifolds in Data Analysis</a></a></li><li><a href=#xlist-hate-a-checklist-based-framework-for-interpretable-and-generalizable-hate-speech-detectionhttpsarxivorgabs260205874v1 aria-label="xList-Hate: A Checklist-Based Framework for Interpretable and Generalizable Hate Speech Detection"><a href=https://arxiv.org/abs/2602.05874v1>xList-Hate: A Checklist-Based Framework for Interpretable and Generalizable Hate Speech Detection</a></a></li><li><a href=#towards-uncertainty-quantification-of-a-model-for-cancer-on-chip-experimentshttpsarxivorgabs260206018v1 aria-label="Towards uncertainty quantification of a model for cancer-on-chip experiments"><a href=https://arxiv.org/abs/2602.06018v1>Towards uncertainty quantification of a model for cancer-on-chip experiments</a></a></li><li><a href=#guardian-safety-filtering-for-systems-with-perception-models-subject-to-adversarial-attackshttpsarxivorgabs260206026v1 aria-label="GUARDIAN: Safety Filtering for Systems with Perception Models Subject to Adversarial Attacks"><a href=https://arxiv.org/abs/2602.06026v1>GUARDIAN: Safety Filtering for Systems with Perception Models Subject to Adversarial Attacks</a></a></li><li><a href=#dissipative-dicke-time-quasicrystalshttpsarxivorgabs260205994v1 aria-label="Dissipative Dicke Time Quasicrystals"><a href=https://arxiv.org/abs/2602.05994v1>Dissipative Dicke Time Quasicrystals</a></a></li><li><a href=#f-grpo-and-beyond-divergence-based-reinforcement-learning-algorithms-for-general-llm-alignmenthttpsarxivorgabs260205946v1 aria-label="$f$-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment"><a href=https://arxiv.org/abs/2602.05946v1>$f$-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment</a></a></li><li><a href=#detective-work-we-shouldnt-have-to-do-practitioner-challenges-in-regulatory-aligned-data-quality-in-machine-learning-systemshttpsarxivorgabs260205944v1 aria-label="&ldquo;Detective Work We Shouldn&rsquo;t Have to Do&rdquo;: Practitioner Challenges in Regulatory-Aligned Data Quality in Machine Learning Systems"><a href=https://arxiv.org/abs/2602.05944v1>&ldquo;Detective Work We Shouldn&rsquo;t Have to Do&rdquo;: Practitioner Challenges in Regulatory-Aligned Data Quality in Machine Learning Systems</a></a></li><li><a href=#lsa-localized-semantic-alignment-for-enhancing-temporal-consistency-in-traffic-video-generationhttpsarxivorgabs260205966v1 aria-label="LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation"><a href=https://arxiv.org/abs/2602.05966v1>LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation</a></a></li><li><a href=#improved-heavy-dark-matter-annihilation-search-from-dwarf-galaxies-with-hawchttpsarxivorgabs260205955v1 aria-label="Improved Heavy Dark Matter Annihilation Search from Dwarf Galaxies with HAWC"><a href=https://arxiv.org/abs/2602.05955v1>Improved Heavy Dark Matter Annihilation Search from Dwarf Galaxies with HAWC</a></a></li><li><a href=#tracing-agn-feedback-power-with-coolwarm-outflow-densities-predictions-and-observational-implicationshttpsarxivorgabs260205954v1 aria-label="Tracing AGN Feedback Power with Cool/Warm Outflow Densities: Predictions and Observational Implications"><a href=https://arxiv.org/abs/2602.05954v1>Tracing AGN Feedback Power with Cool/Warm Outflow Densities: Predictions and Observational Implications</a></a></li><li><a href=#higher-order-adaptive-behaviors-outperform-pairwise-strategies-in-mitigating-contagion-dynamicshttpsarxivorgabs260205915v1 aria-label="Higher-order adaptive behaviors outperform pairwise strategies in mitigating contagion dynamics"><a href=https://arxiv.org/abs/2602.05915v1>Higher-order adaptive behaviors outperform pairwise strategies in mitigating contagion dynamics</a></a></li><li><a href=#exact-recovery-in-the-data-block-modelhttpsarxivorgabs260205852v1 aria-label="Exact Recovery in the Data Block Model"><a href=https://arxiv.org/abs/2602.05852v1>Exact Recovery in the Data Block Model</a></a></li></ul></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h1 id=weekly-paper-notes>Weekly Paper Notes<a hidden class=anchor aria-hidden=true href=#weekly-paper-notes>#</a></h1><h2 id=-multilingual>üîç multilingual<a hidden class=anchor aria-hidden=true href=#-multilingual>#</a></h2><h3 id=self-improving-multilingual-long-reasoning-via-translation-reasoning-integrated-traininghttpsarxivorgabs260205940v1><a href=https://arxiv.org/abs/2602.05940v1>Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training</a><a hidden class=anchor aria-hidden=true href=#self-improving-multilingual-long-reasoning-via-translation-reasoning-integrated-traininghttpsarxivorgabs260205940v1>#</a></h3><p><strong>Authors:</strong> Junxiao Liu, Zhijun Wang, Yixiao Li, Zhejian Lai, Liqian Huang, Xin Huang, Xue Han, Junlan Feng, Shujian Huang
<strong>Venue:</strong> arXiv (2026)</p><p>Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05940v1">üìÑ Download PDF</a></p><hr><h3 id=polyglots-or-multitudes-multilingual-llm-answers-to-value-laden-multiple-choice-questionshttpsarxivorgabs260205932v1><a href=https://arxiv.org/abs/2602.05932v1>Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions</a><a hidden class=anchor aria-hidden=true href=#polyglots-or-multitudes-multilingual-llm-answers-to-value-laden-multiple-choice-questionshttpsarxivorgabs260205932v1>#</a></h3><p><strong>Authors:</strong> L√©o Labat, Etienne Ollion, Fran√ßois Yvon
<strong>Venue:</strong> arXiv (2026)</p><p>Multiple-Choice Questions (MCQs) are often used to assess knowledge, reasoning abilities, and even values encoded in large language models (LLMs). While the effect of multilingualism has been studied on LLM factual recall, this paper seeks to investigate the less explored question of language-induced variation in value-laden MCQ responses. Are multilingual LLMs consistent in their responses across languages, i.e. behave like theoretical polyglots, or do they answer value-laden MCQs depending on the language of the question, like a multitude of monolingual models expressing different values through a single model? We release a new corpus, the Multilingual European Value Survey (MEVS), which, unlike prior work relying on machine translation or ad hoc prompts, solely comprises human-translated survey questions aligned in 8 European languages. We administer a subset of those questions to over thirty multilingual LLMs of various sizes, manufacturers and alignment-fine-tuning status under comprehensive, controlled prompt variations including answer order, symbol type, and tail character. Our results show that while larger, instruction-tuned models display higher overall consistency, the robustness of their responses varies greatly across questions, with certain MCQs eliciting total agreement within and across models while others leave LLM answers split. Language-specific behavior seems to arise in all consistent, instruction-fine-tuned models, but only on certain questions, warranting a further study of the selective effect of preference fine-tuning.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05932v1">üìÑ Download PDF</a></p><hr><h3 id=dr-kernel-reinforcement-learning-done-right-for-triton-kernel-generationshttpsarxivorgabs260205885v1><a href=https://arxiv.org/abs/2602.05885v1>Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations</a><a hidden class=anchor aria-hidden=true href=#dr-kernel-reinforcement-learning-done-right-for-triton-kernel-generationshttpsarxivorgabs260205885v1>#</a></h3><p><strong>Authors:</strong> Wei Liu, Jiawei Xu, Yingru Li, Longtao Zheng, Tianjian Li, Qian Liu, Junxian He
<strong>Venue:</strong> arXiv (2026)</p><p>High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in <a href=https://www.github.com/hkust-nlp/KernelGYM>https://www.github.com/hkust-nlp/KernelGYM</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05885v1">üìÑ Download PDF</a></p><hr><h3 id=eurollm-22b-technical-reporthttpsarxivorgabs260205879v1><a href=https://arxiv.org/abs/2602.05879v1>EuroLLM-22B: Technical Report</a><a hidden class=anchor aria-hidden=true href=#eurollm-22b-technical-reporthttpsarxivorgabs260205879v1>#</a></h3><p><strong>Authors:</strong> Miguel Moura Ramos, Duarte M. Alves, Hippolyte Gisserot-Boukhlef, Jo√£o Alves, Pedro Henrique Martins, Patrick Fernandes, Jos√© Pombal, Nuno M. Guerreiro, Ricardo Rei, Nicolas Boizard, Amin Farajian, Mateusz Klimaszewski, Jos√© G. C. de Souza, Barry Haddow, Fran√ßois Yvon, Pierre Colombo, Alexandra Birch, Andr√© F. T. Martins
<strong>Venue:</strong> arXiv (2026)</p><p>This report presents EuroLLM-22B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-22B&rsquo;s development, including tokenizer design, architectural specifications, data filtering, and training procedures. Across a broad set of multilingual benchmarks, EuroLLM-22B demonstrates strong performance in reasoning, instruction following, and translation, achieving results competitive with models of comparable size. To support future research, we release our base and instruction-tuned models, our multilingual web pretraining data and updated EuroBlocks instruction datasets, as well as our pre-training and evaluation codebases.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05879v1">üìÑ Download PDF</a></p><hr><h3 id=whispers-of-the-butterfly-a-research-through-design-exploration-of-in-situ-conversational-ai-guidance-in-large-scale-outdoor-mr-exhibitionshttpsarxivorgabs260205826v1><a href=https://arxiv.org/abs/2602.05826v1>Whispers of the Butterfly: A Research-through-Design Exploration of In-Situ Conversational AI Guidance in Large-Scale Outdoor MR Exhibitions</a><a hidden class=anchor aria-hidden=true href=#whispers-of-the-butterfly-a-research-through-design-exploration-of-in-situ-conversational-ai-guidance-in-large-scale-outdoor-mr-exhibitionshttpsarxivorgabs260205826v1>#</a></h3><p><strong>Authors:</strong> Dongyijie Primo Pan, Shuyue Li, Yawei Zhao, Junkun Long, Hao Li, Pan Hui
<strong>Venue:</strong> arXiv (2026)</p><p>Large-scale outdoor mixed reality (MR) art exhibitions distribute curated virtual works across open public spaces, but interpretation rarely scales without turning exploration into a scripted tour. Through Research-through-Design, we created Dream-Butterfly, an in-situ conversational AI docent embodied as a small non-human companion that visitors summon for multilingual, exhibition-grounded explanations. We deployed Dream-Butterfly in a large-scale outdoor MR exhibition at a public university campus in southern China, and conducted an in-the-wild between-subject study (N=24) comparing a primarily human-led tour with an AI-led tour while keeping staff for safety in both conditions. Combining questionnaires and semi-structured interviews, we characterize how shifting the primary explanation channel reshapes explanation access, perceived responsiveness, immersion, and workload, and how visitors negotiate responsibility handoffs among staff, the AI guide, and themselves. We distill transferable design implications for configuring mixed human-AI guiding roles and embodying conversational agents in mobile, safety-constrained outdoor MR exhibitions.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05826v1">üìÑ Download PDF</a></p><hr><h3 id=fimi-a-domain-specific-language-model-for-indian-finance-ecosystemhttpsarxivorgabs260205794v1><a href=https://arxiv.org/abs/2602.05794v1>FiMI: A Domain-Specific Language Model for Indian Finance Ecosystem</a><a hidden class=anchor aria-hidden=true href=#fimi-a-domain-specific-language-model-for-indian-finance-ecosystemhttpsarxivorgabs260205794v1>#</a></h3><p><strong>Authors:</strong> Aboli Kathar, Aman Kumar, Anusha Kamath, Araveeti Srujan, Ashish Sharma, Chandra Bhushan, Dilip Asbe, Divya Sorate, Duddu Prasanth Kumar, Evan Acharya, Harsh Sharma, Hrithik Kadam, Kanishk Singla, Keyur Doshi, Kiran Praveen, Kolisetty Krishna SK, Krishanu Adhikary, Lokesh MPT, Mayurdeep Sonowal, Nadeem Shaikh, Navya Prakash, Nimit Kothari, Nitin Kukreja, Prashant Devadiga, Rakesh Paul, Ratanjeet Pratap Chauhan, Raunak Kalani, Raviraj Joshi, Shamanth MH, Shantanu Pandey, Shubham Soni, Siddharth Dixit, Smriti Jopat, Sunil Patel, Suraj Singh, Suvradip Paul, Tulasi Pilla, Utkarsh Vaidya, Vineeth Nambiar, Vishal Kanvaty, Yatharth Dedhia
<strong>Venue:</strong> arXiv (2026)</p><p>We present FiMI (Finance Model for India), a domain-specialized financial language model developed for Indian digital payment systems. We develop two model variants: FiMI Base and FiMI Instruct. FiMI adapts the Mistral Small 24B architecture through a multi-stage training pipeline, beginning with continuous pre-training on 68 Billion tokens of curated financial, multilingual (English, Hindi, Hinglish), and synthetic data. This is followed by instruction fine-tuning and domain-specific supervised fine-tuning focused on multi-turn, tool-driven conversations that model real-world workflows, such as transaction disputes and mandate lifecycle management. Evaluations reveal that FiMI Base achieves a 20% improvement over the Mistral Small 24B Base model on finance reasoning benchmark, while FiMI Instruct outperforms the Mistral Small 24B Instruct model by 87% on domain-specific tool-calling. Moreover, FiMI achieves these significant domain gains while maintaining comparable performance to models of similar size on general benchmarks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05794v1">üìÑ Download PDF</a></p><hr><h3 id=bhashasetu-cross-lingual-knowledge-transfer-from-high-resource-to-extreme-low-resource-languageshttpsarxivorgabs260205599v1><a href=https://arxiv.org/abs/2602.05599v1>BhashaSetu: Cross-Lingual Knowledge Transfer from High-Resource to Extreme Low-Resource Languages</a><a hidden class=anchor aria-hidden=true href=#bhashasetu-cross-lingual-knowledge-transfer-from-high-resource-to-extreme-low-resource-languageshttpsarxivorgabs260205599v1>#</a></h3><p><strong>Authors:</strong> Subhadip Maji, Arnab Bhattacharya
<strong>Venue:</strong> arXiv (2026)</p><p>Despite remarkable advances in natural language processing, developing effective systems for low-resource languages remains a formidable challenge, with performances typically lagging far behind high-resource counterparts due to data scarcity and insufficient linguistic resources. Cross-lingual knowledge transfer has emerged as a promising approach to address this challenge by leveraging resources from high-resource languages. In this paper, we investigate methods for transferring linguistic knowledge from high-resource languages to low-resource languages, where the number of labeled training instances is in hundreds. We focus on sentence-level and word-level tasks. We introduce a novel method, GETR (Graph-Enhanced Token Representation) for cross-lingual knowledge transfer along with two adopted baselines (a) augmentation in hidden layers and (b) token embedding transfer through token translation. Experimental results demonstrate that our GNN-based approach significantly outperforms existing multilingual and cross-lingual baseline methods, achieving 13 percentage point improvements on truly low-resource languages (Mizo, Khasi) for POS tagging, and 20 and 27 percentage point improvements in macro-F1 on simulated low-resource languages (Marathi, Bangla, Malayalam) across sentiment classification and NER tasks respectively. We also present a detailed analysis of the transfer mechanisms and identify key factors that contribute to successful knowledge transfer in this linguistic context.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05599v1">üìÑ Download PDF</a></p><hr><h3 id=cross-lingual-empirical-evaluation-of-large-language-models-for-arabic-medical-taskshttpsarxivorgabs260205374v1><a href=https://arxiv.org/abs/2602.05374v1>Cross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks</a><a hidden class=anchor aria-hidden=true href=#cross-lingual-empirical-evaluation-of-large-language-models-for-arabic-medical-taskshttpsarxivorgabs260205374v1>#</a></h3><p><strong>Authors:</strong> Chaimae Abouzahir, Congbo Ma, Nizar Habash, Farah E. Shamout
<strong>Venue:</strong> arXiv (2026)</p><p>In recent years, Large Language Models (LLMs) have become widely used in medical applications, such as clinical decision support, medical education, and medical question answering. Yet, these models are often English-centric, limiting their robustness and reliability for linguistically diverse communities. Recent work has highlighted discrepancies in performance in low-resource languages for various medical tasks, but the underlying causes remain poorly understood. In this study, we conduct a cross-lingual empirical analysis of LLM performance on Arabic and English medical question and answering. Our findings reveal a persistent language-driven performance gap that intensifies with increasing task complexity. Tokenization analysis exposes structural fragmentation in Arabic medical text, while reliability analysis suggests that model-reported confidence and explanations exhibit limited correlation with correctness. Together, these findings underscore the need for language-aware design and evaluation strategies in LLMs for medical tasks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05374v1">üìÑ Download PDF</a></p><hr><h3 id=multilingual-extraction-and-recognition-of-implicit-discourse-relations-in-speech-and-texthttpsarxivorgabs260205107v1><a href=https://arxiv.org/abs/2602.05107v1>Multilingual Extraction and Recognition of Implicit Discourse Relations in Speech and Text</a><a hidden class=anchor aria-hidden=true href=#multilingual-extraction-and-recognition-of-implicit-discourse-relations-in-speech-and-texthttpsarxivorgabs260205107v1>#</a></h3><p><strong>Authors:</strong> Ahmed Ruby, Christian Hardmeier, Sara Stymne
<strong>Venue:</strong> arXiv (2026)</p><p>Implicit discourse relation classification is a challenging task, as it requires inferring meaning from context. While contextual cues can be distributed across modalities and vary across languages, they are not always captured by text alone. To address this, we introduce an automatic method for distantly related and unrelated language pairs to construct a multilingual and multimodal dataset for implicit discourse relations in English, French, and Spanish. For classification, we propose a multimodal approach that integrates textual and acoustic information through Qwen2-Audio, allowing joint modeling of text and audio for implicit discourse relation classification across languages. We find that while text-based models outperform audio-based models, integrating both modalities can enhance performance, and cross-lingual transfer can provide substantial improvements for low-resource languages.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05107v1">üìÑ Download PDF</a></p><hr><h3 id=be-my-cheese-cultural-nuance-benchmarking-for-machine-translation-in-multilingual-llmshttpsarxivorgabs260204729v1><a href=https://arxiv.org/abs/2602.04729v1>&ldquo;Be My Cheese?&rdquo;: Cultural Nuance Benchmarking for Machine Translation in Multilingual LLMs</a><a hidden class=anchor aria-hidden=true href=#be-my-cheese-cultural-nuance-benchmarking-for-machine-translation-in-multilingual-llmshttpsarxivorgabs260204729v1>#</a></h3><p><strong>Authors:</strong> Madison Van Doren, Casey Ford, Jennifer Barajas, Cory Holland
<strong>Venue:</strong> arXiv (2026)</p><p>We present a large-scale human evaluation benchmark for assessing cultural localisation in machine translation produced by state-of-the-art multilingual large language models (LLMs). Existing MT benchmarks emphasise token-level and grammatical accuracy, but of ten overlook pragmatic and culturally grounded competencies required for real-world localisation. Building on a pilot study of 87 translations across 20 languages, we evaluate 7 multilingual LLMs across 15 target languages with 5 native-speaker raters per language. Raters scored both full-text translations and segment-level instances of culturally nuanced language (idioms, puns, holidays, and culturally embedded concepts) on an ordinal 0-3 quality scale; segment ratings additionally included an NA option for untranslated segments.
Across full-text evaluations, mean overall quality is modest (1.68/3): GPT-5 (2.10/3), Claude Sonnet 3.7 (1.97/3), and Mistral Medium 3.1 (1.84/3) form the strongest tier with fewer catastrophic failures. Segment-level results show sharp category effects: holidays (2.20/3) and cultural concepts (2.19/3) translate substantially better than idioms (1.65/3) and puns (1.45/3), and idioms are most likely to be left untranslated. These findings demonstrate a persistent gap between grammatical adequacy and cultural resonance. To our knowledge, this is the first multilingual, human-annotated benchmark focused explicitly on cultural nuance in translation and localisation, highlighting the need for culturally informed training data, improved cross-lingual pragmatics, and evaluation paradigms that better reflect real-world communicative competence.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.04729v1">üìÑ Download PDF</a></p><hr><h3 id=tokenization-and-morphological-fidelity-in-uralic-nlp-a-cross-lingual-evaluationhttpsarxivorgabs260204241v1><a href=https://arxiv.org/abs/2602.04241v1>Tokenization and Morphological Fidelity in Uralic NLP: A Cross-Lingual Evaluation</a><a hidden class=anchor aria-hidden=true href=#tokenization-and-morphological-fidelity-in-uralic-nlp-a-cross-lingual-evaluationhttpsarxivorgabs260204241v1>#</a></h3><p><strong>Authors:</strong> Nuo Xu, Ahrii Kim
<strong>Venue:</strong> arXiv (2026)</p><p>Subword tokenization critically affects Natural Language Processing (NLP) performance, yet its behavior in morphologically rich and low-resource language families remains under-explored. This study systematically compares three subword paradigms &ndash; Byte Pair Encoding (BPE), Overlap BPE (OBPE), and Unigram Language Model &ndash; across six Uralic languages with varying resource availability and typological diversity. Using part-of-speech (POS) tagging as a controlled downstream task, we show that OBPE consistently achieves stronger morphological alignment and higher tagging accuracy than conventional methods, particularly within the Latin-script group. These gains arise from reduced fragmentation in open-class categories and a better balance across the frequency spectrum. Transfer efficacy further depends on the downstream tagging architecture, interacting with both training volume and genealogical proximity. Taken together, these findings highlight that morphology-sensitive tokenization is not merely a preprocessing choice but a decisive factor in enabling effective cross-lingual transfer for agglutinative, low-resource languages.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.04241v1">üìÑ Download PDF</a></p><hr><h3 id=bagging-based-model-merging-for-robust-general-text-embeddingshttpsarxivorgabs260205787v1><a href=https://arxiv.org/abs/2602.05787v1>Bagging-Based Model Merging for Robust General Text Embeddings</a><a hidden class=anchor aria-hidden=true href=#bagging-based-model-merging-for-robust-general-text-embeddingshttpsarxivorgabs260205787v1>#</a></h3><p><strong>Authors:</strong> Hengran Zhang, Keping Bi, Jiafeng Guo, Jiaming Zhang, Wenbo Yang, Daiting Shi, Xueqi Cheng
<strong>Venue:</strong> arXiv (2026)</p><p>General-purpose text embedding models underpin a wide range of NLP and information retrieval applications, and are typically trained on large-scale multi-task corpora to encourage broad generalization. However, it remains unclear how different multi-task training strategies compare in practice, and how to efficiently adapt embedding models as new domains and data types continually emerge. In this work, we present a systematic study of multi-task training for text embeddings from two perspectives: data scheduling and model merging. We compare batch-level shuffling, sequential training variants, two-stage training, and multiple merging granularities, and find that simple batch-level shuffling consistently yields the strongest overall performance, suggesting that task conflicts are limited and training datasets are largely complementary. Despite its effectiveness, batch-level shuffling exhibits two practical limitations: suboptimal out-of-domain (OOD) generalization and poor suitability for incremental learning due to expensive full retraining. To address these issues, we propose Bagging-based rObust mOdel Merging (\modelname), which trains multiple embedding models on sampled subsets and merges them into a single model, improving robustness while retaining single-model inference efficiency. Moreover, \modelname naturally supports efficient incremental updates by training lightweight update models on new data with a small historical subset and merging them into the existing model. Experiments across diverse embedding benchmarks demonstrate that \modelname consistently improves both in-domain and OOD performance over full-corpus batch-level shuffling, while substantially reducing training cost in incremental learning settings.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05787v1">üìÑ Download PDF</a></p><hr><h3 id=evaluating-the-impact-of-word-embeddings-on-similarity-scoring-in-practical-information-retrievalhttpsarxivorgabs260205734v1><a href=https://arxiv.org/abs/2602.05734v1>Evaluating the impact of word embeddings on similarity scoring in practical information retrieval</a><a hidden class=anchor aria-hidden=true href=#evaluating-the-impact-of-word-embeddings-on-similarity-scoring-in-practical-information-retrievalhttpsarxivorgabs260205734v1>#</a></h3><p><strong>Authors:</strong> Niall McCarroll, Kevin Curran, Eugene McNamee, Angela Clist, Andrew Brammer
<strong>Venue:</strong> arXiv (2026)</p><p>Search behaviour is characterised using synonymy and polysemy as users often want to search information based on meaning. Semantic representation strategies represent a move towards richer associative connections that can adequately capture this complex usage of language. Vector Space Modelling (VSM) and neural word embeddings play a crucial role in modern machine learning and Natural Language Processing (NLP) pipelines. Embeddings use distributional semantics to represent words, sentences, paragraphs or entire documents as vectors in high dimensional spaces. This can be leveraged by Information Retrieval (IR) systems to exploit the semantic relatedness between queries and answers.
This paper evaluates an alternative approach to measuring query statement similarity that moves away from the common similarity measure of centroids of neural word embeddings. Motivated by the Word Movers Distance (WMD) model, similarity is evaluated using the distance between individual words of queries and statements. Results from ranked query and response statements demonstrate significant gains in accuracy using the combined approach of similarity ranking through WMD with the word embedding techniques. The top performing WMD + GloVe combination outperforms all other state-of-the-art retrieval models including Doc2Vec and the baseline LSA model. Along with the significant gains in performance of similarity ranking through WMD, we conclude that the use of pre-trained word embeddings, trained on vast amounts of data, result in domain agnostic language processing solutions that are portable to diverse business use-cases.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05734v1">üìÑ Download PDF</a></p><hr><h3 id=mederrbench-a-fine-grained-multilingual-benchmark-for-medical-error-detection-and-correction-with-clinical-expert-annotationshttpsarxivorgabs260205692v1><a href=https://arxiv.org/abs/2602.05692v1>MedErrBench: A Fine-Grained Multilingual Benchmark for Medical Error Detection and Correction with Clinical Expert Annotations</a><a hidden class=anchor aria-hidden=true href=#mederrbench-a-fine-grained-multilingual-benchmark-for-medical-error-detection-and-correction-with-clinical-expert-annotationshttpsarxivorgabs260205692v1>#</a></h3><p><strong>Authors:</strong> Congbo Ma, Yichun Zhang, Yousef Al-Jazzazi, Ahamed Foisal, Laasya Sharma, Yousra Sadqi, Khaled Saleh, Jihad Mallat, Farah E. Shamout
<strong>Venue:</strong> arXiv (2026)</p><p>Inaccuracies in existing or generated clinical text may lead to serious adverse consequences, especially if it is a misdiagnosis or incorrect treatment suggestion. With Large Language Models (LLMs) increasingly being used across diverse healthcare applications, comprehensive evaluation through dedicated benchmarks is crucial. However, such datasets remain scarce, especially across diverse languages and contexts. In this paper, we introduce MedErrBench, the first multilingual benchmark for error detection, localization, and correction, developed under the guidance of experienced clinicians. Based on an expanded taxonomy of ten common error types, MedErrBench covers English, Arabic and Chinese, with natural clinical cases annotated and reviewed by domain experts. We assessed the performance of a range of general-purpose, language-specific, and medical-domain language models across all three tasks. Our results reveal notable performance gaps, particularly in non-English settings, highlighting the need for clinically grounded, language-aware systems. By making MedErrBench and our evaluation protocols publicly-available, we aim to advance multilingual clinical NLP to promote safer and more equitable AI-based healthcare globally. The dataset is available in the supplementary material. An anonymized version of the dataset is available at: <a href=https://github.com/congboma/MedErrBench>https://github.com/congboma/MedErrBench</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05692v1">üìÑ Download PDF</a></p><hr><h3 id=castle-a-comprehensive-benchmark-for-evaluating-student-tailored-personalized-safety-in-large-language-modelshttpsarxivorgabs260205633v1><a href=https://arxiv.org/abs/2602.05633v1>CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models</a><a hidden class=anchor aria-hidden=true href=#castle-a-comprehensive-benchmark-for-evaluating-student-tailored-personalized-safety-in-large-language-modelshttpsarxivorgabs260205633v1>#</a></h3><p><strong>Authors:</strong> Rui Jia, Ruiyi Lan, Fengrui Liu, Zhongxiang Dai, Bo Jiang, Jing Shao, Jingyuan Chen, Guandong Xu, Fei Wu, Min Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>Large language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and psychological, thereby posing potential safety risks to vulnerable groups. Existing safety evaluations primarily rely on context-independent metrics such as factual accuracy, bias, or toxicity, which fail to capture the divergent harms that the same response might cause across different student attributes. To address this gap, we propose the concept of Student-Tailored Personalized Safety and construct CASTLE based on educational theories. This benchmark covers 15 educational safety risks and 14 student attributes, comprising 92,908 bilingual scenarios. We further design three evaluation metrics: Risk Sensitivity, measuring the model ability to detect risks; Emotional Empathy, evaluating the model capacity to recognize student states; and Student Alignment, assessing the match between model responses and student attributes. Experiments on 18 SOTA LLMs demonstrate that CASTLE poses a significant challenge: all models scored below an average safety rating of 2.3 out of 5, indicating substantial deficiencies in personalized safety assurance.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05633v1">üìÑ Download PDF</a></p><hr><h3 id=reporting-and-reviewing-llm-integrated-systems-in-hci-challenges-and-considerationshttpsarxivorgabs260205128v1><a href=https://arxiv.org/abs/2602.05128v1>Reporting and Reviewing LLM-Integrated Systems in HCI: Challenges and Considerations</a><a hidden class=anchor aria-hidden=true href=#reporting-and-reviewing-llm-integrated-systems-in-hci-challenges-and-considerationshttpsarxivorgabs260205128v1>#</a></h3><p><strong>Authors:</strong> Karla Felix Navarro, Eugene Syriani, Ian Arawjo
<strong>Venue:</strong> arXiv (2026)</p><p>What should HCI scholars consider when reporting and reviewing papers that involve LLM-integrated systems? We interview 18 authors of LLM-integrated system papers on their authoring and reviewing experiences. We find that norms of trust-building between authors and reviewers appear to be eroded by the uncertainty of LLM behavior and hyperbolic rhetoric surrounding AI. Authors perceive that reviewers apply uniquely skeptical and inconsistent standards towards papers that report LLM-integrated systems, and mitigate mistrust by adding technical evaluations, justifying usage, and de-emphasizing LLM presence. Authors&rsquo; views challenge blanket directives to report all prompts and use open models, arguing that prompt reporting is context-dependent and justifying proprietary model usage despite ethical concerns. Finally, some tensions in peer review appear to stem from clashes between the norms and values of HCI and ML/NLP communities, particularly around what constitutes a contribution and an appropriate level of technical rigor. Based on our findings and additional feedback from six expert HCI researchers, we present a set of guidelines and considerations for authors, reviewers, and HCI communities around reporting and reviewing papers that involve LLM-integrated systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05128v1">üìÑ Download PDF</a></p><hr><h3 id=on-dual-connectivity-in-6g-leo-constellationshttpsarxivorgabs260204825v1><a href=https://arxiv.org/abs/2602.04825v1>On Dual Connectivity in 6G Leo Constellations</a><a hidden class=anchor aria-hidden=true href=#on-dual-connectivity-in-6g-leo-constellationshttpsarxivorgabs260204825v1>#</a></h3><p><strong>Authors:</strong> Achilles Machumilane, Alberto Gotta
<strong>Venue:</strong> arXiv (2026)</p><p>Dual connectivity (DC) has garnered significant attention in 5G evolution, allowing for enhancing throughput and reliability by leveraging the channel conditions of two paths. However, when the paths exhibit different delays, such as in terrestrial and non-terrestrial integrated networks with multi-orbit topologies or in networks characterized by frequent topology changes, like Low Earth Orbit (LEO) satellite constellations with different elevation angles, traffic delivery may experience packet reordering or triggering congestion control mechanisms. Additionally, real-time traffic may experience packet drops if their arrival exceeds a play-out threshold. Different techniques have been proposed to address these issues, such as packet duplication, packet switching, and network coding for traffic scheduling in DC. However, if not accurately designed, these techniques can lead to resource waste, encoding/decoding delays, and computational overhead, undermining DC&rsquo;s intended benefits. This paper provides a mathematical framework for calculating the average end-to-end packet loss in case of a loss process modeled with a Discrete Markov Chain - typical of a wireless channel - when combining packet duplication and packet switching or when network coding is employed in DC. Such metrics help derive optimal policies with full knowledge of the underlying loss process to be compared to empirical models learned through Machine Learning algorithms.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.04825v1">üìÑ Download PDF</a></p><hr><h3 id=hydrodynamic-simulations-of-expanded-warm-dense-foil-heated-by-pulsed-powerhttpsarxivorgabs260203458v2><a href=https://arxiv.org/abs/2602.03458v2>Hydrodynamic simulations of expanded warm dense foil heated by pulsed-power</a><a hidden class=anchor aria-hidden=true href=#hydrodynamic-simulations-of-expanded-warm-dense-foil-heated-by-pulsed-powerhttpsarxivorgabs260203458v2>#</a></h3><p><strong>Authors:</strong> Luc Revello, Laurent Videau, Fr√©d√©ric Zucchini, Mathurin Lagr√©e, Christophe Blancard, Benjamin Jodar
<strong>Venue:</strong> arXiv (2026)</p><p>Warm Dense Matter lies at the frontier between condensed matter and plasma, and plays a central role in various fields ranging from planetary science to inertial confinement fusion. Improving our understanding of this regime requires experimental data that can be directly compared with theoretical and numerical models over a broad range of conditions. In this work, a pulsed-power experiment is described in which thin metallic foils, confined within a sapphire cell, are Joule-heated to achieve the expanded warm dense matter regime. Designing such an experiment is challenging, as it requires simultaneously predicting the electrical response of the pulsed-power driver and the hydrodynamic evolution of the heated material. To tackle this challenge, a modeling framework has been developed that couples an electrical description of the pulsed-power system, including the driver, the switching stages and the load with a one-dimensional hydrodynamic code. This coupling allows the electrical energy deposition and the load thermodynamic evolution to be consistently linked through the material electrical conductivity. This approach takes advantage of the simplicity of a 1D geometry while retaining the essential physics and allowing to reproduce various measurements with good accuracy, such as expansion velocity, current and voltage. This numerical approach therefore constitutes a robust and efficient method for designing and optimizing future Warm Dense Matter experiments using pulsed-power facilities.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.03458v2">üìÑ Download PDF</a></p><hr><h3 id=dziribot-rag-based-intelligent-conversational-agent-for-algerian-arabic-dialecthttpsarxivorgabs260202270v1><a href=https://arxiv.org/abs/2602.02270v1>dziribot: rag based intelligent conversational agent for algerian arabic dialect</a><a hidden class=anchor aria-hidden=true href=#dziribot-rag-based-intelligent-conversational-agent-for-algerian-arabic-dialecthttpsarxivorgabs260202270v1>#</a></h3><p><strong>Authors:</strong> El Batoul Bechiri, Dihia Lanasri
<strong>Venue:</strong> arXiv (2026)</p><p>The rapid digitalization of customer service has intensified the demand for conversational agents capable of providing accurate and natural interactions. In the Algerian context, this is complicated by the linguistic complexity of Darja, a dialect characterized by non-standardized orthography, extensive code-switching with French, and the simultaneous use of Arabic and Latin (Arabizi) scripts. This paper introduces DziriBOT, a hybrid intelligent conversational agent specifically engineered to overcome these challenges. We propose a multi-layered architecture that integrates specialized Natural Language Understanding (NLU) with Retrieval-Augmented Generation (RAG), allowing for both structured service flows and dynamic, knowledge-intensive responses grounded in curated enterprise documentation. To address the low-resource nature of Darja, we systematically evaluate three distinct approaches: a sparse-feature Rasa pipeline, classical machine learning baselines, and transformer-based fine-tuning. Our experimental results demonstrate that the fine-tuned DziriBERT model achieves state-of-the-art performance. These results significantly outperform traditional baselines, particularly in handling orthographic noise and rare intents. Ultimately, DziriBOT provides a robust, scalable solution that bridges the gap between formal language models and the linguistic realities of Algerian users, offering a blueprint for dialect-aware automation in the regional market.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.02270v1">üìÑ Download PDF</a></p><hr><h3 id=adaptive-quantum-safe-cryptography-for-6g-vehicular-networks-via-context-aware-optimizationhttpsarxivorgabs260201342v1><a href=https://arxiv.org/abs/2602.01342v1>Adaptive Quantum-Safe Cryptography for 6G Vehicular Networks via Context-Aware Optimization</a><a hidden class=anchor aria-hidden=true href=#adaptive-quantum-safe-cryptography-for-6g-vehicular-networks-via-context-aware-optimizationhttpsarxivorgabs260201342v1>#</a></h3><p><strong>Authors:</strong> Poushali Sengupta, Mayank Raikwar, Sabita Maharjan, Frank Eliassen, Yan Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>Powerful quantum computers in the future may be able to break the security used for communication between vehicles and other devices (Vehicle-to-Everything, or V2X). New security methods called post-quantum cryptography can help protect these systems, but they often require more computing power and can slow down communication, posing a challenge for fast 6G vehicle networks. In this paper, we propose an adaptive post-quantum cryptography (PQC) framework that predicts short-term mobility and channel variations and dynamically selects suitable lattice-, code-, or hash-based PQC configurations using a predictive multi-objective evolutionary algorithm (APMOEA) to meet vehicular latency and security constraints.However, frequent cryptographic reconfiguration in dynamic vehicular environments introduces new attack surfaces during algorithm transitions. A secure monotonic-upgrade protocol prevents downgrade, replay, and desynchronization attacks during transitions. Theoretical results show decision stability under bounded prediction error, latency boundedness under mobility drift, and correctness under small forecast noise. These results demonstrate a practical path toward quantum-safe cryptography in future 6G vehicular networks. Through extensive experiments based on realistic mobility (LuST), weather (ERA5), and NR-V2X channel traces, we show that the proposed framework reduces end-to-end latency by up to 27%, lowers communication overhead by up to 65%, and effectively stabilizes cryptographic switching behavior using reinforcement learning. Moreover, under the evaluated adversarial scenarios, the monotonic-upgrade protocol successfully prevents downgrade, replay, and desynchronization attacks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.01342v1">üìÑ Download PDF</a></p><hr><h3 id=e-globe-scalable-Œµ-global-verification-of-neural-networks-via-tight-upper-bounds-and-pattern-aware-branchinghttpsarxivorgabs260205068v1><a href=https://arxiv.org/abs/2602.05068v1>E-Globe: Scalable $Œµ$-Global Verification of Neural Networks via Tight Upper Bounds and Pattern-Aware Branching</a><a hidden class=anchor aria-hidden=true href=#e-globe-scalable-Œµ-global-verification-of-neural-networks-via-tight-upper-bounds-and-pattern-aware-branchinghttpsarxivorgabs260205068v1>#</a></h3><p><strong>Authors:</strong> Wenting Li, Saif R. Kazi, Russell Bent, Duo Zhou, Huan Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>Neural networks achieve strong empirical performance, but robustness concerns still hinder deployment in safety-critical applications. Formal verification provides robustness guarantees, but current methods face a scalability-completeness trade-off. We propose a hybrid verifier in a branch-and-bound (BaB) framework that efficiently tightens both upper and lower bounds until an $Œµ-$global optimum is reached or early stop is triggered. The key is an exact nonlinear program with complementarity constraints (NLP-CC) for upper bounding that preserves the ReLU input-output graph, so any feasible solution yields a valid counterexample and enables rapid pruning of unsafe subproblems. We further accelerate verification with (i) warm-started NLP solves requiring minimal constraint-matrix updates and (ii) pattern-aligned strong branching that prioritizes splits most effective at tightening relaxations. We also provide conditions under which NLP-CC upper bounds are tight. Experiments on MNIST and CIFAR-10 show markedly tighter upper bounds than PGD across perturbation radii spanning up to three orders of magnitude, fast per-node solves in practice, and substantial end-to-end speedups over MIP-based verification, amplified by warm-starting, GPU batching, and pattern-aligned branching.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05068v1">üìÑ Download PDF</a></p><hr><h3 id=shared-lora-subspaces-for-almost-strict-continual-learninghttpsarxivorgabs260206043v1><a href=https://arxiv.org/abs/2602.06043v1>Shared LoRA Subspaces for almost Strict Continual Learning</a><a hidden class=anchor aria-hidden=true href=#shared-lora-subspaces-for-almost-strict-continual-learninghttpsarxivorgabs260206043v1>#</a></h3><p><strong>Authors:</strong> Prakhar Kaushik, Ankit Vaidya, Shravan Chaudhari, Rama Chellappa, Alan Yuille
<strong>Venue:</strong> arXiv (2026)</p><p>Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06043v1">üìÑ Download PDF</a></p><hr><h3 id=predicting-camera-pose-from-perspective-descriptions-for-spatial-reasoninghttpsarxivorgabs260206041v1><a href=https://arxiv.org/abs/2602.06041v1>Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning</a><a hidden class=anchor aria-hidden=true href=#predicting-camera-pose-from-perspective-descriptions-for-spatial-reasoninghttpsarxivorgabs260206041v1>#</a></h3><p><strong>Authors:</strong> Xuejun Zhang, Aditi Tiwari, Zhenhailong Wang, Heng Ji
<strong>Venue:</strong> arXiv (2026)</p><p>Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20¬∞ and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06041v1">üìÑ Download PDF</a></p><hr><h3 id=dytopo-dynamic-topology-routing-for-multi-agent-reasoning-via-semantic-matchinghttpsarxivorgabs260206039v1><a href=https://arxiv.org/abs/2602.06039v1>DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching</a><a hidden class=anchor aria-hidden=true href=#dytopo-dynamic-topology-routing-for-multi-agent-reasoning-via-semantic-matchinghttpsarxivorgabs260206039v1>#</a></h3><p><strong>Authors:</strong> Yuxing Lu, Yucheng Hu, Xukai Zhao, Jiuxin Cao
<strong>Venue:</strong> arXiv (2026)</p><p>Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager&rsquo;s round goal, each agent outputs lightweight natural-language query (need) and \key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06039v1">üìÑ Download PDF</a></p><hr><h3 id=swimbird-eliciting-switchable-reasoning-mode-in-hybrid-autoregressive-mllmshttpsarxivorgabs260206040v1><a href=https://arxiv.org/abs/2602.06040v1>SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs</a><a hidden class=anchor aria-hidden=true href=#swimbird-eliciting-switchable-reasoning-mode-in-hybrid-autoregressive-mllmshttpsarxivorgabs260206040v1>#</a></h3><p><strong>Authors:</strong> Jintao Tong, Shilin Yan, Hongwei Xue, Xiaojun Tang, Kunyu Shi, Guannan Zhang, Ruixuan Li, Yixiong Zou
<strong>Venue:</strong> arXiv (2026)</p><p>Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as &ldquo;visual thoughts&rdquo; into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06040v1">üìÑ Download PDF</a></p><hr><h3 id=commcp-efficient-multi-agent-coordination-via-llm-based-communication-with-conformal-predictionhttpsarxivorgabs260206038v1><a href=https://arxiv.org/abs/2602.06038v1>CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction</a><a hidden class=anchor aria-hidden=true href=#commcp-efficient-multi-agent-coordination-via-llm-based-communication-with-conformal-predictionhttpsarxivorgabs260206038v1>#</a></h3><p><strong>Authors:</strong> Xiaopan Zhang, Zejin Wang, Zhixu Li, Jianpeng Yao, Jiachen Li
<strong>Venue:</strong> arXiv (2026)</p><p>To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: <a href=https://comm-cp.github.io>https://comm-cp.github.io</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06038v1">üìÑ Download PDF</a></p><hr><h3 id=thinking-with-geometry-active-geometry-integration-for-spatial-reasoninghttpsarxivorgabs260206037v1><a href=https://arxiv.org/abs/2602.06037v1>Thinking with Geometry: Active Geometry Integration for Spatial Reasoning</a><a hidden class=anchor aria-hidden=true href=#thinking-with-geometry-active-geometry-integration-for-spatial-reasoninghttpsarxivorgabs260206037v1>#</a></h3><p><strong>Authors:</strong> Haoyuan Li, Qihang Cao, Tao Tang, Kun Xiang, Zihan Guo, Jianhua Han, Hang Xu, Xiaodan Liang
<strong>Venue:</strong> arXiv (2026)</p><p>Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at <a href=https://github.com/Li-Hao-yuan/GeoThinker>https://github.com/Li-Hao-yuan/GeoThinker</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06037v1">üìÑ Download PDF</a></p><hr><h3 id=correctness-optimized-residual-activation-lens-coral-transferrable-and-calibration-aware-inference-time-steeringhttpsarxivorgabs260206022v1><a href=https://arxiv.org/abs/2602.06022v1>Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering</a><a hidden class=anchor aria-hidden=true href=#correctness-optimized-residual-activation-lens-coral-transferrable-and-calibration-aware-inference-time-steeringhttpsarxivorgabs260206022v1>#</a></h3><p><strong>Authors:</strong> Miranda Muqing Miao, Young-Min Cho, Lyle Ungar
<strong>Venue:</strong> arXiv (2026)</p><p>Large language models (LLMs) exhibit persistent miscalibration, especially after instruction tuning and preference alignment. Modified training objectives can improve calibration, but retraining is expensive. Inference-time steering offers a lightweight alternative, yet most existing methods optimize proxies for correctness rather than correctness itself. We introduce CORAL (Correctness-Optimized Residual Activation Lens), a regularized inference-time steering method that captures distributed correctness signals from model internal activations using weight-decay MLP probes. We evaluate CORAL across three 7B-parameter models and find that it consistently improves accuracy by 10% and expected calibration error (ECE) by 50% on average. We additionally demonstrate that these gains transfer without retraining to the complete published test sets of four held-out benchmarks (ARC-Challenge, HellaSwag, Math-MC, OpenBookQA), averaging 14% accuracy improvements and 49% ECE improvements. Our results support the hypothesis that distributed information in model internals can be extracted using regularized probes when individual neurons are insufficient. CORAL thus provides a compute-efficient, transferable, and calibration-aware approach to improve MCQA performance during inference.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06022v1">üìÑ Download PDF</a></p><hr><h3 id=modeling-integrated-frequency-shifters-and-beam-splittershttpsarxivorgabs260206003v1><a href=https://arxiv.org/abs/2602.06003v1>Modeling integrated frequency shifters and beam splitters</a><a hidden class=anchor aria-hidden=true href=#modeling-integrated-frequency-shifters-and-beam-splittershttpsarxivorgabs260206003v1>#</a></h3><p><strong>Authors:</strong> Manuel H. Mu√±oz-Arias, Kevin J. Randles, Nils T. Otterstrom, Paul S. Davids, Michael Gehl, Mohan Sarovar
<strong>Venue:</strong> arXiv (2026)</p><p>Photonic quantum computing is a strong contender in the race to fault-tolerance. Recent proposals using qubits encoded in frequency modes promise a large reduction in hardware footprint, and have garnered much attention. In this encoding, linear optics, i.e., beam splitters and phase shifters, is necessarily not energy-conserving, and is costly to implement. In this work, we present designs of frequency-mode beam splitters based on modulated arrays of coupled resonators. We develop a methodology to construct their effective transfer matrices based on the SLH formalism for quantum input-output networks. Our methodology is flexible and highly composable, allowing us to define $N$-mode beam splitters either natively based on arrays of $N$-resonators of arbitrary connectivity or as networks of interconnected $l$-mode beam splitters, with $l&lt;N$. We apply our methodology to analyze a two-resonator device, a frequency-domain phase shifter and a Mach-Zehnder interferometer obtained from composing these devices, a four-resonator device, and present a formal no-go theorem on the possibility of natively generating certain $N$-mode frequency-domain beam splitters with arrays of $N$-resonators.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06003v1">üìÑ Download PDF</a></p><hr><h3 id=giant-bubbles-of-fisher-zeros-in-the-quantum-xy-chainhttpsarxivorgabs260205899v1><a href=https://arxiv.org/abs/2602.05899v1>Giant bubbles of Fisher zeros in the quantum XY chain</a><a hidden class=anchor aria-hidden=true href=#giant-bubbles-of-fisher-zeros-in-the-quantum-xy-chainhttpsarxivorgabs260205899v1>#</a></h3><p><strong>Authors:</strong> Songtai Lv, Yang Liu, Erhai Zhao, Haiyuan Zou, Tao Xiang
<strong>Venue:</strong> arXiv (2026)</p><p>We demonstrate an alternative approach based on complex-valued inverse temperature and partition function to probe quantum phases of matter with nontrivial spectra and dynamics. It leverages thermofield dynamics (TFD) to quantitatively characterize quantum and thermal fluctuations, and exploit the correspondence between low-energy excitations and Fisher zeros. Using the quantum XY chain in an external field as a testbed, we show that the oscillatory gap behavior manifests as oscillations in the long-time dynamics of the TFD spectral form factor. We also identify giant bubbles, i.e. large-scale closed lines, of Fisher-zeros near the gapless XX limit. They provide a characteristic energy scale that seems to contradict the predictions of the low energy theory of a featureless Luttinger liquid. We identify this energy scale and relate the motion of these giant bubbles with varying external field to the transfer of spectral weight from high to low energies. The deep connection between Fisher zeros, dynamics, and excitations opens up promising avenues for understanding the unconventional gap behaviors in strongly correlated many-body systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05899v1">üìÑ Download PDF</a></p><hr><h3 id=ui-mem-self-evolving-experience-memory-for-online-reinforcement-learning-in-mobile-gui-agentshttpsarxivorgabs260205832v1><a href=https://arxiv.org/abs/2602.05832v1>UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents</a><a hidden class=anchor aria-hidden=true href=#ui-mem-self-evolving-experience-memory-for-online-reinforcement-learning-in-mobile-gui-agentshttpsarxivorgabs260205832v1>#</a></h3><p><strong>Authors:</strong> Han Xiao, Guozhi Wang, Hao Wang, Shilong Liu, Yuxiang Chai, Yue Pan, Yufeng Zhou, Xiaoxin Chen, Yafei Wen, Hongsheng Li
<strong>Venue:</strong> arXiv (2026)</p><p>Online Reinforcement Learning (RL) offers a promising paradigm for enhancing GUI agents through direct environment interaction. However, its effectiveness is severely hindered by inefficient credit assignment in long-horizon tasks and repetitive errors across tasks due to the lack of experience transfer. To address these challenges, we propose UI-Mem, a novel framework that enhances GUI online RL with a Hierarchical Experience Memory. Unlike traditional replay buffers, our memory accumulates structured knowledge, including high-level workflows, subtask skills, and failure patterns. These experiences are stored as parameterized templates that enable cross-task and cross-application transfer. To effectively integrate memory guidance into online RL, we introduce Stratified Group Sampling, which injects varying levels of guidance across trajectories within each rollout group to maintain outcome diversity, driving the unguided policy toward internalizing guided behaviors. Furthermore, a Self-Evolving Loop continuously abstracts novel strategies and errors to keep the memory aligned with the agent&rsquo;s evolving policy. Experiments on online GUI benchmarks demonstrate that UI-Mem significantly outperforms traditional RL baselines and static reuse strategies, with strong generalization to unseen applications. Project page: <a href=https://ui-mem.github.io>https://ui-mem.github.io</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05832v1">üìÑ Download PDF</a></p><hr><h3 id=v-retrver-evidence-driven-agentic-reasoning-for-universal-multimodal-retrievalhttpsarxivorgabs260206034v1><a href=https://arxiv.org/abs/2602.06034v1>V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval</a><a hidden class=anchor aria-hidden=true href=#v-retrver-evidence-driven-agentic-reasoning-for-universal-multimodal-retrievalhttpsarxivorgabs260206034v1>#</a></h3><p><strong>Authors:</strong> Dongyang Chen, Chaoyang Wang, Dezhao SU, Xi Xiao, Zeyu Zhang, Jing Xiong, Qing Li, Yuzhang Shang, Shichao Ka
<strong>Venue:</strong> arXiv (2026)</p><p>Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06034v1">üìÑ Download PDF</a></p><hr><h3 id=physicsagentabm-physics-guided-generative-agent-based-modelinghttpsarxivorgabs260206030v1><a href=https://arxiv.org/abs/2602.06030v1>PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling</a><a hidden class=anchor aria-hidden=true href=#physicsagentabm-physics-guided-generative-agent-based-modelinghttpsarxivorgabs260206030v1>#</a></h3><p><strong>Authors:</strong> Kavana Venkatesh, Yinhan He, Jundong Li, Jiaming Cui
<strong>Venue:</strong> arXiv (2026)</p><p>Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06030v1">üìÑ Download PDF</a></p><hr><h3 id=curiosity-is-knowledge-self-consistent-learning-and-no-regret-optimization-with-active-inferencehttpsarxivorgabs260206029v1><a href=https://arxiv.org/abs/2602.06029v1>Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference</a><a hidden class=anchor aria-hidden=true href=#curiosity-is-knowledge-self-consistent-learning-and-no-regret-optimization-with-active-inferencehttpsarxivorgabs260206029v1>#</a></h3><p><strong>Authors:</strong> Yingke Li, Anjali Parashar, Enlu Zhou, Chuchu Fan
<strong>Venue:</strong> arXiv (2026)</p><p>Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement&ndash;sufficient curiosity&ndash;simultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic-pragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06029v1">üìÑ Download PDF</a></p><hr><h3 id=diffusion-models-generalization-can-be-characterized-by-inductive-biases-toward-a-data-dependent-ridge-manifoldhttpsarxivorgabs260206021v1><a href=https://arxiv.org/abs/2602.06021v1>Diffusion Model&rsquo;s Generalization Can Be Characterized by Inductive Biases toward a Data-Dependent Ridge Manifold</a><a hidden class=anchor aria-hidden=true href=#diffusion-models-generalization-can-be-characterized-by-inductive-biases-toward-a-data-dependent-ridge-manifoldhttpsarxivorgabs260206021v1>#</a></h3><p><strong>Authors:</strong> Ye He, Yitong Qiu, Molei Tao
<strong>Venue:</strong> arXiv (2026)</p><p>When a diffusion model is not memorizing the training data set, how does it generalize exactly? A quantitative understanding of the distribution it generates would be beneficial to, for example, an assessment of the model&rsquo;s performance for downstream applications. We thus explicitly characterize what diffusion model generates, by proposing a log-density ridge manifold and quantifying how the generated data relate to this manifold as inference dynamics progresses. More precisely, inference undergoes a reach-align-slide process centered around the ridge manifold: trajectories first reach a neighborhood of the manifold, then align as being pushed toward or away from the manifold in normal directions, and finally slide along the manifold in tangent directions. Within the scope of this general behavior, different training errors will lead to different normal and tangent motions, which can be quantified, and these detailed motions characterize when inter-mode generations emerge. More detailed understanding of training dynamics will lead to more accurate quantification of the generation inductive bias, and an example of random feature model will be considered, for which we can explicitly illustrate how diffusion model&rsquo;s inductive biases originate as a composition of architectural bias and training accuracy, and how they evolve with the inference dynamics. Experiments on synthetic multimodal distributions and MNIST latent diffusion support the predicted directional effects, in both low- and high-dimensions.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06021v1">üìÑ Download PDF</a></p><hr><h3 id=mambavf-state-space-model-for-efficient-video-fusionhttpsarxivorgabs260206017v1><a href=https://arxiv.org/abs/2602.06017v1>MambaVF: State Space Model for Efficient Video Fusion</a><a hidden class=anchor aria-hidden=true href=#mambavf-state-space-model-for-efficient-video-fusionhttpsarxivorgabs260206017v1>#</a></h3><p><strong>Authors:</strong> Zixiang Zhao, Yukun Cui, Lilun Deng, Haowen Bai, Haotong Qin, Tao Feng, Konrad Schindler
<strong>Venue:</strong> arXiv (2026)</p><p>Video fusion is a fundamental technique in various video processing tasks. However, existing video fusion methods heavily rely on optical flow estimation and feature warping, resulting in severe computational overhead and limited scalability. This paper presents MambaVF, an efficient video fusion framework based on state space models (SSMs) that performs temporal modeling without explicit motion estimation. First, by reformulating video fusion as a sequential state update process, MambaVF captures long-range temporal dependencies with linear complexity while significantly reducing computation and memory costs. Second, MambaVF proposes a lightweight SSM-based fusion module that replaces conventional flow-guided alignment via a spatio-temporal bidirectional scanning mechanism. This module enables efficient information aggregation across frames. Extensive experiments across multiple benchmarks demonstrate that our MambaVF achieves state-of-the-art performance in multi-exposure, multi-focus, infrared-visible, and medical video fusion tasks. We highlight that MambaVF enjoys high efficiency, reducing up to 92.25% of parameters and 88.79% of computational FLOPs and a 2.1x speedup compared to existing methods. Project page: <a href=https://mambavf.github.io>https://mambavf.github.io</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06017v1">üìÑ Download PDF</a></p><hr><h3 id=sage-benchmarking-and-improving-retrieval-for-deep-research-agentshttpsarxivorgabs260205975v1><a href=https://arxiv.org/abs/2602.05975v1>SAGE: Benchmarking and Improving Retrieval for Deep Research Agents</a><a hidden class=anchor aria-hidden=true href=#sage-benchmarking-and-improving-retrieval-for-deep-research-agentshttpsarxivorgabs260205975v1>#</a></h3><p><strong>Authors:</strong> Tiansheng Hu, Yilun Zhao, Canyu Zhang, Arman Cohan, Chen Zhao
<strong>Venue:</strong> arXiv (2026)</p><p>Deep research agents have emerged as powerful systems for addressing complex queries. Meanwhile, LLM-based retrievers have demonstrated strong capability in following instructions or reasoning. This raises a critical question: can LLM-based retrievers effectively contribute to deep research agent workflows? To investigate this, we introduce SAGE, a benchmark for scientific literature retrieval comprising 1,200 queries across four scientific domains, with a 200,000 paper retrieval corpus.We evaluate six deep research agents and find that all systems struggle with reasoning-intensive retrieval. Using DR Tulu as backbone, we further compare BM25 and LLM-based retrievers (i.e., ReasonIR and gte-Qwen2-7B-instruct) as alternative search tools. Surprisingly, BM25 significantly outperforms LLM-based retrievers by approximately 30%, as existing agents generate keyword-oriented sub-queries. To improve performance, we propose a corpus-level test-time scaling framework that uses LLMs to augment documents with metadata and keywords, making retrieval easier for off-the-shelf retrievers. This yields 8% and 2% gains on short-form and open-ended questions, respectively.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05975v1">üìÑ Download PDF</a></p><hr><h3 id=characterizing-human-semantic-navigation-in-concept-production-as-trajectories-in-embedding-spacehttpsarxivorgabs260205971v1><a href=https://arxiv.org/abs/2602.05971v1>Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space</a><a hidden class=anchor aria-hidden=true href=#characterizing-human-semantic-navigation-in-concept-production-as-trajectories-in-embedding-spacehttpsarxivorgabs260205971v1>#</a></h3><p><strong>Authors:</strong> Felipe D. Toro-Hern√°ndez, Jesuino Vieira Filho, Rodrigo M. Cabral-Carvalho
<strong>Venue:</strong> arXiv (2026)</p><p>Semantic representations can be framed as a structured, dynamic knowledge space through which humans navigate to retrieve and manipulate meaning. To investigate how humans traverse this geometry, we introduce a framework that represents concept production as navigation through embedding space. Using different transformer text embedding models, we construct participant-specific semantic trajectories based on cumulative embeddings and extract geometric and dynamical metrics, including distance to next, distance to centroid, entropy, velocity, and acceleration. These measures capture both scalar and directional aspects of semantic navigation, providing a computationally grounded view of semantic representation search as movement in a geometric space. We evaluate the framework on four datasets across different languages, spanning different property generation tasks: Neurodegenerative, Swear verbal fluency, Property listing task in Italian, and in German. Across these contexts, our approach distinguishes between clinical groups and concept types, offering a mathematical framework that requires minimal human intervention compared to typical labor-intensive linguistic pre-processing methods. Comparison with a non-cumulative approach reveals that cumulative embeddings work best for longer trajectories, whereas shorter ones may provide too little context, favoring the non-cumulative alternative. Critically, different embedding models yielded similar results, highlighting similarities between different learned representations despite different training pipelines. By framing semantic navigation as a structured trajectory through embedding space, bridging cognitive modeling with learned representation, thereby establishing a pipeline for quantifying semantic representation dynamics with applications in clinical research, cross-linguistic analysis, and the assessment of artificial cognition.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05971v1">üìÑ Download PDF</a></p><hr><h3 id=agentictagger-structured-item-representation-for-recommendation-with-llm-agentshttpsarxivorgabs260205945v1><a href=https://arxiv.org/abs/2602.05945v1>AgenticTagger: Structured Item Representation for Recommendation with LLM Agents</a><a hidden class=anchor aria-hidden=true href=#agentictagger-structured-item-representation-for-recommendation-with-llm-agentshttpsarxivorgabs260205945v1>#</a></h3><p><strong>Authors:</strong> Zhouhang Xie, Bo Peng, Zhankui He, Ziqi Chen, Alice Han, Isabella Ye, Benjamin Coleman, Noveen Sachdeva, Fernando Pereira, Julian McAuley, Wang-Cheng Kang, Derek Zhiyuan Cheng, Beidou Wang, Randolph Brown
<strong>Venue:</strong> arXiv (2026)</p><p>High-quality representations are a core requirement for effective recommendation. In this work, we study the problem of LLM-based descriptor generation, i.e., keyphrase-like natural language item representation generation frameworks with minimal constraints on downstream applications. We propose AgenticTagger, a framework that queries LLMs for representing items with sequences of text descriptors. However, open-ended generation provides little control over the generation space, leading to high cardinality, low-performance descriptors that renders downstream modeling challenging. To this end, AgenticTagger features two core stages: (1) a vocabulary building stage where a set of hierarchical, low-cardinality, and high-quality descriptors is identified, and (2) a vocabulary assignment stage where LLMs assign in-vocabulary descriptors to items. To effectively and efficiently ground vocabulary in the item corpus of interest, we design a multi-agent reflection mechanism where an architect LLM iteratively refines the vocabulary guided by parallelized feedback from annotator LLMs that validates the vocabulary against item data. Experiments on public and private data show AgenticTagger brings consistent improvements across diverse recommendation scenarios, including generative and term-based retrieval, ranking, and controllability-oriented, critique-based recommendation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05945v1">üìÑ Download PDF</a></p><hr><h2 id=-linguistics>üîç linguistics<a hidden class=anchor aria-hidden=true href=#-linguistics>#</a></h2><h3 id=pseudo-invertible-neural-networkshttpsarxivorgabs260206042v1><a href=https://arxiv.org/abs/2602.06042v1>Pseudo-Invertible Neural Networks</a><a hidden class=anchor aria-hidden=true href=#pseudo-invertible-neural-networkshttpsarxivorgabs260206042v1>#</a></h3><p><strong>Authors:</strong> Yamit Ehrlich, Nimrod Berman, Assaf Shocher
<strong>Venue:</strong> arXiv (2026)</p><p>The Moore-Penrose Pseudo-inverse (PInv) serves as the fundamental solution for linear systems. In this paper, we propose a natural generalization of PInv to the nonlinear regime in general and to neural networks in particular. We introduce Surjective Pseudo-invertible Neural Networks (SPNN), a class of architectures explicitly designed to admit a tractable non-linear PInv. The proposed non-linear PInv and its implementation in SPNN satisfy fundamental geometric properties. One such property is null-space projection or &ldquo;Back-Projection&rdquo;, $x&rsquo; = x + A^\dagger(y-Ax)$, which moves a sample $x$ to its closest consistent state $x&rsquo;$ satisfying $Ax=y$. We formalize Non-Linear Back-Projection (NLBP), a method that guarantees the same consistency constraint for non-linear mappings $f(x)=y$ via our defined PInv. We leverage SPNNs to expand the scope of zero-shot inverse problems. Diffusion-based null-space projection has revolutionized zero-shot solving for linear inverse problems by exploiting closed-form back-projection. We extend this method to non-linear degradations. Here, &ldquo;degradation&rdquo; is broadly generalized to include any non-linear loss of information, spanning from optical distortions to semantic abstractions like classification. This approach enables zero-shot inversion of complex degradations and allows precise semantic control over generative outputs without retraining the diffusion prior.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06042v1">üìÑ Download PDF</a></p><hr><h3 id=context-forcing-consistent-autoregressive-video-generation-with-long-contexthttpsarxivorgabs260206028v1><a href=https://arxiv.org/abs/2602.06028v1>Context Forcing: Consistent Autoregressive Video Generation with Long Context</a><a hidden class=anchor aria-hidden=true href=#context-forcing-consistent-autoregressive-video-generation-with-long-contexthttpsarxivorgabs260206028v1>#</a></h3><p><strong>Authors:</strong> Shuo Chen, Cong Wei, Sun Sun, Ping Nie, Kai Zhou, Ge Zhang, Ming-Hsuan Yang, Wenhu Chen
<strong>Venue:</strong> arXiv (2026)</p><p>Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical \textbf{student-teacher mismatch}: the teacher&rsquo;s inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student&rsquo;s context length. To resolve this, we propose \textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds &ndash; 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06028v1">üìÑ Download PDF</a></p><hr><h3 id=act-dr6planck-impact-on-inflation-with-non-zero-vacuum-expectation-value-and-the-post-inflationary-behaviorhttpsarxivorgabs260206027v1><a href=https://arxiv.org/abs/2602.06027v1>ACT DR6+Planck impact on inflation with non-zero vacuum expectation value and the post-inflationary behavior</a><a hidden class=anchor aria-hidden=true href=#act-dr6planck-impact-on-inflation-with-non-zero-vacuum-expectation-value-and-the-post-inflationary-behaviorhttpsarxivorgabs260206027v1>#</a></h3><p><strong>Authors:</strong> F. B. M. dos Santos, J. G. Rodrigues, G. Rodrigues, C. Siqueira, J. S. Alcaniz
<strong>Venue:</strong> arXiv (2026)</p><p>The impact of the most recent cosmic microwave background (CMB) data from the Atacama Cosmology Telescope (ACT) is studied for a model of cosmic inflation which predicts a non-zero vacuum expectation value (VEV) $M$ for a large-field regime. Since lower values of $M$ are compatible with the higher spectral index $n_s$ provided by the ACT+Planck joint analysis, we establish new limits on this parameter while also considering further CMB data from the latest BICEP/Keck Array release for CMB polarization modes. We find $\log_{10}M/M_{Pl}=-2.5^{+1.1}<em>{-1.3}$ at 68% confidence level, compatible with $M/M</em>{Pl}\simeq 0.003$, which is interesting for post-inflationary processes, such as preheating. We conduct a lattice simulation for the inflaton field for the first few e-folds, as the model is compatible with the production of relics such as oscillons, which are possible candidates as sources of gravitational waves and primordial black holes. We find that the model indeed produces localized, quasi-spherical structures compatible with oscillons, which might lead to signatures detectable by future experiments. However, in agreement with recent works, we find that although the abundance of gravitational waves that could be generated in this regime has an amplitude well within the sensitivities of these detectors, the frequency range is on the GHz limit, away from the expected frequencies. Finally, we estimate the impact of a coupling of the type $yœÜœÉ^2$ to the inflaton, in the realization of perturbative reheating, directly impacting the predictions of the model, as lower values of $M$ are consistent with both the entire allowed temperature range, and the limits imposed by BICEP/Keck Array+Planck+ACT.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06027v1">üìÑ Download PDF</a></p><hr><h3 id=learning-query-aware-budget-tier-routing-for-runtime-agent-memoryhttpsarxivorgabs260206025v1><a href=https://arxiv.org/abs/2602.06025v1>Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory</a><a hidden class=anchor aria-hidden=true href=#learning-query-aware-budget-tier-routing-for-runtime-agent-memoryhttpsarxivorgabs260206025v1>#</a></h3><p><strong>Authors:</strong> Haozhen Zhang, Haodong Yue, Tao Feng, Quanyu Long, Jianzhu Bao, Bowen Jin, Weizhi Zhang, Xiao Li, Jiaxuan You, Chengwei Qin, Wenya Wang
<strong>Venue:</strong> arXiv (2026)</p><p>Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \textsc{Low}/\textsc{Mid}/\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06025v1">üìÑ Download PDF</a></p><hr><h3 id=mechanisms-of-ai-protein-folding-in-esmfoldhttpsarxivorgabs260206020v1><a href=https://arxiv.org/abs/2602.06020v1>Mechanisms of AI Protein Folding in ESMFold</a><a hidden class=anchor aria-hidden=true href=#mechanisms-of-ai-protein-folding-in-esmfoldhttpsarxivorgabs260206020v1>#</a></h3><p><strong>Authors:</strong> Kevin Lu, Jannik Brinkmann, Stefan Huber, Aaron Mueller, Yonatan Belinkov, David Bau, Chris Wendler
<strong>Venue:</strong> arXiv (2026)</p><p>How do protein structure prediction models fold proteins? We investigate this question by tracing how ESMFold folds a beta hairpin, a prevalent structural motif. Through counterfactual interventions on model latents, we identify two computational stages in the folding trunk. In the first stage, early blocks initialize pairwise biochemical signals: residue identities and associated biochemical features such as charge flow from sequence representations into pairwise representations. In the second stage, late blocks develop pairwise spatial features: distance and contact information accumulate in the pairwise representation. We demonstrate that the mechanisms underlying structural decisions of ESMFold can be localized, traced through interpretable representations, and manipulated with strong causal effects.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06020v1">üìÑ Download PDF</a></p><hr><h3 id=convex-unions-and-completions-from-simplicial-pseudomanifoldshttpsarxivorgabs260206016v1><a href=https://arxiv.org/abs/2602.06016v1>Convex unions and completions from simplicial pseudomanifolds</a><a hidden class=anchor aria-hidden=true href=#convex-unions-and-completions-from-simplicial-pseudomanifoldshttpsarxivorgabs260206016v1>#</a></h3><p><strong>Authors:</strong> Soohyun Park
<strong>Venue:</strong> arXiv (2026)</p><p>While intersections of convex sets are convex, their unions have rather complicated behavior. Some natural contexts where they appear include duality arguments involving boundaries of convex sets and valuations, which have an Euler characteristic-like structure. However, there are certain settings where the convexity property itself is important to consider. For example, this includes (preservation of) positivity properties of divisors on toric varieties under blowdowns. In the case of (restrictions of) conormal bundles, this can be interpreted in terms of interactions between local convexity data stored in rational equivalence relations. We consider generalizations to realizations of simplicial pseudomanifolds and replace rational equivalence with effects of PL homeomorphisms.
Decomposing the PL homeomorphisms into edge subdivisions and contractions, we characterize the space of suitable contraction points compatible with local convexity properties in terms of convex unions and completions. This gives rise to certain external edge subdivisions that make this <code>contraction space'' of the starting edge empty, which is unexpected given the expected </code>increased convexity&rsquo;&rsquo; from edge subdivisions. We also obtain strong affine/linear restrictions on realizations of facets containing nearby edges preserving local convexity. This implies that contracting certain nearby edges results in a very large or very small contraction space of the starting edge. As for boundary behavior, there are parallels between effects of PL homeomorphisms on induced 4-cycles in the 1-skeleton. Finally, we find effects of PL homeomorphisms and suspensions on analogues of local convexity properties stored by linear systems of parameters. This indicates that simplicial spheres PL homeomorphic to the boundary of a cross polytope store record local convexity changes in the most natural way.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06016v1">üìÑ Download PDF</a></p><hr><h3 id=agenticpay-a-multi-agent-llm-negotiation-system-for-buyer-seller-transactionshttpsarxivorgabs260206008v1><a href=https://arxiv.org/abs/2602.06008v1>AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions</a><a hidden class=anchor aria-hidden=true href=#agenticpay-a-multi-agent-llm-negotiation-system-for-buyer-seller-transactionshttpsarxivorgabs260206008v1>#</a></h3><p><strong>Authors:</strong> Xianyang Liu, Shangding Gu, Dawn Song
<strong>Venue:</strong> arXiv (2026)</p><p>Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: <a href=https://github.com/SafeRL-Lab/AgenticPay>https://github.com/SafeRL-Lab/AgenticPay</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06008v1">üìÑ Download PDF</a></p><hr><h3 id=quantum-noise-scaling-in-continuously-operating-multiparameter-sensorshttpsarxivorgabs260205991v1><a href=https://arxiv.org/abs/2602.05991v1>Quantum noise scaling in continuously operating multiparameter sensors</a><a hidden class=anchor aria-hidden=true href=#quantum-noise-scaling-in-continuously-operating-multiparameter-sensorshttpsarxivorgabs260205991v1>#</a></h3><p><strong>Authors:</strong> Aleksandra Sierant, Diana M√©ndez-Avalos, Santiago Tabares Giraldo, Morgan W. Mitchell
<strong>Venue:</strong> arXiv (2026)</p><p>We experimentally investigate the quantum noise mechanisms that limit continuously operating multiparameter quantum sensors. Using a hybrid rf-dc optically pumped magnetometer, we map the photon shot noise, spin projection noise, and measurement back-action noise over an order of magnitude in probe power and a factor of three in pump power while remaining quantum-noise-limited. We observe linear, quadratic, and cubic scaling of the respective total noise powers with probe photon flux, together with a quadratic dependence of back-action on pump photon flux, in quantitative agreement with a stochastic Bloch-equation model. At higher probe powers, additional probe-induced relaxation modifies the spin-noise spectrum while preserving the integrated noise scaling. Our results reveal fundamental, resource-dependent trade-offs unique to continuously monitored multiparameter sensors and establish experimentally the quantum limits governing their optimal operation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05991v1">üìÑ Download PDF</a></p><hr><h3 id=geometry-and-dynamical-morphology-of-growing-bacterial-colonieshttpsarxivorgabs260205958v1><a href=https://arxiv.org/abs/2602.05958v1>Geometry and dynamical morphology of growing bacterial colonies</a><a hidden class=anchor aria-hidden=true href=#geometry-and-dynamical-morphology-of-growing-bacterial-colonieshttpsarxivorgabs260205958v1>#</a></h3><p><strong>Authors:</strong> Benjamin Evert Himberg, Sanghita Sengupta
<strong>Venue:</strong> arXiv (2026)</p><p>We study non-equilibrium bacterial colony growth using a geometry-first, time-resolved analysis of morphology. From time-lapse microscopy data, we track the coupled evolution of area, perimeter, and boundary-sensitive shape descriptors along the full growth history. We find that non-equilibrium growth can exhibit extended intervals of compact area&ndash;perimeter scaling with exponent $Œ±\approx 2$, consistent with growth governed by a single effective geometric length scale, as well as time-localized breakdowns of this scaling during ongoing growth. These breakdowns coincide with transient boundary reorganization while bulk area growth remains sustained. Our results demonstrate that visually distinct morphologies can arise within the same geometric growth regime, and that departures from single-scale behavior reflect intrinsic dynamical restructuring rather than growth arrest. More broadly, this work establishes time-resolved geometry as a coarse-grained framework for identifying when non-equilibrium growth departs from single-scale geometric constraints in living systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05958v1">üìÑ Download PDF</a></p><hr><h3 id=a-powheg-generator-for-di-jet-production-in-polarized-proton-proton-collisionshttpsarxivorgabs260205949v1><a href=https://arxiv.org/abs/2602.05949v1>A POWHEG generator for di-jet production in polarized proton-proton collisions</a><a hidden class=anchor aria-hidden=true href=#a-powheg-generator-for-di-jet-production-in-polarized-proton-proton-collisionshttpsarxivorgabs260205949v1>#</a></h3><p><strong>Authors:</strong> Ignacio Borsa, David Betz, Barbara J√§ger
<strong>Venue:</strong> arXiv (2026)</p><p>We present a new Monte-Carlo generator for the simulation of di-jet production in polarized proton-proton collisions at the next-to-leading order in QCD matched to parton showers using the framework of the POWHEG BOX. With this program we compute a variety of observables of immediate relevance for the spin program of the Relativistic Heavy Ion Collider at Brookhaven National Laboratory. While parton-shower effects are generally small, we find that in some search regions their inclusion improves agreement of predictions with data. Moreover, we provide a critical assessment of selection criteria applied in experiment in the light of perturbative stability.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05949v1">üìÑ Download PDF</a></p><hr><h3 id=interprior-scaling-generative-control-for-physics-based-human-object-interactionshttpsarxivorgabs260206035v1><a href=https://arxiv.org/abs/2602.06035v1>InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions</a><a hidden class=anchor aria-hidden=true href=#interprior-scaling-generative-control-for-physics-based-human-object-interactionshttpsarxivorgabs260206035v1>#</a></h3><p><strong>Authors:</strong> Sirui Xu, Samuel Schulter, Morteza Ziyadi, Xialin He, Xiaohan Fei, Yu-Xiong Wang, Liangyan Gui
<strong>Venue:</strong> arXiv (2026)</p><p>Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06035v1">üìÑ Download PDF</a></p><hr><h3 id=can-vision-language-models-learn-intuitive-physics-from-interactionhttpsarxivorgabs260206033v1><a href=https://arxiv.org/abs/2602.06033v1>Can vision language models learn intuitive physics from interaction?</a><a hidden class=anchor aria-hidden=true href=#can-vision-language-models-learn-intuitive-physics-from-interactionhttpsarxivorgabs260206033v1>#</a></h3><p><strong>Authors:</strong> Luca M. Schulze Buschoff, Konstantinos Voudouris, Can Demircan, Eric Schulz
<strong>Venue:</strong> arXiv (2026)</p><p>Pre-trained vision language models do not have good intuitions about the physical world. Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks. However, fine-tuned models do not appear to learn robust physical rules that can generalize to new contexts. Based on research in cognitive science, we hypothesize that models need to interact with an environment to properly learn its physical dynamics. We train models that learn through interaction with the environment using reinforcement learning. While learning from interaction allows models to improve their within-task performance, it fails to produce models with generalizable physical intuitions. We find that models trained on one task do not reliably generalize to related tasks, even if the tasks share visual statistics and physical principles, and regardless of whether the models are trained through interaction.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06033v1">üìÑ Download PDF</a></p><hr><h3 id=splat-and-distill-augmenting-teachers-with-feed-forward-3d-reconstruction-for-3d-aware-distillationhttpsarxivorgabs260206032v1><a href=https://arxiv.org/abs/2602.06032v1>Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation</a><a hidden class=anchor aria-hidden=true href=#splat-and-distill-augmenting-teachers-with-feed-forward-3d-reconstruction-for-3d-aware-distillationhttpsarxivorgabs260206032v1>#</a></h3><p><strong>Authors:</strong> David Shavin, Sagie Benaim
<strong>Venue:</strong> arXiv (2026)</p><p>Vision Foundation Models (VFMs) have achieved remarkable success when applied to various downstream 2D tasks. Despite their effectiveness, they often exhibit a critical lack of 3D awareness. To this end, we introduce Splat and Distill, a framework that instills robust 3D awareness into 2D VFMs by augmenting the teacher model with a fast, feed-forward 3D reconstruction pipeline. Given 2D features produced by a teacher model, our method first lifts these features into an explicit 3D Gaussian representation, in a feedforward manner. These 3D features are then <code>splatted" onto novel viewpoints, producing a set of novel 2D feature maps used to supervise the student model, </code>distilling" geometrically grounded knowledge. By replacing slow per-scene optimization of prior work with our feed-forward lifting approach, our framework avoids feature-averaging artifacts, creating a dynamic learning process where the teacher&rsquo;s consistency improves alongside that of the student. We conduct a comprehensive evaluation on a suite of downstream tasks, including monocular depth estimation, surface normal estimation, multi-view correspondence, and semantic segmentation. Our method significantly outperforms prior works, not only achieving substantial gains in 3D awareness but also enhancing the underlying semantic richness of 2D features. Project page is available at <a href=https://davidshavin4.github.io/Splat-and-Distill/>https://davidshavin4.github.io/Splat-and-Distill/</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06032v1">üìÑ Download PDF</a></p><hr><h3 id=a-systematic-evaluation-of-large-language-models-for-ptsd-severity-estimation-the-role-of-contextual-knowledge-and-modeling-strategieshttpsarxivorgabs260206015v1><a href=https://arxiv.org/abs/2602.06015v1>A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies</a><a hidden class=anchor aria-hidden=true href=#a-systematic-evaluation-of-large-language-models-for-ptsd-severity-estimation-the-role-of-contextual-knowledge-and-modeling-strategieshttpsarxivorgabs260206015v1>#</a></h3><p><strong>Authors:</strong> Panagiotis Kaliosis, Adithya V Ganesan, Oscar N. E. Kjell, Whitney Ringwald, Scott Feltman, Melissa A. Carr, Dimitris Samaras, Camilo Ruggero, Benjamin J. Luft, Roman Kotov, Andrew H. Schwartz
<strong>Venue:</strong> arXiv (2026)</p><p>Large language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracy. In this study, we utilize a clinical dataset of natural language narratives and self-reported PTSD severity scores from 1,437 individuals to comprehensively evaluate the performance of 11 state-of-the-art LLMs. To understand the factors affecting accuracy, we systematically varied (i) contextual knowledge like subscale definitions, distribution summary, and interview questions, and (ii) modeling strategies including zero-shot vs few shot, amount of reasoning effort, model sizes, structured subscales vs direct scalar prediction, output rescaling and nine ensemble methods. Our findings indicate that (a) LLMs are most accurate when provided with detailed construct definitions and context of the narrative; (b) increased reasoning effort leads to better estimation accuracy; (c) performance of open-weight models (Llama, Deepseek), plateau beyond 70B parameters while closed-weight (o3-mini, gpt-5) models improve with newer generations; and (d) best performance is achieved when ensembling a supervised model with the zero-shot LLMs. Taken together, the results suggest choice of contextual knowledge and modeling strategies is important for deploying LLMs to accurately assess mental health.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06015v1">üìÑ Download PDF</a></p><hr><h3 id=lattices-from-pointed-building-sets-generalized-ornamentation-latticeshttpsarxivorgabs260206004v1><a href=https://arxiv.org/abs/2602.06004v1>Lattices from Pointed Building Sets: Generalized Ornamentation Lattices</a><a hidden class=anchor aria-hidden=true href=#lattices-from-pointed-building-sets-generalized-ornamentation-latticeshttpsarxivorgabs260206004v1>#</a></h3><p><strong>Authors:</strong> Andrew Sack
<strong>Venue:</strong> arXiv (2026)</p><p>We introduce a novel combinatorial structure called pointed building sets, which can be viewed as families of lattices equipped with compatibility relations. To each pointed building set $\mathsf{B}$, we associate a complete lattice $\mathcal{O}(\mathsf{B})$, called the ornamentation lattice of $\mathsf{B}$.
Special cases of this construction have already proven useful in understanding the structure of three families of posets: operahedron lattices, the affine Tamari lattice, and hypergraphic posets of subhypergraphs of the path hypergraph of an increasing tree.
The goal of this paper is to establish the theory of these generalized ornamentations. We examine several natural classes of pointed building sets which recover classical lattices such as the Tamari lattice, the lattice of topologies ordered by coarsening, and the lattice of naturally labeled partial orders. Furthermore, several theoretical directions are explored, including inverse limits and group actions. Notably, this leads to a straightforward construction of inverse limits of Tamari lattices, yielding infinite analogs of the Tamari lattice.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06004v1">üìÑ Download PDF</a></p><hr><h3 id=clouding-the-mirror-stealthy-prompt-injection-attacks-targeting-llm-based-phishing-detectionhttpsarxivorgabs260205484v1><a href=https://arxiv.org/abs/2602.05484v1>Clouding the Mirror: Stealthy Prompt Injection Attacks Targeting LLM-based Phishing Detection</a><a hidden class=anchor aria-hidden=true href=#clouding-the-mirror-stealthy-prompt-injection-attacks-targeting-llm-based-phishing-detectionhttpsarxivorgabs260205484v1>#</a></h3><p><strong>Authors:</strong> Takashi Koide, Hiroki Nakano, Daiki Chiba
<strong>Venue:</strong> arXiv (2026)</p><p>Phishing sites continue to grow in volume and sophistication. Recent work leverages large language models (LLMs) to analyze URLs, HTML, and rendered content to decide whether a website is a phishing site. While these approaches are promising, LLMs are inherently vulnerable to prompt injection (PI). Because attackers can fully control various elements of phishing sites, this creates the potential for PI that exploits the perceptual asymmetry between LLMs and humans: instructions imperceptible to end users can still be parsed by the LLM and can stealthily manipulate its judgment. The specific risks of PI in phishing detection and effective mitigation strategies remain largely unexplored. This paper presents the first comprehensive evaluation of PI against multimodal LLM-based phishing detection. We introduce a two-dimensional taxonomy, defined by Attack Techniques and Attack Surfaces, that captures realistic PI strategies. Using this taxonomy, we implement diverse attacks and empirically study several representative LLM-based detection systems. The results show that phishing detection with state-of-the-art models such as GPT-5 remains vulnerable to PI. We then propose InjectDefuser, a defense framework that combines prompt hardening, allowlist-based retrieval augmentation, and output validation. Across multiple models, InjectDefuser significantly reduces attack success rates. Our findings clarify the PI risk landscape and offer practical defenses that improve the reliability of next-generation phishing countermeasures.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05484v1">üìÑ Download PDF</a></p><hr><h3 id=beyond-length-context-aware-expansion-and-independence-as-developmentally-sensitive-evaluation-in-child-utteranceshttpsarxivorgabs260205392v1><a href=https://arxiv.org/abs/2602.05392v1>Beyond Length: Context-Aware Expansion and Independence as Developmentally Sensitive Evaluation in Child Utterances</a><a hidden class=anchor aria-hidden=true href=#beyond-length-context-aware-expansion-and-independence-as-developmentally-sensitive-evaluation-in-child-utteranceshttpsarxivorgabs260205392v1>#</a></h3><p><strong>Authors:</strong> Jiyun Chun, Eric Fosler-Lussier, Michael White, Andrew Perrault
<strong>Venue:</strong> arXiv (2026)</p><p>Evaluating the quality of children&rsquo;s utterances in adult-child dialogue remains challenging due to insufficient context-sensitive metrics. Common proxies such as Mean Length of Utterance (MLU), lexical diversity (vocd-D), and readability indices (Flesch-Kincaid Grade Level, Gunning Fog Index) are dominated by length and ignore conversational context, missing aspects of response quality such as reasoning depth, topic maintenance, and discourse planning. We introduce an LLM-as-a-judge framework that first classifies the Previous Adult Utterance Type and then scores the child&rsquo;s response along two axes: Expansion (contextual elaboration and inferential depth) and Independence (the child&rsquo;s contribution to advancing the discourse). These axes reflect fundamental dimensions in child language development, where Expansion captures elaboration, clause combining, and causal and contrastive connectives. Independence captures initiative, topic control, decreasing reliance on adult scaffolding through growing self-regulation, and audience design. We establish developmental validity by showing age-related patterns and demonstrate predictive value by improving age estimation over common baselines. We further confirm semantic sensitivity by detecting differences tied to discourse relations. Our metrics align with human judgments, enabling large-scale evaluation. This shifts child utterance assessment from simply measuring length to evaluating how meaningfully the child&rsquo;s speech contributes to and advances the conversation within its context.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05392v1">üìÑ Download PDF</a></p><hr><h3 id=dolphin-v2-universal-document-parsing-via-scalable-anchor-promptinghttpsarxivorgabs260205384v1><a href=https://arxiv.org/abs/2602.05384v1>Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting</a><a hidden class=anchor aria-hidden=true href=#dolphin-v2-universal-document-parsing-via-scalable-anchor-promptinghttpsarxivorgabs260205384v1>#</a></h3><p><strong>Authors:</strong> Hao Feng, Wei Shi, Ke Zhang, Xiang Fei, Lei Liao, Dingkang Yang, Yongkun Du, Xuecheng Wu, Jingqun Tang, Yang Liu, Hong Chen, Can Huang
<strong>Venue:</strong> arXiv (2026)</p><p>Document parsing has garnered widespread attention as vision-language models (VLMs) advance OCR capabilities. However, the field remains fragmented across dozens of specialized models with varying strengths, forcing users to navigate complex model selection and limiting system scalability. Moreover, existing two-stage approaches depend on axis-aligned bounding boxes for layout detection, failing to handle distorted or photographed documents effectively. To this end, we present Dolphin-v2, a two-stage document image parsing model that substantially improves upon the original Dolphin. In the first stage, Dolphin-v2 jointly performs document type classification (digital-born versus photographed) alongside layout analysis. For digital-born documents, it conducts finer-grained element detection with reading order prediction. In the second stage, we employ a hybrid parsing strategy: photographed documents are parsed holistically as complete pages to handle geometric distortions, while digital-born documents undergo element-wise parallel parsing guided by the detected layout anchors, enabling efficient content extraction. Compared with the original Dolphin, Dolphin-v2 introduces several crucial enhancements: (1) robust parsing of photographed documents via holistic page-level understanding, (2) finer-grained element detection (21 categories) with semantic attribute extraction such as author information and document metadata, and (3) code block recognition with indentation preservation, which existing systems typically lack. Comprehensive evaluations are conducted on DocPTBench, OmniDocBench, and our self-constructed RealDoc-160 benchmark. The results demonstrate substantial improvements: +14.78 points overall on the challenging OmniDocBench and 91% error reduction on photographed documents, while maintaining efficient inference through parallel processing.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05384v1">üìÑ Download PDF</a></p><hr><h3 id=deepread-document-structure-aware-reasoning-to-enhance-agentic-searchhttpsarxivorgabs260205014v1><a href=https://arxiv.org/abs/2602.05014v1>DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search</a><a hidden class=anchor aria-hidden=true href=#deepread-document-structure-aware-reasoning-to-enhance-agentic-searchhttpsarxivorgabs260205014v1>#</a></h3><p><strong>Authors:</strong> Zhanli Li, Huiwen Tian, Lvzhou Luo, Yixuan Cao, Ping Luo
<strong>Venue:</strong> arXiv (2026)</p><p>With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search frameworks commonly treat long documents as flat collections of chunks, underutilizing document-native priors such as hierarchical organization and sequential discourse structure. We introduce DeepRead, a structure-aware, multi-turn document reasoning agent that explicitly operationalizes these priors for long-document question answering. DeepRead leverages LLM-based OCR model to convert PDFs into structured Markdown that preserves headings and paragraph boundaries. It then indexes documents at the paragraph level and assigns each paragraph a coordinate-style metadata key encoding its section identity and in-section order. Building on this representation, DeepRead equips the LLM with two complementary tools: a Retrieve tool that localizes relevant paragraphs while exposing their structural coordinates (with lightweight scanning context), and a ReadSection tool that enables contiguous, order-preserving reading within a specified section and paragraph range. Our experiments demonstrate that DeepRead achieves significant improvements over Search-o1-style agentic search in document question answering. The synergistic effect between retrieval and reading tools is also validated. Our fine-grained behavioral analysis reveals a reading and reasoning paradigm resembling human-like ``locate then read&rsquo;&rsquo; behavior.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05014v1">üìÑ Download PDF</a></p><hr><h3 id=exploiting-contextual-information-to-improve-stance-detection-in-informal-political-discourse-with-llmshttpsarxivorgabs260204750v1><a href=https://arxiv.org/abs/2602.04750v1>Exploiting contextual information to improve stance detection in informal political discourse with LLMs</a><a hidden class=anchor aria-hidden=true href=#exploiting-contextual-information-to-improve-stance-detection-in-informal-political-discourse-with-llmshttpsarxivorgabs260204750v1>#</a></h3><p><strong>Authors:</strong> Arman Engin Sucu, Yixiang Zhou, Mario A. Nascimento, Tony Mullen
<strong>Venue:</strong> arXiv (2026)</p><p>This study investigates the use of Large Language Models (LLMs) for political stance detection in informal online discourse, where language is often sarcastic, ambiguous, and context-dependent. We explore whether providing contextual information, specifically user profile summaries derived from historical posts, can improve classification accuracy. Using a real-world political forum dataset, we generate structured profiles that summarize users&rsquo; ideological leaning, recurring topics, and linguistic patterns. We evaluate seven state-of-the-art LLMs across baseline and context-enriched setups through a comprehensive cross-model evaluation. Our findings show that contextual prompts significantly boost accuracy, with improvements ranging from +17.5% to +38.5%, achieving up to 74% accuracy that surpasses previous approaches. We also analyze how profile size and post selection strategies affect performance, showing that strategically chosen political content yields better results than larger, randomly selected contexts. These findings underscore the value of incorporating user-level context to enhance LLM performance in nuanced political classification tasks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.04750v1">üìÑ Download PDF</a></p><hr><h3 id=rise-video-can-video-generators-decode-implicit-world-ruleshttpsarxivorgabs260205986v1><a href=https://arxiv.org/abs/2602.05986v1>RISE-Video: Can Video Generators Decode Implicit World Rules?</a><a hidden class=anchor aria-hidden=true href=#rise-video-can-video-generators-decode-implicit-world-ruleshttpsarxivorgabs260205986v1>#</a></h3><p><strong>Authors:</strong> Mingxin Liu, Shuran Ma, Shibei Meng, Xiangyu Zhao, Zicheng Zhang, Shaofeng Zhang, Zhihang Zhong, Peixian Chen, Haoyu Cao, Xing Sun, Haodong Duan, Xue Yang
<strong>Venue:</strong> arXiv (2026)</p><p>While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \textit{Reasoning Alignment}, \textit{Temporal Consistency}, \textit{Physical Rationality}, and \textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05986v1">üìÑ Download PDF</a></p><hr><h3 id=does-cosmology-require-hermiticity-in-quantum-mechanicshttpsarxivorgabs260205973v1><a href=https://arxiv.org/abs/2602.05973v1>Does Cosmology require Hermiticity in Quantum Mechanics?</a><a hidden class=anchor aria-hidden=true href=#does-cosmology-require-hermiticity-in-quantum-mechanicshttpsarxivorgabs260205973v1>#</a></h3><p><strong>Authors:</strong> Oem Trivedi, Alfredo Gurrola
<strong>Venue:</strong> arXiv (2026)</p><p>We explore the consequences of allowing non-Hermitian structures in quantum cosmology by extending the Wheeler DeWitt framework beyond strictly Hermitian dynamics. Using a controlled semiclassical reduction, we show how anti Hermitian contributions propagate into both early universe primordial fluctuations and late-time structure growth as effective damping or gain terms. Confronting this framework with inflationary observables, growth of structure and the observed near flatness of the universe, we derive strong infrared constraints that suppress non Hermiticity across cosmic history. We demonstrate that these bounds are mutually consistent between early and late-time probes and can be partially relaxed in theories beyond General Relativity. Our results establish cosmology as a novel arena for testing foundational aspects of quantum mechanics and suggest that Hermiticity may emerge dynamically along the semiclassical branch describing our universe.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05973v1">üìÑ Download PDF</a></p><hr><h3 id=transformers-are-born-biased-structural-inductive-biases-at-random-initialization-and-their-practical-consequenceshttpsarxivorgabs260205927v1><a href=https://arxiv.org/abs/2602.05927v1>Transformers Are Born Biased: Structural Inductive Biases at Random Initialization and Their Practical Consequences</a><a hidden class=anchor aria-hidden=true href=#transformers-are-born-biased-structural-inductive-biases-at-random-initialization-and-their-practical-consequenceshttpsarxivorgabs260205927v1>#</a></h3><p><strong>Authors:</strong> Siquan Li, Yao Tong, Haonan Wang, Tianyang Hu
<strong>Venue:</strong> arXiv (2026)</p><p>Transformers underpin modern large language models (LLMs) and are commonly assumed to be behaviorally unstructured at random initialization, with all meaningful preferences emerging only through large-scale training. We challenge this assumption by showing that randomly initialized transformers already exhibit strong and systematic structural biases. In particular, untrained models display extreme token preferences: across random input sequences, certain tokens are predicted with probabilities orders of magnitude larger.
We provide a mechanistic explanation for this phenomenon by dissecting the transformer architecture at initialization. We show that extreme token preference arises from a contraction of token representations along a random seed-dependent direction. This contraction is driven by two interacting forces: (i) asymmetric nonlinear activations in MLP sublayers induce global (inter-sequence) representation concentration, and (ii) self-attention further amplifies this effect through local (intra-sequence) aggregation. Together, these mechanisms align hidden representations along a direction determined solely by the random initialization, producing highly non-uniform next-token predictions.
Beyond mechanistic insight, we demonstrate that these initialization-induced biases persist throughout training, forming a stable and intrinsic model identity. Leveraging this property, we introduce SeedPrint, a fingerprinting method that can reliably distinguish models that differ only in their random initialization, even after extensive training and under substantial distribution shift. Finally, we identify a fundamental positional discrepancy inherent to the attention mechanism&rsquo;s intra-sequence contraction that is causally linked to the attention-sink phenomenon. This discovery provides a principled explanation for the emergence of sinks and offers a pathway for their control.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05927v1">üìÑ Download PDF</a></p><hr><h3 id=on-particle-dynamics-in-steady-axial-rotor-flowshttpsarxivorgabs260205872v1><a href=https://arxiv.org/abs/2602.05872v1>On particle dynamics in steady axial rotor flows</a><a hidden class=anchor aria-hidden=true href=#on-particle-dynamics-in-steady-axial-rotor-flowshttpsarxivorgabs260205872v1>#</a></h3><p><strong>Authors:</strong> Francesco Caccia, Alberto Guardone
<strong>Venue:</strong> arXiv (2026)</p><p>We investigate the effect of rotor velocity induction on the distribution of particles impinging on rotor blades and model the delayed response of a particle to the rotor-induced velocity field. We consider as reference a wind turbine rotor and a small-scale propeller in axial flow conditions. We first show that the classical 2D modeling of the multi-phase flow can generate a systematic error with respect to the 3D solution. We consider two limiting cases: particles in equilibrium with the rotor-induced velocity field, where the carrier phase is computed using the section&rsquo;s aerodynamic velocity vector, and induction-independent particles, where the geometric velocity vector is used. The 3D solution differs from the two limiting cases when particles are in partial equilibrium with the induced velocity. We introduce an induction Stokes number $\mathit{Stk}<em>\text{ind}$ and identify a transition regime between the two limiting solutions for $0.1 \lesssim \mathit{Stk}</em>\text{ind} \lesssim 10$. Then, we present a simple 1D delay model to evaluate the induced component of the particle velocity at the rotor disk as a function $\mathit{Stk}_\text{ind}$. We validate the model by showing that it allows capturing the transition regime in 2D simulations. The model only requires knowledge of the aerodynamic and geometric velocity vectors, i.e., of the axial and tangential induction factors.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05872v1">üìÑ Download PDF</a></p><hr><h3 id=odysseyarena-benchmarking-large-language-models-for-long-horizon-active-and-inductive-interactionshttpsarxivorgabs260205843v1><a href=https://arxiv.org/abs/2602.05843v1>OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions</a><a hidden class=anchor aria-hidden=true href=#odysseyarena-benchmarking-large-language-models-for-long-horizon-active-and-inductive-interactionshttpsarxivorgabs260205843v1>#</a></h3><p><strong>Authors:</strong> Fangzhi Xu, Hang Yan, Qiushi Sun, Jinyang Wu, Zixian Huang, Muye Huang, Jingyang Gong, Zichen Ding, Kanzhi Cheng, Yian Wang, Xinyu Che, Zeyi Sun, Jian Zhang, Zhangyue Yin, Haoran Luo, Xuanjing Huang, Ben Kao, Jun Liu, Qika Lin
<strong>Venue:</strong> arXiv (2026)</p><p>The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent&rsquo;s inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at <a href=https://github.com/xufangzhi/Odyssey-Arena>https://github.com/xufangzhi/Odyssey-Arena</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05843v1">üìÑ Download PDF</a></p><hr><h3 id=mining-generalizable-activation-functionshttpsarxivorgabs260205688v1><a href=https://arxiv.org/abs/2602.05688v1>Mining Generalizable Activation Functions</a><a hidden class=anchor aria-hidden=true href=#mining-generalizable-activation-functionshttpsarxivorgabs260205688v1>#</a></h3><p><strong>Authors:</strong> Alex Vitvitskyi, Michael Boratko, Matej Grcic, Razvan Pascanu, Deep Shah, Petar Veliƒçkoviƒá
<strong>Venue:</strong> arXiv (2026)</p><p>The choice of activation function is an active area of research, with different proposals aimed at improving optimization, while maintaining expressivity. Additionally, the activation function can significantly alter the implicit inductive bias of the architecture, controlling its non-linear behavior. In this paper, in line with previous work, we argue that evolutionary search provides a useful framework for finding new activation functions, while we also make two novel observations. The first is that modern pipelines, such as AlphaEvolve, which relies on frontier LLMs as a mutator operator, allows for a much wider and flexible search space; e.g., over all possible python functions within a certain FLOP budget, eliminating the need for manually constructed search spaces. In addition, these pipelines will be biased towards meaningful activation functions, given their ability to represent common knowledge, leading to a potentially more efficient search of the space. The second observation is that, through this framework, one can target not only performance improvements but also activation functions that encode particular inductive biases. This can be done by using performance on out-of-distribution data as a fitness function, reflecting the degree to which the architecture respects the inherent structure in the data in a manner independent of distribution shifts. We carry an empirical exploration of this proposal and show that relatively small scale synthetic datasets can be sufficient for AlphaEvolve to discover meaningful activations.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05688v1">üìÑ Download PDF</a></p><hr><h3 id=generative-ontology-when-structured-knowledge-learns-to-createhttpsarxivorgabs260205636v1><a href=https://arxiv.org/abs/2602.05636v1>Generative Ontology: When Structured Knowledge Learns to Create</a><a hidden class=anchor aria-hidden=true href=#generative-ontology-when-structured-knowledge-learns-to-createhttpsarxivorgabs260205636v1>#</a></h3><p><strong>Authors:</strong> Benny Cheung
<strong>Venue:</strong> arXiv (2026)</p><p>Traditional ontologies excel at describing domain structure but cannot generate novel artifacts. Large language models generate fluently but produce outputs that lack structural validity, hallucinating mechanisms without components, goals without end conditions. We introduce Generative Ontology, a framework that synthesizes these complementary strengths: ontology provides the grammar; the LLM provides the creativity.
Generative Ontology encodes domain knowledge as executable Pydantic schemas that constrain LLM generation via DSPy signatures. A multi-agent pipeline assigns specialized roles to different ontology domains: a Mechanics Architect designs game systems, a Theme Weaver integrates narrative, a Balance Critic identifies exploits. Each agent carrying a professional &ldquo;anxiety&rdquo; that prevents shallow, agreeable outputs. Retrieval-augmented generation grounds novel designs in precedents from existing exemplars, while iterative validation ensures coherence between mechanisms and components.
We demonstrate the framework through GameGrammar, a system for generating complete tabletop game designs. Given a thematic prompt (&ldquo;bioluminescent fungi competing in a cave ecosystem&rdquo;), the pipeline produces structurally complete, playable game specifications with mechanisms, components, victory conditions, and setup instructions. These outputs satisfy ontological constraints while remaining genuinely creative.
The pattern generalizes beyond games. Any domain with expert vocabulary, validity constraints, and accumulated exemplars (music composition, software architecture, culinary arts) is a candidate for Generative Ontology. We argue that constraints do not limit creativity but enable it: just as grammar makes poetry possible, ontology makes structured generation possible.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05636v1">üìÑ Download PDF</a></p><hr><h2 id=-psycholinguistics>üîç psycholinguistics<a hidden class=anchor aria-hidden=true href=#-psycholinguistics>#</a></h2><h3 id=chunky-post-training-data-driven-failures-of-generalizationhttpsarxivorgabs260205910v1><a href=https://arxiv.org/abs/2602.05910v1>Chunky Post-Training: Data Driven Failures of Generalization</a><a hidden class=anchor aria-hidden=true href=#chunky-post-training-data-driven-failures-of-generalizationhttpsarxivorgabs260205910v1>#</a></h3><p><strong>Authors:</strong> Seoirse Murray, Allison Qi, Timothy Qian, John Schulman, Collin Burns, Sara Price
<strong>Venue:</strong> arXiv (2026)</p><p>LLM post-training involves many diverse datasets, each targeting a specific behavior. But these datasets encode incidental patterns alongside intended ones: correlations between formatting and content, narrow phrasings across diverse problems, and implicit associations arising from the discrete data curation process. These patterns are often invisible to developers yet salient to models, producing behaviors that surprise their creators, such as rejecting true facts presented in a particular question format. We call this chunky post-training: the model learns spurious correlations as a result of distinct chunks of post-training data. We introduce SURF, a black-box pipeline which surfaces these unintended behaviors at run time, and TURF, a tool that traces these failures back to specific post-training data. Applying these tools to frontier models (Claude 4.5, GPT-5.1, Grok 4.1, Gemini 3) and open models (T√ºlu 3), we show that chunky post-training produces miscalibrated behaviors, which often result from imbalanced or underspecified chunks of post-training data.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05910v1">üìÑ Download PDF</a></p><hr><h3 id=visualizing-the-loss-landscapes-of-physics-informed-neural-networkshttpsarxivorgabs260205849v1><a href=https://arxiv.org/abs/2602.05849v1>Visualizing the loss landscapes of physics-informed neural networks</a><a hidden class=anchor aria-hidden=true href=#visualizing-the-loss-landscapes-of-physics-informed-neural-networkshttpsarxivorgabs260205849v1>#</a></h3><p><strong>Authors:</strong> Conor Rowan, Finn Murphy-Blanchard
<strong>Venue:</strong> arXiv (2026)</p><p>Training a neural network requires navigating a high-dimensional, non-convex loss surface to find parameters that minimize this loss. In many ways, it is surprising that optimizers such as stochastic gradient descent and ADAM can reliably locate minima which perform well on both the training and test data. To understand the success of training, a &ldquo;loss landscape&rdquo; community has emerged to study the geometry of the loss function and the dynamics of optimization, often using visualization techniques. However, these loss landscape studies have mostly been limited to machine learning for image classification. In the newer field of physics-informed machine learning, little work has been conducted to visualize the landscapes of losses defined not by regression to large data sets, but by differential operators acting on state fields discretized by neural networks. In this work, we provide a comprehensive review of the loss landscape literature, as well as a discussion of the few existing physics-informed works which investigate the loss landscape. We then use a number of the techniques we survey to empirically investigate the landscapes defined by the Deep Ritz and squared residual forms of the physics loss function. We find that the loss landscapes of physics-informed neural networks have many of the same properties as the data-driven classification problems studied in the literature. Unexpectedly, we find that the two formulations of the physics loss often give rise to similar landscapes, which appear smooth, well-conditioned, and convex in the vicinity of the solution. The purpose of this work is to introduce the loss landscape perspective to the scientific machine learning community, compare the Deep Ritz and the strong form losses, and to challenge prevailing intuitions about the complexity of the loss landscapes of physics-informed networks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05849v1">üìÑ Download PDF</a></p><hr><h3 id=hidden-simplicity-in-ads-spinning-mellin-amplitudes-via-scaffoldinghttpsarxivorgabs260205568v1><a href=https://arxiv.org/abs/2602.05568v1>Hidden simplicity in AdS spinning Mellin amplitudes via scaffolding</a><a hidden class=anchor aria-hidden=true href=#hidden-simplicity-in-ads-spinning-mellin-amplitudes-via-scaffoldinghttpsarxivorgabs260205568v1>#</a></h3><p><strong>Authors:</strong> Song He, Xiang Li, Yuyu Mo, Dongyu Yang
<strong>Venue:</strong> arXiv (2026)</p><p>We uncover surprising hidden simplicity in Mellin amplitudes for tree-level AdS holographic correlators for spinning operators, such as AdS &ldquo;gluons&rdquo; and &ldquo;gravitons&rdquo; (spin 1 and 2). We define Mellin amplitudes with $n$ spinning operators via the so-called &ldquo;scaffolding&rdquo; of $2n$-scalar ones with specific projection operators for each spin state, which are rational functions of Mellin variables of $2n$ scalars generalizing flat-space scaffolding amplitudes. We classify possible three-point structures with spin 1 and 2 which take the same form as massive three-point amplitudes in flat space, and match with special solutions such as those extracted from 6-scalar ones in $\mathrm{AdS}_5\times S^3$ or $\mathrm{AdS}_5\times S^5$. Focusing on $\mathrm{AdS}_5$ gluons, we directly bootstrap spinning amplitudes in scaffolding form up to $n=6$ gluons (which amounts to $2n=12$ scalars) using factorizations, multi-linearity and flat-space limit. The results take a remarkably simple form in analogy with flat-space amplitudes, which can be constructed from familiar 3- and 4-vertices as well as propagators of massive spin-1 particles. Surprisingly, we find that vertices with any descendant levels are proportional to the primary ones with nice combinatorial coefficients, which makes manifest the correct flat-space limit in the simplest possible way.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05568v1">üìÑ Download PDF</a></p><hr><h3 id=data-centric-interpretability-for-llm-based-multi-agent-reinforcement-learninghttpsarxivorgabs260205183v1><a href=https://arxiv.org/abs/2602.05183v1>Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning</a><a hidden class=anchor aria-hidden=true href=#data-centric-interpretability-for-llm-based-multi-agent-reinforcement-learninghttpsarxivorgabs260205183v1>#</a></h3><p><strong>Authors:</strong> John Yan, Michael Yu, Yuqi Sun, Alexander Duffy, Tyler Marques, Matthew Lyle Olson
<strong>Venue:</strong> arXiv (2026)</p><p>Large language models (LLMs) are increasingly trained in complex Reinforcement Learning, multi-agent environments, making it difficult to understand how behavior changes over training. Sparse Autoencoders (SAEs) have recently shown to be useful for data-centric interpretability. In this work, we analyze large-scale reinforcement learning training runs from the sophisticated environment of Full-Press Diplomacy by applying pretrained SAEs, alongside LLM-summarizer methods. We introduce Meta-Autointerp, a method for grouping SAE features into interpretable hypotheses about training dynamics. We discover fine-grained behaviors including role-playing patterns, degenerate outputs, language switching, alongside high-level strategic behaviors and environment-specific bugs. Through automated evaluation, we validate that 90% of discovered SAE Meta-Features are significant, and find a surprising reward hacking behavior. However, through two user studies, we find that even subjectively interesting and seemingly helpful SAE features may be worse than useless to humans, along with most LLM generated hypotheses. However, a subset of SAE-derived hypotheses are predictively useful for downstream tasks. We further provide validation by augmenting an untrained agent&rsquo;s system prompt, improving the score by +14.2%. Overall, we show that SAEs and LLM-summarizer provide complementary views into agent behavior, and together our framework forms a practical starting point for future data-centric interpretability work on ensuring trustworthy LLM behavior throughout training.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05183v1">üìÑ Download PDF</a></p><hr><h3 id=a-surprising-discrepancy-in-the-regularity-of-conjugacies-between-generalized-interval-exchange-transformations-and-their-inverses-at-freezinghttpsarxivorgabs260204993v1><a href=https://arxiv.org/abs/2602.04993v1>A surprising discrepancy in the regularity of conjugacies between generalized interval exchange transformations and their inverses at freezing</a><a hidden class=anchor aria-hidden=true href=#a-surprising-discrepancy-in-the-regularity-of-conjugacies-between-generalized-interval-exchange-transformations-and-their-inverses-at-freezinghttpsarxivorgabs260204993v1>#</a></h3><p><strong>Authors:</strong> Krzysztof FrƒÖczek, ≈Åukasz Kotlewski
<strong>Venue:</strong> arXiv (2026)</p><p>Generalized interval exchange transformations (GIETs) are semi-conjugate to interval exchange transformations (IETs) when the Rauzy-Veech combinatorics is $\infty$-complete. When this semi-conjugacy is a homeomorphism, a fundamental problem is to understand the regularity of the conjugacy and its inverse. Contrary to the usual expectation that their H√∂lder regularities degenerate simultaneously, we exhibit a strongly asymmetric behavior. For self-similar IETs of hyperbolic periodic type and a natural one-parameter central family of affine IET deformations obtained via a freezing (zero-temperature) limit, the conjugacy becomes arbitrarily irregular while its inverse remains uniformly H√∂lder. Using thermodynamic formalism for renormalization and zero-temperature limits, we obtain sharp asymptotics for Hausdorff dimensions of invariant and conformal measures and for the supremal H√∂lder exponents of the conjugacy and its inverse.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.04993v1">üìÑ Download PDF</a></p><hr><h3 id=artificial-intelligence-as-strange-intelligence-against-linear-models-of-intelligencehttpsarxivorgabs260204986v1><a href=https://arxiv.org/abs/2602.04986v1>Artificial Intelligence as Strange Intelligence: Against Linear Models of Intelligence</a><a hidden class=anchor aria-hidden=true href=#artificial-intelligence-as-strange-intelligence-against-linear-models-of-intelligencehttpsarxivorgabs260204986v1>#</a></h3><p><strong>Authors:</strong> Kendra Chilson, Eric Schwitzgebel
<strong>Venue:</strong> arXiv (2026)</p><p>We endorse and expand upon Susan Schneider&rsquo;s critique of the linear model of AI progress and introduce two novel concepts: &ldquo;familiar intelligence&rdquo; and &ldquo;strange intelligence&rdquo;. AI intelligence is likely to be strange intelligence, defying familiar patterns of ability and inability, combining superhuman capacities in some domains with subhuman performance in other domains, and even within domains sometimes combining superhuman insight with surprising errors that few humans would make. We develop and defend a nonlinear model of intelligence on which &ldquo;general intelligence&rdquo; is not a unified capacity but instead the ability to achieve a broad range of goals in a broad range of environments, in a manner that defies nonarbitrary reduction to a single linear quantity. We conclude with implications for adversarial testing approaches to evaluating AI capacities. If AI is strange intelligence, we should expect that even the most capable systems will sometimes fail in seemingly obvious tasks. On a nonlinear model of AI intelligence, such errors on their own do not demonstrate a system&rsquo;s lack of outstanding general intelligence. Conversely, excellent performance on one type of task, such as an IQ test, cannot warrant assumptions of broad capacities beyond that task domain.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.04986v1">üìÑ Download PDF</a></p><hr><h3 id=dflash-block-diffusion-for-flash-speculative-decodinghttpsarxivorgabs260206036v1><a href=https://arxiv.org/abs/2602.06036v1>DFlash: Block Diffusion for Flash Speculative Decoding</a><a hidden class=anchor aria-hidden=true href=#dflash-block-diffusion-for-flash-speculative-decodinghttpsarxivorgabs260206036v1>#</a></h3><p><strong>Authors:</strong> Jian Chen, Yesheng Liang, Zhijian Liu
<strong>Venue:</strong> arXiv (2026)</p><p>Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, DFlash enables efficient drafting with high-quality outputs and higher acceptance rates. Experiments show that DFlash achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.5x higher speedup than the state-of-the-art speculative decoding method EAGLE-3.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06036v1">üìÑ Download PDF</a></p><hr><h3 id=learning-to-share-selective-memory-for-efficient-parallel-agentic-systemshttpsarxivorgabs260205965v1><a href=https://arxiv.org/abs/2602.05965v1>Learning to Share: Selective Memory for Efficient Parallel Agentic Systems</a><a hidden class=anchor aria-hidden=true href=#learning-to-share-selective-memory-for-efficient-parallel-agentic-systemshttpsarxivorgabs260205965v1>#</a></h3><p><strong>Authors:</strong> Joseph Fioresi, Parth Parag Kulkarni, Ashmal Vayani, Song Wang, Mubarak Shah
<strong>Venue:</strong> arXiv (2026)</p><p>Agentic systems solve complex tasks by coordinating multiple agents that iteratively reason, invoke tools, and exchange intermediate results. To improve robustness and solution quality, recent approaches deploy multiple agent teams running in parallel to explore diverse reasoning trajectories. However, parallel execution comes at a significant computational cost: when different teams independently reason about similar sub-problems or execute analogous steps, they repeatedly perform substantial overlapping computation. To address these limitations, in this paper, we propose Learning to Share (LTS), a learned shared-memory mechanism for parallel agentic frameworks that enables selective cross-team information reuse while controlling context growth. LTS introduces a global memory bank accessible to all teams and a lightweight controller that decides whether intermediate agent steps should be added to memory or not. The controller is trained using stepwise reinforcement learning with usage-aware credit assignment, allowing it to identify information that is globally useful across parallel executions. Experiments on the AssistantBench and GAIA benchmarks show that LTS significantly reduces overall runtime while matching or improving task performance compared to memory-free parallel baselines, demonstrating that learned memory admission is an effective strategy for improving the efficiency of parallel agentic systems. Project page: <a href=https://joefioresi718.github.io/LTS_webpage/>https://joefioresi718.github.io/LTS_webpage/</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05965v1">üìÑ Download PDF</a></p><hr><h3 id=photonic-neuromorphic-processing-with-coupled-spiking-silicon-microringshttpsarxivorgabs260205918v1><a href=https://arxiv.org/abs/2602.05918v1>Photonic neuromorphic processing with coupled spiking silicon microrings</a><a hidden class=anchor aria-hidden=true href=#photonic-neuromorphic-processing-with-coupled-spiking-silicon-microringshttpsarxivorgabs260205918v1>#</a></h3><p><strong>Authors:</strong> Giovanni Donati, Stefano Biasi, Lorenzo Pavesi, Antonio Hurtado
<strong>Venue:</strong> arXiv (2026)</p><p>Understanding the physical computing mechanisms of individual network nodes is essential for scaling neuromorphic photonic architectures. This work proposes a compact passive nonlinear photonic core based on a Side-Coupled Integrated Spaced Sequence of Resonators (SCISSOR) made of three nominally equal microrings and investigate its computing capabilities. Its nonlinearities and internal feedback enable analogue, spiking, and bistable responses that are accessed by tuning the injection power and wavelength. Implemented as a single nonlinear node in a time-multiplexed reservoir computing, the SCISSOR achieves error-free classification on the Iris dataset and accuracies above 97% on the Sonar task, using both analogue and digital reservoir representations with 150 virtual nodes. In the digital scheme, spiking dynamics naturally generate sparse reservoir states, enabling efficient classification even with a single spike. Intriguingly, optimal operating points are at the boundaries where sharp transitions in dynamical complexity and/or output power occur. In these points, the SCISSOR supports high task-performance, opening novel strategies for future on-chip training. Spiking and thermal bistabilities also participate to enhance the computational performance at low injected powers below 4 mW. These results suggest optical coupled microring resonators as effective building blocks for future edge computing and neuromorphic photonic systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05918v1">üìÑ Download PDF</a></p><hr><h3 id=regularized-calibration-with-successive-rounding-for-post-training-quantizationhttpsarxivorgabs260205902v1><a href=https://arxiv.org/abs/2602.05902v1>Regularized Calibration with Successive Rounding for Post-Training Quantization</a><a hidden class=anchor aria-hidden=true href=#regularized-calibration-with-successive-rounding-for-post-training-quantizationhttpsarxivorgabs260205902v1>#</a></h3><p><strong>Authors:</strong> Seohyeon Cha, Huancheng Chen, Dongjun Kim, Haoran Zhang, Kevin Chan, Gustavo de Veciana, Haris Vikalo
<strong>Venue:</strong> arXiv (2026)</p><p>Large language models (LLMs) deliver robust performance across diverse applications, yet their deployment often faces challenges due to the memory and latency costs of storing and accessing billions of parameters. Post-training quantization (PTQ) enables efficient inference by mapping pretrained weights to low-bit formats without retraining, but its effectiveness depends critically on both the quantization objective and the rounding procedure used to obtain low-bit weight representations. In this work, we show that interpolating between symmetric and asymmetric calibration acts as a form of regularization that preserves the standard quadratic structure used in PTQ while providing robustness to activation mismatch. Building on this perspective, we derive a simple successive rounding procedure that naturally incorporates asymmetric calibration, as well as a bounded-search extension that allows for an explicit trade-off between quantization quality and the compute cost. Experiments across multiple LLM families, quantization bit-widths, and benchmarks demonstrate that the proposed bounded search based on a regularized asymmetric calibration objective consistently improves perplexity and accuracy over PTQ baselines, while incurring only modest and controllable additional computational cost.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05902v1">üìÑ Download PDF</a></p><hr><h3 id=fhaim-fully-homomorphic-aim-for-private-synthetic-data-generationhttpsarxivorgabs260205838v1><a href=https://arxiv.org/abs/2602.05838v1>FHAIM: Fully Homomorphic AIM For Private Synthetic Data Generation</a><a hidden class=anchor aria-hidden=true href=#fhaim-fully-homomorphic-aim-for-private-synthetic-data-generationhttpsarxivorgabs260205838v1>#</a></h3><p><strong>Authors:</strong> Mayank Kumar, Qian Lou, Paulo Barreto, Martine De Cock, Sikha Pentyala
<strong>Venue:</strong> arXiv (2026)</p><p>Data is the lifeblood of AI, yet much of the most valuable data remains locked in silos due to privacy and regulations. As a result, AI remains heavily underutilized in many of the most important domains, including healthcare, education, and finance. Synthetic data generation (SDG), i.e. the generation of artificial data with a synthesizer trained on real data, offers an appealing solution to make data available while mitigating privacy concerns, however existing SDG-as-a-service workflow require data holders to trust providers with access to private data.We propose FHAIM, the first fully homomorphic encryption (FHE) framework for training a marginal-based synthetic data generator on encrypted tabular data. FHAIM adapts the widely used AIM algorithm to the FHE setting using novel FHE protocols, ensuring that the private data remains encrypted throughout and is released only with differential privacy guarantees. Our empirical analysis show that FHAIM preserves the performance of AIM while maintaining feasible runtimes.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05838v1">üìÑ Download PDF</a></p><hr><h3 id=large-data-acquisition-and-analytics-at-synchrotron-radiation-facilitieshttpsarxivorgabs260205837v1><a href=https://arxiv.org/abs/2602.05837v1>Large Data Acquisition and Analytics at Synchrotron Radiation Facilities</a><a hidden class=anchor aria-hidden=true href=#large-data-acquisition-and-analytics-at-synchrotron-radiation-facilitieshttpsarxivorgabs260205837v1>#</a></h3><p><strong>Authors:</strong> Aashish Panta, Giorgio Scorzelli, Amy A. Gooch, Werner Sun, Katherine S. Shanks, Suchismita Sarker, Devin Bougie, Keara Soloway, Rolf Verberg, Tracy Berman, Glenn Tarcea, John Allison, Michela Taufer, Valerio Pascucci
<strong>Venue:</strong> arXiv (2026)</p><p>Synchrotron facilities like the Cornell High Energy Synchrotron Source (CHESS) generate massive data volumes from complex beamline experiments, but face challenges such as limited access time, the need for on-site experiment monitoring, and managing terabytes of data per user group. We present the design, deployment, and evaluation of a framework that addresses CHESS&rsquo;s data acquisition and management issues. Deployed on a secure CHESS server, our system provides real time, web-based tools for remote experiment monitoring and data quality assessment, improving operational efficiency. Implemented across three beamlines (ID3A, ID3B, ID4B), the framework managed 50-100 TB of data and over 10 million files in late 2024. Testing with 43 research groups and 86 dashboards showed reduced overhead, improved accessibility, and streamlined data workflows. Our paper highlights the development, deployment, and evaluation of our framework and its transformative impact on synchrotron data acquisition.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05837v1">üìÑ Download PDF</a></p><hr><h3 id=sparcd-a-spectral-graph-framework-for-revealing-differential-functional-connectivity-in-fmri-datahttpsarxivorgabs260205807v1><a href=https://arxiv.org/abs/2602.05807v1>SpARCD: A Spectral Graph Framework for Revealing Differential Functional Connectivity in fMRI Data</a><a hidden class=anchor aria-hidden=true href=#sparcd-a-spectral-graph-framework-for-revealing-differential-functional-connectivity-in-fmri-datahttpsarxivorgabs260205807v1>#</a></h3><p><strong>Authors:</strong> Shira Yoffe, Ziv Ben-Zion, Talma Hendler, Malka Gorfine, Ariel Jaffe
<strong>Venue:</strong> arXiv (2026)</p><p>Identifying brain regions that exhibit altered functional connectivity across cognitive or emotional states is a key problem in neuroscience. Existing methods, such as edge-wise testing, seed-based psychophysiological interaction (PPI) analysis, or correlation network comparison, typically suffer from low statistical power, arbitrary thresholding, and limited ability to capture distributed or nonlinear dependence patterns. We propose SpARCD (Spectral Analysis of Revealing Connectivity Differences), a novel statistical framework for detecting differences in brain connectivity between two experimental conditions. SpARCD leverages distance correlation, a dependence measure sensitive to both linear and nonlinear associations, to construct a weighted graph for each condition. It then constructs a differential operator via spectral filtering and uncovers connectivity changes by computing its leading eigenvectors. Inference is achieved via a permutation-based testing scheme that yields interpretable, region-level significance maps. Extensive simulation studies demonstrate that SpARCD achieves superior power relative to conventional edge-wise or univariate approaches, particularly in the presence of complex dependency structures. Application to fMRI data from 113 early PTSD patients performing an emotional face-matching task reveals distinct networks associated with emotional reactivity and regulatory processes. Overall, SpARCD provides a statistically rigorous and computationally efficient framework for comparing high-dimensional connectivity structures, with broad applicability to neuroimaging and other network-based scientific domains.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05807v1">üìÑ Download PDF</a></p><hr><h3 id=from-vision-to-decision-neuromorphic-control-for-autonomous-navigation-and-trackinghttpsarxivorgabs260205683v1><a href=https://arxiv.org/abs/2602.05683v1>From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking</a><a hidden class=anchor aria-hidden=true href=#from-vision-to-decision-neuromorphic-control-for-autonomous-navigation-and-trackinghttpsarxivorgabs260205683v1>#</a></h3><p><strong>Authors:</strong> Chuwei Wang, Eduardo Sebasti√°n, Amanda Prorok, Anastasia Bizyaeva
<strong>Venue:</strong> arXiv (2026)</p><p>Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05683v1">üìÑ Download PDF</a></p><hr><h3 id=a-hybrid-data-driven-algorithm-for-real-time-friction-force-estimation-in-hydraulic-cylindershttpsarxivorgabs260205967v1><a href=https://arxiv.org/abs/2602.05967v1>A Hybrid Data-Driven Algorithm for Real-Time Friction Force Estimation in Hydraulic Cylinders</a><a hidden class=anchor aria-hidden=true href=#a-hybrid-data-driven-algorithm-for-real-time-friction-force-estimation-in-hydraulic-cylindershttpsarxivorgabs260205967v1>#</a></h3><p><strong>Authors:</strong> Mohamad Amin Jamshidi, Mehrbod Zarifi, Zolfa Anvari, Hamed Ghafarirad, Mohammad Zareinejad
<strong>Venue:</strong> arXiv (2026)</p><p>Hydraulic systems are widely utilized in industrial applications due to their high force generation, precise control, and ability to function in harsh environments. Hydraulic cylinders, as actuators in these systems, apply force and position through the displacement of hydraulic fluid, but their operation is significantly influenced by friction force. Achieving precision in hydraulic cylinders requires an accurate friction model under various operating conditions. Existing analytical models, often derived from experimental tests, necessitate the identification or estimation of influencing factors but are limited in adaptability and computational efficiency. This research introduces a data-driven, hybrid algorithm based on Long Short-Term Memory (LSTM) networks and Random Forests for nonlinear friction force estimation. The algorithm effectively combines feature detection and estimation processes using training data acquired from an experimental hydraulic test setup. It achieves a consistent and stable model error of less than 10% across diverse operating conditions and external load variations, ensuring robust performance in complex situations. The computational cost of the algorithm is 1.51 milliseconds per estimation, making it suitable for real-time applications. The proposed method addresses the limitations of analytical models by delivering high precision and computational efficiency. The algorithm&rsquo;s performance is validated through detailed analysis and experimental results, including direct comparisons with the LuGre model. The comparison highlights that while the LuGre model offers a theoretical foundation for friction modeling, its performance is limited by its inability to dynamically adjust to varying operational conditions of the hydraulic cylinder, further emphasizing the advantages of the proposed hybrid approach in real-time applications.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05967v1">üìÑ Download PDF</a></p><hr><h3 id=prompting-destiny-negotiating-socialization-and-growth-in-an-llm-mediated-speculative-gameworldhttpsarxivorgabs260205864v1><a href=https://arxiv.org/abs/2602.05864v1>Prompting Destiny: Negotiating Socialization and Growth in an LLM-Mediated Speculative Gameworld</a><a hidden class=anchor aria-hidden=true href=#prompting-destiny-negotiating-socialization-and-growth-in-an-llm-mediated-speculative-gameworldhttpsarxivorgabs260205864v1>#</a></h3><p><strong>Authors:</strong> Mandi Yang, Zhiqi Gao, Yibo Meng, Dongyijie Primo Pan
<strong>Venue:</strong> arXiv (2026)</p><p>We present an LLM-mediated role-playing game that supports reflection on socialization, moral responsibility, and educational role positioning. Grounded in socialization theory, the game follows a four-season structure in which players guide a child prince through morally charged situations and compare the LLM-mediated NPC&rsquo;s differentiated responses across stages, helping them reason about how educational guidance shifts with socialization. To approximate real educational contexts and reduce score-chasing, the system hides real-time evaluative scores and provides delayed, end-of-stage growth feedback as reflective prompts. We conducted a user study (N=12) with gameplay logs and post-game interviews, analyzed via reflexive thematic analysis. Findings show how players negotiated responsibility and role positioning, and reveal an entry-load tension between open-ended expression and sustained engagement. We contribute design knowledge on translating sociological models of socialization into reflective AI-mediated game systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05864v1">üìÑ Download PDF</a></p><hr><h2 id=-llm>üîç llm<a hidden class=anchor aria-hidden=true href=#-llm>#</a></h2><h3 id=multi-token-prediction-via-self-distillationhttpsarxivorgabs260206019v1><a href=https://arxiv.org/abs/2602.06019v1>Multi-Token Prediction via Self-Distillation</a><a hidden class=anchor aria-hidden=true href=#multi-token-prediction-via-self-distillationhttpsarxivorgabs260206019v1>#</a></h3><p><strong>Authors:</strong> John Kirchenbauer, Abhimanyu Hans, Brian Bartoldson, Micah Goldblum, Ashwinee Panda, Tom Goldstein
<strong>Venue:</strong> arXiv (2026)</p><p>Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online distillation objective. The final model retains the exact same implementation as the pretrained initial checkpoint and is deployable without the addition of any auxiliary verifier or other specialized inference code. On GSM8K, our method produces models that can decode more than $3\times$ faster on average at $&lt;5%$ drop in accuracy relative to single token decoding performance.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06019v1">üìÑ Download PDF</a></p><hr><h3 id=genarena-how-can-we-achieve-human-aligned-evaluation-for-visual-generation-taskshttpsarxivorgabs260206013v1><a href=https://arxiv.org/abs/2602.06013v1>GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?</a><a hidden class=anchor aria-hidden=true href=#genarena-how-can-we-achieve-human-aligned-evaluation-for-visual-generation-taskshttpsarxivorgabs260206013v1>#</a></h3><p><strong>Authors:</strong> Ruihang Li, Leigang Qu, Jingxu Zhang, Dongnan Gui, Mengde Xu, Xiaosong Zhang, Han Hu, Wenjie Wang, Jiaqi Wang
<strong>Venue:</strong> arXiv (2026)</p><p>The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06013v1">üìÑ Download PDF</a></p><hr><h3 id=orthogonal-self-attentionhttpsarxivorgabs260205996v1><a href=https://arxiv.org/abs/2602.05996v1>Orthogonal Self-Attention</a><a hidden class=anchor aria-hidden=true href=#orthogonal-self-attentionhttpsarxivorgabs260205996v1>#</a></h3><p><strong>Authors:</strong> Leo Zhang, James Martens
<strong>Venue:</strong> arXiv (2026)</p><p>Softmax Self-Attention (SSA) is a key component of Transformer architectures. However, when utilised within skipless architectures, which aim to improve representation learning, recent work has highlighted the inherent instability of SSA due to inducing rank collapse and poorly-conditioned Jacobians. In this work, we design a novel attention mechanism: Orthogonal Self-Attention (OSA), which aims to bypass these issues with SSA, in order to allow for (non-causal) Transformers without skip connections and normalisation layers to be more easily trained. In particular, OSA parametrises the attention matrix to be orthogonal via mapping a skew-symmetric matrix, formed from query-key values, through the matrix exponential. We show that this can be practically implemented, by exploiting the low-rank structure of our query-key values, resulting in the computational complexity and memory cost of OSA scaling linearly with sequence length. Furthermore, we derive an initialisation scheme for which we prove ensures that the Jacobian of OSA is well-conditioned.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05996v1">üìÑ Download PDF</a></p><hr><h3 id=ap-ood-attention-pooling-for-out-of-distribution-detectionhttpsarxivorgabs260206031v1><a href=https://arxiv.org/abs/2602.06031v1>AP-OOD: Attention Pooling for Out-of-Distribution Detection</a><a hidden class=anchor aria-hidden=true href=#ap-ood-attention-pooling-for-out-of-distribution-detectionhttpsarxivorgabs260206031v1>#</a></h3><p><strong>Authors:</strong> Claus Hofmann, Christian Huber, Bernhard Lehner, Daniel Klotz, Sepp Hochreiter, Werner Zellinger
<strong>Venue:</strong> arXiv (2026)</p><p>Out-of-distribution (OOD) detection, which maps high-dimensional data into a scalar OOD score, is critical for the reliable deployment of machine learning models. A key challenge in recent research is how to effectively leverage and aggregate token embeddings from language models to obtain the OOD score. In this work, we propose AP-OOD, a novel OOD detection method for natural language that goes beyond simple average-based aggregation by exploiting token-level information. AP-OOD is a semi-supervised approach that flexibly interpolates between unsupervised and supervised settings, enabling the use of limited auxiliary outlier data. Empirically, AP-OOD sets a new state of the art in OOD detection for text: in the unsupervised setting, it reduces the FPR95 (false positive rate at 95% true positives) from 27.84% to 4.67% on XSUM summarization, and from 77.08% to 70.37% on WMT15 En-Fr translation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06031v1">üìÑ Download PDF</a></p><hr><h3 id=breaking-symmetry-bottlenecks-in-gnn-readoutshttpsarxivorgabs260205950v1><a href=https://arxiv.org/abs/2602.05950v1>Breaking Symmetry Bottlenecks in GNN Readouts</a><a hidden class=anchor aria-hidden=true href=#breaking-symmetry-bottlenecks-in-gnn-readoutshttpsarxivorgabs260205950v1>#</a></h3><p><strong>Authors:</strong> Mouad Talhi, Arne Wolf, Anthea Monod
<strong>Venue:</strong> arXiv (2026)</p><p>Graph neural networks (GNNs) are widely used for learning on structured data, yet their ability to distinguish non-isomorphic graphs is fundamentally limited. These limitations are usually attributed to message passing; in this work we show that an independent bottleneck arises at the readout stage. Using finite-dimensional representation theory, we prove that all linear permutation-invariant readouts, including sum and mean pooling, factor through the Reynolds (group-averaging) operator and therefore project node embeddings onto the fixed subspace of the permutation action, erasing all non-trivial symmetry-aware components regardless of encoder expressivity. This yields both a new expressivity barrier and an interpretable characterization of what global pooling preserves or destroys. To overcome this collapse, we introduce projector-based invariant readouts that decompose node representations into symmetry-aware channels and summarize them with nonlinear invariant statistics, preserving permutation invariance while retaining information provably invisible to averaging. Empirically, swapping only the readout enables fixed encoders to separate WL-hard graph pairs and improves performance across multiple benchmarks, demonstrating that readout design is a decisive and under-appreciated factor in GNN expressivity.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05950v1">üìÑ Download PDF</a></p><hr><h2 id=-neuroscience>üîç neuroscience<a hidden class=anchor aria-hidden=true href=#-neuroscience>#</a></h2><h3 id=visrefiner-learning-from-visual-differences-for-screenshot-to-code-generationhttpsarxivorgabs260205998v1><a href=https://arxiv.org/abs/2602.05998v1>VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation</a><a hidden class=anchor aria-hidden=true href=#visrefiner-learning-from-visual-differences-for-screenshot-to-code-generationhttpsarxivorgabs260205998v1>#</a></h3><p><strong>Authors:</strong> Jie Deng, Kaichun Yao, Libo Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>Screenshot-to-code generation aims to translate user interface screenshots into executable frontend code that faithfully reproduces the target layout and style. Existing multimodal large language models perform this mapping directly from screenshots but are trained without observing the visual outcomes of their generated code. In contrast, human developers iteratively render their implementation, compare it with the design, and learn how visual differences relate to code changes. Inspired by this process, we propose VisRefiner, a training framework that enables models to learn from visual differences between rendered predictions and reference designs. We construct difference-aligned supervision that associates visual discrepancies with corresponding code edits, allowing the model to understand how appearance variations arise from implementation changes. Building on this, we introduce a reinforcement learning stage for self-refinement, where the model improves its generated code by observing both the rendered output and the target design, identifying their visual differences, and updating the code accordingly. Experiments show that VisRefiner substantially improves single-step generation quality and layout fidelity, while also endowing models with strong self-refinement ability. These results demonstrate the effectiveness of learning from visual differences for advancing screenshot-to-code generation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05998v1">üìÑ Download PDF</a></p><hr><h3 id=the-effects-of-non-ideal-mixing-in-planetary-magma-oceans-and-atmosphereshttpsarxivorgabs260205917v1><a href=https://arxiv.org/abs/2602.05917v1>The Effects of Non-ideal Mixing in Planetary Magma Oceans and Atmospheres</a><a hidden class=anchor aria-hidden=true href=#the-effects-of-non-ideal-mixing-in-planetary-magma-oceans-and-atmosphereshttpsarxivorgabs260205917v1>#</a></h3><p><strong>Authors:</strong> Aaron Werlen, Edward D. Young, Hilke E. Schlichting, Caroline Dorn, Anat Shahar
<strong>Venue:</strong> arXiv (2026)</p><p>Sub-Neptunes with hydrogen-rich envelopes are expected to sustain long-lived magma oceans that continuously exchange volatiles with their overlying atmospheres. Capturing these interactions is key to understanding the chemical evolution and present-day diversity of sub-Neptunes, super-Earths, and terrestrial planets, particularly in light of new JWST observations and upcoming missions. Recent advances in both geochemistry and astrophysics now allow the integration of experimental constraints and thermodynamic models across melt, metal, and gas phases. Here we extend a global chemical equilibrium model to include non-ideal behavior in all three phases. Our framework combines fugacity corrections for gas species with activity coefficients for silicate and metal species, enabling a fully coupled description of volatile partitioning. We show that for planetary embryos (0.5 M$_\oplus$ at 2350 K), non-ideality introduces only modest corrections to atmosphere-magma ocean interface (AMOI) pressures, volatile inventories, and interior compositions. In contrast, for sub-Neptunes with higher temperatures ($\approx$ 3000 K) and pressures, non-ideal effects are more pronounced, though still modest in absolute terms$-$typically within 20% and at most a factor of two. Including activity and fugacity coefficients simultaneously increases the AMOI pressure, enhances water retention in the mantle and the envelope. Our results demonstrate that non-ideality must be treated globally: applying corrections to only one phase leads to incomplete or even misleading trends. These findings highlight the importance of self-consistent global thermodynamic treatments for interpreting atmospheric spectra and interior structures of sub-Neptunes and super-Earths.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05917v1">üìÑ Download PDF</a></p><hr><h3 id=darwin-dynamic-agentically-rewriting-self-improving-networkhttpsarxivorgabs260205848v1><a href=https://arxiv.org/abs/2602.05848v1>DARWIN: Dynamic Agentically Rewriting Self-Improving Network</a><a hidden class=anchor aria-hidden=true href=#darwin-dynamic-agentically-rewriting-self-improving-networkhttpsarxivorgabs260205848v1>#</a></h3><p><strong>Authors:</strong> Henry Jiang
<strong>Venue:</strong> arXiv (2026)</p><p>DARWIN is an evolutionary GPT model, utilizing a genetic-algorithm like optimization structure with several independent GPT agents being trained individually using unique training code. Each iteration, the GPT models are prompted to modify the training code of one another in an attempt to improve their performance in a mutation-like manner, and the best GPT agents are then benchmarked and selected for the next iteration by genetic algorithm. For demonstration purposes and due to budget and time constraints, OpenAI API is used to prompt training code improvements and the nanoGPT framework is used as the training code. DARWIN also utilizes persistent JSON-based memory files to track previous reasoning and changes to code to correlate with improvement to model performance. and a bidirectional interface for HITL intervention allowing the model to request upgrades such as additional datasets, training scripts, and restructuring of file hierarchies. In experiments, DARWIN achieved a 1.26 percent improvement in model FLOPS utilization (MFU) and a 2.07 percent improvement to perplexity in 5 iterations of training over baseline configurations, demonstrating promising capabilities as a foundation for scaling evolutionary GPT training.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05848v1">üìÑ Download PDF</a></p><hr><h3 id=where-does-warm-up-come-from-adaptive-scheduling-for-norm-constrained-optimizershttpsarxivorgabs260205813v1><a href=https://arxiv.org/abs/2602.05813v1>Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers</a><a hidden class=anchor aria-hidden=true href=#where-does-warm-up-come-from-adaptive-scheduling-for-norm-constrained-optimizershttpsarxivorgabs260205813v1>#</a></h3><p><strong>Authors:</strong> Artem Riabinin, Andrey Veprikov, Arman Bolatov, Martin Tak√°ƒç, Aleksandr Beznosikov
<strong>Venue:</strong> arXiv (2026)</p><p>We study adaptive learning rate scheduling for norm-constrained optimizers (e.g., Muon and Lion). We introduce a generalized smoothness assumption under which local curvature decreases with the suboptimality gap and empirically verify that this behavior holds along optimization trajectories. Under this assumption, we establish convergence guarantees under an appropriate choice of learning rate, for which warm-up followed by decay arises naturally from the proof rather than being imposed heuristically.
Building on this theory, we develop a practical learning rate scheduler that relies only on standard hyperparameters and adapts the warm-up duration automatically at the beginning of training. We evaluate this method on large language model pretraining with LLaMA architectures and show that our adaptive warm-up selection consistently outperforms or at least matches the best manually tuned warm-up schedules across all considered setups, without additional hyperparameter search. Our source code is available at <a href=https://github.com/brain-lab-research/llm-baselines/tree/warmup>https://github.com/brain-lab-research/llm-baselines/tree/warmup</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05813v1">üìÑ Download PDF</a></p><hr><h3 id=assessing-the-sensitivity-of-niobium--and-tantalum-based-superconducting-qubits-to-infrared-radiationhttpsarxivorgabs260205806v1><a href=https://arxiv.org/abs/2602.05806v1>Assessing the Sensitivity of Niobium- and Tantalum-Based Superconducting Qubits to Infrared Radiation</a><a hidden class=anchor aria-hidden=true href=#assessing-the-sensitivity-of-niobium--and-tantalum-based-superconducting-qubits-to-infrared-radiationhttpsarxivorgabs260205806v1>#</a></h3><p><strong>Authors:</strong> Michael Kerschbaum, Felix Wagner, Uro≈° Ognjanoviƒá, Giovanni Vio, Kuno Knapp, Dante Colao Zanuz, Alexander Flasby, Mohsen Bahrami Panah, Andreas Wallraff, Jean-Claude Besse
<strong>Venue:</strong> arXiv (2026)</p><p>The use of tantalum films for superconducting qubits has recently extended qubit coherence times significantly, primarily due to reduced dielectric losses at the metal-air interface. However, the choice of base material also influences the sensitivity to quasiparticle-induced decoherence. In this study, we investigate quasiparticle tunneling rates in niobium and tantalum-based offset-charge-sensitive qubits. Using a source of thermal radiation, we characterize the sensitivity of either material to infrared radiation and explore the impact of the infrared background through the targeted use of in-line filters in the wiring and ambient infrared absorbers. We identify both radiation channels as significant contributions to decoherence for tantalum but not for niobium qubits and achieve tunneling rates of 100 Hz and 300 Hz for niobium and tantalum respectively upon installation of infrared filters. Additionally, we find a time-dependence in the observed tunneling rates on the scale of days, which we interpret as evidence of slowly cooling, thermally radiating components in the experimental setup. Our findings indicate that continued improvements in coherence times may require renewed attention to radiative backgrounds and experimental setup design, especially when introducing new material platforms.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05806v1">üìÑ Download PDF</a></p><hr><h3 id=a-perturbation-correction-method-based-on-local-randomized-neural-networks-for-quasi-linear-interface-problemshttpsarxivorgabs260205800v1><a href=https://arxiv.org/abs/2602.05800v1>A Perturbation-Correction Method Based on Local Randomized Neural Networks for Quasi-Linear Interface Problems</a><a hidden class=anchor aria-hidden=true href=#a-perturbation-correction-method-based-on-local-randomized-neural-networks-for-quasi-linear-interface-problemshttpsarxivorgabs260205800v1>#</a></h3><p><strong>Authors:</strong> Siyuan Lang, Zhiyue Zhang
<strong>Venue:</strong> arXiv (2026)</p><p>For quasi-linear interface problems with discontinuous diffusion coefficients, the nonconvex objective functional often leads to optimization stagnation in randomized neural network approximations. This paper Proposes a perturbation-correction framework based on Loacal Randomized Neural Networks(LRaNNs) to overcome this limitation. In the initialization step, a satisisfactory based approximation is obtained by minimizing the original nonconvex residual, typically stagnating at a moderate accuracy level. Subsequently, in the correction step, a correction term is determined by solving a subproblem governed by a perturbation expansion around the base approximation. This reformulation yields a convex optimization problem for the output coefficients, which guarantees rapic convergence. We rigorously derive an a posteriori error estitmate, demonstrating that the total generalization error is governed by the discrete residual norm, quadrature error, and a controllable truncation error. Numerical experiments on nonlinear diffusion problems with irregular moving interfaces, gradient-dependent diffusivities, and high-contrast media demonstrate that the proposed method effectively overcomes the optimization plateau. The correction step yields a significant improvement of 4-6 order of magnitude in L^2 accuracy.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05800v1">üìÑ Download PDF</a></p><hr><h2 id=-data_resources>üîç data_resources<a hidden class=anchor aria-hidden=true href=#-data_resources>#</a></h2><h3 id=supply-vs-demand-in-community-based-fact-checking-on-social-mediahttpsarxivorgabs260206005v1><a href=https://arxiv.org/abs/2602.06005v1>Supply vs. Demand in Community-Based Fact-Checking on Social Media</a><a hidden class=anchor aria-hidden=true href=#supply-vs-demand-in-community-based-fact-checking-on-social-mediahttpsarxivorgabs260206005v1>#</a></h3><p><strong>Authors:</strong> Moritz Pilarski, Nicolas Pr√∂llochs
<strong>Venue:</strong> arXiv (2026)</p><p>Fact-checking ecosystems on social media depend on the interplay between what users want checked and what contributors are willing to supply. Prior research has largely examined these forces in isolation, yet it remains unclear to what extent supply meets demand. We address this gap with an empirical analysis of a unique dataset of 1.1 million fact-checks and fact-checking requests from X&rsquo;s Community Notes platform between June 2024 and May 2025. We find that requests disproportionately target highly visible posts - those with more views and engagement and authored by influential accounts - whereas fact-checks are distributed more broadly across languages, sentiments, and topics. Using a quasi-experimental survival analysis, we further estimate the effect of displaying requests on subsequent note creation. Results show that requests significantly accelerate contributions from Top Writers. Altogether, our findings highlight a gap between the content that attracts requests for fact-checking and the content that ultimately receives fact-checks, while showing that user requests can steer contributors toward greater alignment. These insights carry important implications for platform governance and future research on online misinformation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06005v1">üìÑ Download PDF</a></p><hr><h3 id=speech-emotion-recognition-leveraging-openais-whisper-representations-and-attentive-pooling-methodshttpsarxivorgabs260206000v1><a href=https://arxiv.org/abs/2602.06000v1>Speech Emotion Recognition Leveraging OpenAI&rsquo;s Whisper Representations and Attentive Pooling Methods</a><a hidden class=anchor aria-hidden=true href=#speech-emotion-recognition-leveraging-openais-whisper-representations-and-attentive-pooling-methodshttpsarxivorgabs260206000v1>#</a></h3><p><strong>Authors:</strong> Ali Shendabadi, Parnia Izadirad, Mostafa Salehi, Mahmoud Bijankhan
<strong>Venue:</strong> arXiv (2026)</p><p>Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasets. Recent studies have leveraged pre-trained models to extract features for downstream tasks such as SER. This work explores the capabilities of Whisper, a pre-trained ASR system, in speech emotion recognition by proposing two attention-based pooling methods, Multi-head Attentive Average Pooling and QKV Pooling, designed to efficiently reduce the dimensionality of Whisper representations while preserving emotional features. We experiment on English and Persian, using the IEMOCAP and ShEMO datasets respectively, with Whisper Tiny and Small. Our multi-head QKV architecture achieves state-of-the-art results on the ShEMO dataset, with a 2.47% improvement in unweighted accuracy. We further compare the performance of different Whisper encoder layers and find that intermediate layers often perform better for SER on the Persian dataset, providing a lightweight and efficient alternative to much larger models such as HuBERT X-Large. Our findings highlight the potential of Whisper as a representation extractor for SER and demonstrate the effectiveness of attention-based pooling for dimension reduction.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06000v1">üìÑ Download PDF</a></p><hr><h3 id=persistent-human-feedback-llms-and-static-analyzers-for-secure-code-generation-and-vulnerability-detectionhttpsarxivorgabs260205868v1><a href=https://arxiv.org/abs/2602.05868v1>Persistent Human Feedback, LLMs, and Static Analyzers for Secure Code Generation and Vulnerability Detection</a><a hidden class=anchor aria-hidden=true href=#persistent-human-feedback-llms-and-static-analyzers-for-secure-code-generation-and-vulnerability-detectionhttpsarxivorgabs260205868v1>#</a></h3><p><strong>Authors:</strong> Ehsan Firouzi, Mohammad Ghafari
<strong>Venue:</strong> arXiv (2026)</p><p>Existing literature heavily relies on static analysis tools to evaluate LLMs for secure code generation and vulnerability detection. We reviewed 1,080 LLM-generated code samples, built a human-validated ground-truth, and compared the outputs of two widely used static security tools, CodeQL and Semgrep, against this corpus. While 61% of the samples were genuinely secure, Semgrep and CodeQL classified 60% and 80% as secure, respectively. Despite the apparent agreement in aggregate statistics, per-sample analysis reveals substantial discrepancies: only 65% of Semgrep&rsquo;s and 61% of CodeQL&rsquo;s reports correctly matched the ground truth. These results question the reliability of static analysis tools as sole evaluators of code security and underscore the need for expert feedback. Building on this insight, we propose a conceptual framework that persistently stores human feedback in a dynamic retrieval-augmented generation pipeline, enabling LLMs to reuse past feedback for secure code generation and vulnerability detection.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05868v1">üìÑ Download PDF</a></p><hr><h3 id=contextbench-a-benchmark-for-context-retrieval-in-coding-agentshttpsarxivorgabs260205892v1><a href=https://arxiv.org/abs/2602.05892v1>ContextBench: A Benchmark for Context Retrieval in Coding Agents</a><a hidden class=anchor aria-hidden=true href=#contextbench-a-benchmark-for-context-retrieval-in-coding-agentshttpsarxivorgabs260205892v1>#</a></h3><p><strong>Authors:</strong> Han Li, Letian Zhu, Bohan Zhang, Rili Feng, Jiaming Wang, Yue Pan, Earl T. Barr, Sarro Federica, Zhaoyang Chu, He Ye
<strong>Venue:</strong> arXiv (2026)</p><p>LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (&ldquo;The Bitter Lesson&rdquo; of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks. Data and code are available at: <a href=https://cioutn.github.io/context-bench/>https://cioutn.github.io/context-bench/</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05892v1">üìÑ Download PDF</a></p><hr><h2 id=-emotion_language>üîç emotion_language<a hidden class=anchor aria-hidden=true href=#-emotion_language>#</a></h2><h3 id=dimensionality-reduction-on-riemannian-manifolds-in-data-analysishttpsarxivorgabs260205936v1><a href=https://arxiv.org/abs/2602.05936v1>Dimensionality Reduction on Riemannian Manifolds in Data Analysis</a><a hidden class=anchor aria-hidden=true href=#dimensionality-reduction-on-riemannian-manifolds-in-data-analysishttpsarxivorgabs260205936v1>#</a></h3><p><strong>Authors:</strong> Alaa El Ichi, Khalide Jbilou
<strong>Venue:</strong> arXiv (2026)</p><p>In this work, we investigate Riemannian geometry based dimensionality reduction methods that respect the underlying manifold structure of the data. In particular, we focus on Principal Geodesic Analysis (PGA) as a nonlinear generalization of PCA for manifold valued data, and extend discriminant analysis through Riemannian adaptations of other known dimensionality reduction methods. These approaches exploit geodesic distances, tangent space representations, and intrinsic statistical measures to achieve more faithful low dimensional embeddings. We also discuss related manifold learning techniques and highlight their theoretical foundations and practical advantages. Experimental results on representative datasets demonstrate that Riemannian methods provide improved representation quality and classification performance compared to their Euclidean counterparts, especially for data constrained to curved spaces such as hyperspheres and symmetric positive definite manifolds. This study underscores the importance of geometry aware dimensionality reduction in modern machine learning and data science applications.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05936v1">üìÑ Download PDF</a></p><hr><h3 id=xlist-hate-a-checklist-based-framework-for-interpretable-and-generalizable-hate-speech-detectionhttpsarxivorgabs260205874v1><a href=https://arxiv.org/abs/2602.05874v1>xList-Hate: A Checklist-Based Framework for Interpretable and Generalizable Hate Speech Detection</a><a hidden class=anchor aria-hidden=true href=#xlist-hate-a-checklist-based-framework-for-interpretable-and-generalizable-hate-speech-detectionhttpsarxivorgabs260205874v1>#</a></h3><p><strong>Authors:</strong> Adri√°n Gir√≥n, Pablo Miralles, Javier Huertas-Tato, Sergio D&rsquo;Antonio, David Camacho
<strong>Venue:</strong> arXiv (2026)</p><p>Hate speech detection is commonly framed as a direct binary classification problem despite being a composite concept defined through multiple interacting factors that vary across legal frameworks, platform policies, and annotation guidelines. As a result, supervised models often overfit dataset-specific definitions and exhibit limited robustness under domain shift and annotation noise.
We introduce xList-Hate, a diagnostic framework that decomposes hate speech detection into a checklist of explicit, concept-level questions grounded in widely shared normative criteria. Each question is independently answered by a large language model (LLM), producing a binary diagnostic representation that captures hateful content features without directly predicting the final label. These diagnostic signals are then aggregated by a lightweight, fully interpretable decision tree, yielding transparent and auditable predictions.
We evaluate it across multiple hate speech benchmarks and model families, comparing it against zero-shot LLM classification and in-domain supervised fine-tuning. While supervised methods typically maximize in-domain performance, we consistently improves cross-dataset robustness and relative performance under domain shift. In addition, qualitative analysis of disagreement cases provides evidence that the framework can be less sensitive to certain forms of annotation inconsistency and contextual ambiguity. Crucially, the approach enables fine-grained interpretability through explicit decision paths and factor-level analysis.
Our results suggest that reframing hate speech detection as a diagnostic reasoning task, rather than a monolithic classification problem, provides a robust, explainable, and extensible alternative for content moderation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05874v1">üìÑ Download PDF</a></p><hr><h3 id=towards-uncertainty-quantification-of-a-model-for-cancer-on-chip-experimentshttpsarxivorgabs260206018v1><a href=https://arxiv.org/abs/2602.06018v1>Towards uncertainty quantification of a model for cancer-on-chip experiments</a><a hidden class=anchor aria-hidden=true href=#towards-uncertainty-quantification-of-a-model-for-cancer-on-chip-experimentshttpsarxivorgabs260206018v1>#</a></h3><p><strong>Authors:</strong> Silvia Bertoluzza, Vittoria Bianchi, Gabriella Bretti, Lorenzo Tamellini, Pietro Zanotti
<strong>Venue:</strong> arXiv (2026)</p><p>This study is a first step towards using data-informed differential models to predict and control the dynamics of cancer-on-chip experiments. We consider a conceptualized one-dimensional device, containing a cancer and a population of white blood cells. The interaction between the cancer and the population of cells is modeled by a chemotaxis model inspired by Keller-Segel-type equations, which is solved by a Hybridized Discontinuous Galerkin method. Our goal is using (synthetic) data to tune the parameters of the governing equations and to assess the uncertainty on the predictions of the dynamics due to the residual uncertainty on the parameters remaining after the tuning procedure. To this end, we apply techniques from uncertainty quantification for parametric differential models. We first perform a global sensitivity analysis using both Sobol and Morris indices to assess how parameter uncertainty impacts model predictions, and fix the value of parameters with negligible impact. Subsequently, we conduct an inverse uncertainty quantification analysis by Bayesian techniques to compute a data-informed probability distribution of the remaining model parameters. Finally, we carry out a forward uncertainty quantification analysis to compute the impact of the updated (residual) parametric uncertainties on the quantities of interest of the model. The whole procedure is sped up by using surrogate models, based on sparse-grids, to approximate the mapping of the uncertain parameters to the quantities of interest.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06018v1">üìÑ Download PDF</a></p><hr><h3 id=guardian-safety-filtering-for-systems-with-perception-models-subject-to-adversarial-attackshttpsarxivorgabs260206026v1><a href=https://arxiv.org/abs/2602.06026v1>GUARDIAN: Safety Filtering for Systems with Perception Models Subject to Adversarial Attacks</a><a hidden class=anchor aria-hidden=true href=#guardian-safety-filtering-for-systems-with-perception-models-subject-to-adversarial-attackshttpsarxivorgabs260206026v1>#</a></h3><p><strong>Authors:</strong> Nicholas Rober, Alex Rose, Jonathan P. How
<strong>Venue:</strong> arXiv (2026)</p><p>Safety filtering is an effective method for enforcing constraints in safety-critical systems, but existing methods typically assume perfect state information. This limitation is especially problematic for systems that rely on neural network (NN)-based state estimators, which can be highly sensitive to noise and adversarial input perturbations. We address these problems by introducing GUARDIAN: Guaranteed Uncertainty-Aware Reachability Defense against Adversarial INterference, a safety filtering framework that provides formal safety guarantees for systems with NN-based state estimators. At runtime, GUARDIAN uses neural network verification tools to provide guaranteed bounds on the system&rsquo;s state estimate given possible perturbations to its observation. It then uses a modified Hamilton-Jacobi reachability formulation to construct a safety filter that adjusts the nominal control input based on the verified state bounds and safety constraints. The result is an uncertainty-aware filter that ensures safety despite the system&rsquo;s reliance on an NN estimator with noisy, possibly adversarial, input observations. Theoretical analysis and numerical experiments demonstrate that GUARDIAN effectively defends systems against adversarial attacks that would otherwise lead to a violation of safety constraints.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.06026v1">üìÑ Download PDF</a></p><hr><h3 id=dissipative-dicke-time-quasicrystalshttpsarxivorgabs260205994v1><a href=https://arxiv.org/abs/2602.05994v1>Dissipative Dicke Time Quasicrystals</a><a hidden class=anchor aria-hidden=true href=#dissipative-dicke-time-quasicrystalshttpsarxivorgabs260205994v1>#</a></h3><p><strong>Authors:</strong> Sk Anisur, Sayan Choudhury
<strong>Venue:</strong> arXiv (2026)</p><p>We investigate the emergence of time quasicrystals (TQCs) in the open Dicke model, subjected to a quasi-periodic Fibonacci drive. TQCs are characterized by a robust sub-harmonic quasi-periodic response that is qualitatively distinct from the external drive. By directly analyzing the dynamics of the system in the thermodynamic limit, we establish the existence of TQC order in this system for a wide parameter regime. Remarkably, we demonstrate that this behavior persists even in the deep quantum regime with only two qubits. We systematically study the dependence of the TQC lifetime, $œÑ^{\ast}$, on the number of qubits and demonstrate that $œÑ^{\ast}$ increases monotonically with the system size. Our work demonstrates that quasi-periodically driven dissipative quantum systems can serve as a powerful platform for realizing novel non-equilibrium phases of matter.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05994v1">üìÑ Download PDF</a></p><hr><h3 id=f-grpo-and-beyond-divergence-based-reinforcement-learning-algorithms-for-general-llm-alignmenthttpsarxivorgabs260205946v1><a href=https://arxiv.org/abs/2602.05946v1>$f$-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment</a><a hidden class=anchor aria-hidden=true href=#f-grpo-and-beyond-divergence-based-reinforcement-learning-algorithms-for-general-llm-alignmenthttpsarxivorgabs260205946v1>#</a></h3><p><strong>Authors:</strong> Rajdeep Haldar, Lantao Mei, Guang Lin, Yue Xing, Qifan Song
<strong>Venue:</strong> arXiv (2026)</p><p>Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose $f$-Group Relative Policy Optimization ($f$-GRPO), a class of on-policy reinforcement learning, and $f$-Hybrid Alignment Loss ($f$-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of $f$-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05946v1">üìÑ Download PDF</a></p><hr><h3 id=detective-work-we-shouldnt-have-to-do-practitioner-challenges-in-regulatory-aligned-data-quality-in-machine-learning-systemshttpsarxivorgabs260205944v1><a href=https://arxiv.org/abs/2602.05944v1>&ldquo;Detective Work We Shouldn&rsquo;t Have to Do&rdquo;: Practitioner Challenges in Regulatory-Aligned Data Quality in Machine Learning Systems</a><a hidden class=anchor aria-hidden=true href=#detective-work-we-shouldnt-have-to-do-practitioner-challenges-in-regulatory-aligned-data-quality-in-machine-learning-systemshttpsarxivorgabs260205944v1>#</a></h3><p><strong>Authors:</strong> Yichun Wang, Kristina Irion, Paul Groth, Hazar Harmouch
<strong>Venue:</strong> arXiv (2026)</p><p>Ensuring data quality in machine learning (ML) systems has become increasingly complex as regulatory requirements expand. In the European Union (EU), frameworks such as the General Data Protection Regulation (GDPR) and the Artificial Intelligence Act (AI Act) articulate data quality requirements that closely parallel technical concerns in ML practice, while also extending to legal obligations related to accountability, risk management, and human rights protection. This paper presents a qualitative interview study with EU-based data practitioners working on ML systems in regulated contexts. Through semi-structured interviews, we investigate how practitioners interpret regulatory-aligned data quality, the challenges they encounter, and the supports they identify as necessary. Our findings reveal persistent gaps between legal principles and engineering workflows, fragmentation across data pipelines, limitations of existing tools, unclear responsibility boundaries between technical and legal teams, and a tendency toward reactive, audit-driven quality practices. We also identify practitioners&rsquo; needs for compliance-aware tooling, clearer governance structures, and cultural shifts toward proactive data governance.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05944v1">üìÑ Download PDF</a></p><hr><h3 id=lsa-localized-semantic-alignment-for-enhancing-temporal-consistency-in-traffic-video-generationhttpsarxivorgabs260205966v1><a href=https://arxiv.org/abs/2602.05966v1>LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation</a><a hidden class=anchor aria-hidden=true href=#lsa-localized-semantic-alignment-for-enhancing-temporal-consistency-in-traffic-video-generationhttpsarxivorgabs260205966v1>#</a></h3><p><strong>Authors:</strong> Mirlan Karimov, Teodora Spasojevic, Markus Braun, Julian Wiederer, Vasileios Belagiannis, Marc Pollefeys
<strong>Venue:</strong> arXiv (2026)</p><p>Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models. LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips. Specifically, we compare the output of an off-the-shelf feature extraction model between the ground-truth and generated video clips localized around dynamic objects inducing a semantic feature consistency loss. We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics. To further test the temporal consistency in generated videos we adapt two additional metrics from object detection task, namely mAP and mIoU. Extensive experiments on nuScenes and KITTI datasets show the effectiveness of our approach in enhancing temporal consistency in video generation without the need for external control signals during inference and any computational overheads.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05966v1">üìÑ Download PDF</a></p><hr><h3 id=improved-heavy-dark-matter-annihilation-search-from-dwarf-galaxies-with-hawchttpsarxivorgabs260205955v1><a href=https://arxiv.org/abs/2602.05955v1>Improved Heavy Dark Matter Annihilation Search from Dwarf Galaxies with HAWC</a><a hidden class=anchor aria-hidden=true href=#improved-heavy-dark-matter-annihilation-search-from-dwarf-galaxies-with-hawchttpsarxivorgabs260205955v1>#</a></h3><p><strong>Authors:</strong> A. Albert, R. Alfaro, C. Alvarez, A. Andr√©s, E. Anita-Rangel, M. Araya, J. C. Arteaga-Vel√°zquez, D. Avila Rojas, H. A. Ayala Solares, R. Babu, P. Bangale, E. Belmont-Moreno, A. Bernal, K. S. Caballero-Mora, T. Capistr√°n, A. Carrami√±ana, F. Carre√≥n, S. Casanova, A. L. Colmenero-Cesar, U. Cotti, J. Cotzomi, S. Couti√±o de Le√≥n, E. De la Fuente, C. de Le√≥n, P. Desiati, N. Di Lalla, R. Diaz Hernandez, M. A. DuVernois, J. C. D√≠az-V√©lez, K. Engel, T. Ergin, C. Espinoza, N. Fraija, S. Fraija, A. Galv√°n-G√°mez, J. A. Garc√≠a-Gonz√°lez, F. Garfias, N. Ghosh, A. Gonzalez Mu√±oz, M. M. Gonz√°lez, J. A. Gonz√°lez, J. A. Goodman, J. Gyeong, J. P. Harding, S. Hern√°ndez-Cadena, I. Herzog, D. Huang, F. Hueyotl-Zahuantitla, P. H√ºntemeyer, A. Iriarte, S. Kaufmann, D. Kieda, K. Leavitt, W. H. Lee, J. Lee, H. Le√≥n Vargas, J. T. Linnemann, A. L. Longinotti, G. Luis-Raya, C. Lundy, K. Malone, O. Martinez, J. Mart√≠nez-Castro, H. Mart√≠nez-Huerta, J. A. Matthews, P. Miranda-Romagnoli, P. E. Mir√≥n-Enriquez, J. A. Morales-Soto, E. Moreno, M. Mostaf√°, M. Najafi, A. Nayerhoda, L. Nellen, M. U. Nisa, R. Noriega-Papaqui, N. Omodei, E. Ponce, Y. P√©rez Araujo, E. G. P√©rez-P√©rez, C. D. Rho, A. Rodriguez Parra, D. Rosa-Gonz√°lez, M. Roth, H. Salazar, D. Salazar-Gallegos, A. Sandoval, M. Schneider, J. Serna-Franco, M. Shin, A. J. Smith, Y. Son, R. W. Springer, O. Tibolla, K. Tollefson, I. Torres, R. Torres-Escobedo, F. Ure√±a-Mena, E. Varela, L. Villase√±or, X. Wang, Z. Wang, I. J. Watson, H. Wu, S. Yu, X. Zhang, H. Zhou
<strong>Venue:</strong> arXiv (2026)</p><p>Understanding dark matter&rsquo;s elusive nature is crucial for the framework of particle physics and expanding the Standard Model. This analysis utilizes the High Altitude Water Cherenkov (HAWC) gamma ray Observatory to indirectly search for dark matter (DM) by studying gamma ray emission from dwarf spheroidal galaxies (dSphs). Selected for their high ratio of dark matter to baryonic matter, dSphs are useful for this type of search owing to the low background emission. In comparison to previous HAWC studies, we significantly improve our sensitivity to DM from dSphs due to improvements to our event reconstruction and reduced hadronic contamination. We expanded the number of dSphs studied, DM annihilation channels into the Standard Model (SM), and the amount of data collected on each previously studied dSph. We searched for DM signals in each dSph using the latest version of the algorithms used to reconstruct data from the primary detector of the HAWC instrument. We report that we do not detect evidence of DM from dSphs, so we place upper limits for the velocity-weighted DM annihilation cross-section ($\langleœÉv \rangle$) on the order of $10^{-23}~\text{cm}^3\text{s}^{-1}$ for a DM mass range of $1-10^4$ TeV.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05955v1">üìÑ Download PDF</a></p><hr><h3 id=tracing-agn-feedback-power-with-coolwarm-outflow-densities-predictions-and-observational-implicationshttpsarxivorgabs260205954v1><a href=https://arxiv.org/abs/2602.05954v1>Tracing AGN Feedback Power with Cool/Warm Outflow Densities: Predictions and Observational Implications</a><a hidden class=anchor aria-hidden=true href=#tracing-agn-feedback-power-with-coolwarm-outflow-densities-predictions-and-observational-implicationshttpsarxivorgabs260205954v1>#</a></h3><p><strong>Authors:</strong> Ivan Almeida, Tiago Costa, Chris M. Harrison, Samuel R. Ward
<strong>Venue:</strong> arXiv (2026)</p><p>Winds launched at the scale of the accretion disc or dusty torus in Active Galactic Nuclei (AGN) are thought to drive energy-conserving outflows that shape galaxy evolution. The key signature of such outflows, the presence of a hot ($T \gtrsim 10^9 , \rm K$), shocked wind component, is hard to detect directly. Observations of AGN outflows typically probe a separate outflow phase: cool/warm gas with $T \lesssim 10^5 , \rm K$. Here, we show that the density of cool outflowing gas scales with AGN luminosity, serving as an indirect diagnostic of the elusive hot, shocked wind. We use hydrodynamic simulations with the moving-mesh code AREPO to target the interaction between a small-scale AGN wind of speed $\approx 10^4 , \rm km , s^{-1}$ and galactic discs containing an idealised, clumpy interstellar medium (ISM). Through a new refinement scheme targeting rapidly-cooling, fast-moving gas, our simulations reach a resolution of $\lesssim 0.1 , \rm pc$ in the cool, outflowing phase. We extract an ensemble of cool clouds from the AGN-driven outflows produced in our simulations, finding that their densities increase systematically with AGN wind power and AGN luminosity. Moreover, the mass distribution and internal properties of these cloudlets appear to be insensitive to the initial properties of the ISM, and shaped mainly by the dynamics of radiative, turbulent mixing layers. The increase in cool outflow density with kinetic wind power and AGN luminosity has profound implications for observational estimates of outflow rates and their scaling with AGN luminosity. Depending on the available outflow and density tracers, observationally-derived outflow rates may be overestimated by orders of magnitude.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05954v1">üìÑ Download PDF</a></p><hr><h3 id=higher-order-adaptive-behaviors-outperform-pairwise-strategies-in-mitigating-contagion-dynamicshttpsarxivorgabs260205915v1><a href=https://arxiv.org/abs/2602.05915v1>Higher-order adaptive behaviors outperform pairwise strategies in mitigating contagion dynamics</a><a hidden class=anchor aria-hidden=true href=#higher-order-adaptive-behaviors-outperform-pairwise-strategies-in-mitigating-contagion-dynamicshttpsarxivorgabs260205915v1>#</a></h3><p><strong>Authors:</strong> Marco Mancastroppa, M√°rton Karsai, Alain Barrat
<strong>Venue:</strong> arXiv (2026)</p><p>When exposed to a contagion phenomenon, individuals may respond to the perceived risk of infection by adopting behavioral changes, aiming to reduce their exposure or their risk of infecting others. The social cost of such adaptive behaviors and their impact on the contagion dynamics have been investigated in pairwise networks, with binary interactions driving both contagion and risk perception. However, contagion and adaptive mechanisms can also be driven by group (higher-order) interactions. Here, we consider several adaptive behaviors triggered by awareness of risk perceived through higher-order and pairwise interactions, and we compare their impact on pairwise and higher-order contagion processes. By numerical simulations and a mean-field analytic approach, we show that adaptive behaviors driven by higher-order information are more effective in limiting the spread of a contagion, than similar mechanisms based on pairwise information. Meanwhile, they also entail a lower social cost, measured as the reduction of the intensity of interactions in the population. Indeed, adaptive mechanisms based on higher-order information lead to a heterogeneous risk perception within the population, producing a higher alert on nodes with large hyperdegree (i.e., participating in many groups), on their neighborhoods, and on large groups. This in turn prevents the spreading process to exploit the properties of these nodes and groups, which tend to drive and sustain the dynamics in the absence of adaptive behaviors.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05915v1">üìÑ Download PDF</a></p><hr><h3 id=exact-recovery-in-the-data-block-modelhttpsarxivorgabs260205852v1><a href=https://arxiv.org/abs/2602.05852v1>Exact Recovery in the Data Block Model</a><a hidden class=anchor aria-hidden=true href=#exact-recovery-in-the-data-block-modelhttpsarxivorgabs260205852v1>#</a></h3><p><strong>Authors:</strong> Amir R. Asadi, Akbar Davoodi, Ramin Javadi, Farzad Parvaresh
<strong>Venue:</strong> arXiv (2026)</p><p>Community detection in networks is a fundamental problem in machine learning and statistical inference, with applications in social networks, biological systems, and communication networks. The stochastic block model (SBM) serves as a canonical framework for studying community structure, and exact recovery, identifying the true communities with high probability, is a central theoretical question. While classical results characterize the phase transition for exact recovery based solely on graph connectivity, many real-world networks contain additional data, such as node attributes or labels. In this work, we study exact recovery in the Data Block Model (DBM), an SBM augmented with node-associated data, as formalized by Asadi, Abbe, and Verd√∫ (2017). We introduce the Chernoff&ndash;TV divergence and use it to characterize a sharp exact recovery threshold for the DBM. We further provide an efficient algorithm that achieves this threshold, along with a matching converse result showing impossibility below the threshold. Finally, simulations validate our findings and demonstrate the benefits of incorporating vertex data as side information in community detection.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2602.05852v1">üìÑ Download PDF</a></p><hr></div><footer class=post-footer><ul class=post-tags></ul></footer><div><div class=pagination__title><span class=pagination__title-h style=font-size:20px>Comments</span><hr></div><div id=tcomment></div><script src=https://cdn.staticfile.org/twikoo/1.6.39/twikoo.all.min.js></script><script>twikoo.init({envId:"https://mangodb-theta.vercel.app/",el:"#tcomment",lang:"en-US",path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://garyforreal.me/zh/>Gary's House</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>Views: <span id=busuanzi_value_site_pv></span>
</span><span id=busuanzi_container_site_uv>Visitors: <span id=busuanzi_value_site_uv></span></span></div></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="Â§çÂà∂";function s(){t.innerHTML="Â∑≤Â§çÂà∂ÔºÅ",setTimeout(()=>{t.innerHTML="Â§çÂà∂"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>