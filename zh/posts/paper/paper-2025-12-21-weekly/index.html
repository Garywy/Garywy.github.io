<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Weekly Paper Notes - 2025-12-21 | Gary's House</title>
<meta name=keywords content><meta name=description content="Weekly Paper Notes
üîç multilingual
From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection
Authors: Hao Li, Yubing Ren, Yanan Cao, Yingjie Li, Fang Fang, Xuebin Wang
Venue: arXiv (2025)
Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness."><meta name=author content="Gary"><link rel=canonical href=https://garyforreal.me/zh/posts/paper/paper-2025-12-21-weekly/><meta name=google-site-verification content="XYZabc"><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><link crossorigin=anonymous href=/assets/css/stylesheet.7e5251d8716d933fafcf3df70d7ecd02729661d6d232fc5dd0b8a85ef75e3409.css integrity="sha256-flJR2HFtkz+vzz33DX7NAnKWYdbSMvxd0LioXvdeNAk=" rel="preload stylesheet" as=style><link rel=icon href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=16x16 href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=32x32 href=https://garyforreal.me/img/Q.jpg><link rel=apple-touch-icon href=https://garyforreal.me/Q.jpg><link rel=mask-icon href=https://garyforreal.me/Q.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://garyforreal.me/zh/posts/paper/paper-2025-12-21-weekly/><link rel=alternate hreflang=en href=https://garyforreal.me/en/posts/paper/paper-2025-12-21-weekly/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@200..900&display=swap" rel=stylesheet><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous referrerpolicy=no-referrer></script><meta property="og:title" content="Weekly Paper Notes - 2025-12-21"><meta property="og:description" content="Weekly Paper Notes
üîç multilingual
From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection
Authors: Hao Li, Yubing Ren, Yanan Cao, Yingjie Li, Fang Fang, Xuebin Wang
Venue: arXiv (2025)
Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness."><meta property="og:type" content="article"><meta property="og:url" content="https://garyforreal.me/zh/posts/paper/paper-2025-12-21-weekly/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-21T15:23:23+00:00"><meta property="article:modified_time" content="2025-12-21T15:23:23+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Weekly Paper Notes - 2025-12-21"><meta name=twitter:description content="Weekly Paper Notes
üîç multilingual
From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection
Authors: Hao Li, Yubing Ren, Yanan Cao, Yingjie Li, Fang Fang, Xuebin Wang
Venue: arXiv (2025)
Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Â∏ñÂ≠ê","item":"https://garyforreal.me/zh/posts/"},{"@type":"ListItem","position":2,"name":"ËÆ∫Êñá","item":"https://garyforreal.me/zh/posts/paper/"},{"@type":"ListItem","position":3,"name":"Weekly Paper Notes - 2025-12-21","item":"https://garyforreal.me/zh/posts/paper/paper-2025-12-21-weekly/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Weekly Paper Notes - 2025-12-21","name":"Weekly Paper Notes - 2025-12-21","description":"Weekly Paper Notes üîç multilingual From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection Authors: Hao Li, Yubing Ren, Yanan Cao, Yingjie Li, Fang Fang, Xuebin Wang Venue: arXiv (2025)\nBenefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.\n","keywords":[],"articleBody":"Weekly Paper Notes üîç multilingual From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection Authors: Hao Li, Yubing Ren, Yanan Cao, Yingjie Li, Fang Fang, Xuebin Wang Venue: arXiv (2025)\nBenefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.\nüìÑ Download PDF\nBridging the Reality Gap: Efficient Adaptation of ASR systems for Challenging Low-Resource Domains Authors: Darshil Chauhan, Adityasinh Solanki, Vansh Patel, Kanav Kapoor, Ritvik Jain, Aditya Bansal, Dhruv Kumar, Prateek Narang Venue: arXiv (2025)\nAutomatic Speech Recognition (ASR) holds immense potential to streamline clinical documentation, such as digitizing handwritten prescriptions and reports, thereby increasing patient throughput and reducing costs in resource-constrained sectors like rural healthcare. However, realizing this utility is currently obstructed by significant technical barriers: strict data privacy constraints, limited computational resources, and severe acoustic domain shifts. We quantify this gap by showing that a robust multilingual model (IndicWav2Vec) degrades to a stark 40.94% Word Error Rate (WER) when deployed on real-world clinical audio (Gram Vaani), rendering it unusable for practical applications. To address these challenges and bring ASR closer to deployment, we propose an efficient, privacy-preserving adaptation framework. We employ Low-Rank Adaptation (LoRA) to enable continual learning from incoming data streams directly on edge devices, ensuring patient data confidentiality. Our strategy yields a 17.1% relative improvement in WER on the target domain. Furthermore, by integrating multi-domain experience replay, we reduce catastrophic forgetting by 47% compared to naive adaptation. These results demonstrate a viable pathway for building reliable, self-improving ASR systems that can operate effectively within the constraints of high-impact real-world environments.\nüìÑ Download PDF\nHearing to Translate: The Effectiveness of Speech Modality Integration into LLMs Authors: Sara Papi, Javier Garcia Gilabert, Zachary Hopton, Vil√©m Zouhar, Carlos Escolano, Gerard I. G√°llego, Jorge Iranzo-S√°nchez, Ahrii Kim, Dominik Mach√°ƒçek, Patricia Schmidtova, Maike Z√ºfle Venue: arXiv (2025)\nAs Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.\nüìÑ Download PDF\nMitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation Authors: Musarrat Zeba, Abdullah Al Mamun, Kishoar Jahan Tithee, Debopom Sutradhar, Mohaimenul Azam Khan Raiaan, Saddam Mukta, Reem E. Mohamed, Md Rafiqul Islam, Yakub Sebastian, Mukhtar Hussain, Sami Azam Venue: arXiv (2025)\nIn healthcare, it is essential for any LLM-generated output to be reliable and accurate, particularly in cases involving decision-making and patient safety. However, the outputs are often unreliable in such critical areas due to the risk of hallucinated outputs from the LLMs. To address this issue, we propose a fact-checking module that operates independently of any LLM, along with a domain-specific summarization model designed to minimize hallucination rates. Our model is fine-tuned using Low-Rank Adaptation (LoRa) on the MIMIC III dataset and is paired with the fact-checking module, which uses numerical tests for correctness and logical checks at a granular level through discrete logic in natural language processing (NLP) to validate facts against electronic health records (EHRs). We trained the LLM model on the full MIMIC-III dataset. For evaluation of the fact-checking module, we sampled 104 summaries, extracted them into 3,786 propositions, and used these as facts. The fact-checking module achieves a precision of 0.8904, a recall of 0.8234, and an F1-score of 0.8556. Additionally, the LLM summary model achieves a ROUGE-1 score of 0.5797 and a BERTScore of 0.9120 for summary quality.\nüìÑ Download PDF\nA Multi-Agent Large Language Model Framework for Automated Qualitative Analysis Authors: Qidi Xu, Nuzha Amjad, Grace Giles, Alexa Cumming, De‚Äôangelo Hermesky, Alexander Wen, Min Ji Kwak, Yejin Kim Venue: arXiv (2025)\nUnderstanding patients experiences is essential for advancing patient centered care, especially in chronic diseases that require ongoing communication. However, qualitative thematic analysis, the primary approach for exploring these experiences, remains labor intensive, subjective, and difficult to scale. In this study, we developed a multi agent large language model framework that automates qualitative thematic analysis through three agents (Instructor, Thematizer, CodebookGenerator), named Collaborative Theme Identification Agent (CoTI). We applied CoTI to 12 heart failure patient interviews to analyze their perceptions of medication intensity. CoTI identified key phrases, themes, and codebook that were more similar to those of the senior investigator than both junior investigators and baseline NLP models. We also implemented CoTI into a user-facing application to enable AI human interaction in qualitative analysis. However, collaboration between CoTI and junior investigators provided only marginal gains, suggesting they may overrely on CoTI and limit their independent critical thinking.\nüìÑ Download PDF\nCross-Language Bias Examination in Large Language Models Authors: Yuxuan Liang, Marwa Mahmoud Venue: arXiv (2025)\nThis study introduces an innovative multilingual bias evaluation framework for assessing bias in Large Language Models, combining explicit bias assessment through the BBQ benchmark with implicit bias measurement using a prompt-based Implicit Association Test. By translating the prompts and word list into five target languages, English, Chinese, Arabic, French, and Spanish, we directly compare different types of bias across languages. The results reveal substantial gaps in bias across languages used in LLMs. For example, Arabic and Spanish consistently show higher levels of stereotype bias, while Chinese and English exhibit lower levels of bias. We also identify contrasting patterns across bias types. Age shows the lowest explicit bias but the highest implicit bias, emphasizing the importance of detecting implicit biases that are undetectable with standard benchmarks. These findings indicate that LLMs vary significantly across languages and bias dimensions. This study fills a key research gap by providing a comprehensive methodology for cross-lingual bias analysis. Ultimately, our work establishes a foundation for the development of equitable multilingual LLMs, ensuring fairness and effectiveness across diverse languages and cultures.\nüìÑ Download PDF\nEmotion Recognition in Signers Authors: Kotaro Funakoshi, Yaoxiong Zhu Venue: arXiv (2025)\nRecognition of signers‚Äô emotions suffers from one theoretical challenge and one practical challenge, namely, the overlap between grammatical and affective facial expressions and the scarcity of data for model training. This paper addresses these two challenges in a cross-lingual setting using our eJSL dataset, a new benchmark dataset for emotion recognition in Japanese Sign Language signers, and BOBSL, a large British Sign Language dataset with subtitles. In eJSL, two signers expressed 78 distinct utterances with each of seven different emotional states, resulting in 1,092 video clips. We empirically demonstrate that 1) textual emotion recognition in spoken language mitigates data scarcity in sign language, 2) temporal segment selection has a significant impact, and 3) incorporating hand motion enhances emotion recognition in signers. Finally we establish a stronger baseline than spoken language LLMs.\nüìÑ Download PDF\nMultilingual and Continuous Backchannel Prediction: A Cross-lingual Study Authors: Koji Inoue, Mikey Elmers, Yahui Fu, Zi Haur Pang, Taiga Mori, Divesh Lala, Keiko Ochi, Tatsuya Kawahara Venue: arXiv (2025)\nWe present a multilingual, continuous backchannel prediction model for Japanese, English, and Chinese, and use it to investigate cross-linguistic timing behavior. The model is Transformer-based and operates at the frame level, jointly trained with auxiliary tasks on approximately 300 hours of dyadic conversations. Across all three languages, the multilingual model matches or surpasses monolingual baselines, indicating that it learns both language-universal cues and language-specific timing patterns. Zero-shot transfer with two-language training remains limited, underscoring substantive cross-lingual differences. Perturbation analyses reveal distinct cue usage: Japanese relies more on short-term linguistic information, whereas English and Chinese are more sensitive to silence duration and prosodic variation; multilingual training encourages shared yet adaptable representations and reduces overreliance on pitch in Chinese. A context-length study further shows that Japanese is relatively robust to shorter contexts, while Chinese benefits markedly from longer contexts. Finally, we integrate the trained model into a real-time processing software, demonstrating CPU-only inference. Together, these findings provide a unified model and empirical evidence for how backchannel timing differs across languages, informing the design of more natural, culturally-aware spoken dialogue systems.\nüìÑ Download PDF\nStutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion Authors: Guransh Singh, Md Shah Fahad Venue: arXiv (2025)\nStuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a ‚Äòblock‚Äô with a ‚Äòprolongation‚Äô) due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve ‚ÄúModality Collapse‚Äù, an ‚ÄúEcho Chamber‚Äù effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.\nüìÑ Download PDF\nScaling Laws for Code: Every Programming Language Matters Authors: Jian Yang, Shawn Guo, Lin Jing, Wei Zhang, Aishan Liu, Chuan Hao, Zhoujun Li, Wayne Xin Zhao, Xianglong Liu, Weifeng Lv, Bryan Dai Venue: arXiv (2025)\nCode large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.\nüìÑ Download PDF\nVenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks Authors: Beitong Zhou, Zhexiao Huang, Yuan Guo, Zhangxuan Gu, Tianyu Xia, Zichen Luo, Fei Tang, Dehan Kong, Yanyi Shang, Suling Ou, Zhenlin Guo, Changhua Meng, Shuheng Shen Venue: arXiv (2025)\nGUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.\nüìÑ Download PDF\nPPSEBM: An Energy-Based Model with Progressive Parameter Selection for Continual Learning Authors: Xiaodi Li, Dingcheng Li, Rujun Gao, Mahmoud Zamani, Feng Mi, Latifur Khan Venue: arXiv (2025)\nContinual learning remains a fundamental challenge in machine learning, requiring models to learn from a stream of tasks without forgetting previously acquired knowledge. A major obstacle in this setting is catastrophic forgetting, where performance on earlier tasks degrades as new tasks are learned. In this paper, we introduce PPSEBM, a novel framework that integrates an Energy-Based Model (EBM) with Progressive Parameter Selection (PPS) to effectively address catastrophic forgetting in continual learning for natural language processing tasks. In PPSEBM, progressive parameter selection allocates distinct, task-specific parameters for each new task, while the EBM generates representative pseudo-samples from prior tasks. These generated samples actively inform and guide the parameter selection process, enhancing the model‚Äôs ability to retain past knowledge while adapting to new tasks. Experimental results on diverse NLP benchmarks demonstrate that PPSEBM outperforms state-of-the-art continual learning methods, offering a promising and robust solution to mitigate catastrophic forgetting.\nüìÑ Download PDF\nAn Empirical Study on Chinese Character Decomposition in Multiword Expression-Aware Neural Machine Translation Authors: Lifeng Han, Gareth J. F. Jones, Alan F. Smeaton Venue: arXiv (2025)\nWord meaning, representation, and interpretation play fundamental roles in natural language understanding (NLU), natural language processing (NLP), and natural language generation (NLG) tasks. Many of the inherent difficulties in these tasks stem from Multi-word Expressions (MWEs), which complicate the tasks by introducing ambiguity, idiomatic expressions, infrequent usage, and a wide range of variations. Significant effort and substantial progress have been made in addressing the challenging nature of MWEs in Western languages, particularly English. This progress is attributed in part to the well-established research communities and the abundant availability of computational resources. However, the same level of progress is not true for language families such as Chinese and closely related Asian languages, which continue to lag behind in this regard. While sub-word modelling has been successfully applied to many Western languages to address rare words improving phrase comprehension, and enhancing machine translation (MT) through techniques like byte-pair encoding (BPE), it cannot be applied directly to ideograph language scripts like Chinese. In this work, we conduct a systematic study of the Chinese character decomposition technology in the context of MWE-aware neural machine translation (NMT). Furthermore, we report experiments to examine how Chinese character decomposition technology contributes to the representation of the original meanings of Chinese words and characters, and how it can effectively address the challenges of translating MWEs.\nüìÑ Download PDF\nMultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services Authors: Lingfeng Tang, Daoping Zhang, Junjie Chen, Peihao Huang, Feng Jin, Chengguang Xu, Yuxin Chen, Feiqiang Sun, Guo Chen Venue: arXiv (2025)\nThe limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM‚Äôs sleep mode by 1.12x to 2.48x.\nüìÑ Download PDF\nMagic state cultivation on a superconducting quantum processor Authors: Emma Rosenfeld, Craig Gidney, Gabrielle Roberts, Alexis Morvan, Nathan Lacroix, Dvir Kafri, Jeffrey Marshall, Ming Li, Volodymyr Sivak, Dmitry Abanin, Amira Abbas, Rajeev Acharya, Laleh Aghababaie Beni, Georg Aigeldinger, Ross Alcaraz, Sayra Alcaraz, Trond I. Andersen, Markus Ansmann, Frank Arute, Kunal Arya, Walt Askew, Nikita Astrakhantsev, Juan Atalaya, Ryan Babbush, Brian Ballard, Joseph C. Bardin, Hector Bates, Andreas Bengtsson, Majid Bigdeli Karimi, Alexander Bilmes, Simon Bilodeau, Felix Borjans, Jenna Bovaird, Dylan Bowers, Leon Brill, Peter Brooks, Michael Broughton, David A. Browne, Brett Buchea, Bob B. Buckley, Tim Burger, Brian Burkett, Nicholas Bushnell, Jamal Busnaina, Anthony Cabrera, Juan Campero, Hung-Shen Chang, Silas Chen, Zijun Chen, Ben Chiaro, Liang-Ying Chih, Agnetta Y. Cleland, Bryan Cochrane, Matt Cockrell, Josh Cogan, Paul Conner, Harold Cook, Rodrigo G. Corti√±as, William Courtney, Alexander L. Crook, Ben Curtin, Martin Damyanov, Sayan Das, Dripto M. Debroy, Sean Demura, Paul Donohoe, Ilya Drozdov, Andrew Dunsworth, Valerie Ehimhen, Alec Eickbusch, Aviv Moshe Elbag, Lior Ella, Mahmoud Elzouka, David Enriquez, Catherine Erickson, Lara Faoro, Vinicius S. Ferreira, Marcos Flores, Leslie Flores Burgos, Sam Fontes, Ebrahim Forati, Jeremiah Ford, Brooks Foxen, Masaya Fukami, Alan Wing Lun Fung, Lenny Fuste, Suhas Ganjam, Gonzalo Garcia, Christopher Garrick, Robert Gasca, Helge Gehring, Robert Geiger, √âlie Genois, William Giang, Dar Gilboa, James E. Goeders, Edward C. Gonzales, Raja Gosula, Stijn J. de Graaf, Alejandro Grajales Dau, Dietrich Graumann, Joel Grebel, Alex Greene, Jonathan A. Gross, Jose Guerrero, Lo√Øck Le Guevel, Tan Ha, Steve Habegger, Tanner Hadick, Ali Hadjikhani, Michael C. Hamilton, Monica Hansen, Matthew P. Harrigan, Sean D. Harrington, Jeanne Hartshorn, Stephen Heslin, Paula Heu, Oscar Higgott, Reno Hiltermann, Jeremy Hilton, Hsin-Yuan Huang, Mike Hucka, Christopher Hudspeth, Ashley Huff, William J. Huggins, Lev B. Ioffe, Evan Jeffrey, Shaun Jevons, Zhang Jiang, Xiaoxuan Jin, Chaitali Joshi, Pavol Juhas, Andreas Kabel, Hui Kang, Kiseo Kang, Amir H. Karamlou, Ryan Kaufman, Kostyantyn Kechedzhi, Tanuj Khattar, Mostafa Khezri, Seon Kim, Paul V. Klimov, Can M. Knaut, Bryce Kobrin, Alexander N. Korotkov, Fedor Kostritsa, John Mark Kreikebaum, Ryuho Kudo, Ben Kueffler, Arun Kumar, Vladislav D. Kurilovich, Vitali Kutsko, Tiano Lange-Dei, Brandon W. Langley, Pavel Laptev, Kim-Ming Lau, Emma Leavell, Justin Ledford, Joy Lee, Kenny Lee, Brian J. Lester, Wendy Leung, Lily Li, Wing Yan Li, Alexander T. Lill, William P. Livingston, Matthew T. Lloyd, Aditya Locharla, Laura De Lorenzo, Erik Lucero, Daniel Lundahl, Aaron Lunt, Sid Madhuk, Aniket Maiti, Ashley Maloney, Salvatore Mandr√†, Leigh S. Martin, Orion Martin, Eric Mascot, Paul Masih Das, Dmitri Maslov, Melvin Mathews, Cameron Maxfield, Jarrod R. McClean, Matt McEwen, Seneca Meeks, Anthony Megrant, Kevin C. Miao, Zlatko K. Minev, Reza Molavi, Sebastian Molina, Shirin Montazeri, Charles Neill, Michael Newman, Anthony Nguyen, Murray Nguyen, Chia-Hung Ni, Murphy Yuezhen Niu, Nicholas Noll, Logan Oas, William D. Oliver, Raymond Orosco, Kristoffer Ottosson, Alice Pagano, Agustin Di Paolo, Sherman Peek, David Peterson, Alex Pizzuto, Elias Portoles, Rebecca Potter, Orion Pritchard, Michael Qian, Chris Quintana, Ganesh Ramachandran, Arpit Ranadive, Matthew J. Reagor, Rachel Resnick, David M. Rhodes, Daniel Riley, Roberto Rodriguez, Emma Ropes, Lucia B. De Rose, Eliott Rosenberg, Dario Rosenstock, Elizabeth Rossi, Pedram Roushan, David A. Rower, Robert Salazar, Kannan Sankaragomathi, Murat Can Sarihan, Max Schaefer, Sebastian Schroeder, Henry F. Schurkus, Aria Shahingohar, Michael J. Shearn, Aaron Shorter, Noah Shutty, Vladimir Shvarts, Spencer Small, W. Clarke Smith, David A. Sobel, Barrett Spells, Sofia Springer, George Sterling, Jordan Suchard, Aaron Szasz, Alexander Sztein, Madeline Taylor, Jothi Priyanka Thiruraman, Douglas Thor, Dogan Timucin, Eifu Tomita, Alfredo Torres, M. Mert Torunbalci, Hao Tran, Abeer Vaishnav, Justin Vargas, Sergey Vdovichev, Guifre Vidal, Benjamin Villalonga, Catherine Vollgraff Heidweiller, Meghan Voorhees, Steven Waltman, Jonathan Waltz, Shannon X. Wang, Danni Wang, Brayden Ware, James D. Watson, Yonghua Wei, Travis Weidel, Theodore White, Kristi Wong, Bryan W. K. Woo, Christopher J. Wood, Maddy Woodson, Cheng Xing, Z. Jamie Yao, Ping Yeh, Bicheng Ying, Juhwan Yoo, Noureldin Yosri, Elliot Young, Grayson Young, Adam Zalcman, Ran Zhang, Yaxing Zhang, Ningfeng Zhu, Nicholas Zobrist, Zhenjie Zou, Hartmut Neven, Sergio Boixo, Cody Jones, Julian Kelly, Alexandre Bourassa, Kevin J. Satzinger Venue: arXiv (2025)\nFault-tolerant quantum computing requires a universal gate set, but the necessary non-Clifford gates represent a significant resource cost for most quantum error correction architectures. Magic state cultivation offers an efficient alternative to resource-intensive distillation protocols; however, testing the proposal‚Äôs assumptions represents a challenging departure from quantum memory experiments. We present an experimental study of magic state cultivation on a superconducting quantum processor. We implement cultivation, including code-switching into a surface code, and develop a fault-tolerant measurement protocol to bound the magic state fidelity. Cultivation reduces the error by a factor of 40, with a state fidelity of 0.9999(1) (retaining 8% of attempts). Our results experimentally establish magic state cultivation as a viable solution to one of quantum computing‚Äôs most significant challenges.\nüìÑ Download PDF\nTransversal Clifford-Hierarchy Gates via Non-Abelian Surface Codes Authors: Alison Warman, Sakura Schafer-Nameki Venue: arXiv (2025)\nWe present a purely 2D transversal realization of phase gates at any level of the Clifford hierarchy, and beyond, using non-Abelian surface codes. Our construction encodes a logical qubit in the quantum double $D(G)$ of a non-Abelian group $G$ on a triangular spatial patch. The logical gate is implemented transversally by stacking on the spatial region a symmetry-protected topological (SPT) phase specified by a group 2-cocycle. The Bravyi‚ÄìK√∂nig theorem limits the unitary gates implementable by constant-depth quantum circuits on Pauli stabilizer codes in $D$ dimensions to the $D$-th level of the Clifford hierarchy. We bypass this, by constructing transversal unitary gates at arbitrary levels of the Clifford hierarchy purely in 2D, without sacrificing locality or fault tolerance, however at the cost of using the quantum double of a non-Abelian group $G$. Specifically, for $G = D_{4N}$, the dihedral group of order $8N$, we realize the phase gate $T^{1/N} = \\mathrm{diag}(1, e^{iœÄ/(4N)})$ in the logical $\\overline{Z}$ basis. For $8N = 2^n$, this gate lies at the $n$-th level of the Clifford hierarchy and, importantly, has a qubit-only realization: we show that it can be constructed in terms of Clifford-hierarchy stabilizers for a code with $n$ physical qubits on each edge of the lattice. We also discuss code-switching to the $\\mathbb{Z}_2 \\times \\mathbb{Z}_2$ and $\\mathbb{Z}_2$ toric codes, which can be utilized for the quantum error correction in this setup.\nüìÑ Download PDF\nLitePT: Lighter Yet Stronger Point Transformer Authors: Yuanwen Yue, Damien Robert, Jianyuan Wang, Sunghwan Hong, Jan Dirk Wegner, Christian Rupprecht, Konrad Schindler Venue: arXiv (2025)\nModern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\\times$ fewer parameters, runs $2\\times$ faster, and uses $2\\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.\nüìÑ Download PDF\nWhen a Nation Speaks: Machine Learning and NLP in People‚Äôs Sentiment Analysis During Bangladesh‚Äôs 2024 Mass Uprising Authors: Md. Samiul Alim, Mahir Shahriar Tamim, Maisha Rahman, Tanvir Ahmed Khan, Md Mushfique Anwar Venue: arXiv (2025)\nSentiment analysis, an emerging research area within natural language processing (NLP), has primarily been explored in contexts like elections and social media trends, but there remains a significant gap in understanding emotional dynamics during civil unrest, particularly in the Bangla language. Our study pioneers sentiment analysis in Bangla during a national crisis by examining public emotions amid Bangladesh‚Äôs 2024 mass uprising. We curated a unique dataset of 2,028 annotated news headlines from major Facebook news portals, classifying them into Outrage, Hope, and Despair. Through Latent Dirichlet Allocation (LDA), we identified prevalent themes like political corruption and public protests, and analyzed how events such as internet blackouts shaped sentiment patterns. It outperformed multilingual transformers (mBERT: 67%, XLM-RoBERTa: 71%) and traditional machine learning methods (SVM and Logistic Regression: both 70%). These results highlight the effectiveness of language-specific models and offer valuable insights into public sentiment during political turmoil.\nüìÑ Download PDF\nThe World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text Authors: Hanlin Wang, Hao Ouyang, Qiuyu Wang, Yue Yu, Yihao Meng, Wen Wang, Ka Leong Cheng, Shuailei Ma, Qingyan Bai, Yixuan Li, Cheng Chen, Yanhong Zeng, Xing Zhu, Yujun Shen, Qifeng Chen Venue: arXiv (2025)\nWe present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories ‚Äì encoding motion, timing, and visibility ‚Äì with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.\nüìÑ Download PDF\nNext-Embedding Prediction Makes Strong Vision Learners Authors: Sihan Xu, Ziqiao Ma, Wenhao Chai, Xuweiyi Chen, Weiyang Jin, Joyce Chai, Saining Xie, Stella X. Yu Venue: arXiv (2025)\nInspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.\nüìÑ Download PDF\nAdaTooler-V: Adaptive Tool-Use for Images and Videos Authors: Chaoyang Wang, Kaituo Feng, Dongyang Chen, Zhongyu Wang, Zhixun Li, Sicheng Gao, Meng Meng, Xu Zhou, Manyuan Zhang, Yuzhang Shang, Xiangyu Yue Venue: arXiv (2025)\nRecent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.\nüìÑ Download PDF\nGenerative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning Authors: Qihao Liu, Luoxin Ye, Wufei Ma, Yu-Cheng Chou, Alan Yuille Venue: arXiv (2025)\nLarge language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice‚Äôs soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.\nüìÑ Download PDF\nConstructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates Authors: Nikhil Prakash, Donghao Ren, Dominik Moritz, Yannick Assogba Venue: arXiv (2025)\nPrior studies investigating the internal workings of LLMs have uncovered sparse subnetworks, often referred to as circuits, that are responsible for performing specific tasks. Additionally, it has been shown that model performance improvement through fine-tuning often results from the strengthening of existing circuits in the model. Taken together, these findings suggest the possibility of intervening directly on such circuits to make precise, task-targeted updates. Motivated by these findings, we propose a novel method called Constructive Circuit Amplification which identifies pivotal tokens from model reasoning traces as well as model components responsible for the desired task, and updates only those components. Applied to mathematical reasoning, it improves accuracy by up to +11.4% across multiple models while modifying as little as 1.59% of model components, with minimal impact on other abilities as measured by MMLU, TriviaQA, and TruthfulQA. These results demonstrate that targeted capabilities can be reliably enhanced by selectively updating a sparse set of model components.\nüìÑ Download PDF\nExploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward Authors: Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen, Tianyi Lin Venue: arXiv (2025)\nThis paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.\nüìÑ Download PDF\nMomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning Authors: Yuanchen Ju, Yongyuan Liang, Yen-Jen Wang, Nandiraju Gireesh, Yuanliang Ju, Seungjae Lee, Qiao Gu, Elvis Hsieh, Furong Huang, Koushil Sreenath Venue: arXiv (2025)\nMobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.\nüìÑ Download PDF\nTiny Recursive Control: Iterative Reasoning for Efficient Optimal Control Authors: Amit Jain, Richard Linares Venue: arXiv (2025)\nNeural network controllers increasingly demand millions of parameters, and language model approaches push into the billions. For embedded aerospace systems with strict power and latency constraints, this scaling is prohibitive. We present Tiny Recursive Control (TRC), a neural architecture based on a counterintuitive principle: capacity can emerge from iteration depth rather than parameter count. TRC applies compact networks (approximately 1.5M parameters) repeatedly through a two-level hierarchical latent structure, refining control sequences by simulating trajectories and correcting based on tracking error. Because the same weights process every refinement step, adding iterations increases computation without increasing memory. We evaluate TRC on nonlinear control problems including oscillator stabilization and powered descent with fuel constraints. Across these domains, TRC achieves near-optimal control costs while requiring only millisecond-scale inference on GPU and under 10~MB memory, two orders of magnitude smaller than language model baselines. These results demonstrate that recursive reasoning, previously confined to discrete tasks, transfers effectively to continuous control synthesis.\nüìÑ Download PDF\nAnalogicity in List Coloring Problems and Interval $k$-$(Œ≥,Œº)$-choosability: A Complexity-Theoretic Study Authors: Simone Ingrid Monteiro Gama, Rosiane de Freitas Rodrigues Venue: arXiv (2025)\nThis work investigates structural and computational aspects of list-based graph coloring under interval constraints. Building on the framework of analogous and p-analogous problems, we show that classical List Coloring, $Œº$-coloring, and $(Œ≥,Œº)$-coloring share strong complexity-preserving correspondences on graph classes closed under pendant-vertex extensions. These equivalences allow hardness and tractability results to transfer directly among the models. Motivated by applications in scheduling and resource allocation with bounded ranges, we introduce the interval-restricted $k$-$(Œ≥,Œº)$-coloring model, where each vertex receives an interval of exactly $k$ consecutive admissible colors. We prove that, although $(Œ≥,Œº)$-coloring is NP-complete even on several well-structured graph classes, its $k$-restricted version becomes polynomial-time solvable for any fixed $k$. Extending this formulation, we define $k$-$(Œ≥,Œº)$-choosability and analyze its expressive power and computational limits. Our results show that the number of admissible list assignments is drastically reduced under interval constraints, yielding a more tractable alternative to classical choosability, even though the general decision problem remains located at high levels of the polynomial hierarchy. Overall, the paper provides a unified view of list-coloring variants through structural reductions, establishes new complexity bounds for interval-based models, and highlights the algorithmic advantages of imposing fixed-size consecutive color ranges.\nüìÑ Download PDF\nPhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence Authors: Xiaopeng Lin, Shijie Lian, Bin Yu, Ruoqi Yang, Changti Wu, Yuzhuo Miao, Yurun Jin, Yukun Shi, Cong Huang, Bojun Cheng, Kai Chen Venue: arXiv (2025)\nRobotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9%), demonstrating effective transfer from human egocentric supervision to downstream robot control.\nüìÑ Download PDF\nFew-Shot Specific Emitter Identification via Integrated Complex Variational Mode Decomposition and Spatial Attention Transfer Authors: Chenyu Zhu, Zeyang Li, Ziyi Xie, Jie Zhang Venue: arXiv (2025)\nSpecific emitter identification (SEI) utilizes passive hardware characteristics to authenticate transmitters, providing a robust physical-layer security solution. However, most deep-learning-based methods rely on extensive data or require prior information, which poses challenges in real-world scenarios with limited labeled data. We propose an integrated complex variational mode decomposition algorithm that decomposes and reconstructs complex-valued signals to approximate the original transmitted signals, thereby enabling more accurate feature extraction. We further utilize a temporal convolutional network to effectively model the sequential signal characteristics, and introduce a spatial attention mechanism to adaptively weight informative signal segments, significantly enhancing identification performance. Additionally, the branch network allows leveraging pre-trained weights from other data while reducing the need for auxiliary datasets. Ablation experiments on the simulated data demonstrate the effectiveness of each component of the model. An accuracy comparison on a public dataset reveals that our method achieves 96% accuracy using only 10 symbols without requiring any prior knowledge.\nüìÑ Download PDF\nDVGT: Driving Visual Geometry Transformer Authors: Sicheng Zuo, Zixun Xie, Wenzhao Zheng, Shaoqing Xu, Fang Li, Shengyin Jiang, Long Chen, Zhi-Xin Yang, Jiwen Lu Venue: arXiv (2025)\nPerceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.\nüìÑ Download PDF\nSFTok: Bridging the Performance Gap in Discrete Tokenizers Authors: Qihang Rao, Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu Venue: arXiv (2025)\nRecent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \\textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \\textbf{self-forcing guided visual reconstruction} and \\textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).\nüìÑ Download PDF\nSceneDiff: A Benchmark and Method for Multiview Object Change Detection Authors: Yuqun Wu, Chih-hao Lin, Henry Che, Aditi Tiwari, Chuhang Zou, Shenlong Wang, Derek Hoiem Venue: arXiv (2025)\nWe investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.\nüìÑ Download PDF\nFlowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos Authors: Mingfei Chen, Yifan Wang, Zhengqin Li, Homanga Bharadhwaj, Yujin Chen, Chuan Qin, Ziyi Kou, Yuan Tian, Eric Whitmire, Rajinder Sodhi, Hrvoje Benko, Eli Shlizerman, Yue Liu Venue: arXiv (2025)\nPrior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.\nüìÑ Download PDF\nFlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction Authors: Shuyuan Tu, Yueming Pan, Yinming Huang, Xintong Han, Zhen Xing, Qi Dai, Kai Qiu, Chong Luo, Zuxuan Wu Venue: arXiv (2025)\nCurrent diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.\nüìÑ Download PDF\nLinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation Authors: Haichao Zhang, Yao Lu, Lichen Wang, Yunzhe Li, Daiwei Chen, Yunpeng Xu, Yun Fu Venue: arXiv (2025)\nVideo Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.\nüìÑ Download PDF\nOPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction Authors: Yuxin Ray Song, Jinzhou Li, Rao Fu, Devin Murphy, Kaichen Zhou, Rishi Shiv, Yaqi Li, Haoyu Xiong, Crystal Elaine Owens, Yilun Du, Yiyue Luo, Xianyi Cheng, Antonio Torralba, Wojciech Matusik, Paul Pu Liang Venue: arXiv (2025)\nThe human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.\nüìÑ Download PDF\nMEPIC: Memory Efficient Position Independent Caching for LLM Serving Authors: Qian Wang, Zahra Yousefijamarani, Morgan Lindsay Heisler, Rongzhi Gu, Bai Xiaolong, Shan Yizhou, Wei Zhang, Wang Lan, Ying Xiong, Yong Zhang, Zhenan Fan Venue: arXiv (2025)\nModern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content. We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes.\nüìÑ Download PDF\nExploration of Augmentation Strategies in Multi-modal Retrieval-Augmented Generation for the Biomedical Domain: A Case Study Evaluating Question Answering in Glycobiology Authors: Primo≈æ Kocbek, Azra Frkatoviƒá-Hod≈æiƒá, Dora Laliƒá, Vivian Hui, Gordan Lauc, Gregor ≈†tiglic Venue: arXiv (2025)\nMulti-modal retrieval-augmented generation (MM-RAG) promises grounded biomedical QA, but it is unclear when to (i) convert figures/tables into text versus (ii) use optical character recognition (OCR)-free visual retrieval that returns page images and leaves interpretation to the generator. We study this trade-off in glycobiology, a visually dense domain. We built a benchmark of 120 multiple-choice questions (MCQs) from 25 papers, stratified by retrieval difficulty (easy text, medium figures/tables, hard cross-evidence). We implemented four augmentations-None, Text RAG, Multi-modal conversion, and late-interaction visual retrieval (ColPali)-using Docling parsing and Qdrant indexing. We evaluated mid-size open-source and frontier proprietary models (e.g., Gemma-3-27B-IT, GPT-4o family). Additional testing used the GPT-5 family and multiple visual retrievers (ColPali/ColQwen/ColFlor). Accuracy with Agresti-Coull 95% confidence intervals (CIs) was computed over 5 runs per configuration. With Gemma-3-27B-IT, Text and Multi-modal augmentation outperformed OCR-free retrieval (0.722-0.740 vs. 0.510 average accuracy). With GPT-4o, Multi-modal achieved 0.808, with Text 0.782 and ColPali 0.745 close behind; within-model differences were small. In follow-on experiments with the GPT-5 family, the best results with ColPali and ColFlor improved by ~2% to 0.828 in both cases. In general, across the GPT-5 family, ColPali, ColQwen, and ColFlor were statistically indistinguishable. GPT-5-nano trailed larger GPT-5 variants by roughly 8-10%. Pipeline choice is capacity-dependent: converting visuals to text lowers the reader burden and is more reliable for mid-size models, whereas OCR-free visual retrieval becomes competitive under frontier models. Among retrievers, ColFlor offers parity with heavier options at a smaller footprint, making it an efficient default when strong generators are available.\nüìÑ Download PDF\nFrom Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs Authors: Shubham Mishra, Samyek Jain, Gorang Mehrishi, Shiv Tiwari, Harsh Sharma, Pratik Narang, Dhruv Kumar Venue: arXiv (2025)\nRetrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.\nüìÑ Download PDF\nCitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs? Authors: Siqi Wang, Chao Liang, Yunfan Gao, Erxin Yu, Sen Li, Yushi Li, Jing Li, Haofen Wang Venue: arXiv (2025)\nVision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., ‚ÄúI am thirsty‚Äù) in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs‚Äô spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping‚Äôs emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling ‚Äúlast-mile‚Äù navigation challenges.\nüìÑ Download PDF\nüîç linguistics Generative Refocusing: Flexible Defocus Control from a Single Image Authors: Chun-Wei Tuan Mu, Jia-Bin Huang, Yu-Lun Liu Venue: arXiv (2025)\nDepth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.\nüìÑ Download PDF\nEasyV2V: A High-quality Instruction-based Video Editing Framework Authors: Jinjie Mai, Chaoyang Wang, Guocheng Gordon Qian, Willi Menapace, Sergey Tulyakov, Bernard Ghanem, Peter Wonka, Ashkan Mirzaei Venue: arXiv (2025)\nWhile image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/\nüìÑ Download PDF\nDifferences That Matter: Auditing Models for Capability Gap Discovery and Rectification Authors: Qihao Liu, Chengzhi Mao, Yaojie Liu, Alan Yuille, Wen-Sheng Chu Venue: arXiv (2025)\nConventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.\nüìÑ Download PDF\nDiscovering gravitational waveform distortions from lensing: a deep dive into GW231123 Authors: Juno C. L. Chan, Jose Mar√≠a Ezquiaga, Rico K. L. Lo, Joey Bowman, Lorena Maga√±a Zertuche, Luka Vujeva Venue: arXiv (2025)\nGravitational waves (GWs) are unique messengers as they travel through the Universe without alteration except for gravitational lensing. Their long wavelengths make them susceptible to diffraction by cosmic structures, providing an unprecedented opportunity to map dark matter substructures. Identifying lensed events requires the analysis of thousands to millions of simulated events to reach high statistical significances. This is computationally prohibitive with standard GW parameter estimation methods. We build on top of state-of-the-art neural posterior algorithms to accelerate the lensed inference from CPU days to minutes with DINGO-lensing. We showcase its capabilities by reanalyzing GW231123, the most promising lensed candidate so far, and find that its statistical significance cannot exceed 4$œÉ$. We observe that 8% of GW231123-like nonlensed simulations favor lensing, which could be explained by the self-similarity of short-duration signals. Still, 58% of GW231123-like lensed simulations have larger support for lensing, showing that higher detection statistics are possible. Although GW231123 exposes the challenges of claiming the first GW lensing detection, our deep-learning methods have demonstrated to be powerful enough to enable the upcoming discovery of lensed GWs.\nüìÑ Download PDF\nIn-Context Algebra Authors: Eric Todd, Jannik Brinkmann, Rohit Gandikota, David Bau Venue: arXiv (2025)\nWe investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.\nüìÑ Download PDF\nInstant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation Authors: Kaiwen Jiang, Xueting Li, Seonwook Park, Ravi Ramamoorthi, Shalini De Mello, Koki Nagano Venue: arXiv (2025)\nPortrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods ‚Äì built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting ‚Äì ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face‚Äôs 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d\nüìÑ Download PDF\nMachine learning assisted high throughput prediction of moir√© materials Authors: Daniel Kaplan, Alexander C. Tyner, Eva Y. Andrei, J. H. Pixley Venue: arXiv (2025)\nThe world of 2D materials is rapidly expanding with new discoveries of stackable and twistable layered systems composed of lattices of different symmetries, orbital character, and structural motifs. Often, however, it is not clear a priori whether a pair of monolayers twisted at a small angle will exhibit correlated or interaction-driven phenomena. The computational cost to make accurate predictions of the single particle states is significant, as small twists require very large unit cells, easily encompassing 10,000 atoms, and therefore implementing a high throughput prediction has been out of reach. Here we show a path to overcome this challenge by introducing a machine learning (ML) based methodology that efficiently estimates the twisted interlayer tunneling at arbitrarily low twist angles through the local-configuration based approach that enables interpolating the local stacking for a range of twist angles using a random forest regression algorithm. We leverage the kernel polynomial method to compute the density of states (DOS) on large real space graphs by reconstructing a lattice model of the twisted bilayer with the ML fitted hoppings. For twisted bilayer graphene (TBG), we show the ability of the method to resolve the magic angle DOS at a substantial improvement in computational time. We use this new technique to scan through the database of stable 2D monolayers (MC2D) and reveal new twistable candidates across the five possible points groups in two-dimensions with a large DOS near the Fermi energy, with potentially exciting interacting physics to be probed in future experiments.\nüìÑ Download PDF\nRevival Dynamics from Equilibrium States: Scars from Chords in SYK Authors: Debarghya Chakraborty, Dario Rosa Venue: arXiv (2025)\nWe develop a novel framework to build quantum many-body scar states in bipartite systems characterized by perfect correlation between the Hamiltonians governing the two sides. By means of a Krylov construction, we build an interaction term which supports a tower of equally-spaced energy eigenstates. This gives rise to finite-time revivals whenever the system is initialized in a purification of a generic equilibrium state. The dynamics is universally characterized, and is largely independent of the specific details of the Hamiltonians defining the individual partitions. By considering the two-sided chord states of the double-scaled SYK model, we find an approximate realization of this framework. We analytically study the revival dynamics, finding rigid motion for wavepackets localized on the spectrum of a single SYK copy. These findings are tested numerically for systems of finite size, showing excellent agreement with the analytical predictions.\nüìÑ Download PDF\nExperimental Measurement of Enhanced Group Delay Silicon Photonic Waveguides Indicative of the Frozen Mode Regime Around the Stationary Inflection Point Authors: Nathaniel Furman, Albert Herrero-Parareda, Anthony Rapp, Ilya Vitebskiy, Ricky Gibson, Bradley J. Thompson, Dean P. Brown, Robert Bedford, Filippo Capolino Venue: arXiv (2025)\nThe dispersion engineering of periodic silicon photonic waveguides presents opportunities for significant group delay enhancement compared to uniform waveguides of comparable length. We describe the spectral response characteristics for measured devices and compare their properties to modeled data. These waveguides support the frozen mode regime (FMR) around near infrared wavelengths and are expected to show enhanced group delays around the FMR resonances. Measurements of fabricated devices provide evidence for enhanced delays and spectral properties associated with the FMR. We study how perturbations to the waveguide model impact agreement with measurements and its meaning for these devices operating in the FMR.\nüìÑ Download PDF\nRayleigh-B√©nard thermal convection in emulsions: a short review Authors: Francesca Pelusi, Andrea Scagliarini, Mauro Sbragaglia, Massimo Bernaschi, Roberto Benzi Venue: arXiv (2025)\nThermally driven emulsions arise in a broad range of natural and industrial contexts, yet their fundamental physical understanding remains only partially established. Emulsions exhibit a complex, concentration-dependent rheology, ranging from Newtonian (dilute emulsions) to yield-stress (concentrated emulsions). In buoyancy-driven flows, the complex structure and rheology of the emulsion are strongly coupled to convective flows, giving rise to fascinating and non-trivial phenomena involving stability, transient dynamics, and morphological evolution of the system. We review recent progress on thermally driven emulsions in the celebrated Rayleigh-B√©nard configuration, offering new perspectives on the behaviour of soft materials in thermal convection.\nüìÑ Download PDF\nConsistent Excesses in the LHC Electroweak SUSY Searches: GUT-based Singlino/Higgsino Interpretation in the NMSSM Authors: Emanuele Bagnaschi, Manimala Chakraborti, Sven Heinemeyer, Ipsita Saha Venue: arXiv (2025)\nThe search for supersymmetric models remains one of the main items on the BSM search program at the LHC, with EW SUSY partners still allowed with masses as low as a few hundred GeV. Over the last years, searches for the ‚Äúgolden channel‚Äù, $pp \\to \\tildeœá^0_2 \\tildeœá^{\\pm}1 \\to \\tildeœá^0_1 Z^{()} \\tildeœá^0_1 W^{\\pm ()}$ show consistent excesses between ATLAS and CMS in the 2soft-lepton and 3soft-lepton plus missing-$E_T$ searches, assuming $m{\\tildeœá^0_2} \\approx m_{\\tildeœá^{\\pm}1} \\gtrsim 200$ GeV and $Œîm{21} := m_{\\tildeœá^0_2} - m_{\\tildeœá^0_1} \\approx 20$ GeV. We interpret these excesses in the framework of the Next-to-Minimal Supersymmetric Standard Model. We assume a singlino dominated lightest neutralino as a Dark Matter (DM) candidate. The second and third lightest neutralinos are higgsino like, with the higgsino mixing parameter $Œº$ being smaller than the soft SUSY-breaking bino and wino masses, $M_1$ and $M_2$. We furthermore assume the approximate GUT relations $M_1 \\sim M_2/2 \\sim M_3/6$, with the implication for our scenario of a gluino mass $m_{\\tilde{g}} \\sim M_3 \\gtrsim 3$ TeV. Scalar masses are assumed to heavy and do not play a role in our analysis. We find that this scenario is in agreement with all relevant experimental constraints, comprising the LHC searches for SUSY particles and additional Higgs bosons, the LHC Higgs-boson rate measurements, the DM direct detection limits and the upper limit on the DM relic density. We demonstrate that this scenario gives an excellent description of the observed excesses in the search for 2and 3soft-leptons plus \\ETmiss, with $m_{\\tildeœá^0_2} \\sim m_{\\tildeœá^0_3} \\sim m_{\\tildeœá^{\\pm}1}$ and $Œîm{21} \\sim 20$ GeV. This constitutes the first explanation of the soft-lepton excesses in a model with GUT relations among the soft SUSY-breaking parameters.\nüìÑ Download PDF\nField Quantisations in Schwarzschild Spacetime: Theory versus Low-Energy Experiments Authors: Viacheslav A. Emelyanov Venue: arXiv (2025)\nNon-relativistic quantum particles in the Earth‚Äôs gravitational field are successfully described by the Schr√∂dinger equation with Newton‚Äôs gravitational potential. Particularly, quantum mechanics is in agreement with such experiments as free fall and quantum interference induced by gravity. However, quantum mechanics is a low-energy approximation to quantum field theory. The latter is successful by the description of high-energy experiments. Gravity is embedded in quantum field theory through the general-covariance principle. This framework is known in the literature as quantum field theory in curved spacetime, where the concept of a quantum particle is, though, ambiguous. In this article, we study in this framework how a Hawking particle moves in the far-horizon region of Schwarzschild spacetime by computing its propagator. We find this propagator differs from that which follows from the path-integral formalism ‚Äì the formalism which adequately describes both free fall and quantum interference induced by gravity.\nüìÑ Download PDF\nSelf-Affine Scaling of Earth‚Äôs Islands Authors: Matthew Oline, Jeremy Hoskins, David Seekell, Mary Silber, B. B. Cael Venue: arXiv (2025)\nEarth‚Äôs relief is approximately self-affine, meaning a zoom-in on a small region looks statistically similar to a large region upon a suitable rescaling. Fractional Brownian surfaces give an idealized self-affine model of Earth‚Äôs relief with one parameter, the Hurst exponent $H$, characterizing the roughness of the surface. To quantitatively assess agreement with Earth elevation data, we compile a large dataset of topographic profiles of islands (N=131,063 with the range of areas covering 8+ orders of magnitude) and obtain four estimates for the Hurst exponent of Earth‚Äôs surface by fitting four statistical laws from the theory of self-affine surfaces concerning islands: (i) distribution of areas, (ii) volume-area relationship, (iii) perimeter-area relationship, and (iv) maximum height-area relationship. The estimated Hurst exponents differ greatly, indicating different fractal scaling behavior for different geometric features, but are sorted in order of increasing expected influence of erosion at the shorelines.\nüìÑ Download PDF\nStereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors Authors: Guibao Shen, Yihua Du, Wenhang Ge, Jing He, Chirui Chang, Donghao Zhou, Zhen Yang, Luozhou Wang, Xin Tao, Ying-Cong Chen Venue: arXiv (2025)\nThe rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint‚Äô‚Äô (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.\nüìÑ Download PDF\nDepth Any Panoramas: A Foundation Model for Panoramic Depth Estimation Authors: Xin Lin, Meixi Song, Dizhe Zhang, Wenxuan Lu, Haodong Li, Bo Du, Ming-Hsuan Yang, Truong Nguyen, Lu Qi Venue: arXiv (2025)\nIn this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \\href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP_website/}\nüìÑ Download PDF\nOpening the House: Datasets for Mixed Doubles Curling Authors: Robyn Ritchie, Alexandre Leblanc, Thomas Loughin Venue: arXiv (2025)\nWe introduce the most comprehensive publicly available datasets for mixed doubles curling, constructed from eleven top-level tournaments from the CurlIT (https://curlit.com/results) Results Booklets spanning 53 countries, 1,112 games, and nearly 70,000 recorded shots. While curling analytics has grown in recent years, mixed doubles remains under-served due to limited access to data. Using a combined text-scraping and image-processing pipeline, we extract and standardize detailed game- and shot-level information, including player statistics, hammer possession, Power Play usage, stone coordinates, and post-shot scoring states. We describe the data engineering workflow, highlight challenges in parsing historical records, and derive additional contextual features that enable rigorous strategic analysis. Using these datasets, we present initial insights into shot selection and success rates, scoring distributions, and team efficiencies, illustrating key differences between mixed doubles and traditional 4-player curling. We highlight various ways to analyze this type of data including from a shot-, end-, game- or team-level to display its versatilely. The resulting resources provide a foundation for advanced performance modeling, strategic evaluation, and future research in mixed doubles curling analytics, supporting broader analytical engagement with this rapidly growing discipline.\nüìÑ Download PDF\nBrepLLM: Native Boundary Representation Understanding with Large Language Models Authors: Liyuan Deng, Hao Guo, Yunpeng Bai, Yongkang Dai, Huaxi Huang, Yilei Shi Venue: arXiv (2025)\nCurrent token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.\nüìÑ Download PDF\nOccupational Tasks, Automation, and Economic Growth: A Modeling and Simulation Approach Authors: Georgios A. Tritsaris Venue: arXiv (2025)\nThe Fourth Industrial Revolution commonly refers to the accelerating technological transformation that has been taking place in the 21st century. Economic growth theories which treat the accumulation of knowledge and its effect on production endogenously remain relevant, yet they have been evolving to explain how the current wave of advancements in automation and artificial intelligence (AI) technology will affect productivity and different occupations. The work contributes to current economic discourse by developing an analytical task-based framework that endogenously integrates knowledge accumulation with frictions that describe technological lock-in and the burden of knowledge generation and validation. The interaction between production (or automation) and growth (or knowledge accumulation) is also described explicitly. To study how automation and AI shape economic outcomes, I rely on high-throughput calculations of the developed model. The effect of the model‚Äôs structural parameters on key variables such as the production output, wages, and labor shares of output is quantified, and possible intervention strategies are briefly discussed. An important result is that wages and labor shares are not directly linked, instead they can be influenced independently through distinct policy levers.\nüìÑ Download PDF\nPrivacy Discourse and Emotional Dynamics in Mental Health Information Interaction on Reddit Authors: Jai Kruthunz Naveen Kumar, Aishwarya Umeshkumar Surani, Harkirat Singh, Sanchari Das Venue: arXiv (2025)\nReddit is a major venue for mental-health information interaction and peer support, where privacy concerns increasingly surface in user discourse. Thus, we analyze privacy-related discussions across 14 mental-health and regulatory subreddits, comprising 10,119 posts and 65,385 comments collected with a custom web scraper. Using lexicon-based sentiment analysis, we quantify emotional alignment between communities via cosine similarity of sentiment distributions, observing high similarity for Bipolar and ADHD (0.877), Anxiety and Depression (0.849), and MentalHealthSupport and MentalIllness (0.989) subreddits. We also construct keyword dictionaries to tag privacy-related themes (e.g., HIPAA, GDPR) and perform temporal analysis from 2020 to 2025, finding a 50% increase in privacy discourse with intermittent regulatory spikes. A chi-square test of independence across subreddit domains indicates significant distributional differences. The results characterize how privacy-oriented discussion co-varies with user sentiment in online mental-health communities.\nüìÑ Download PDF\nThe Moralization Corpus: Frame-Based Annotation and Analysis of Moralizing Speech Acts across Diverse Text Genres Authors: Maria Becker, Mirko Sommer, Lars Tapken, Yi Wan Teh, Bruno Brocai Venue: arXiv (2025)\nMoralizations - arguments that invoke moral values to justify demands or positions - are a yet underexplored form of persuasive communication. We present the Moralization Corpus, a novel multi-genre dataset designed to analyze how moral values are strategically used in argumentative discourse. Moralizations are pragmatically complex and often implicit, posing significant challenges for both human annotators and NLP systems. We develop a frame-based annotation scheme that captures the constitutive elements of moralizations - moral values, demands, and discourse protagonists - and apply it to a diverse set of German texts, including political debates, news articles, and online discussions. The corpus enables fine-grained analysis of moralizing language across communicative formats and domains. We further evaluate several large language models (LLMs) under varied prompting conditions for the task of moralization detection and moralization component extraction and compare it to human annotations in order to investigate the challenges of automatic and manual analysis of moralizations. Results show that detailed prompt instructions has a greater effect than few-shot or explanation-based prompting, and that moralization remains a highly subjective and context-sensitive task. We release all data, annotation guidelines, and code to foster future interdisciplinary research on moral discourse and moral reasoning in NLP.\nüìÑ Download PDF\nProbing scalar and pseudoscalar new physics using rare kaon decays Authors: G. D‚ÄôAmbrosio, A. M. Iyer, F. Mahmoudi, S. Neshatpour Venue: arXiv (2025)\nRare kaon decays provide sensitive tests of new physics. In this work, we focus on scalar and pseudoscalar operators, analysing the $K\\to œÄ\\ell^+\\ell^-$ and $K\\to \\ell^+\\ell^-$ decays. We highlight the complementary role of different modes: $K^+\\toœÄ^+\\ell^+\\ell^-$, in particular the forward-backward asymmetry in the muon channel as a clean probe of scalar effects, the stringent constraints from $K_L\\to Œº^+Œº^-$, and the discovery potential of future measurements of $K_S\\to Œº^+Œº^-$ and $K_L\\to œÄ^0 \\ell^+\\ell^-$. The interplay between charged and neutral modes underscores the complementarity of NA62, the LHCb upgrade, and KOTO-II.\nüìÑ Download PDF\nMany-body contextuality and self-testing quantum matter via nonlocal games Authors: Oliver Hart, David T. Stephen, Evan Wickenden, Rahul Nandkishore Venue: arXiv (2025)\nContextuality is arguably the fundamental property that makes quantum mechanics different from classical physics. It is responsible for quantum computational speedups in both magic-state-injection-based and measurement-based models of computation, and can be directly probed in a many-body setting by multiplayer nonlocal quantum games. Here, we discuss a family of games that can be won with certainty when performing single-site Pauli measurements on a state that is a codeword of a Calderbank-Shor-Steane (CSS) error-correcting quantum code. We show that these games require deterministic computation of a code-dependent Boolean function, and that the classical probability of success is upper bounded by a generalized notion of nonlinearity/nonquadraticity. This success probability quantifies the state‚Äôs contextuality, and is computed via the function‚Äôs (generalized) Walsh-Hadamard spectrum. To calculate this, we introduce an efficient, many-body-physics-inspired method that involves identifying the symmetries of an auxiliary hypergraph state. We compute the classical probability of success for several paradigmatic CSS codes and relate it to both classical statistical mechanics models and to strange correlators of symmetry-protected topological states. We also consider CSS submeasurement games, which can only be won with certainty by sharing the appropriate codeword up to local isometries. These games therefore enable self-testing, which we illustrate explicitly for the 2D toric code. We also discuss how submeasurement games enable an extensive notion of contextuality in many-body states.\nüìÑ Download PDF\nWiedemann-Franz violation and thermal Hall effect in kagome metal TbCr6Ge6 Authors: Jhinkyu Choi, Mohan B. Neupane, L. H. Vilela-Le√£o, Bishnu P. Belbase, Arjun Unnikrishnan, Syeda Neha Zaidi, Jukka I. V√§yrynen, Arnab Banerjee Venue: arXiv (2025)\nThe thermal Hall effect has emerged as a powerful probe of exotic excitations in correlated quantum materials, providing access to charge-neutral heat carriers that remain invisible to electrical transport. To directly examine how heat and charge respond in relation within a kagome metal, we investigate the ferrimagnetic rare-earth 1-6-6 compound TbCr6Ge6 using the Wiedemann-Franz (WF) framework. We observe a dramatic breakdown of the WF law across the ferrimagnetic transition, where both longitudinal and transverse Lorenz ratios, L_{xx,xy} = Œ∫_{xx,xy} / (T œÉ_{xx,xy}), deviate strongly from the Sommerfeld value L_0. After a partial recovery toward L_0 near 5-7 K, the Lorenz ratios are sharply suppressed well below L_0 despite a metallic charge response. We further find a pronounced low-temperature suppression of both L_{xx} and L_{xy} and a sign-changing transverse Lorenz ratio, indicating a clear decoupling between heat and charge transport and signaling substantial contributions from charge-neutral excitations whose Berry-curvature-driven transverse response evolves with temperature and magnetic field. TbCr6Ge6 thus provides a tunable metallic platform in which exchange-driven ferrimagnetism governs both longitudinal and transverse thermal responses, enabling controlled departures from Wiedemann-Franz behavior over an experimentally accessible temperature and field range.\nüìÑ Download PDF\nNonstabilizerness in Stark many-body localization Authors: Han-Ze Li, Yi-Rui Zhang, Yu-Jun Zhao, Xuyang Huang, Jian-Xin Zhong Venue: arXiv (2025)\nQuantum many-body disorder-free localization can suppress transport while still allowing the buildup of computationally costly non-Clifford resources. In a transverse-field Ising chain realizing disorder-free Stark many-body localization, we show that the stabilizer R√©nyi entropy remains nonzero and grows slowly to a finite plateau deep in the strong Stark-field regime, with strong initial-state selectivity. As the Stark field strength increases, long-time magic and entanglement consistently signal a crossover from ergodic to constrained localized dynamics. These results establish nonstabilizerness (``magic‚Äô‚Äô) as a practical complexity probe for disorder-free ergodicity breaking and constrained localization, with direct relevance to benchmarking and designing near-term quantum simulators, and fill a gap in the understanding of nonstabilizerness in disorder-free many-body localization.\nüìÑ Download PDF\nTOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge Authors: Khurram Khalil, Khaza Anuarul Hoque Venue: arXiv (2025)\nLarge Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.\nüìÑ Download PDF\nFighting non-locality with non-locality: microcausality and boundary conditions in QED Authors: Philipp A. Hoehn, Josh Kirklin Venue: arXiv (2025)\nIn gauge theories, globally charged observables necessarily depend non-locally on the kinematical fields, with this dependence extending to the asymptotic boundary of spacetime. Despite this, we show that a subset of such observables can be consistently regarded as local to the bulk, in a manner that respects microcausality and leaves locality properties of uncharged observables untouched. A sufficient condition for this is to impose kinematically non-local boundary conditions on the large gauge sector of the theory, and to invoke a relational notion of localisation for observables. This reveals a relatively underappreciated link between boundary conditions, and different notions of microcausality and locality. We develop this point through a detailed case study in scalar QED, describing non-local boundary conditions that allow a large family of observables on a codimension-1 bulk surface to be viewed as local to that surface, despite being dressed by asymptotic Wilson lines. We show that this property continues to hold within a perturbative quantisation of the theory, and we argue that this leads to a consistent local net of algebras that includes these charged observables in bulk algebras. We explain how this setup may be understood in terms of a preferred dynamical reference frame for small gauge transformations appearing in the boundary conditions. Many features of the theory (such as microcausality, the vacuum state, and the net of algebras of observables) depend on the choice of this frame, and we briefly discuss some repercussions of this for algebraic formulations of QFT. While our analysis is performed in QED, we expect our results to carry over qualitatively to more complicated theories including gravity.\nüìÑ Download PDF\nGrowing Self-Similar Markov Trees Authors: Nicolas Curien, William Fleurat, Adrianus Twigt Venue: arXiv (2025)\nCan we obtain a Brownian CRT of mass $1/2$ from a CRT of mass $1$ by cutting certain branches? In this paper, we will answer that question in the much more general setting of self-similar Markov trees. Self-similar Markov trees (ssMt) are random decorated trees that encode the genealogy of a system of particles carrying positive labels, and where particles undergo splitting and growth depending on their labels in a self-similar fashion. Introduced and developed in the recent monograph (Bertoin-Curien-Riera, 2024), they provide a broad generalization of Brownian and stable continuum random trees and arise naturally in various models of random geometry such as the Brownian sphere/disk. The law of a ssMt is characterized by its quadruplet $(\\mathrm{a}, œÉ^2, \\boldsymbolŒõ; Œ±)$, which specifies the features of the underlying growth-fragmentation mechanism, together with the initial decoration $x\u003e0$. In this work, we focus on special cases of ssMt in which the trees started from different initial values $x\u003e0$ can be coupled into a continuous, increasing family of nested subtrees. In the case of the Brownian and stable continuum random trees, this yields surprisingly simple novel dynamics corresponding to the scaling limit of the leaf-growth algorithms of Luczak-Winkler and Caraceni-Stauffer.\nüìÑ Download PDF\nAn exciton interacting with the phonons of an electronic Wigner crystal Authors: Jens Havgaard Nyhegn, Esben Rohan Christensen, Georg M. Bruun Venue: arXiv (2025)\nWith the advent of atomically thin and tunable van der Waals materials, a two-dimensional electronic Wigner crystal has recently been observed. The smoking gun signal was the appearance of an umklapp branch in optical exciton spectroscopy coming from the periodic potential generated by the Wigner crystal assumed to be static. Vibrations of the Wigner crystal however leads to a gapless phonon spectrum, which may affect the exciton spectrum. To explore this, we develop a field theoretical description of an exciton interacting with electrons forming a Wigner crystal including the coupling to the phonons. We show that importance of the exciton-phonon coupling scales with the exciton-electron interaction strength relative to the typical phonon energy squared. The motion of the exciton leads to two kinds of scattering processes, where the exciton emits a phonon either staying within the same Bloch band (intraband scattering) or changing its band (interband scattering). Using a non-perturbative self-consistent Born approximation, we demonstrate that these scattering processes lead to the formation of quasiparticles (polarons) consisting of the exciton in Bloch states dressed by Wigner crystal phonons. The energy shift and damping of these polarons depend on the electron density in a non-trivial way since it affects both the exciton-phonon interaction strength, as well as the phonon and exciton spectra. In particular, the damping is strongly affected by whether the polaron energy is inside the gapless phonon scattering continuum or not. Using these results, we finally analyse their effects on the observed spectral properties of the exciton.\nüìÑ Download PDF\nElectric field diagnostics in a continuous rf plasma using Rydberg-EIT Authors: Bineet Dash, Xinyan Xiang, Dingkun Feng, Eric Paradis, Georg Raithel Venue: arXiv (2025)\nWe present a non-invasive spectroscopic technique to measure electric fields in plasma, leveraging large polarizabilities and Stark shifts of Rydberg atoms. Rydberg Stark shifts are measured with high precision using narrow-linewidth lasers via Electromagnetically Induced Transparency (EIT) of rubidium vapor seeded into a continuous, inductively coupled radio-frequency (rf) plasma in a few mTorr of argon gas. Without plasma, the Rydberg-EIT spectra exhibit rf modulation sidebands caused by electric- and magnetic-dipole transitions in the rf drive coil. With the plasma present, the rf modulation sidebands vanish due to screening of the rf drive field from the plasma interior. The lineshapes of the EIT spectra in the plasma reflect the plasma‚Äôs Holtsmark microfield distribution, allowing us to determine plasma density and collisional line broadening over a range of pressures and rf drive powers. The work is expected to have applications in non-invasive spatio-temporal electric-field diagnostics of low-pressure plasma, plasma sheaths, process plasma and dusty plasma.\nüìÑ Download PDF\nGrammar-Forced Translation of Natural Language to Temporal Logic using LLMs Authors: William English, Dominic Simon, Sumit Kumar Jha, Rickard Ewetz Venue: arXiv (2025)\nTranslating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.\nüìÑ Download PDF\nSCOPE: Simple Coil Optimization for Plasma and Engineering Authors: Nathan Welch, Chris Marsden Venue: arXiv (2025)\nDesigning superconducting coils for a tokamak fusion device is a highly coupled, non-linear design problem. The coils have many disparate engineering requirements from structural to power electronics, as well strict limits placed on the system by the high temperature superconducting (HTS) cables. Simultaneously, the coils must be able to contain multiple plasma scenarios from inception, through ramp up, to flat top, and ramp down, all whilst applying a large, controlled, inductive voltage to drive current. In addition, we wish to optimize divertor separatrices to increase the likelihood of designing a suitable divertor strikepoint. Lastly, the physical limits of the entire tokamak must be taken into account and space reserved for support structures, access for maintenance schemes, and installation limits. The method outlined here uses a combined simulated annealing method to find optimal coil sizes and positions with a constrained quadratic or quartic optimization for the coil currents. The method is designed to optimize coils for multiple scenarios simultaneously, including ramp-ups, to avoid over optimization of a single design point. A key enabler is the efficient implementation that allows millions of evaluations to be performed in a few hours with modest computational power. This optimization method is part of a larger, iterative workflow which enables further, detailed design work to feedback on the optimization.\nüìÑ Download PDF\nSmile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors Authors: Kejun Liu, Yuanyuan Liu, Lin Wei, Chang Tang, Yibing Zhan, Zijing Chen, Zhe Chen Venue: arXiv (2025)\nEmotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.\nüìÑ Download PDF\nBlack-Start Power Capacity Sizing and Control Strategy for an Islanded DFIG Wind-to-Hydrogen System Authors: Bosen Yang, Kang Ma, Jin Lin, Yonghua Song Venue: arXiv (2025)\nThis paper proposes a black-start method for an off-grid wind-to-hydrogen (W2H) system comprising a wind farm based on Doubly-Fed Induction Generators (DFIGs), proton exchange membrane fuel cells (PEMFCs) serving as the black-start power source, and a hydrogen production industry. The PEMFC is installed within the hydrogen industry to facilitate direct access to hydrogen fuel. Based on the microgrid topology and black-start scheme, this study innovatively sizes the rated capacity of the PEMFC through power flow analysis. The capacity must be sufficient to charge passive components such as transmission lines and transformers, provide rotor excitation, and supply wind turbine (WT) and electrolyzer (ELZ) auxiliaries during startup. The proposed system integrates wind-hydrogen coordinated control (WHCC) and hydrogen-storage coordinated control (HSCC). Under maximum power point tracking (MPPT) of the WTs, the ELZ follows power fluctuations to absorb wind output, ensuring stable voltage and frequency. Fixed-frequency control applied to either the DFIG or PEMFC converters enables DFIGs to retain conventional grid-following (GFL) operation, reducing converter development costs. For both control modes, this paper establishes the black-start sequence and formulates a comprehensive coordinated control strategy for the entire system. The entire control system is validated through simulations in MATLAB/Simulink. Results confirm that the calculated PEMFC capacity supports reliable black-start, while the black-start control strategy ensures smooth system self-startup. Furthermore, the coordinated control strategy maintains stable frequency and voltage under fluctuating wind power, demonstrating the practicality and robustness of the proposed approach.\nüìÑ Download PDF\nGRHayL: a modern, infrastructure-agnostic, extensible library for GRMHD simulations Authors: Samuel Cupp, Leonardo R. Werneck, Terrence Pierre Jacques, Samuel Tootle, Zachariah B. Etienne Venue: arXiv (2025)\nInterpreting multi-messenger signals from neutron stars and black holes requires reliable general-relativistic magnetohydrodynamics (GRMHD) simulations across rapidly evolving high-performance-computing platforms, yet key algorithms are routinely rewritten within infrastructure-specific numerical-relativity codes, hindering verification and reuse. We present the General Relativistic Hydrodynamics Library (GRHayL), a modular, infrastructure-agnostic GR(M)HD library providing conservative-to-primitive recovery, reconstruction, flux/source and induction operators, equations of state, and neutrino leakage through an intuitive interface. GRHayL refactors and extends the mature IllinoisGRMHD code into reusable pointwise and stencil-wise kernels, enabling rapid development and cross-code validation in diverse frameworks, while easing adoption of new microphysics and future accelerators. We implement the same kernels in the Einstein Toolkit (Carpet and CarpetX) and BlackHoles@Home, demonstrating portability with minimal duplication. Validation combines continuous-integration unit tests with cross-infrastructure comparisons of analytic GRMHD Riemann problems, dynamical Tolman-Oppenheimer-Volkoff evolutions, and binary neutron-star mergers, showing comparable or improved behavior over legacy IllinoisGRMHD and established Einstein Toolkit codes.\nüìÑ Download PDF\nüîç psycholinguistics Solving the Dirac equation on a GPU for strong-field processes in multidimensional background fields Authors: Greger Torgrimsson Venue: arXiv (2025)\nIn this paper, we show how to solve the Dirac equation, $(iŒ≥^Œº[\\partial_Œº+ieA_Œº(t,{\\bf x})]-m)œà=0$, on a GPU. This is orders of magnitude faster than solving it on CPU and allows us to consider background fields, $A_Œº(t,{\\bf x})$, that depend on $2+1$ or even $3+1$ coordinates. Our approach is conveniently implemented using the computational library JAX. We show how to obtain the probabilities of Schwinger and nonlinear Breit-Wheeler pair production from these solutions using a scattered-wave-function approach and compare the results with the worldline-instanton approximations.\nüìÑ Download PDF\nASKAP discovery of a 30 kpc bipolar outflow from the edge-on disk of the nearby spiral galaxy ESO 130-G012 Authors: Baerbel S. Koribalski, Roland M. Crocker, Ildar Khabibullin, Anna Ivleva, Klaus Dolag, Umberto Maio, Ralf-Juergen Dettmar, Jacco Th. van Loon, Stanislav Shabala Venue: arXiv (2025)\nWe present the discovery of a large-scale, limb-brightened outflow, extending at least 30 kpc above and below the star-forming disk of the edge-on galaxy ESO 130-G012 (D = 16.9 Mpc). Partially obscured by Galactic foreground stars and dust, this optically unremarkable, low-mass galaxy reveals one of the largest known hourglass-shaped outflows from the full extent of its bright stellar disk. The outflow was discovered in 944 MHz radio continuum images from the Australian Square Kilometre Array Pathfinder (ASKAP) obtained as part of the ‚ÄúEvolutionary Map of the Universe‚Äù (EMU) project. Its height is at least 3x that of the stellar disk diameter (~10 kpc), while its shape and size most resemble the large biconical, edge-brightened FUV and X-ray outflows in the nearby starburst galaxy NGC 3079. The large-scale, hourglass-shaped outflow of ESO 130-G012 appears to be hollow and originates from the star-forming disk, expanding into the halo with speeds close to the escape velocity before likely returning to the disk. Given ESO 130-G012‚Äôs modest star formation rate, the height of the outflow is surprising and unusual, likely made possible by the galaxy‚Äôs relatively low gravitational potential. Follow-up observations are expected to detect hot gas inside the bipolar outflow cones and magnetic fields along the X-shaped outflow wings. Neutral gas may also be lifted above the inner disk by the outflow.\nüìÑ Download PDF\nOn right units of special inverse monoids Authors: Igor Dolinka, Robert D. Gray Venue: arXiv (2025)\nWe study the class of monoids that arise as the submonoid of right units of finitely presented special inverse monoids (SIMs). Gray and Ru≈°kuc (2024) gave the first example of a finitely presented SIM whose submonoid of right units does not admit a decomposition into a free product of the group of units and a finite rank free monoid. In the first part of this paper we prove a general result which shows that the only instances where the right units of a finitely presented SIM can admit such a free product decomposition is when their group of units is finitely presented. In showing this, we establish some general results about finite generation and presentability of subgroups of SIMs. In particular, we give an exact characterisation of when an arbitrary subgroup is finitely generated in terms of connectedness properties of unions of its cosets in its $\\mathscr{R}$-class, and also a characterisation of when an arbitrary subgroup is finitely presented. We also give a sufficient condition for finite generation and presentability of an arbitrary subgroup given in terms of a geometric finiteness property called boundary width. As a consequence, we show that the classes of monoids of right units of finitely presented SIMs and prefix monoids of finitely presented groups are independent. In the second part of the paper, we show that every finitely generated submonoid of a finitely RC-presented monoid is isomorphic to a submonoid $N$ of a finitely presented SIM $M$ such that $N$ is a submonoid of the right units of $M$, and $N$ contains the group of units of $M$. This result generalises and extends the classification of groups of units of finitely presented SIMs recently obtained by Gray and Kambites (2025). From this, we derive a number of surprising properties of RC-presentations for right cancellative monoids contrasting the classical theory of monoid presentations.\nüìÑ Download PDF\nWave-packet dynamics in pseudo-Hermitian lattices: Coexistence of Hermitian and non-Hermitian wavefronts Authors: Alon Beck, Moshe Goldstein Venue: arXiv (2025)\nThis paper investigates wave-packet dynamics in non-Hermitian lattice systems and reveals a surprising phenomenon: The simultaneous propagation of two distinct wavefronts, one traveling at the non-Hermitian velocity and the other at the Hermitian velocity. We show that this dual-front behavior arises naturally in systems governed by a pseudo-Hermitian Hamiltonian. Using the paradigmatic Hatano-Nelson model as our primary example, we demonstrate that this coexistence is essential for understanding a wide array of unconventional dynamical effects, including abrupt ``non-Hermitian reflections‚Äô‚Äô, sudden shifts of Gaussian wave-packets, and disorder-induced emergent packets seeded by the small initial tails. We present analytic predictions that closely match numerical simulations. These results may offer new insight into the topology of non-Hermitian systems and point toward measurable experimental consequences.\nüìÑ Download PDF\nOffline Multi-Task Multi-Objective Data-Driven Evolutionary Algorithm with Language Surrogate Model and Implicit Q-Learning Authors: Xian-Rong Zhang, Yue-Jiao Gong, Zeyuan Ma, Jun Zhang Venue: arXiv (2025)\nData-driven evolutionary algorithms has shown surprising results in addressing expensive optimization problems through robust surrogate modeling. Though promising, existing surrogate modeling schemes may encounter limitations in complex optimization problems with many sub-objectives, which rely on repeated and tedious approximation. To address such technical gap, we propose Q-MetaSur as a plug-and-play surrogate modeling scheme capable of providing unified and generalized surrogate learning. Specifically, we consider multi-task-multi-objective optimization~(MTMOO) in offline setting. Several key designs are proposed: 1) we transform objective approximation into sequence-to-sequence modeling where MTMOO problem can be represented by tenxual tokenization. To operate under such auto-regressive modeling, we introduce a Large Language Model-based surrogate model that first encodes a MTMOO instance and then decodes objective values of unseen decision variables. To ensure stability in training the proposed model, we propose a two-stage offline training strategy that operates as a synergy of supervised tuning and RL fine-tuning, which first exploits offline dataset to fit existing knowledge and then leverages RL to enhance model‚Äôs generalization performance. Extensive empirical results on the CEC2019 benchmark demonstrate that Q-MetaSur not only outperforms representative surrogate baselines in objective approximation accuracy, but also helps underlying evolutionary algorithms achieve both desired optimization convergence and improved pareto optimality.\nüìÑ Download PDF\nAutonomous Learning of Attractors for Neuromorphic Computing with Wien Bridge Oscillator Networks Authors: Riley Acker, Aman Desai, Garrett Kenyon, Frank Barrows Venue: arXiv (2025)\nWe present an oscillatory neuromorphic primitive implemented with networks of coupled Wien bridge oscillators and tunable resistive couplings. Phase relationships between oscillators encode patterns, and a local Hebbian learning rule continuously adapts the couplings, allowing learning and recall to emerge from the same ongoing analog dynamics rather than from separate training and inference phases. Using a Kuramoto-style phase model with an effective energy function, we show that learned phase patterns form attractor states and validate this behavior in simulation and hardware. We further realize a 2-4-2 architecture with a hidden layer of oscillators, whose bipartite visible-hidden coupling allows multiple internal configurations to produce the same visible phase states. When inputs are switched, transient spikes in energy followed by relaxation indicate how the network can reduce surprise by reshaping its energy landscape. These results support coupled oscillator circuits as a hardware platform for energy-based neuromorphic computing with autonomous, continuous learning.\nüìÑ Download PDF\nMaximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics Authors: Aaron Wei, Milad Jalali, Danica J. Sutherland Venue: arXiv (2025)\nExisting two-sample testing techniques, particularly those based on choosing a kernel for the Maximum Mean Discrepancy (MMD), often assume equal sample sizes from the two distributions. Applying these methods in practice can require discarding valuable data, unnecessarily reducing test power. We address this long-standing limitation by extending the theory of generalized U-statistics and applying it to the usual MMD estimator, resulting in new characterization of the asymptotic distributions of the MMD estimator with unequal sample sizes (particularly outside the proportional regimes required by previous partial results). This generalization also provides a new criterion for optimizing the power of an MMD test with unequal sample sizes. Our approach preserves all available data, enhancing test accuracy and applicability in realistic settings. Along the way, we give much cleaner characterizations of the variance of MMD estimators, revealing something that might be surprising to those in the area: while zero MMD implies a degenerate estimator, it is sometimes possible to have a degenerate estimator with nonzero MMD as well; we give a construction and a proof that it does not happen in common situations.\nüìÑ Download PDF\nToward 6G Downlink NOMA: CRC-Aided GRAND for Noise-Resilient NOMA Decoding in Beyond-5G Networks Authors: Emirhan Zor, Bora Bozkurt, Ferkan Yilmaz Venue: arXiv (2025)\nNon-Orthogonal Multiple Access (NOMA) technology has emerged as a promising technology to enable massive connectivity and enhanced spectral efficiency in next-generation wireless networks. In this study, we propose a novel two-user downlink power-domain NOMA framework that integrates a Cyclic Redundancy Check (CRC)-aided Guessing Random Additive Noise Decoding (GRAND) with successive interference cancellation (SIC). Unlike conventional SIC methods, which are susceptible to error propagation when there is low power disparity between users, the proposed scheme leverages GRAND‚Äôs noise-centric strategy to systematically rank and test candidate error patterns until the correct codeword is identified. In this architecture, CRC is utilized not only to detect errors but also to aid the decoding process, effectively eliminating the need for separate Forward Error Correction (FEC) codes and reducing overall system overhead. Furthermore, the strong user enhances its decoding performance by applying SIC that is reinforced by GRAND-based decoding of the weaker user‚Äôs signals, thereby minimizing error propagation and increasing throughput. Comprehensive simulation results over both Additive White Gaussian Noise (AWGN) and Rayleigh fading channels, under varying power allocations and user distances, show that the CRC-aided GRAND-NOMA approach significantly improves the Bit Error Rate (BER) performance compared to state-of-the-art NOMA decoding techniques. These findings underscore the potential of integrating universal decoding methods like GRAND into interference-limited multiuser environments for robust future wireless networks.\nüìÑ Download PDF\nConnecting current and future dual AGN searches to LISA and PTA gravitational wave detections Authors: Nianyi Chen, Yihao Zhou, Ekaterine Dadiani, Tiziana Di Matteo, Cici Wang, Antonella Palmese, Yue Shen, Junyao Li, Adi Foord, Simeon Bird, Yueying Ni, Yanhui Yang, Rupert Croft Venue: arXiv (2025)\nDual active galactic nuclei (DAGN) mark an observable stage of massive black hole (MBH) pairing in galaxy mergers and are precursors to the MBH binaries that generate low-frequency gravitational waves. Using the large-volume ASTRID cosmological simulation, we construct DAGN catalogs matched to current (COSMOS-Web, DESI) and forthcoming (AXIS, Roman) searches. With realistic selection functions applied, ASTRID reproduces observed dual fractions, separations, and host-galaxy properties across redshifts. We predict a substantial population of small-separation (\u003c5 kpc) duals that current surveys fail to capture, indicating that the apparent paucity of sub-kpc systems in COSMOS-Web is driven primarily by selection effects rather than a physical deficit. By following each simulated dual forward in time, we show that dual AGN are robust tracers of MBH mergers: ~30-70% coalesce within $\\lesssim 1$ Gyr, and 20-60% of these mergers produce gravitational-wave signals detectable by LISA. Duals accessible to AXIS and Roman are the progenitors of ~10% of low-redshift LISA events and ~30% of the PTA-band stochastic background. Massive green-valley galaxies with moderate-luminosity AGN, together with massive star-forming hosts containing bright quasars at $z\u003e1$, emerge as the most likely environments for imminent MBH binaries. These results provide a unified cosmological framework linking dual AGN demographics, MBH binary formation, and gravitational-wave emission, and they identify concrete, high-priority targets for coordinated electromagnetic and GW searches in upcoming multi-messenger surveys.\nüìÑ Download PDF\nA MAPS Detector for High Resolution Low Dose EBSD Authors: Barnaby D. A. Levin, Kalani Moore, Nicol√≤ M. Della Ventura, McLean P. Echlin, Tresa M. Pollock, Daniel S. Gianola Venue: arXiv (2025)\nThe use of highly sensitive pixelated direct detectors has dramatically improved the performance of high energy instrumentation such as transmission electron microscopy. Here, we describe a recently developed monolithic active pixel sensor designed for low energy scanning electron microscopy applications. This detector enables electron backscatter diffraction (EBSD) at lower energies and dose than are accessible with existing scintillator-coupled detectors, expanding grain orientation mapping capabilities to materials such as ceramics that are poor electron conductors. The high detector sensitivity allows collection of rich diffraction information - providing dislocation defect contrast that is otherwise not accessible via EBSD. Indeed, even the energy of single electron interaction events can be measured with this detector, which we demonstrate to energy filter diffraction patterns revealing details of how diffraction occurs at low energy.\nüìÑ Download PDF\nEnhancing Kinematics Understanding Through a Real-Time Graph-Based Motion Video Game Authors: Mateo Dutra, Marcos Abreu, Mart√≠n Monteiro, Silvia Sguilla, Cecilia Stari, Alvaro Suarez, Arturo C. Marti Venue: arXiv (2025)\nKinematics is a core topic in early physics courses, yet students often struggle to interpret motion and its graphical representations. To tackle these difficulties, we developed MissionMotion, a physical-computational videogame where students reproduce target motion graphs using real-time data from their own movements or from sensors connected through micro:bit or Arduino. The system displays both the target and the user-generated graph, providing immediate visual feedback and a score based on similarity. We piloted the environment with ninth-grade students in different school contexts and evaluated their experience using the MEEGA+ instrument. The results show strong engagement, positive perceptions of usability, and evidence that the game promotes reflection on motion graphs in ways that rarely emerge in traditional lessons. MissionMotion runs on any web-enabled device and all materials are openly available, offering teachers an accessible tool to integrate experimentation, computational thinking, and playful learning into physics classrooms.\nüìÑ Download PDF\nDelay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint Authors: Endar Suprih Wihidayat, Sieteng Soh, Kwan-Wu Chin, Duc-son Pham Venue: arXiv (2025)\nIn this paper, the Multi-stage Edge Server Upgrade (M-ESU) is proposed as a new network planning problem, involving the upgrading of an existing multi-access edge computing (MEC) system through multiple stages (e.g., over several years). More precisely, the problem considers two key decisions: (i) whether to deploy additional edge servers or upgrade those already installed, and (ii) how tasks should be offloaded so that the average number of tasks that meet their delay requirement is maximized. The framework specifically involves: (i) deployment of new servers combined with capacity upgrades for existing servers, and (ii) the optimal task offloading to maximize the average number of tasks with a delay requirement. It also considers the following constraints: (i) budget per stage, (ii) server deployment and upgrade cost (in $) and cost depreciation rate, (iii) computation resource of servers, (iv) number of tasks and their growth rate (in %), and (v) the increase in task sizes and stricter delay requirements over time. We present two solutions: a Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). MILP yields the optimal solution for small networks, whereas M-ESU/H is used in large-scale networks. For small networks, the simulation results show that the solution computed by M-ESU/H is within 1.25% of the optimal solution while running several orders of magnitude faster. For large networks, M-ESU/H is compared against three alternative heuristic solutions that consider only server deployment, or giving priority to server deployment or upgrade. Our experiments show that M-ESU/H yields up to 21.57% improvement in task satisfaction under identical budget and demand growth conditions, confirming its scalability and practical value for long-term MEC systems.\nüìÑ Download PDF\nThe Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI Authors: Otman A. Basir Venue: arXiv (2025)\nArtificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.\nüìÑ Download PDF\nPlausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error Authors: Claudia Vale Oliveira, Nelson Zagalo, Filipe Silva, Anabela Brandao, Syeda Faryal Hussain Khurrum, Joaquim Santos Venue: arXiv (2025)\nLarge language models (LLMs) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi LLM evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that LLM errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for LLM assessment, digital literacy, and the design of trustworthy human AI communication.\nüìÑ Download PDF\nCyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences Authors: Giovanni Adorni Venue: arXiv (2025)\nGenerative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona-lisation of teachers. This paper proposes \\emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \\emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures. We articulate three pillars for cyber-humanist design, \\emph{reflexive competence}, \\emph{algorithmic citizenship}, and \\emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \\emph{prompt-based learning} and a new \\emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.\nüìÑ Download PDF\nThe hands of time: Moving my body to keep time order in the brain Authors: Julien Lagarde Venue: arXiv (2025)\nThe brain is very often viewed as a network of cells, mostly neurons. Here we introduce a conjecture, in the spirit of a philosophical though experiment, which proposes that the present cannot be obtained from within such networks, and that this limitation imposes burdens on network efficiency in information processing. We aim to argue this conjecture imposes recurrent contacts from within the brain to outside in the physical world via behavior, which create a flow of time stamps. This though experiment may contribute to make the divide between the foci toward inside versus outside, for example opposing ecological psychology and many frameworks adopted in neurosciences, superfluous. This piece proposes an ambulation triggered by a thought experiment: What if I was a neuron listening to another one and talking to a third? It is a modest echo to fruitful thought experiments, like Molyneux problem, the imitation game and the anti-sequel Chinese room, key gedankenexperiments in an elevator, or the cogito.\nüìÑ Download PDF\nNeedle in the Web: A Benchmark for Retrieving Targeted Web Pages in the Wild Authors: Yumeng Wang, Tianyu Fan, Lingrui Xu, Chao Huang Venue: arXiv (2025)\nLarge Language Models (LLMs) have evolved from simple chatbots into sophisticated agents capable of automating complex real-world tasks, where browsing and reasoning over live web content is key to assessing retrieval and cognitive skills. Existing benchmarks like BrowseComp and xBench-DeepSearch emphasize complex reasoning searches requiring multi-hop synthesis but neglect Fuzzy Exploratory Search, namely queries that are vague and multifaceted, where users seek the most relevant webpage rather than a single factual answer. To address this gap, we introduce Needle in the Web, a novel benchmark specifically designed to evaluate modern search agents and LLM-based systems on their ability to retrieve and reason over real-world web content in response to ambiguous, exploratory queries under varying levels of difficulty. Needle in the Web comprises 663 questions spanning seven distinct domains. To ensure high query quality and answer uniqueness, we employ a flexible methodology that reliably generates queries of controllable difficulty based on factual claims of web contents. We benchmark three leading LLMs and three agent-based search systems on Needle in the Web, finding that most models struggle: many achieve below 35% accuracy, and none consistently excel across domains or difficulty levels. These findings reveal that Needle in the Web presents a significant challenge for current search systems and highlights the open problem of effective fuzzy retrieval under semantic ambiguity.\nüìÑ Download PDF\nUnraveling persistent urban-rural gaps: A long-term provincial analysis of residential heating and cooling loads Authors: Qinwen Tang, Ran Yan, Nan Zhou, Minda Ma Venue: arXiv (2025)\nWith global climate change and rising demand for thermal comfort, space heating and cooling have become increasingly critical to achieving carbon neutrality in the building sector. This study presents a first attempt to develop a bottom-up regional building energy model based on prototype buildings simulated in EnergyPlus, to assess space heating and cooling loads of urban and rural residential buildings across 30 Chinese provinces from 1980 to 2024. The results indicate that: (1) Guangdong recorded the highest cooling loads in 2020, reaching 76.5 TWh/a in urban areas and 63.0 TWh/a in rural areas; Henan exhibited the highest rural heating load at 174.6 TWh/a, while urban heating loads were highest in provinces such as Liaoning and Shandong. (2) From 1980 to 2024, average cooling loads increased from 12.4 to 15.1 kWh/m2/a in urban areas but declined from 22.63 to 19.87 kWh/m2/a in rural areas. Over the same period, average heating loads decreased from 44.08 to 39.92 kWh/m2/a in urban areas and from 100.15 to 72.42 kWh/m2/a in rural areas. (3) Urban residential building stock has surpassed rural stock in 22 provinces in recent years, compared with only 4 provinces in 2000, and the presence of 12 urban energy-efficiency standards versus only one rural standard further highlights substantial envelope performance gaps. Collectively, these dynamics have led to pronounced and persistent urban-rural disparities in residential heating and cooling loads. These findings underscore the need for differentiated standards and region-specific clean heating strategies, while providing a transferable modeling framework to inform targeted energy-saving policies and support the building sector‚Äôs transition toward carbon neutrality.\nüìÑ Download PDF\nHypervelocity Impact Debris Cloud Trajectory-Planning based on Additive Manufactured Lattice Structures Authors: Bilin Zheng, Xiao Kang, Xiaoyu Zhang, Hao Zhou, Mengchuan Xu, Chang Liu Venue: arXiv (2025)\nSpace debris and micrometeoroid (MMOD) impacts pose a serious threat to the safe operation of spacecraft. However, traditional protective structures typically suffer from limitations such as excessive thickness and inadequate load-bearing capacity. Guided by the design concepts of debris-cloud deflection and hierarchical energy dissipation, this study proposes a trajectory-planning lattice protective structure. First, the lattice parameters and geometry were designed according to the functional relationship between the incident angle and the transmitted/ricochet trajectory angles. Subsequently, multi-angle hypervelocity impact experiments were carried out to evaluate the proposed lattice protection structure. In combination with post-impact CT three-dimensional reconstruction and smoothed particle hydrodynamics (SPH) numerical simulations, the protective mechanisms of the lattice structure were systematically characterized and clarified. The results demonstrate that, for three oblique incidence conditions, the lattice structure remained intact and significantly deflected the debris-cloud momentum direction while effectively dissipating its kinetic energy. The angled plates with gradient designs enabled continuous changes in the momentum direction and stepwise kinetic energy dissipation through multiple cycles of debrisation, dispersion, and trajectory deflection. This research presents a novel, engineering-ready approach for spacecraft MMOD protection and validates the potential of trajectory-planning lattice structures for hypervelocity impact defense.\nüìÑ Download PDF\nüîç llm Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning Authors: Andrew Wagenmaker, Perry Dong, Raymond Tsao, Chelsea Finn, Sergey Levine Venue: arXiv (2025)\nStandard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) ‚Äì which trains a policy to directly match the actions played by the demonstrator ‚Äì can fail to ensure coverage over the demonstrator‚Äôs actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator‚Äôs behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator‚Äôs actions, enabling more effective finetuning. Furthermore, this policy ‚Äì which we refer to as the posterior behavioral cloning (PostBC) policy ‚Äì achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains ‚Äì relying only on standard supervised learning ‚Äì and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.\nüìÑ Download PDF\nVIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization Authors: Xiaoyan Cong, Haotian Yang, Angtian Wang, Yizhi Wang, Yiding Yang, Canyu Zhang, Chongyang Ma Venue: arXiv (2025)\nInstruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io\nüìÑ Download PDF\nImpacts of Racial Bias in Historical Training Data for News AI Authors: Rahul Bhargava, Malene Hornstrup Jespersen, Emily Boardman Ndulue, Vivica Dsouza Venue: arXiv (2025)\nAI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning ‚Äúblacks‚Äù thematic topic label. Through quantitative and qualitative means we investigate this label‚Äôs use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the ‚Äúblacks‚Äù label operates partially as a general ‚Äúracism detector‚Äù across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.\nüìÑ Download PDF\nChecking the HAL Interface Specification Continuously, Right from the Start Authors: Manuel Bentele, Onur Altinordu, Jan K√∂rner, Andreas Podelski, Axel Sikora Venue: arXiv (2025)\nThe correct use of a Hardware Abstraction Layer (HAL) interface in embedded applications is crucial to prevent malfunctions, crashes, or even hardware damage. Software model checking has been successfully applied to check interface specifications in application programs, but its employment in industrial practice is hindered by its unpredictability (whether it succeeds for a given application program or not). In this paper, we present a novel approach to address this problem by checking the HAL interface specification continuously and right from the start of the development. I.e., we develop an embedded application in several iterations without a formal connection between the steps. The steps start from a program skeleton which does nothing but calling HAL functions. Actual functionality is added consecutively. The HAL interface specification is checked in each step of the sequence. The idea of the approach is to exploit a specific feature of software model checking: Its attempt to compute exactly the abstraction that is needed for the check to succeed may carry over from one step to the next, even if there is no formal connection between the steps. The experience from a preliminary experimental evaluation of our approach in the development of embedded applications is very promising. Following our approach, the check succeeds in each step and in particular in the final application program.\nüìÑ Download PDF\nMultimer Embedding for Molecular Crystals Utilizing up to Tetramer Interactions Authors: Alexander List, A. Daniel Boese, Johannes Hoja Venue: arXiv (2025)\nMolecular crystals possess a highly complex crystallographic landscape which in many cases results in the experimental observation of multiple crystal structures for the same compound. Accurate results can often be obtained for such systems by employing periodic density functional theory using hybrid functionals; however, this is not always computationally feasible. One possibility to circumvent these expensive periodic calculations is the utilization of multimer embedding methods. Therein, the fully periodic crystal is described at a lower level of theory, and subsequently monomer energies, dimer interaction energies, etc. are corrected via high-level calculations. In this paper, we further extend such a multimer embedding approach by one multimer order for all investigated properties, allowing us to compute lattice energies up to the tetramer embedding level, and atomic forces, the stress tensor, and harmonic phonons up to the trimer level. We test the significance of including these higher-order multimers by embedding PBE0+MBD multimers into periodic PBE+MBD calculations utilizing the X23 benchmark set of molecular crystals and comparing the results to explicit periodic PBE0+MBD calculations. We show that tetramer interactions systematically improve the lattice energy approximation and explore multiple possibilities for multimer selection. Furthermore, we confirm that trimer interactions are crucial for the description of the stress tensor, yielding cell volumes within 1 % of those of PBE0+MBD. Subsequently, this also results in an improvement of the description of vibrational properties, giving on average gamma point frequencies within 1.3 wave numbers and vibrational free energies within 0.3 kJ/mol of the PBE0+MBD results.\nüìÑ Download PDF\nAlchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection Authors: Kaixin Ding, Yang Zhou, Xi Chen, Miao Yang, Jiarong Ou, Rui Chen, Xin Tao, Hengshuang Zhao Venue: arXiv (2025)\nRecent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose Alchemist, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample‚Äôs influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.\nüìÑ Download PDF\nüîç neuroscience An evacuation simulator for pedestrian dynamics based on the Social Force Model Authors: Juli√°n L√≥pez, Virginia Mazzone, M. Leticia Rubio Puzzo, Juan Cruz Moreno Venue: arXiv (2025)\nThe evacuation of pedestrians from enclosed spaces represents a key problem in safety engineering and infrastructure design. Analyzing the collective dynamics that emerge during evacuation processes requires simulation tools capable of capturing individual interactions and spatial constraints realistically. In this work, we present \\textit{SiCoBioNa}, an open-source evacuation simulator based on the Social Force Model (SFM). The software provides an intuitive graphical interface that allows users to configure pedestrian properties, spatial geometries, and initial conditions without requiring prior expertise in numerical modeling techniques. The SFM framework enables the representation of goal-oriented motion, interpersonal interactions, and interactions with fixed obstacles. The simulator generates both quantitative data and visual outputs, facilitating the analysis of evacuation dynamics and the evaluation of different spatial configurations. Due to its modular and extensible design, \\textit{SiCoBioNa} serves as a reproducible research tool for studies on pedestrian dynamics providing practical support for evacuation planning.\nüìÑ Download PDF\nPrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy Authors: Ripan Kumar Kundu, Istiak Ahmed, Khaza Anuarul Hoque Venue: arXiv (2025)\nThe convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43% and 39%, respectively, for cybersickness tasks while preserving model utility with up to 97% accuracy using Transformer models. Furthermore, it improves inference time by up to ~2x compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay.\nüìÑ Download PDF\nüîç data_resources Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures Authors: Yehor Tereshchenko, Mika H√§m√§l√§inen, Svitlana Myroniuk Venue: arXiv (2025)\nThe evaluation of Large Language Models (LLMs) for translation tasks has primarily focused on high-resource languages, leaving a significant gap in understanding their performance on low-resource and endangered languages. This study presents a comprehensive comparison of OpenAI‚Äôs GPT models, specifically examining the differences between reasoning and non-reasoning architectures for translating between Finnish and four low-resource Uralic languages: Komi-Zyrian, Moksha, Erzya, and Udmurt. Using a parallel corpus of literary texts, we evaluate model willingness to attempt translation through refusal rate analysis across different model architectures. Our findings reveal significant performance variations between reasoning and non-reasoning models, with reasoning models showing 16 percentage points lower refusal rates. The results provide valuable insights for researchers and practitioners working with Uralic languages and contribute to the broader understanding of reasoning model capabilities for endangered language preservation.\nüìÑ Download PDF\nSigma-Moe-Tiny Technical Report Authors: Qingguo Hu, Zhenghao Lin, Ziyue Yang, Yucheng Ding, Xiao Liu, Yuting Jiang, Ruizhe Wang, Tianyu Chen, Zhongxin Guo, Yifan Xiong, Rui Gao, Lei Qu, Jinsong Su, Peng Cheng, Yeyun Gong Venue: arXiv (2025)\nMixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures. Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny Code: https://github.com/microsoft/ltp-megatron-lm\nüìÑ Download PDF\nWeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning Authors: Wendong Bi, Yirong Mao, Xianglong Liu, Kai Tian, Jian Zhang, Hanjie Wang, Wenhui Que Venue: arXiv (2025)\nPersonalized music recommendation in conversational scenarios usually requires a deep understanding of user preferences and nuanced musical context, yet existing methods often struggle with balancing specialized domain knowledge and flexible tool integration. This paper proposes WeMusic-Agent, a training framework for efficient LLM-based conversational music recommendation. By integrating the knowledge internalization and agentic boundary learning, the framework aims to teach the model to intelligently decide when to leverage internalized knowledge and when to call specialized tools (e.g., music retrieval APIs, music recommendation systems). Under this framework, we present WeMusic-Agent-M1, an agentic model that internalizes extensive musical knowledge via continued pretraining on 50B music-related corpus while acquiring the ability to invoke external tools when necessary. Additionally, considering the lack of open-source benchmarks for conversational music recommendation, we also construct a benchmark for personalized music recommendations derived from real-world data in WeChat Listen. This benchmark enables comprehensive evaluation across multiple dimensions, including relevance, personalization, and diversity of the recommendations. Experiments on real-world data demonstrate that WeMusic-Agent achieves significant improvements over existing models.\nüìÑ Download PDF\nMultimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image Authors: Yushi Hu, Reyhane Askari-Hemmat, Melissa Hall, Emily Dinan, Luke Zettlemoyer, Marjan Ghazvininejad Venue: arXiv (2025)\nReward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (‚Äúthinking-with-images‚Äù), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to \u003e90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.\nüìÑ Download PDF\nüîç emotion_language On the Universal Representation Property of Spiking Neural Networks Authors: Shayan Hundrieser, Philipp Tuchel, Insung Kong, Johannes Schmidt-Hieber Venue: arXiv (2025)\nInspired by biology, spiking neural networks (SNNs) process information via discrete spikes over time, offering an energy-efficient alternative to the classical computing paradigm and classical artificial neural networks (ANNs). In this work, we analyze the representational power of SNNs by viewing them as sequence-to-sequence processors of spikes, i.e., systems that transform a stream of input spikes into a stream of output spikes. We establish the universal representation property for a natural class of spike train functions. Our results are fully quantitative, constructive, and near-optimal in the number of required weights and neurons. The analysis reveals that SNNs are particularly well-suited to represent functions with few inputs, low temporal complexity, or compositions of such functions. The latter is of particular interest, as it indicates that deep SNNs can efficiently capture composite functions via a modular design. As an application of our results, we discuss spike train classification. Overall, these results contribute to a rigorous foundation for understanding the capabilities and limitations of spike-based neuromorphic systems.\nüìÑ Download PDF\nWhat Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels Authors: Aditya Yadavalli, Tiago Pimentel, Tamar I Regev, Ethan Wilcox, Alex Warstadt Venue: arXiv (2025)\nProsody ‚Äì the melody of speech ‚Äì conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance‚Äôs meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel ‚Äì and by implication the prosodic channel ‚Äì transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.\nüìÑ Download PDF\nGinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation Authors: William English, Chase Walker, Dominic Simon, Rickard Ewetz Venue: arXiv (2025)\nNatural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\\mathcal{P}$. We decompose the grounding task hierarchically ‚Äì first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5%$, a $1.4\\times$ improvement over SOTA.\nüìÑ Download PDF\nPattern recognition in complex systems via vector-field representations of spatio-temporal data Authors: Ingrid Amaranta Membrillo Solis, Maria van Rossem, Tristan Madeleine, Tetiana Orlova, Nina Podoliak, Giampaolo D‚ÄôAlessandro, Jacek Brodzki, Malgosia Kaczmarek Venue: arXiv (2025)\nA complex system comprises multiple interacting entities whose interdependencies form a unified whole, exhibiting emergent behaviours not present in individual components. Examples include the human brain, living cells, soft matter, Earth‚Äôs climate, ecosystems, and the economy. These systems exhibit high-dimensional, non-linear dynamics, making their modelling, classification, and prediction particularly challenging. Advances in information technology have enabled data-driven approaches to studying such systems. However, the sheer volume and complexity of spatio-temporal data often hinder traditional methods like dimensionality reduction, phase-space reconstruction, and attractor characterisation. This paper introduces a geometric framework for analysing spatio-temporal data from complex systems, grounded in the theory of vector fields over discrete measure spaces. We propose a two-parameter family of metrics suitable for data analysis and machine learning applications. The framework supports time-dependent images, image gradients, and real- or vector-valued functions defined on graphs and simplicial complexes. We validate our approach using data from numerical simulations of biological and physical systems on flat and curved domains. Our results show that the proposed metrics, combined with multidimensional scaling, effectively address key analytical challenges. They enable dimensionality reduction, mode decomposition, phase-space reconstruction, and attractor characterisation. Our findings offer a robust pathway for understanding complex dynamical systems, especially in contexts where traditional modelling is impractical but abundant experimental data are available.\nüìÑ Download PDF\nHow Good is Post-Hoc Watermarking With Language Model Rephrasing? Authors: Pierre Fernandez, Tom Sander, Hady Elsahar, Hongyan Chang, Tom√°≈° Souƒçek, Valeriu Lacatusu, Tuan Tran, Sylvestre-Alvise Rebuffi, Alexandre Mourachko Venue: arXiv (2025)\nGeneration-time text watermarking embeds statistical signals into text for traceability of AI-generated content. We explore post-hoc watermarking where an LLM rewrites existing text while applying generation-time watermarking, to protect copyrighted documents, or detect their use in training or RAG via watermark radioactivity. Unlike generation-time approaches, which is constrained by how LLMs are served, this setting offers additional degrees of freedom for both generation and detection. We investigate how allocating compute (through larger rephrasing models, beam search, multi-candidate generation, or entropy filtering at detection) affects the quality-detectability trade-off. Our strategies achieve strong detectability and semantic fidelity on open-ended text such as books. Among our findings, the simple Gumbel-max scheme surprisingly outperforms more recent alternatives under nucleus sampling, and most methods benefit significantly from beam search. However, most approaches struggle when watermarking verifiable text such as code, where we counterintuitively find that smaller models outperform larger ones. This study reveals both the potential and limitations of post-hoc watermarking, laying groundwork for practical applications and future research.\nüìÑ Download PDF\nLearning Confidence Ellipsoids and Applications to Robust Subspace Recovery Authors: Chao Gao, Liren Shan, Vaidehi Srinivas, Aravindan Vijayaraghavan Venue: arXiv (2025)\nWe study the problem of finding confidence ellipsoids for an arbitrary distribution in high dimensions. Given samples from a distribution $D$ and a confidence parameter $Œ±$, the goal is to find the smallest volume ellipsoid $E$ which has probability mass $\\Pr_{D}[E] \\ge 1-Œ±$. Ellipsoids are a highly expressive class of confidence sets as they can capture correlations in the distribution, and can approximate any convex set. This problem has been studied in many different communities. In statistics, this is the classic minimum volume estimator introduced by Rousseeuw as a robust non-parametric estimator of location and scatter. However in high dimensions, it becomes NP-hard to obtain any non-trivial approximation factor in volume when the condition number $Œ≤$ of the ellipsoid (ratio of the largest to the smallest axis length) goes to $\\infty$. This motivates the focus of our paper: can we efficiently find confidence ellipsoids with volume approximation guarantees when compared to ellipsoids of bounded condition number $Œ≤$? Our main result is a polynomial time algorithm that finds an ellipsoid $E$ whose volume is within a $O(Œ≤^{Œ≥d})$ multiplicative factor of the volume of best $Œ≤$-conditioned ellipsoid while covering at least $1-O(Œ±/Œ≥)$ probability mass for any $Œ≥\u003c Œ±$. We complement this with a computational hardness result that shows that such a dependence seems necessary up to constants in the exponent. The algorithm and analysis uses the rich primal-dual structure of the minimum volume enclosing ellipsoid and the geometric Brascamp-Lieb inequality. As a consequence, we obtain the first polynomial time algorithm with approximation guarantees on worst-case instances of the robust subspace recovery problem.\nüìÑ Download PDF\nM-PhyGs: Multi-Material Object Dynamics from Video Authors: Norika Wada, Kohei Yamashita, Ryo Kawahara, Ko Nishino Venue: arXiv (2025)\nKnowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.\nüìÑ Download PDF\nA Radio Search for Star-Planet Interaction in TOI-540 and SPECULOOS-3 Authors: Kevin N. Ortiz Ceballos, Yvette Cendes, Edo Berger Venue: arXiv (2025)\nWe present the first targeted centimeter-band radio observations of two recently-discovered exoplanet systems that are prime candidates for magnetic star-planet interaction (SPI): TOI-540 and SPECULOOS-3. The targets were selected due to the small orbital separation of their known planets, as well as for indications of stellar magnetic activity, given that for SPI radio emission may be strongest when a sufficiently magnetized star hosts a close-in planet. The deep, multi-epoch Very Large Array (SPECULOOS-3) and MeerKAT (TOI-540) observations yield non-detections, with $3œÉ$ limits of $\\lesssim 7.5$ $Œº$Jy ($4-8$ GHz) and $\\lesssim 30-80$ $Œº$Jy ($0.8-1.7$ GHz), respectively. For SPECULOOS-3 b we rule out observable SPI for most of its orbit, while for TOI-540 b we sample a narrower range, around planetary transit. We model possible planetary magnetic field strength constraints for both systems, and conclude that our observations are sensitive enough to sample SPI emission in these systems if present and directed at us, even for a planetary field of only $\\sim 1$ G.\nüìÑ Download PDF\nOn the Edge of Core (Non-)Emptiness: An Automated Reasoning Approach to Approval-Based Multi-Winner Voting Authors: Ratip Emin Berker, Emanuel Tewolde, Vincent Conitzer, Mingyu Guo, Marijn Heule, Lirong Xia Venue: arXiv (2025)\nCore stability is a natural and well-studied notion for group fairness in multi-winner voting, where the task is to select a committee from a pool of candidates. We study the setting where voters either approve or disapprove of each candidate; here, it remains a major open problem whether a core-stable committee always exists. In this work, we develop an approach based on mixed-integer linear programming for deciding whether and when core-stable committees are guaranteed to exist. In contrast to SAT-based approaches popular in computational social choice, our method can produce proofs for a specific number of candidates independent of the number of voters. In addition to these computational gains, our program lends itself to a novel duality-based reformulation of the core stability problem, from which we obtain new existence results in special cases. Further, we use our framework to reveal previously unknown relationships between core stability and other desirable properties, such as notions of priceability.\nüìÑ Download PDF\nVeblen effects and broken windows in an environmental OLG model Authors: Nicol√°s Blampied, Alessia Cafferata, Marwil J. Davila-Fernandez Venue: arXiv (2025)\nCan constantly comparing ourselves to others lead to overconsumption, ultimately increasing the ecological footprint? How do social comparisons shape green preferences over time? To answer these questions, we develop an environmental Overlapping Generations (OLG) model that explicitly accounts for Veblen effects and allows green preferences to be updated asynchronously, influenced by past environmental conditions and relative status considerations. We show that, along the optimal path, positional spending leads to overconsumption, which is detrimental to the environment. Taxing consumption is counterproductive as it does not directly address the social comparisons issue, leaving the problem unchanged. When the Veblenian mechanism is weak, the introduction of a materialistic ``secular trend‚Äô‚Äô ‚Äì that lowers the importance placed on the public good ‚Äì gives rise to two stable equilibria separated by a saddle: one in which agents care about environmental quality as much as consuming, and the other in which they derive utility solely from the latter. Studying the basins of attraction reveals that green investments are highly fragile. Our numerical experiments further indicated that, when Veblen effects are strong, the model depicts endogenous, persistent, aperiodic oscillations. In this case, green preferences fluctuate close to zero, and environmental quality is very low. Taken together, these findings suggest environmental vulnerability grows in parallel with status-driven consumption.\nüìÑ Download PDF\nA Survey on Spatio-Temporal Knowledge Graph Models Authors: Philipp Plamper, Hanna K√∂pcke, Anika Gro√ü Venue: arXiv (2025)\nMany complex real-world systems exhibit inherently intertwined temporal and spatial characteristics. Spatio-temporal knowledge graphs (STKGs) have therefore emerged as a powerful representation paradigm, as they integrate entities, relationships, time and space within a unified graph structure. They are increasingly applied across diverse domains, including environmental systems and urban, transportation, social and human mobility networks. However, modeling STKGs remains challenging: their foundations span classical graph theory as well as temporal and spatial graph models, which have evolved independently across different research communities and follow heterogeneous modeling assumptions and terminologies. As a result, existing approaches often lack conceptual alignment, generalizability and reusability. This survey provides a systematic review of spatio-temporal knowledge graph models, tracing their origins in static, temporal and spatial graph modeling. We analyze existing approaches along key modeling dimensions, including edge semantics, temporal and spatial annotation strategies, temporal and spatial semantics and relate these choices to their respective application domains. Our analysis reveals that unified modeling frameworks are largely absent and that most current models are tailored to specific use cases rather than designed for reuse or long-term knowledge preservation. Based on these findings, we derive modeling guidelines and identify open challenges to guide future research.\nüìÑ Download PDF\nThe Motile Josephson Array: Bridging Active Turbulence and Superconductivity Authors: Magnus F Ivarsen Venue: arXiv (2025)\nRecent minimalist modeling has demonstrated that overdamped polar chiral active matter can support an emergent, inviscid Euler turbulence, despite the system‚Äôs strictly dissipative microscopic nature. In this letter, we establish the statistical mechanical foundation for the emergent inertial regime by deriving a formal isomorphism between the model‚Äôs agent dynamics and the overdamped Langevin equation for disordered Josephson junctions. We identify the trapped agent state as a macroscopic superconducting phase governed by the Adler equation. The validity of this mapping is confirmed analytically by a disorder-broadened Adler-Ohmic crossover in the system‚Äôs slip velocity, corresponding to the saddle-node bifurcation of phase-locking systems. These results define the new minimal chiral flocking model as a motile, disordered Josephson array, bridging active turbulence and quantum superconductivity.\nüìÑ Download PDF\n","wordCount":"25376","inLanguage":"zh","datePublished":"2025-12-21T15:23:23.007415Z","dateModified":"2025-12-21T15:23:23.007415Z","author":{"@type":"Person","name":"Gary"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://garyforreal.me/zh/posts/paper/paper-2025-12-21-weekly/"},"publisher":{"@type":"Organization","name":"Gary's House","logo":{"@type":"ImageObject","url":"https://garyforreal.me/img/Q.jpg"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><script src=https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://garyforreal.me/zh/ accesskey=h title="Gary's Blog (Alt + H)"><img src=https://garyforreal.me/img/me.jpg alt aria-label=logo height=35>Gary's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://garyforreal.me/en/ title=English aria-label=English>English</a></li></ul></div></div><ul id=menu><li><a href=https://garyforreal.me/zh/search title="üîçÊêúÁ¥¢ (Alt + /)" accesskey=/><span>üîçÊêúÁ¥¢</span></a></li><li><a href=https://garyforreal.me/zh/ title=üè†‰∏ªÈ°µ><span>üè†‰∏ªÈ°µ</span></a></li><li><a href=https://garyforreal.me/zh/posts/ title=üìöÊñáÁ´†><span>üìöÊñáÁ´†</span></a></li><li><a href=https://garyforreal.me/zh/archives/ title=‚è±Â≠òÊ°£><span>‚è±Â≠òÊ°£</span></a></li><li><a href=https://garyforreal.me/zh/music/ title=üéµÈü≥‰πê><span>üéµÈü≥‰πê</span></a></li><li><a href=https://garyforreal.me/zh/about title=üôãüèª‚Äç‚ôÇÔ∏èÂÖ≥‰∫é><span>üôãüèª‚Äç‚ôÇÔ∏èÂÖ≥‰∫é</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://garyforreal.me/zh/>‰∏ªÈ°µ</a>&nbsp;¬ª&nbsp;<a href=https://garyforreal.me/zh/posts/>Â∏ñÂ≠ê</a>&nbsp;¬ª&nbsp;<a href=https://garyforreal.me/zh/posts/paper/>ËÆ∫Êñá</a></div><h1 class="post-title entry-hint-parent">Weekly Paper Notes - 2025-12-21</h1><div class=post-meta><span title='2025-12-21 15:23:23.007415 +0000 UTC'>2025-12-21</span>&nbsp;¬∑&nbsp;120 ÂàÜÈíü&nbsp;¬∑&nbsp;Gary&nbsp;|&nbsp;ËØ≠Ë®Ä:<ul class=i18n_list><li><a href=https://garyforreal.me/en/posts/paper/paper-2025-12-21-weekly/>English</a></li></ul><div class=meta-item>&nbsp¬∑&nbsp
        <span id=busuanzi_container_page_pv>Êú¨ÊñáÈòÖËØªÈáè<span id=busuanzi_value_page_pv></span>Ê¨°</span></div></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>ÁõÆÂΩï</span></summary><div class=inner><ul><li><a href=#weekly-paper-notes aria-label="Weekly Paper Notes">Weekly Paper Notes</a><ul><li><a href=#-multilingual aria-label="üîç multilingual">üîç multilingual</a><ul><li><a href=#from-essence-to-defense-adaptive-semantic-aware-watermarking-for-embedding-as-a-service-copyright-protectionhttpsarxivorgabs251216439v1 aria-label="From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection"><a href=https://arxiv.org/abs/2512.16439v1>From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection</a></a></li><li><a href=#bridging-the-reality-gap-efficient-adaptation-of-asr-systems-for-challenging-low-resource-domainshttpsarxivorgabs251216401v1 aria-label="Bridging the Reality Gap: Efficient Adaptation of ASR systems for Challenging Low-Resource Domains"><a href=https://arxiv.org/abs/2512.16401v1>Bridging the Reality Gap: Efficient Adaptation of ASR systems for Challenging Low-Resource Domains</a></a></li><li><a href=#hearing-to-translate-the-effectiveness-of-speech-modality-integration-into-llmshttpsarxivorgabs251216378v1 aria-label="Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs"><a href=https://arxiv.org/abs/2512.16378v1>Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs</a></a></li><li><a href=#mitigating-hallucinations-in-healthcare-llms-with-granular-fact-checking-and-domain-specific-adaptationhttpsarxivorgabs251216189v1 aria-label="Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation"><a href=https://arxiv.org/abs/2512.16189v1>Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation</a></a></li><li><a href=#a-multi-agent-large-language-model-framework-for-automated-qualitative-analysishttpsarxivorgabs251216063v1 aria-label="A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis"><a href=https://arxiv.org/abs/2512.16063v1>A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis</a></a></li><li><a href=#cross-language-bias-examination-in-large-language-modelshttpsarxivorgabs251216029v1 aria-label="Cross-Language Bias Examination in Large Language Models"><a href=https://arxiv.org/abs/2512.16029v1>Cross-Language Bias Examination in Large Language Models</a></a></li><li><a href=#emotion-recognition-in-signershttpsarxivorgabs251215376v1 aria-label="Emotion Recognition in Signers"><a href=https://arxiv.org/abs/2512.15376v1>Emotion Recognition in Signers</a></a></li><li><a href=#multilingual-and-continuous-backchannel-prediction-a-cross-lingual-studyhttpsarxivorgabs251214085v1 aria-label="Multilingual and Continuous Backchannel Prediction: A Cross-lingual Study"><a href=https://arxiv.org/abs/2512.14085v1>Multilingual and Continuous Backchannel Prediction: A Cross-lingual Study</a></a></li><li><a href=#stutterfuse-mitigating-modality-collapse-in-stuttering-detection-with-jaccard-weighted-metric-learning-and-gated-fusionhttpsarxivorgabs251213632v1 aria-label="StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion"><a href=https://arxiv.org/abs/2512.13632v1>StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion</a></a></li><li><a href=#scaling-laws-for-code-every-programming-language-mattershttpsarxivorgabs251213472v1 aria-label="Scaling Laws for Code: Every Programming Language Matters"><a href=https://arxiv.org/abs/2512.13472v1>Scaling Laws for Code: Every Programming Language Matters</a></a></li><li><a href=#venusbench-gd-a-comprehensive-multi-platform-gui-benchmark-for-diverse-grounding-taskshttpsarxivorgabs251216501v1 aria-label="VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks"><a href=https://arxiv.org/abs/2512.16501v1>VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks</a></a></li><li><a href=#ppsebm-an-energy-based-model-with-progressive-parameter-selection-for-continual-learninghttpsarxivorgabs251215658v1 aria-label="PPSEBM: An Energy-Based Model with Progressive Parameter Selection for Continual Learning"><a href=https://arxiv.org/abs/2512.15658v1>PPSEBM: An Energy-Based Model with Progressive Parameter Selection for Continual Learning</a></a></li><li><a href=#an-empirical-study-on-chinese-character-decomposition-in-multiword-expression-aware-neural-machine-translationhttpsarxivorgabs251215556v1 aria-label="An Empirical Study on Chinese Character Decomposition in Multiword Expression-Aware Neural Machine Translation"><a href=https://arxiv.org/abs/2512.15556v1>An Empirical Study on Chinese Character Decomposition in Multiword Expression-Aware Neural Machine Translation</a></a></li><li><a href=#multipath-transfer-engine-breaking-gpu-and-host-memory-bandwidth-bottlenecks-in-llm-serviceshttpsarxivorgabs251216056v1 aria-label="MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services"><a href=https://arxiv.org/abs/2512.16056v1>MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services</a></a></li><li><a href=#magic-state-cultivation-on-a-superconducting-quantum-processorhttpsarxivorgabs251213908v1 aria-label="Magic state cultivation on a superconducting quantum processor"><a href=https://arxiv.org/abs/2512.13908v1>Magic state cultivation on a superconducting quantum processor</a></a></li><li><a href=#transversal-clifford-hierarchy-gates-via-non-abelian-surface-codeshttpsarxivorgabs251213777v1 aria-label="Transversal Clifford-Hierarchy Gates via Non-Abelian Surface Codes"><a href=https://arxiv.org/abs/2512.13777v1>Transversal Clifford-Hierarchy Gates via Non-Abelian Surface Codes</a></a></li><li><a href=#litept-lighter-yet-stronger-point-transformerhttpsarxivorgabs251213689v1 aria-label="LitePT: Lighter Yet Stronger Point Transformer"><a href=https://arxiv.org/abs/2512.13689v1>LitePT: Lighter Yet Stronger Point Transformer</a></a></li><li><a href=#when-a-nation-speaks-machine-learning-and-nlp-in-peoples-sentiment-analysis-during-bangladeshs-2024-mass-uprisinghttpsarxivorgabs251215547v1 aria-label="When a Nation Speaks: Machine Learning and NLP in People&rsquo;s Sentiment Analysis During Bangladesh&rsquo;s 2024 Mass Uprising"><a href=https://arxiv.org/abs/2512.15547v1>When a Nation Speaks: Machine Learning and NLP in People&rsquo;s Sentiment Analysis During Bangladesh&rsquo;s 2024 Mass Uprising</a></a></li><li><a href=#the-world-is-your-canvas-painting-promptable-events-with-reference-images-trajectories-and-texthttpsarxivorgabs251216924v1 aria-label="The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text"><a href=https://arxiv.org/abs/2512.16924v1>The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</a></a></li><li><a href=#next-embedding-prediction-makes-strong-vision-learnershttpsarxivorgabs251216922v1 aria-label="Next-Embedding Prediction Makes Strong Vision Learners"><a href=https://arxiv.org/abs/2512.16922v1>Next-Embedding Prediction Makes Strong Vision Learners</a></a></li><li><a href=#adatooler-v-adaptive-tool-use-for-images-and-videoshttpsarxivorgabs251216918v1 aria-label="AdaTooler-V: Adaptive Tool-Use for Images and Videos"><a href=https://arxiv.org/abs/2512.16918v1>AdaTooler-V: Adaptive Tool-Use for Images and Videos</a></a></li><li><a href=#generative-adversarial-reasoner-enhancing-llm-reasoning-with-adversarial-reinforcement-learninghttpsarxivorgabs251216917v1 aria-label="Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning"><a href=https://arxiv.org/abs/2512.16917v1>Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning</a></a></li><li><a href=#constructive-circuit-amplification-improving-math-reasoning-in-llms-via-targeted-sub-network-updateshttpsarxivorgabs251216914v1 aria-label="Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates"><a href=https://arxiv.org/abs/2512.16914v1>Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates</a></a></li><li><a href=#exploration-vs-exploitation-rethinking-rlvr-through-clipping-entropy-and-spurious-rewardhttpsarxivorgabs251216912v1 aria-label="Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward"><a href=https://arxiv.org/abs/2512.16912v1>Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</a></a></li><li><a href=#momagraph-state-aware-unified-scene-graphs-with-vision-language-model-for-embodied-task-planninghttpsarxivorgabs251216909v1 aria-label="MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning"><a href=https://arxiv.org/abs/2512.16909v1>MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning</a></a></li><li><a href=#tiny-recursive-control-iterative-reasoning-for-efficient-optimal-controlhttpsarxivorgabs251216824v1 aria-label="Tiny Recursive Control: Iterative Reasoning for Efficient Optimal Control"><a href=https://arxiv.org/abs/2512.16824v1>Tiny Recursive Control: Iterative Reasoning for Efficient Optimal Control</a></a></li><li><a href=#analogicity-in-list-coloring-problems-and-interval-k-%ce%b3%ce%bc-choosability-a-complexity-theoretic-studyhttpsarxivorgabs251216807v1 aria-label="Analogicity in List Coloring Problems and Interval $k$-$(Œ≥,Œº)$-choosability: A Complexity-Theoretic Study"><a href=https://arxiv.org/abs/2512.16807v1>Analogicity in List Coloring Problems and Interval $k$-$(Œ≥,Œº)$-choosability: A Complexity-Theoretic Study</a></a></li><li><a href=#physbrain-human-egocentric-data-as-a-bridge-from-vision-language-models-to-physical-intelligencehttpsarxivorgabs251216793v1 aria-label="PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence"><a href=https://arxiv.org/abs/2512.16793v1>PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence</a></a></li><li><a href=#few-shot-specific-emitter-identification-via-integrated-complex-variational-mode-decomposition-and-spatial-attention-transferhttpsarxivorgabs251216786v1 aria-label="Few-Shot Specific Emitter Identification via Integrated Complex Variational Mode Decomposition and Spatial Attention Transfer"><a href=https://arxiv.org/abs/2512.16786v1>Few-Shot Specific Emitter Identification via Integrated Complex Variational Mode Decomposition and Spatial Attention Transfer</a></a></li><li><a href=#dvgt-driving-visual-geometry-transformerhttpsarxivorgabs251216919v1 aria-label="DVGT: Driving Visual Geometry Transformer"><a href=https://arxiv.org/abs/2512.16919v1>DVGT: Driving Visual Geometry Transformer</a></a></li><li><a href=#sftok-bridging-the-performance-gap-in-discrete-tokenizershttpsarxivorgabs251216910v1 aria-label="SFTok: Bridging the Performance Gap in Discrete Tokenizers"><a href=https://arxiv.org/abs/2512.16910v1>SFTok: Bridging the Performance Gap in Discrete Tokenizers</a></a></li><li><a href=#scenediff-a-benchmark-and-method-for-multiview-object-change-detectionhttpsarxivorgabs251216908v1 aria-label="SceneDiff: A Benchmark and Method for Multiview Object Change Detection"><a href=https://arxiv.org/abs/2512.16908v1>SceneDiff: A Benchmark and Method for Multiview Object Change Detection</a></a></li><li><a href=#flowing-from-reasoning-to-motion-learning-3d-hand-trajectory-prediction-from-egocentric-human-interaction-videoshttpsarxivorgabs251216907v1 aria-label="Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos"><a href=https://arxiv.org/abs/2512.16907v1>Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos</a></a></li><li><a href=#flashportrait-6x-faster-infinite-portrait-animation-with-adaptive-latent-predictionhttpsarxivorgabs251216900v1 aria-label="FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction"><a href=https://arxiv.org/abs/2512.16900v1>FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction</a></a></li><li><a href=#linkedout-linking-world-knowledge-representation-out-of-video-llm-for-next-generation-video-recommendationhttpsarxivorgabs251216891v1 aria-label="LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation"><a href=https://arxiv.org/abs/2512.16891v1>LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation</a></a></li><li><a href=#opentouch-bringing-full-hand-touch-to-real-world-interactionhttpsarxivorgabs251216842v1 aria-label="OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction"><a href=https://arxiv.org/abs/2512.16842v1>OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction</a></a></li><li><a href=#mepic-memory-efficient-position-independent-caching-for-llm-servinghttpsarxivorgabs251216822v1 aria-label="MEPIC: Memory Efficient Position Independent Caching for LLM Serving"><a href=https://arxiv.org/abs/2512.16822v1>MEPIC: Memory Efficient Position Independent Caching for LLM Serving</a></a></li><li><a href=#exploration-of-augmentation-strategies-in-multi-modal-retrieval-augmented-generation-for-the-biomedical-domain-a-case-study-evaluating-question-answering-in-glycobiologyhttpsarxivorgabs251216802v1 aria-label="Exploration of Augmentation Strategies in Multi-modal Retrieval-Augmented Generation for the Biomedical Domain: A Case Study Evaluating Question Answering in Glycobiology"><a href=https://arxiv.org/abs/2512.16802v1>Exploration of Augmentation Strategies in Multi-modal Retrieval-Augmented Generation for the Biomedical Domain: A Case Study Evaluating Question Answering in Glycobiology</a></a></li><li><a href=#from-facts-to-conclusions--integrating-deductive-reasoning-in-retrieval-augmented-llmshttpsarxivorgabs251216795v1 aria-label="From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs"><a href=https://arxiv.org/abs/2512.16795v1>From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs</a></a></li><li><a href=#cityseeker-how-do-vlms-explore-embodied-urban-navigation-with-implicit-human-needshttpsarxivorgabs251216755v1 aria-label="CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?"><a href=https://arxiv.org/abs/2512.16755v1>CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?</a></a></li></ul></li><li><a href=#-linguistics aria-label="üîç linguistics">üîç linguistics</a><ul><li><a href=#generative-refocusing-flexible-defocus-control-from-a-single-imagehttpsarxivorgabs251216923v1 aria-label="Generative Refocusing: Flexible Defocus Control from a Single Image"><a href=https://arxiv.org/abs/2512.16923v1>Generative Refocusing: Flexible Defocus Control from a Single Image</a></a></li><li><a href=#easyv2v-a-high-quality-instruction-based-video-editing-frameworkhttpsarxivorgabs251216920v1 aria-label="EasyV2V: A High-quality Instruction-based Video Editing Framework"><a href=https://arxiv.org/abs/2512.16920v1>EasyV2V: A High-quality Instruction-based Video Editing Framework</a></a></li><li><a href=#differences-that-matter-auditing-models-for-capability-gap-discovery-and-rectificationhttpsarxivorgabs251216921v1 aria-label="Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification"><a href=https://arxiv.org/abs/2512.16921v1>Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</a></a></li><li><a href=#discovering-gravitational-waveform-distortions-from-lensing-a-deep-dive-into-gw231123httpsarxivorgabs251216916v1 aria-label="Discovering gravitational waveform distortions from lensing: a deep dive into GW231123"><a href=https://arxiv.org/abs/2512.16916v1>Discovering gravitational waveform distortions from lensing: a deep dive into GW231123</a></a></li><li><a href=#in-context-algebrahttpsarxivorgabs251216902v1 aria-label="In-Context Algebra"><a href=https://arxiv.org/abs/2512.16902v1>In-Context Algebra</a></a></li><li><a href=#instant-expressive-gaussian-head-avatar-via-3d-aware-expression-distillationhttpsarxivorgabs251216893v1 aria-label="Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation"><a href=https://arxiv.org/abs/2512.16893v1>Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation</a></a></li><li><a href=#machine-learning-assisted-high-throughput-prediction-of-moir%c3%a9-materialshttpsarxivorgabs251216892v1 aria-label="Machine learning assisted high throughput prediction of moir√© materials"><a href=https://arxiv.org/abs/2512.16892v1>Machine learning assisted high throughput prediction of moir√© materials</a></a></li><li><a href=#revival-dynamics-from-equilibrium-states-scars-from-chords-in-sykhttpsarxivorgabs251216836v1 aria-label="Revival Dynamics from Equilibrium States: Scars from Chords in SYK"><a href=https://arxiv.org/abs/2512.16836v1>Revival Dynamics from Equilibrium States: Scars from Chords in SYK</a></a></li><li><a href=#experimental-measurement-of-enhanced-group-delay-silicon-photonic-waveguides-indicative-of-the-frozen-mode-regime-around-the-stationary-inflection-pointhttpsarxivorgabs251216831v1 aria-label="Experimental Measurement of Enhanced Group Delay Silicon Photonic Waveguides Indicative of the Frozen Mode Regime Around the Stationary Inflection Point"><a href=https://arxiv.org/abs/2512.16831v1>Experimental Measurement of Enhanced Group Delay Silicon Photonic Waveguides Indicative of the Frozen Mode Regime Around the Stationary Inflection Point</a></a></li><li><a href=#rayleigh-b%c3%a9nard-thermal-convection-in-emulsions-a-short-reviewhttpsarxivorgabs251216830v1 aria-label="Rayleigh-B√©nard thermal convection in emulsions: a short review"><a href=https://arxiv.org/abs/2512.16830v1>Rayleigh-B√©nard thermal convection in emulsions: a short review</a></a></li><li><a href=#consistent-excesses-in-the-lhc-electroweak-susy-searches-gut-based-singlinohiggsino-interpretation-in-the-nmssmhttpsarxivorgabs251216783v1 aria-label="Consistent Excesses in the LHC Electroweak SUSY Searches: GUT-based Singlino/Higgsino Interpretation in the NMSSM"><a href=https://arxiv.org/abs/2512.16783v1>Consistent Excesses in the LHC Electroweak SUSY Searches: GUT-based Singlino/Higgsino Interpretation in the NMSSM</a></a></li><li><a href=#field-quantisations-in-schwarzschild-spacetime-theory-versus-low-energy-experimentshttpsarxivorgabs251216667v1 aria-label="Field Quantisations in Schwarzschild Spacetime: Theory versus Low-Energy Experiments"><a href=https://arxiv.org/abs/2512.16667v1>Field Quantisations in Schwarzschild Spacetime: Theory versus Low-Energy Experiments</a></a></li><li><a href=#self-affine-scaling-of-earths-islandshttpsarxivorgabs251216659v1 aria-label="Self-Affine Scaling of Earth&rsquo;s Islands"><a href=https://arxiv.org/abs/2512.16659v1>Self-Affine Scaling of Earth&rsquo;s Islands</a></a></li><li><a href=#stereopilot-learning-unified-and-efficient-stereo-conversion-via-generative-priorshttpsarxivorgabs251216915v1 aria-label="StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors"><a href=https://arxiv.org/abs/2512.16915v1>StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors</a></a></li><li><a href=#depth-any-panoramas-a-foundation-model-for-panoramic-depth-estimationhttpsarxivorgabs251216913v1 aria-label="Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation"><a href=https://arxiv.org/abs/2512.16913v1>Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation</a></a></li><li><a href=#opening-the-house-datasets-for-mixed-doubles-curlinghttpsarxivorgabs251216574v1 aria-label="Opening the House: Datasets for Mixed Doubles Curling"><a href=https://arxiv.org/abs/2512.16574v1>Opening the House: Datasets for Mixed Doubles Curling</a></a></li><li><a href=#brepllm-native-boundary-representation-understanding-with-large-language-modelshttpsarxivorgabs251216413v1 aria-label="BrepLLM: Native Boundary Representation Understanding with Large Language Models"><a href=https://arxiv.org/abs/2512.16413v1>BrepLLM: Native Boundary Representation Understanding with Large Language Models</a></a></li><li><a href=#occupational-tasks-automation-and-economic-growth-a-modeling-and-simulation-approachhttpsarxivorgabs251216261v1 aria-label="Occupational Tasks, Automation, and Economic Growth: A Modeling and Simulation Approach"><a href=https://arxiv.org/abs/2512.16261v1>Occupational Tasks, Automation, and Economic Growth: A Modeling and Simulation Approach</a></a></li><li><a href=#privacy-discourse-and-emotional-dynamics-in-mental-health-information-interaction-on-reddithttpsarxivorgabs251215945v1 aria-label="Privacy Discourse and Emotional Dynamics in Mental Health Information Interaction on Reddit"><a href=https://arxiv.org/abs/2512.15945v1>Privacy Discourse and Emotional Dynamics in Mental Health Information Interaction on Reddit</a></a></li><li><a href=#the-moralization-corpus-frame-based-annotation-and-analysis-of-moralizing-speech-acts-across-diverse-text-genreshttpsarxivorgabs251215248v1 aria-label="The Moralization Corpus: Frame-Based Annotation and Analysis of Moralizing Speech Acts across Diverse Text Genres"><a href=https://arxiv.org/abs/2512.15248v1>The Moralization Corpus: Frame-Based Annotation and Analysis of Moralizing Speech Acts across Diverse Text Genres</a></a></li><li><a href=#probing-scalar-and-pseudoscalar-new-physics-using-rare-kaon-decayshttpsarxivorgabs251216903v1 aria-label="Probing scalar and pseudoscalar new physics using rare kaon decays"><a href=https://arxiv.org/abs/2512.16903v1>Probing scalar and pseudoscalar new physics using rare kaon decays</a></a></li><li><a href=#many-body-contextuality-and-self-testing-quantum-matter-via-nonlocal-gameshttpsarxivorgabs251216886v1 aria-label="Many-body contextuality and self-testing quantum matter via nonlocal games"><a href=https://arxiv.org/abs/2512.16886v1>Many-body contextuality and self-testing quantum matter via nonlocal games</a></a></li><li><a href=#wiedemann-franz-violation-and-thermal-hall-effect-in-kagome-metal-tbcr6ge6httpsarxivorgabs251216868v1 aria-label="Wiedemann-Franz violation and thermal Hall effect in kagome metal TbCr6Ge6"><a href=https://arxiv.org/abs/2512.16868v1>Wiedemann-Franz violation and thermal Hall effect in kagome metal TbCr6Ge6</a></a></li><li><a href=#nonstabilizerness-in-stark-many-body-localizationhttpsarxivorgabs251216859v1 aria-label="Nonstabilizerness in Stark many-body localization"><a href=https://arxiv.org/abs/2512.16859v1>Nonstabilizerness in Stark many-body localization</a></a></li><li><a href=#toggle-temporal-logic-guided-large-language-model-compression-for-edgehttpsarxivorgabs251216855v1 aria-label="TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge"><a href=https://arxiv.org/abs/2512.16855v1>TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge</a></a></li><li><a href=#fighting-non-locality-with-non-locality-microcausality-and-boundary-conditions-in-qedhttpsarxivorgabs251216898v1 aria-label="Fighting non-locality with non-locality: microcausality and boundary conditions in QED"><a href=https://arxiv.org/abs/2512.16898v1>Fighting non-locality with non-locality: microcausality and boundary conditions in QED</a></a></li><li><a href=#growing-self-similar-markov-treeshttpsarxivorgabs251216894v1 aria-label="Growing Self-Similar Markov Trees"><a href=https://arxiv.org/abs/2512.16894v1>Growing Self-Similar Markov Trees</a></a></li><li><a href=#an-exciton-interacting-with-the-phonons-of-an-electronic-wigner-crystalhttpsarxivorgabs251216888v1 aria-label="An exciton interacting with the phonons of an electronic Wigner crystal"><a href=https://arxiv.org/abs/2512.16888v1>An exciton interacting with the phonons of an electronic Wigner crystal</a></a></li><li><a href=#electric-field-diagnostics-in-a-continuous-rf-plasma-using-rydberg-eithttpsarxivorgabs251216867v1 aria-label="Electric field diagnostics in a continuous rf plasma using Rydberg-EIT"><a href=https://arxiv.org/abs/2512.16867v1>Electric field diagnostics in a continuous rf plasma using Rydberg-EIT</a></a></li><li><a href=#grammar-forced-translation-of-natural-language-to-temporal-logic-using-llmshttpsarxivorgabs251216814v1 aria-label="Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs"><a href=https://arxiv.org/abs/2512.16814v1>Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs</a></a></li><li><a href=#scope-simple-coil-optimization-for-plasma-and-engineeringhttpsarxivorgabs251216546v1 aria-label="SCOPE: Simple Coil Optimization for Plasma and Engineering"><a href=https://arxiv.org/abs/2512.16546v1>SCOPE: Simple Coil Optimization for Plasma and Engineering</a></a></li><li><a href=#smile-on-the-face-sadness-in-the-eyes-bridging-the-emotion-gap-with-a-multimodal-dataset-of-eye-and-facial-behaviorshttpsarxivorgabs251216485v1 aria-label="Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors"><a href=https://arxiv.org/abs/2512.16485v1>Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors</a></a></li><li><a href=#black-start-power-capacity-sizing-and-control-strategy-for-an-islanded-dfig-wind-to-hydrogen-systemhttpsarxivorgabs251216263v1 aria-label="Black-Start Power Capacity Sizing and Control Strategy for an Islanded DFIG Wind-to-Hydrogen System"><a href=https://arxiv.org/abs/2512.16263v1>Black-Start Power Capacity Sizing and Control Strategy for an Islanded DFIG Wind-to-Hydrogen System</a></a></li><li><a href=#grhayl-a-modern-infrastructure-agnostic-extensible-library-for-grmhd-simulationshttpsarxivorgabs251215846v1 aria-label="GRHayL: a modern, infrastructure-agnostic, extensible library for GRMHD simulations"><a href=https://arxiv.org/abs/2512.15846v1>GRHayL: a modern, infrastructure-agnostic, extensible library for GRMHD simulations</a></a></li></ul></li><li><a href=#-psycholinguistics aria-label="üîç psycholinguistics">üîç psycholinguistics</a><ul><li><a href=#solving-the-dirac-equation-on-a-gpu-for-strong-field-processes-in-multidimensional-background-fieldshttpsarxivorgabs251216889v1 aria-label="Solving the Dirac equation on a GPU for strong-field processes in multidimensional background fields"><a href=https://arxiv.org/abs/2512.16889v1>Solving the Dirac equation on a GPU for strong-field processes in multidimensional background fields</a></a></li><li><a href=#askap-discovery-of-a-30-kpc-bipolar-outflow-from-the-edge-on-disk-of-the-nearby-spiral-galaxy-eso-130-g012httpsarxivorgabs251215991v1 aria-label="ASKAP discovery of a 30 kpc bipolar outflow from the edge-on disk of the nearby spiral galaxy ESO 130-G012"><a href=https://arxiv.org/abs/2512.15991v1>ASKAP discovery of a 30 kpc bipolar outflow from the edge-on disk of the nearby spiral galaxy ESO 130-G012</a></a></li><li><a href=#on-right-units-of-special-inverse-monoidshttpsarxivorgabs251215591v1 aria-label="On right units of special inverse monoids"><a href=https://arxiv.org/abs/2512.15591v1>On right units of special inverse monoids</a></a></li><li><a href=#wave-packet-dynamics-in-pseudo-hermitian-lattices-coexistence-of-hermitian-and-non-hermitian-wavefrontshttpsarxivorgabs251215333v1 aria-label="Wave-packet dynamics in pseudo-Hermitian lattices: Coexistence of Hermitian and non-Hermitian wavefronts"><a href=https://arxiv.org/abs/2512.15333v1>Wave-packet dynamics in pseudo-Hermitian lattices: Coexistence of Hermitian and non-Hermitian wavefronts</a></a></li><li><a href=#offline-multi-task-multi-objective-data-driven-evolutionary-algorithm-with-language-surrogate-model-and-implicit-q-learninghttpsarxivorgabs251215149v1 aria-label="Offline Multi-Task Multi-Objective Data-Driven Evolutionary Algorithm with Language Surrogate Model and Implicit Q-Learning"><a href=https://arxiv.org/abs/2512.15149v1>Offline Multi-Task Multi-Objective Data-Driven Evolutionary Algorithm with Language Surrogate Model and Implicit Q-Learning</a></a></li><li><a href=#autonomous-learning-of-attractors-for-neuromorphic-computing-with-wien-bridge-oscillator-networkshttpsarxivorgabs251214869v1 aria-label="Autonomous Learning of Attractors for Neuromorphic Computing with Wien Bridge Oscillator Networks"><a href=https://arxiv.org/abs/2512.14869v1>Autonomous Learning of Attractors for Neuromorphic Computing with Wien Bridge Oscillator Networks</a></a></li><li><a href=#maximum-mean-discrepancy-with-unequal-sample-sizes-via-generalized-u-statisticshttpsarxivorgabs251213997v1 aria-label="Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics"><a href=https://arxiv.org/abs/2512.13997v1>Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics</a></a></li><li><a href=#toward-6g-downlink-noma-crc-aided-grand-for-noise-resilient-noma-decoding-in-beyond-5g-networkshttpsarxivorgabs251216860v1 aria-label="Toward 6G Downlink NOMA: CRC-Aided GRAND for Noise-Resilient NOMA Decoding in Beyond-5G Networks"><a href=https://arxiv.org/abs/2512.16860v1>Toward 6G Downlink NOMA: CRC-Aided GRAND for Noise-Resilient NOMA Decoding in Beyond-5G Networks</a></a></li><li><a href=#connecting-current-and-future-dual-agn-searches-to-lisa-and-pta-gravitational-wave-detectionshttpsarxivorgabs251216844v1 aria-label="Connecting current and future dual AGN searches to LISA and PTA gravitational wave detections"><a href=https://arxiv.org/abs/2512.16844v1>Connecting current and future dual AGN searches to LISA and PTA gravitational wave detections</a></a></li><li><a href=#a-maps-detector-for-high-resolution-low-dose-ebsdhttpsarxivorgabs251216839v1 aria-label="A MAPS Detector for High Resolution Low Dose EBSD"><a href=https://arxiv.org/abs/2512.16839v1>A MAPS Detector for High Resolution Low Dose EBSD</a></a></li><li><a href=#enhancing-kinematics-understanding-through-a-real-time-graph-based-motion-video-gamehttpsarxivorgabs251216838v1 aria-label="Enhancing Kinematics Understanding Through a Real-Time Graph-Based Motion Video Game"><a href=https://arxiv.org/abs/2512.16838v1>Enhancing Kinematics Understanding Through a Real-Time Graph-Based Motion Video Game</a></a></li><li><a href=#delay-aware-multi-stage-edge-server-upgrade-with-budget-constrainthttpsarxivorgabs251216792v1 aria-label="Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint"><a href=https://arxiv.org/abs/2512.16792v1>Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint</a></a></li><li><a href=#the-social-responsibility-stack-a-control-theoretic-architecture-for-governing-socio-technical-aihttpsarxivorgabs251216873v1 aria-label="The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI"><a href=https://arxiv.org/abs/2512.16873v1>The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI</a></a></li><li><a href=#plausibility-as-failure-how-llms-and-humans-co-construct-epistemic-errorhttpsarxivorgabs251216750v1 aria-label="Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error"><a href=https://arxiv.org/abs/2512.16750v1>Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error</a></a></li><li><a href=#cyber-humanism-in-education-reclaiming-agency-through-ai-and-learning-scienceshttpsarxivorgabs251216701v1 aria-label="Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences"><a href=https://arxiv.org/abs/2512.16701v1>Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences</a></a></li><li><a href=#the-hands-of-time-moving-my-body-to-keep-time-order-in-the-brainhttpsarxivorgabs251216616v1 aria-label="The hands of time: Moving my body to keep time order in the brain"><a href=https://arxiv.org/abs/2512.16616v1>The hands of time: Moving my body to keep time order in the brain</a></a></li><li><a href=#needle-in-the-web-a-benchmark-for-retrieving-targeted-web-pages-in-the-wildhttpsarxivorgabs251216553v1 aria-label="Needle in the Web: A Benchmark for Retrieving Targeted Web Pages in the Wild"><a href=https://arxiv.org/abs/2512.16553v1>Needle in the Web: A Benchmark for Retrieving Targeted Web Pages in the Wild</a></a></li><li><a href=#unraveling-persistent-urban-rural-gaps-a-long-term-provincial-analysis-of-residential-heating-and-cooling-loadshttpsarxivorgabs251216779v1 aria-label="Unraveling persistent urban-rural gaps: A long-term provincial analysis of residential heating and cooling loads"><a href=https://arxiv.org/abs/2512.16779v1>Unraveling persistent urban-rural gaps: A long-term provincial analysis of residential heating and cooling loads</a></a></li><li><a href=#hypervelocity-impact-debris-cloud-trajectory-planning-based-on-additive-manufactured-lattice-structureshttpsarxivorgabs251216611v1 aria-label="Hypervelocity Impact Debris Cloud Trajectory-Planning based on Additive Manufactured Lattice Structures"><a href=https://arxiv.org/abs/2512.16611v1>Hypervelocity Impact Debris Cloud Trajectory-Planning based on Additive Manufactured Lattice Structures</a></a></li></ul></li><li><a href=#-llm aria-label="üîç llm">üîç llm</a><ul><li><a href=#posterior-behavioral-cloning-pretraining-bc-policies-for-efficient-rl-finetuninghttpsarxivorgabs251216911v1 aria-label="Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning"><a href=https://arxiv.org/abs/2512.16911v1>Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning</a></a></li><li><a href=#viva-vlm-guided-instruction-based-video-editing-with-reward-optimizationhttpsarxivorgabs251216906v1 aria-label="VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization"><a href=https://arxiv.org/abs/2512.16906v1>VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization</a></a></li><li><a href=#impacts-of-racial-bias-in-historical-training-data-for-news-aihttpsarxivorgabs251216901v1 aria-label="Impacts of Racial Bias in Historical Training Data for News AI"><a href=https://arxiv.org/abs/2512.16901v1>Impacts of Racial Bias in Historical Training Data for News AI</a></a></li><li><a href=#checking-the-hal-interface-specification-continuously-right-from-the-starthttpsarxivorgabs251216897v1 aria-label="Checking the HAL Interface Specification Continuously, Right from the Start"><a href=https://arxiv.org/abs/2512.16897v1>Checking the HAL Interface Specification Continuously, Right from the Start</a></a></li><li><a href=#multimer-embedding-for-molecular-crystals-utilizing-up-to-tetramer-interactionshttpsarxivorgabs251216877v1 aria-label="Multimer Embedding for Molecular Crystals Utilizing up to Tetramer Interactions"><a href=https://arxiv.org/abs/2512.16877v1>Multimer Embedding for Molecular Crystals Utilizing up to Tetramer Interactions</a></a></li><li><a href=#alchemist-unlocking-efficiency-in-text-to-image-model-training-via-meta-gradient-data-selectionhttpsarxivorgabs251216905v1 aria-label="Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection"><a href=https://arxiv.org/abs/2512.16905v1>Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection</a></a></li></ul></li><li><a href=#-neuroscience aria-label="üîç neuroscience">üîç neuroscience</a><ul><li><a href=#an-evacuation-simulator-for-pedestrian-dynamics-based-on-the-social-force-modelhttpsarxivorgabs251216887v1 aria-label="An evacuation simulator for pedestrian dynamics based on the Social Force Model"><a href=https://arxiv.org/abs/2512.16887v1>An evacuation simulator for pedestrian dynamics based on the Social Force Model</a></a></li><li><a href=#privatexr-defending-privacy-attacks-in-extended-reality-through-explainable-ai-guided-differential-privacyhttpsarxivorgabs251216851v1 aria-label="PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy"><a href=https://arxiv.org/abs/2512.16851v1>PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy</a></a></li></ul></li><li><a href=#-data_resources aria-label="üîç data_resources">üîç data_resources</a><ul><li><a href=#evaluating-openai-gpt-models-for-translation-of-endangered-uralic-languages-a-comparison-of-reasoning-and-non-reasoning-architectureshttpsarxivorgabs251216287v1 aria-label="Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures"><a href=https://arxiv.org/abs/2512.16287v1>Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures</a></a></li><li><a href=#sigma-moe-tiny-technical-reporthttpsarxivorgabs251216248v1 aria-label="Sigma-Moe-Tiny Technical Report"><a href=https://arxiv.org/abs/2512.16248v1>Sigma-Moe-Tiny Technical Report</a></a></li><li><a href=#wemusic-agent-efficient-conversational-music-recommendation-via-knowledge-internalization-and-agentic-boundary-learninghttpsarxivorgabs251216108v1 aria-label="WeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning"><a href=https://arxiv.org/abs/2512.16108v1>WeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning</a></a></li><li><a href=#multimodal-rewardbench-2-evaluating-omni-reward-models-for-interleaved-text-and-imagehttpsarxivorgabs251216899v1 aria-label="Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image"><a href=https://arxiv.org/abs/2512.16899v1>Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image</a></a></li></ul></li><li><a href=#-emotion_language aria-label="üîç emotion_language">üîç emotion_language</a><ul><li><a href=#on-the-universal-representation-property-of-spiking-neural-networkshttpsarxivorgabs251216872v1 aria-label="On the Universal Representation Property of Spiking Neural Networks"><a href=https://arxiv.org/abs/2512.16872v1>On the Universal Representation Property of Spiking Neural Networks</a></a></li><li><a href=#what-do-prosody-and-text-convey-characterizing-how-meaningful-information-is-distributed-across-multiple-channelshttpsarxivorgabs251216832v1 aria-label="What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels"><a href=https://arxiv.org/abs/2512.16832v1>What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels</a></a></li><li><a href=#ginsign-grounding-natural-language-into-system-signatures-for-temporal-logic-translationhttpsarxivorgabs251216770v1 aria-label="GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation"><a href=https://arxiv.org/abs/2512.16770v1>GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation</a></a></li><li><a href=#pattern-recognition-in-complex-systems-via-vector-field-representations-of-spatio-temporal-datahttpsarxivorgabs251216763v1 aria-label="Pattern recognition in complex systems via vector-field representations of spatio-temporal data"><a href=https://arxiv.org/abs/2512.16763v1>Pattern recognition in complex systems via vector-field representations of spatio-temporal data</a></a></li><li><a href=#how-good-is-post-hoc-watermarking-with-language-model-rephrasinghttpsarxivorgabs251216904v1 aria-label="How Good is Post-Hoc Watermarking With Language Model Rephrasing?"><a href=https://arxiv.org/abs/2512.16904v1>How Good is Post-Hoc Watermarking With Language Model Rephrasing?</a></a></li><li><a href=#learning-confidence-ellipsoids-and-applications-to-robust-subspace-recoveryhttpsarxivorgabs251216875v1 aria-label="Learning Confidence Ellipsoids and Applications to Robust Subspace Recovery"><a href=https://arxiv.org/abs/2512.16875v1>Learning Confidence Ellipsoids and Applications to Robust Subspace Recovery</a></a></li><li><a href=#m-phygs-multi-material-object-dynamics-from-videohttpsarxivorgabs251216885v1 aria-label="M-PhyGs: Multi-Material Object Dynamics from Video"><a href=https://arxiv.org/abs/2512.16885v1>M-PhyGs: Multi-Material Object Dynamics from Video</a></a></li><li><a href=#a-radio-search-for-star-planet-interaction-in-toi-540-and-speculoos-3httpsarxivorgabs251216852v1 aria-label="A Radio Search for Star-Planet Interaction in TOI-540 and SPECULOOS-3"><a href=https://arxiv.org/abs/2512.16852v1>A Radio Search for Star-Planet Interaction in TOI-540 and SPECULOOS-3</a></a></li><li><a href=#on-the-edge-of-core-non-emptiness-an-automated-reasoning-approach-to-approval-based-multi-winner-votinghttpsarxivorgabs251216895v1 aria-label="On the Edge of Core (Non-)Emptiness: An Automated Reasoning Approach to Approval-Based Multi-Winner Voting"><a href=https://arxiv.org/abs/2512.16895v1>On the Edge of Core (Non-)Emptiness: An Automated Reasoning Approach to Approval-Based Multi-Winner Voting</a></a></li><li><a href=#veblen-effects-and-broken-windows-in-an-environmental-olg-modelhttpsarxivorgabs251216806v1 aria-label="Veblen effects and broken windows in an environmental OLG model"><a href=https://arxiv.org/abs/2512.16806v1>Veblen effects and broken windows in an environmental OLG model</a></a></li><li><a href=#a-survey-on-spatio-temporal-knowledge-graph-modelshttpsarxivorgabs251216487v1 aria-label="A Survey on Spatio-Temporal Knowledge Graph Models"><a href=https://arxiv.org/abs/2512.16487v1>A Survey on Spatio-Temporal Knowledge Graph Models</a></a></li><li><a href=#the-motile-josephson-array-bridging-active-turbulence-and-superconductivityhttpsarxivorgabs251216884v1 aria-label="The Motile Josephson Array: Bridging Active Turbulence and Superconductivity"><a href=https://arxiv.org/abs/2512.16884v1>The Motile Josephson Array: Bridging Active Turbulence and Superconductivity</a></a></li></ul></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h1 id=weekly-paper-notes>Weekly Paper Notes<a hidden class=anchor aria-hidden=true href=#weekly-paper-notes>#</a></h1><h2 id=-multilingual>üîç multilingual<a hidden class=anchor aria-hidden=true href=#-multilingual>#</a></h2><h3 id=from-essence-to-defense-adaptive-semantic-aware-watermarking-for-embedding-as-a-service-copyright-protectionhttpsarxivorgabs251216439v1><a href=https://arxiv.org/abs/2512.16439v1>From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection</a><a hidden class=anchor aria-hidden=true href=#from-essence-to-defense-adaptive-semantic-aware-watermarking-for-embedding-as-a-service-copyright-protectionhttpsarxivorgabs251216439v1>#</a></h3><p><strong>Authors:</strong> Hao Li, Yubing Ren, Yanan Cao, Yingjie Li, Fang Fang, Xuebin Wang
<strong>Venue:</strong> arXiv (2025)</p><p>Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16439v1">üìÑ Download PDF</a></p><hr><h3 id=bridging-the-reality-gap-efficient-adaptation-of-asr-systems-for-challenging-low-resource-domainshttpsarxivorgabs251216401v1><a href=https://arxiv.org/abs/2512.16401v1>Bridging the Reality Gap: Efficient Adaptation of ASR systems for Challenging Low-Resource Domains</a><a hidden class=anchor aria-hidden=true href=#bridging-the-reality-gap-efficient-adaptation-of-asr-systems-for-challenging-low-resource-domainshttpsarxivorgabs251216401v1>#</a></h3><p><strong>Authors:</strong> Darshil Chauhan, Adityasinh Solanki, Vansh Patel, Kanav Kapoor, Ritvik Jain, Aditya Bansal, Dhruv Kumar, Prateek Narang
<strong>Venue:</strong> arXiv (2025)</p><p>Automatic Speech Recognition (ASR) holds immense potential to streamline clinical documentation, such as digitizing handwritten prescriptions and reports, thereby increasing patient throughput and reducing costs in resource-constrained sectors like rural healthcare. However, realizing this utility is currently obstructed by significant technical barriers: strict data privacy constraints, limited computational resources, and severe acoustic domain shifts. We quantify this gap by showing that a robust multilingual model (IndicWav2Vec) degrades to a stark 40.94% Word Error Rate (WER) when deployed on real-world clinical audio (Gram Vaani), rendering it unusable for practical applications. To address these challenges and bring ASR closer to deployment, we propose an efficient, privacy-preserving adaptation framework. We employ Low-Rank Adaptation (LoRA) to enable continual learning from incoming data streams directly on edge devices, ensuring patient data confidentiality. Our strategy yields a 17.1% relative improvement in WER on the target domain. Furthermore, by integrating multi-domain experience replay, we reduce catastrophic forgetting by 47% compared to naive adaptation. These results demonstrate a viable pathway for building reliable, self-improving ASR systems that can operate effectively within the constraints of high-impact real-world environments.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16401v1">üìÑ Download PDF</a></p><hr><h3 id=hearing-to-translate-the-effectiveness-of-speech-modality-integration-into-llmshttpsarxivorgabs251216378v1><a href=https://arxiv.org/abs/2512.16378v1>Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs</a><a hidden class=anchor aria-hidden=true href=#hearing-to-translate-the-effectiveness-of-speech-modality-integration-into-llmshttpsarxivorgabs251216378v1>#</a></h3><p><strong>Authors:</strong> Sara Papi, Javier Garcia Gilabert, Zachary Hopton, Vil√©m Zouhar, Carlos Escolano, Gerard I. G√°llego, Jorge Iranzo-S√°nchez, Ahrii Kim, Dominik Mach√°ƒçek, Patricia Schmidtova, Maike Z√ºfle
<strong>Venue:</strong> arXiv (2025)</p><p>As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16378v1">üìÑ Download PDF</a></p><hr><h3 id=mitigating-hallucinations-in-healthcare-llms-with-granular-fact-checking-and-domain-specific-adaptationhttpsarxivorgabs251216189v1><a href=https://arxiv.org/abs/2512.16189v1>Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation</a><a hidden class=anchor aria-hidden=true href=#mitigating-hallucinations-in-healthcare-llms-with-granular-fact-checking-and-domain-specific-adaptationhttpsarxivorgabs251216189v1>#</a></h3><p><strong>Authors:</strong> Musarrat Zeba, Abdullah Al Mamun, Kishoar Jahan Tithee, Debopom Sutradhar, Mohaimenul Azam Khan Raiaan, Saddam Mukta, Reem E. Mohamed, Md Rafiqul Islam, Yakub Sebastian, Mukhtar Hussain, Sami Azam
<strong>Venue:</strong> arXiv (2025)</p><p>In healthcare, it is essential for any LLM-generated output to be reliable and accurate, particularly in cases involving decision-making and patient safety. However, the outputs are often unreliable in such critical areas due to the risk of hallucinated outputs from the LLMs. To address this issue, we propose a fact-checking module that operates independently of any LLM, along with a domain-specific summarization model designed to minimize hallucination rates. Our model is fine-tuned using Low-Rank Adaptation (LoRa) on the MIMIC III dataset and is paired with the fact-checking module, which uses numerical tests for correctness and logical checks at a granular level through discrete logic in natural language processing (NLP) to validate facts against electronic health records (EHRs). We trained the LLM model on the full MIMIC-III dataset. For evaluation of the fact-checking module, we sampled 104 summaries, extracted them into 3,786 propositions, and used these as facts. The fact-checking module achieves a precision of 0.8904, a recall of 0.8234, and an F1-score of 0.8556. Additionally, the LLM summary model achieves a ROUGE-1 score of 0.5797 and a BERTScore of 0.9120 for summary quality.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16189v1">üìÑ Download PDF</a></p><hr><h3 id=a-multi-agent-large-language-model-framework-for-automated-qualitative-analysishttpsarxivorgabs251216063v1><a href=https://arxiv.org/abs/2512.16063v1>A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis</a><a hidden class=anchor aria-hidden=true href=#a-multi-agent-large-language-model-framework-for-automated-qualitative-analysishttpsarxivorgabs251216063v1>#</a></h3><p><strong>Authors:</strong> Qidi Xu, Nuzha Amjad, Grace Giles, Alexa Cumming, De&rsquo;angelo Hermesky, Alexander Wen, Min Ji Kwak, Yejin Kim
<strong>Venue:</strong> arXiv (2025)</p><p>Understanding patients experiences is essential for advancing patient centered care, especially in chronic diseases that require ongoing communication. However, qualitative thematic analysis, the primary approach for exploring these experiences, remains labor intensive, subjective, and difficult to scale. In this study, we developed a multi agent large language model framework that automates qualitative thematic analysis through three agents (Instructor, Thematizer, CodebookGenerator), named Collaborative Theme Identification Agent (CoTI). We applied CoTI to 12 heart failure patient interviews to analyze their perceptions of medication intensity. CoTI identified key phrases, themes, and codebook that were more similar to those of the senior investigator than both junior investigators and baseline NLP models. We also implemented CoTI into a user-facing application to enable AI human interaction in qualitative analysis. However, collaboration between CoTI and junior investigators provided only marginal gains, suggesting they may overrely on CoTI and limit their independent critical thinking.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16063v1">üìÑ Download PDF</a></p><hr><h3 id=cross-language-bias-examination-in-large-language-modelshttpsarxivorgabs251216029v1><a href=https://arxiv.org/abs/2512.16029v1>Cross-Language Bias Examination in Large Language Models</a><a hidden class=anchor aria-hidden=true href=#cross-language-bias-examination-in-large-language-modelshttpsarxivorgabs251216029v1>#</a></h3><p><strong>Authors:</strong> Yuxuan Liang, Marwa Mahmoud
<strong>Venue:</strong> arXiv (2025)</p><p>This study introduces an innovative multilingual bias evaluation framework for assessing bias in Large Language Models, combining explicit bias assessment through the BBQ benchmark with implicit bias measurement using a prompt-based Implicit Association Test. By translating the prompts and word list into five target languages, English, Chinese, Arabic, French, and Spanish, we directly compare different types of bias across languages. The results reveal substantial gaps in bias across languages used in LLMs. For example, Arabic and Spanish consistently show higher levels of stereotype bias, while Chinese and English exhibit lower levels of bias. We also identify contrasting patterns across bias types. Age shows the lowest explicit bias but the highest implicit bias, emphasizing the importance of detecting implicit biases that are undetectable with standard benchmarks. These findings indicate that LLMs vary significantly across languages and bias dimensions. This study fills a key research gap by providing a comprehensive methodology for cross-lingual bias analysis. Ultimately, our work establishes a foundation for the development of equitable multilingual LLMs, ensuring fairness and effectiveness across diverse languages and cultures.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16029v1">üìÑ Download PDF</a></p><hr><h3 id=emotion-recognition-in-signershttpsarxivorgabs251215376v1><a href=https://arxiv.org/abs/2512.15376v1>Emotion Recognition in Signers</a><a hidden class=anchor aria-hidden=true href=#emotion-recognition-in-signershttpsarxivorgabs251215376v1>#</a></h3><p><strong>Authors:</strong> Kotaro Funakoshi, Yaoxiong Zhu
<strong>Venue:</strong> arXiv (2025)</p><p>Recognition of signers&rsquo; emotions suffers from one theoretical challenge and one practical challenge, namely, the overlap between grammatical and affective facial expressions and the scarcity of data for model training. This paper addresses these two challenges in a cross-lingual setting using our eJSL dataset, a new benchmark dataset for emotion recognition in Japanese Sign Language signers, and BOBSL, a large British Sign Language dataset with subtitles. In eJSL, two signers expressed 78 distinct utterances with each of seven different emotional states, resulting in 1,092 video clips. We empirically demonstrate that 1) textual emotion recognition in spoken language mitigates data scarcity in sign language, 2) temporal segment selection has a significant impact, and 3) incorporating hand motion enhances emotion recognition in signers. Finally we establish a stronger baseline than spoken language LLMs.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.15376v1">üìÑ Download PDF</a></p><hr><h3 id=multilingual-and-continuous-backchannel-prediction-a-cross-lingual-studyhttpsarxivorgabs251214085v1><a href=https://arxiv.org/abs/2512.14085v1>Multilingual and Continuous Backchannel Prediction: A Cross-lingual Study</a><a hidden class=anchor aria-hidden=true href=#multilingual-and-continuous-backchannel-prediction-a-cross-lingual-studyhttpsarxivorgabs251214085v1>#</a></h3><p><strong>Authors:</strong> Koji Inoue, Mikey Elmers, Yahui Fu, Zi Haur Pang, Taiga Mori, Divesh Lala, Keiko Ochi, Tatsuya Kawahara
<strong>Venue:</strong> arXiv (2025)</p><p>We present a multilingual, continuous backchannel prediction model for Japanese, English, and Chinese, and use it to investigate cross-linguistic timing behavior. The model is Transformer-based and operates at the frame level, jointly trained with auxiliary tasks on approximately 300 hours of dyadic conversations. Across all three languages, the multilingual model matches or surpasses monolingual baselines, indicating that it learns both language-universal cues and language-specific timing patterns. Zero-shot transfer with two-language training remains limited, underscoring substantive cross-lingual differences. Perturbation analyses reveal distinct cue usage: Japanese relies more on short-term linguistic information, whereas English and Chinese are more sensitive to silence duration and prosodic variation; multilingual training encourages shared yet adaptable representations and reduces overreliance on pitch in Chinese. A context-length study further shows that Japanese is relatively robust to shorter contexts, while Chinese benefits markedly from longer contexts. Finally, we integrate the trained model into a real-time processing software, demonstrating CPU-only inference. Together, these findings provide a unified model and empirical evidence for how backchannel timing differs across languages, informing the design of more natural, culturally-aware spoken dialogue systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.14085v1">üìÑ Download PDF</a></p><hr><h3 id=stutterfuse-mitigating-modality-collapse-in-stuttering-detection-with-jaccard-weighted-metric-learning-and-gated-fusionhttpsarxivorgabs251213632v1><a href=https://arxiv.org/abs/2512.13632v1>StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion</a><a hidden class=anchor aria-hidden=true href=#stutterfuse-mitigating-modality-collapse-in-stuttering-detection-with-jaccard-weighted-metric-learning-and-gated-fusionhttpsarxivorgabs251213632v1>#</a></h3><p><strong>Authors:</strong> Guransh Singh, Md Shah Fahad
<strong>Venue:</strong> arXiv (2025)</p><p>Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a &lsquo;block&rsquo; with a &lsquo;prolongation&rsquo;) due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve &ldquo;Modality Collapse&rdquo;, an &ldquo;Echo Chamber&rdquo; effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.13632v1">üìÑ Download PDF</a></p><hr><h3 id=scaling-laws-for-code-every-programming-language-mattershttpsarxivorgabs251213472v1><a href=https://arxiv.org/abs/2512.13472v1>Scaling Laws for Code: Every Programming Language Matters</a><a hidden class=anchor aria-hidden=true href=#scaling-laws-for-code-every-programming-language-mattershttpsarxivorgabs251213472v1>#</a></h3><p><strong>Authors:</strong> Jian Yang, Shawn Guo, Lin Jing, Wei Zhang, Aishan Liu, Chuan Hao, Zhoujun Li, Wayne Xin Zhao, Xianglong Liu, Weifeng Lv, Bryan Dai
<strong>Venue:</strong> arXiv (2025)</p><p>Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.13472v1">üìÑ Download PDF</a></p><hr><h3 id=venusbench-gd-a-comprehensive-multi-platform-gui-benchmark-for-diverse-grounding-taskshttpsarxivorgabs251216501v1><a href=https://arxiv.org/abs/2512.16501v1>VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks</a><a hidden class=anchor aria-hidden=true href=#venusbench-gd-a-comprehensive-multi-platform-gui-benchmark-for-diverse-grounding-taskshttpsarxivorgabs251216501v1>#</a></h3><p><strong>Authors:</strong> Beitong Zhou, Zhexiao Huang, Yuan Guo, Zhangxuan Gu, Tianyu Xia, Zichen Luo, Fei Tang, Dehan Kong, Yanyi Shang, Suling Ou, Zhenlin Guo, Changhua Meng, Shuheng Shen
<strong>Venue:</strong> arXiv (2025)</p><p>GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16501v1">üìÑ Download PDF</a></p><hr><h3 id=ppsebm-an-energy-based-model-with-progressive-parameter-selection-for-continual-learninghttpsarxivorgabs251215658v1><a href=https://arxiv.org/abs/2512.15658v1>PPSEBM: An Energy-Based Model with Progressive Parameter Selection for Continual Learning</a><a hidden class=anchor aria-hidden=true href=#ppsebm-an-energy-based-model-with-progressive-parameter-selection-for-continual-learninghttpsarxivorgabs251215658v1>#</a></h3><p><strong>Authors:</strong> Xiaodi Li, Dingcheng Li, Rujun Gao, Mahmoud Zamani, Feng Mi, Latifur Khan
<strong>Venue:</strong> arXiv (2025)</p><p>Continual learning remains a fundamental challenge in machine learning, requiring models to learn from a stream of tasks without forgetting previously acquired knowledge. A major obstacle in this setting is catastrophic forgetting, where performance on earlier tasks degrades as new tasks are learned. In this paper, we introduce PPSEBM, a novel framework that integrates an Energy-Based Model (EBM) with Progressive Parameter Selection (PPS) to effectively address catastrophic forgetting in continual learning for natural language processing tasks. In PPSEBM, progressive parameter selection allocates distinct, task-specific parameters for each new task, while the EBM generates representative pseudo-samples from prior tasks. These generated samples actively inform and guide the parameter selection process, enhancing the model&rsquo;s ability to retain past knowledge while adapting to new tasks. Experimental results on diverse NLP benchmarks demonstrate that PPSEBM outperforms state-of-the-art continual learning methods, offering a promising and robust solution to mitigate catastrophic forgetting.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.15658v1">üìÑ Download PDF</a></p><hr><h3 id=an-empirical-study-on-chinese-character-decomposition-in-multiword-expression-aware-neural-machine-translationhttpsarxivorgabs251215556v1><a href=https://arxiv.org/abs/2512.15556v1>An Empirical Study on Chinese Character Decomposition in Multiword Expression-Aware Neural Machine Translation</a><a hidden class=anchor aria-hidden=true href=#an-empirical-study-on-chinese-character-decomposition-in-multiword-expression-aware-neural-machine-translationhttpsarxivorgabs251215556v1>#</a></h3><p><strong>Authors:</strong> Lifeng Han, Gareth J. F. Jones, Alan F. Smeaton
<strong>Venue:</strong> arXiv (2025)</p><p>Word meaning, representation, and interpretation play fundamental roles in natural language understanding (NLU), natural language processing (NLP), and natural language generation (NLG) tasks. Many of the inherent difficulties in these tasks stem from Multi-word Expressions (MWEs), which complicate the tasks by introducing ambiguity, idiomatic expressions, infrequent usage, and a wide range of variations. Significant effort and substantial progress have been made in addressing the challenging nature of MWEs in Western languages, particularly English. This progress is attributed in part to the well-established research communities and the abundant availability of computational resources. However, the same level of progress is not true for language families such as Chinese and closely related Asian languages, which continue to lag behind in this regard. While sub-word modelling has been successfully applied to many Western languages to address rare words improving phrase comprehension, and enhancing machine translation (MT) through techniques like byte-pair encoding (BPE), it cannot be applied directly to ideograph language scripts like Chinese. In this work, we conduct a systematic study of the Chinese character decomposition technology in the context of MWE-aware neural machine translation (NMT). Furthermore, we report experiments to examine how Chinese character decomposition technology contributes to the representation of the original meanings of Chinese words and characters, and how it can effectively address the challenges of translating MWEs.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.15556v1">üìÑ Download PDF</a></p><hr><h3 id=multipath-transfer-engine-breaking-gpu-and-host-memory-bandwidth-bottlenecks-in-llm-serviceshttpsarxivorgabs251216056v1><a href=https://arxiv.org/abs/2512.16056v1>MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services</a><a hidden class=anchor aria-hidden=true href=#multipath-transfer-engine-breaking-gpu-and-host-memory-bandwidth-bottlenecks-in-llm-serviceshttpsarxivorgabs251216056v1>#</a></h3><p><strong>Authors:</strong> Lingfeng Tang, Daoping Zhang, Junjie Chen, Peihao Huang, Feng Jin, Chengguang Xu, Yuxin Chen, Feiqiang Sun, Guo Chen
<strong>Venue:</strong> arXiv (2025)</p><p>The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM&rsquo;s sleep mode by 1.12x to 2.48x.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16056v1">üìÑ Download PDF</a></p><hr><h3 id=magic-state-cultivation-on-a-superconducting-quantum-processorhttpsarxivorgabs251213908v1><a href=https://arxiv.org/abs/2512.13908v1>Magic state cultivation on a superconducting quantum processor</a><a hidden class=anchor aria-hidden=true href=#magic-state-cultivation-on-a-superconducting-quantum-processorhttpsarxivorgabs251213908v1>#</a></h3><p><strong>Authors:</strong> Emma Rosenfeld, Craig Gidney, Gabrielle Roberts, Alexis Morvan, Nathan Lacroix, Dvir Kafri, Jeffrey Marshall, Ming Li, Volodymyr Sivak, Dmitry Abanin, Amira Abbas, Rajeev Acharya, Laleh Aghababaie Beni, Georg Aigeldinger, Ross Alcaraz, Sayra Alcaraz, Trond I. Andersen, Markus Ansmann, Frank Arute, Kunal Arya, Walt Askew, Nikita Astrakhantsev, Juan Atalaya, Ryan Babbush, Brian Ballard, Joseph C. Bardin, Hector Bates, Andreas Bengtsson, Majid Bigdeli Karimi, Alexander Bilmes, Simon Bilodeau, Felix Borjans, Jenna Bovaird, Dylan Bowers, Leon Brill, Peter Brooks, Michael Broughton, David A. Browne, Brett Buchea, Bob B. Buckley, Tim Burger, Brian Burkett, Nicholas Bushnell, Jamal Busnaina, Anthony Cabrera, Juan Campero, Hung-Shen Chang, Silas Chen, Zijun Chen, Ben Chiaro, Liang-Ying Chih, Agnetta Y. Cleland, Bryan Cochrane, Matt Cockrell, Josh Cogan, Paul Conner, Harold Cook, Rodrigo G. Corti√±as, William Courtney, Alexander L. Crook, Ben Curtin, Martin Damyanov, Sayan Das, Dripto M. Debroy, Sean Demura, Paul Donohoe, Ilya Drozdov, Andrew Dunsworth, Valerie Ehimhen, Alec Eickbusch, Aviv Moshe Elbag, Lior Ella, Mahmoud Elzouka, David Enriquez, Catherine Erickson, Lara Faoro, Vinicius S. Ferreira, Marcos Flores, Leslie Flores Burgos, Sam Fontes, Ebrahim Forati, Jeremiah Ford, Brooks Foxen, Masaya Fukami, Alan Wing Lun Fung, Lenny Fuste, Suhas Ganjam, Gonzalo Garcia, Christopher Garrick, Robert Gasca, Helge Gehring, Robert Geiger, √âlie Genois, William Giang, Dar Gilboa, James E. Goeders, Edward C. Gonzales, Raja Gosula, Stijn J. de Graaf, Alejandro Grajales Dau, Dietrich Graumann, Joel Grebel, Alex Greene, Jonathan A. Gross, Jose Guerrero, Lo√Øck Le Guevel, Tan Ha, Steve Habegger, Tanner Hadick, Ali Hadjikhani, Michael C. Hamilton, Monica Hansen, Matthew P. Harrigan, Sean D. Harrington, Jeanne Hartshorn, Stephen Heslin, Paula Heu, Oscar Higgott, Reno Hiltermann, Jeremy Hilton, Hsin-Yuan Huang, Mike Hucka, Christopher Hudspeth, Ashley Huff, William J. Huggins, Lev B. Ioffe, Evan Jeffrey, Shaun Jevons, Zhang Jiang, Xiaoxuan Jin, Chaitali Joshi, Pavol Juhas, Andreas Kabel, Hui Kang, Kiseo Kang, Amir H. Karamlou, Ryan Kaufman, Kostyantyn Kechedzhi, Tanuj Khattar, Mostafa Khezri, Seon Kim, Paul V. Klimov, Can M. Knaut, Bryce Kobrin, Alexander N. Korotkov, Fedor Kostritsa, John Mark Kreikebaum, Ryuho Kudo, Ben Kueffler, Arun Kumar, Vladislav D. Kurilovich, Vitali Kutsko, Tiano Lange-Dei, Brandon W. Langley, Pavel Laptev, Kim-Ming Lau, Emma Leavell, Justin Ledford, Joy Lee, Kenny Lee, Brian J. Lester, Wendy Leung, Lily Li, Wing Yan Li, Alexander T. Lill, William P. Livingston, Matthew T. Lloyd, Aditya Locharla, Laura De Lorenzo, Erik Lucero, Daniel Lundahl, Aaron Lunt, Sid Madhuk, Aniket Maiti, Ashley Maloney, Salvatore Mandr√†, Leigh S. Martin, Orion Martin, Eric Mascot, Paul Masih Das, Dmitri Maslov, Melvin Mathews, Cameron Maxfield, Jarrod R. McClean, Matt McEwen, Seneca Meeks, Anthony Megrant, Kevin C. Miao, Zlatko K. Minev, Reza Molavi, Sebastian Molina, Shirin Montazeri, Charles Neill, Michael Newman, Anthony Nguyen, Murray Nguyen, Chia-Hung Ni, Murphy Yuezhen Niu, Nicholas Noll, Logan Oas, William D. Oliver, Raymond Orosco, Kristoffer Ottosson, Alice Pagano, Agustin Di Paolo, Sherman Peek, David Peterson, Alex Pizzuto, Elias Portoles, Rebecca Potter, Orion Pritchard, Michael Qian, Chris Quintana, Ganesh Ramachandran, Arpit Ranadive, Matthew J. Reagor, Rachel Resnick, David M. Rhodes, Daniel Riley, Roberto Rodriguez, Emma Ropes, Lucia B. De Rose, Eliott Rosenberg, Dario Rosenstock, Elizabeth Rossi, Pedram Roushan, David A. Rower, Robert Salazar, Kannan Sankaragomathi, Murat Can Sarihan, Max Schaefer, Sebastian Schroeder, Henry F. Schurkus, Aria Shahingohar, Michael J. Shearn, Aaron Shorter, Noah Shutty, Vladimir Shvarts, Spencer Small, W. Clarke Smith, David A. Sobel, Barrett Spells, Sofia Springer, George Sterling, Jordan Suchard, Aaron Szasz, Alexander Sztein, Madeline Taylor, Jothi Priyanka Thiruraman, Douglas Thor, Dogan Timucin, Eifu Tomita, Alfredo Torres, M. Mert Torunbalci, Hao Tran, Abeer Vaishnav, Justin Vargas, Sergey Vdovichev, Guifre Vidal, Benjamin Villalonga, Catherine Vollgraff Heidweiller, Meghan Voorhees, Steven Waltman, Jonathan Waltz, Shannon X. Wang, Danni Wang, Brayden Ware, James D. Watson, Yonghua Wei, Travis Weidel, Theodore White, Kristi Wong, Bryan W. K. Woo, Christopher J. Wood, Maddy Woodson, Cheng Xing, Z. Jamie Yao, Ping Yeh, Bicheng Ying, Juhwan Yoo, Noureldin Yosri, Elliot Young, Grayson Young, Adam Zalcman, Ran Zhang, Yaxing Zhang, Ningfeng Zhu, Nicholas Zobrist, Zhenjie Zou, Hartmut Neven, Sergio Boixo, Cody Jones, Julian Kelly, Alexandre Bourassa, Kevin J. Satzinger
<strong>Venue:</strong> arXiv (2025)</p><p>Fault-tolerant quantum computing requires a universal gate set, but the necessary non-Clifford gates represent a significant resource cost for most quantum error correction architectures. Magic state cultivation offers an efficient alternative to resource-intensive distillation protocols; however, testing the proposal&rsquo;s assumptions represents a challenging departure from quantum memory experiments. We present an experimental study of magic state cultivation on a superconducting quantum processor. We implement cultivation, including code-switching into a surface code, and develop a fault-tolerant measurement protocol to bound the magic state fidelity. Cultivation reduces the error by a factor of 40, with a state fidelity of 0.9999(1) (retaining 8% of attempts). Our results experimentally establish magic state cultivation as a viable solution to one of quantum computing&rsquo;s most significant challenges.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.13908v1">üìÑ Download PDF</a></p><hr><h3 id=transversal-clifford-hierarchy-gates-via-non-abelian-surface-codeshttpsarxivorgabs251213777v1><a href=https://arxiv.org/abs/2512.13777v1>Transversal Clifford-Hierarchy Gates via Non-Abelian Surface Codes</a><a hidden class=anchor aria-hidden=true href=#transversal-clifford-hierarchy-gates-via-non-abelian-surface-codeshttpsarxivorgabs251213777v1>#</a></h3><p><strong>Authors:</strong> Alison Warman, Sakura Schafer-Nameki
<strong>Venue:</strong> arXiv (2025)</p><p>We present a purely 2D transversal realization of phase gates at any level of the Clifford hierarchy, and beyond, using non-Abelian surface codes. Our construction encodes a logical qubit in the quantum double $D(G)$ of a non-Abelian group $G$ on a triangular spatial patch. The logical gate is implemented transversally by stacking on the spatial region a symmetry-protected topological (SPT) phase specified by a group 2-cocycle. The Bravyi&ndash;K√∂nig theorem limits the unitary gates implementable by constant-depth quantum circuits on Pauli stabilizer codes in $D$ dimensions to the $D$-th level of the Clifford hierarchy. We bypass this, by constructing transversal unitary gates at arbitrary levels of the Clifford hierarchy purely in 2D, without sacrificing locality or fault tolerance, however at the cost of using the quantum double of a non-Abelian group $G$. Specifically, for $G = D_{4N}$, the dihedral group of order $8N$, we realize the phase gate $T^{1/N} = \mathrm{diag}(1, e^{iœÄ/(4N)})$ in the logical $\overline{Z}$ basis. For $8N = 2^n$, this gate lies at the $n$-th level of the Clifford hierarchy and, importantly, has a qubit-only realization: we show that it can be constructed in terms of Clifford-hierarchy stabilizers for a code with $n$ physical qubits on each edge of the lattice. We also discuss code-switching to the $\mathbb{Z}_2 \times \mathbb{Z}_2$ and $\mathbb{Z}_2$ toric codes, which can be utilized for the quantum error correction in this setup.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.13777v1">üìÑ Download PDF</a></p><hr><h3 id=litept-lighter-yet-stronger-point-transformerhttpsarxivorgabs251213689v1><a href=https://arxiv.org/abs/2512.13689v1>LitePT: Lighter Yet Stronger Point Transformer</a><a hidden class=anchor aria-hidden=true href=#litept-lighter-yet-stronger-point-transformerhttpsarxivorgabs251213689v1>#</a></h3><p><strong>Authors:</strong> Yuanwen Yue, Damien Robert, Jianyuan Wang, Sunghwan Hong, Jan Dirk Wegner, Christian Rupprecht, Konrad Schindler
<strong>Venue:</strong> arXiv (2025)</p><p>Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\times$ fewer parameters, runs $2\times$ faster, and uses $2\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: <a href=https://github.com/prs-eth/LitePT>https://github.com/prs-eth/LitePT</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.13689v1">üìÑ Download PDF</a></p><hr><h3 id=when-a-nation-speaks-machine-learning-and-nlp-in-peoples-sentiment-analysis-during-bangladeshs-2024-mass-uprisinghttpsarxivorgabs251215547v1><a href=https://arxiv.org/abs/2512.15547v1>When a Nation Speaks: Machine Learning and NLP in People&rsquo;s Sentiment Analysis During Bangladesh&rsquo;s 2024 Mass Uprising</a><a hidden class=anchor aria-hidden=true href=#when-a-nation-speaks-machine-learning-and-nlp-in-peoples-sentiment-analysis-during-bangladeshs-2024-mass-uprisinghttpsarxivorgabs251215547v1>#</a></h3><p><strong>Authors:</strong> Md. Samiul Alim, Mahir Shahriar Tamim, Maisha Rahman, Tanvir Ahmed Khan, Md Mushfique Anwar
<strong>Venue:</strong> arXiv (2025)</p><p>Sentiment analysis, an emerging research area within natural language processing (NLP), has primarily been explored in contexts like elections and social media trends, but there remains a significant gap in understanding emotional dynamics during civil unrest, particularly in the Bangla language. Our study pioneers sentiment analysis in Bangla during a national crisis by examining public emotions amid Bangladesh&rsquo;s 2024 mass uprising. We curated a unique dataset of 2,028 annotated news headlines from major Facebook news portals, classifying them into Outrage, Hope, and Despair. Through Latent Dirichlet Allocation (LDA), we identified prevalent themes like political corruption and public protests, and analyzed how events such as internet blackouts shaped sentiment patterns. It outperformed multilingual transformers (mBERT: 67%, XLM-RoBERTa: 71%) and traditional machine learning methods (SVM and Logistic Regression: both 70%). These results highlight the effectiveness of language-specific models and offer valuable insights into public sentiment during political turmoil.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.15547v1">üìÑ Download PDF</a></p><hr><h3 id=the-world-is-your-canvas-painting-promptable-events-with-reference-images-trajectories-and-texthttpsarxivorgabs251216924v1><a href=https://arxiv.org/abs/2512.16924v1>The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</a><a hidden class=anchor aria-hidden=true href=#the-world-is-your-canvas-painting-promptable-events-with-reference-images-trajectories-and-texthttpsarxivorgabs251216924v1>#</a></h3><p><strong>Authors:</strong> Hanlin Wang, Hao Ouyang, Qiuyu Wang, Yue Yu, Yihao Meng, Wen Wang, Ka Leong Cheng, Shuailei Ma, Qingyan Bai, Yixuan Li, Cheng Chen, Yanhong Zeng, Xing Zhu, Yujun Shen, Qifeng Chen
<strong>Venue:</strong> arXiv (2025)</p><p>We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories &ndash; encoding motion, timing, and visibility &ndash; with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: <a href=https://worldcanvas.github.io/>https://worldcanvas.github.io/</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16924v1">üìÑ Download PDF</a></p><hr><h3 id=next-embedding-prediction-makes-strong-vision-learnershttpsarxivorgabs251216922v1><a href=https://arxiv.org/abs/2512.16922v1>Next-Embedding Prediction Makes Strong Vision Learners</a><a hidden class=anchor aria-hidden=true href=#next-embedding-prediction-makes-strong-vision-learnershttpsarxivorgabs251216922v1>#</a></h3><p><strong>Authors:</strong> Sihan Xu, Ziqiao Ma, Wenhao Chai, Xuweiyi Chen, Weiyang Jin, Joyce Chai, Saining Xie, Stella X. Yu
<strong>Venue:</strong> arXiv (2025)</p><p>Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16922v1">üìÑ Download PDF</a></p><hr><h3 id=adatooler-v-adaptive-tool-use-for-images-and-videoshttpsarxivorgabs251216918v1><a href=https://arxiv.org/abs/2512.16918v1>AdaTooler-V: Adaptive Tool-Use for Images and Videos</a><a hidden class=anchor aria-hidden=true href=#adatooler-v-adaptive-tool-use-for-images-and-videoshttpsarxivorgabs251216918v1>#</a></h3><p><strong>Authors:</strong> Chaoyang Wang, Kaituo Feng, Dongyang Chen, Zhongyu Wang, Zhixun Li, Sicheng Gao, Meng Meng, Xu Zhou, Manyuan Zhang, Yuzhang Shang, Xiangyu Yue
<strong>Venue:</strong> arXiv (2025)</p><p>Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16918v1">üìÑ Download PDF</a></p><hr><h3 id=generative-adversarial-reasoner-enhancing-llm-reasoning-with-adversarial-reinforcement-learninghttpsarxivorgabs251216917v1><a href=https://arxiv.org/abs/2512.16917v1>Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning</a><a hidden class=anchor aria-hidden=true href=#generative-adversarial-reasoner-enhancing-llm-reasoning-with-adversarial-reinforcement-learninghttpsarxivorgabs251216917v1>#</a></h3><p><strong>Authors:</strong> Qihao Liu, Luoxin Ye, Wufei Ma, Yu-Cheng Chou, Alan Yuille
<strong>Venue:</strong> arXiv (2025)</p><p>Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice&rsquo;s soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16917v1">üìÑ Download PDF</a></p><hr><h3 id=constructive-circuit-amplification-improving-math-reasoning-in-llms-via-targeted-sub-network-updateshttpsarxivorgabs251216914v1><a href=https://arxiv.org/abs/2512.16914v1>Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates</a><a hidden class=anchor aria-hidden=true href=#constructive-circuit-amplification-improving-math-reasoning-in-llms-via-targeted-sub-network-updateshttpsarxivorgabs251216914v1>#</a></h3><p><strong>Authors:</strong> Nikhil Prakash, Donghao Ren, Dominik Moritz, Yannick Assogba
<strong>Venue:</strong> arXiv (2025)</p><p>Prior studies investigating the internal workings of LLMs have uncovered sparse subnetworks, often referred to as circuits, that are responsible for performing specific tasks. Additionally, it has been shown that model performance improvement through fine-tuning often results from the strengthening of existing circuits in the model. Taken together, these findings suggest the possibility of intervening directly on such circuits to make precise, task-targeted updates. Motivated by these findings, we propose a novel method called Constructive Circuit Amplification which identifies pivotal tokens from model reasoning traces as well as model components responsible for the desired task, and updates only those components. Applied to mathematical reasoning, it improves accuracy by up to +11.4% across multiple models while modifying as little as 1.59% of model components, with minimal impact on other abilities as measured by MMLU, TriviaQA, and TruthfulQA. These results demonstrate that targeted capabilities can be reliably enhanced by selectively updating a sparse set of model components.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16914v1">üìÑ Download PDF</a></p><hr><h3 id=exploration-vs-exploitation-rethinking-rlvr-through-clipping-entropy-and-spurious-rewardhttpsarxivorgabs251216912v1><a href=https://arxiv.org/abs/2512.16912v1>Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</a><a hidden class=anchor aria-hidden=true href=#exploration-vs-exploitation-rethinking-rlvr-through-clipping-entropy-and-spurious-rewardhttpsarxivorgabs251216912v1>#</a></h3><p><strong>Authors:</strong> Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen, Tianyi Lin
<strong>Venue:</strong> arXiv (2025)</p><p>This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16912v1">üìÑ Download PDF</a></p><hr><h3 id=momagraph-state-aware-unified-scene-graphs-with-vision-language-model-for-embodied-task-planninghttpsarxivorgabs251216909v1><a href=https://arxiv.org/abs/2512.16909v1>MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning</a><a hidden class=anchor aria-hidden=true href=#momagraph-state-aware-unified-scene-graphs-with-vision-language-model-for-embodied-task-planninghttpsarxivorgabs251216909v1>#</a></h3><p><strong>Authors:</strong> Yuanchen Ju, Yongyuan Liang, Yen-Jen Wang, Nandiraju Gireesh, Yuanliang Ju, Seungjae Lee, Qiao Gu, Elvis Hsieh, Furong Huang, Koushil Sreenath
<strong>Venue:</strong> arXiv (2025)</p><p>Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16909v1">üìÑ Download PDF</a></p><hr><h3 id=tiny-recursive-control-iterative-reasoning-for-efficient-optimal-controlhttpsarxivorgabs251216824v1><a href=https://arxiv.org/abs/2512.16824v1>Tiny Recursive Control: Iterative Reasoning for Efficient Optimal Control</a><a hidden class=anchor aria-hidden=true href=#tiny-recursive-control-iterative-reasoning-for-efficient-optimal-controlhttpsarxivorgabs251216824v1>#</a></h3><p><strong>Authors:</strong> Amit Jain, Richard Linares
<strong>Venue:</strong> arXiv (2025)</p><p>Neural network controllers increasingly demand millions of parameters, and language model approaches push into the billions. For embedded aerospace systems with strict power and latency constraints, this scaling is prohibitive. We present Tiny Recursive Control (TRC), a neural architecture based on a counterintuitive principle: capacity can emerge from iteration depth rather than parameter count. TRC applies compact networks (approximately 1.5M parameters) repeatedly through a two-level hierarchical latent structure, refining control sequences by simulating trajectories and correcting based on tracking error. Because the same weights process every refinement step, adding iterations increases computation without increasing memory. We evaluate TRC on nonlinear control problems including oscillator stabilization and powered descent with fuel constraints. Across these domains, TRC achieves near-optimal control costs while requiring only millisecond-scale inference on GPU and under 10~MB memory, two orders of magnitude smaller than language model baselines. These results demonstrate that recursive reasoning, previously confined to discrete tasks, transfers effectively to continuous control synthesis.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16824v1">üìÑ Download PDF</a></p><hr><h3 id=analogicity-in-list-coloring-problems-and-interval-k-Œ≥Œº-choosability-a-complexity-theoretic-studyhttpsarxivorgabs251216807v1><a href=https://arxiv.org/abs/2512.16807v1>Analogicity in List Coloring Problems and Interval $k$-$(Œ≥,Œº)$-choosability: A Complexity-Theoretic Study</a><a hidden class=anchor aria-hidden=true href=#analogicity-in-list-coloring-problems-and-interval-k-Œ≥Œº-choosability-a-complexity-theoretic-studyhttpsarxivorgabs251216807v1>#</a></h3><p><strong>Authors:</strong> Simone Ingrid Monteiro Gama, Rosiane de Freitas Rodrigues
<strong>Venue:</strong> arXiv (2025)</p><p>This work investigates structural and computational aspects of list-based graph coloring under interval constraints. Building on the framework of analogous and p-analogous problems, we show that classical List Coloring, $Œº$-coloring, and $(Œ≥,Œº)$-coloring share strong complexity-preserving correspondences on graph classes closed under pendant-vertex extensions. These equivalences allow hardness and tractability results to transfer directly among the models. Motivated by applications in scheduling and resource allocation with bounded ranges, we introduce the interval-restricted $k$-$(Œ≥,Œº)$-coloring model, where each vertex receives an interval of exactly $k$ consecutive admissible colors. We prove that, although $(Œ≥,Œº)$-coloring is NP-complete even on several well-structured graph classes, its $k$-restricted version becomes polynomial-time solvable for any fixed $k$. Extending this formulation, we define $k$-$(Œ≥,Œº)$-choosability and analyze its expressive power and computational limits. Our results show that the number of admissible list assignments is drastically reduced under interval constraints, yielding a more tractable alternative to classical choosability, even though the general decision problem remains located at high levels of the polynomial hierarchy. Overall, the paper provides a unified view of list-coloring variants through structural reductions, establishes new complexity bounds for interval-based models, and highlights the algorithmic advantages of imposing fixed-size consecutive color ranges.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16807v1">üìÑ Download PDF</a></p><hr><h3 id=physbrain-human-egocentric-data-as-a-bridge-from-vision-language-models-to-physical-intelligencehttpsarxivorgabs251216793v1><a href=https://arxiv.org/abs/2512.16793v1>PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence</a><a hidden class=anchor aria-hidden=true href=#physbrain-human-egocentric-data-as-a-bridge-from-vision-language-models-to-physical-intelligencehttpsarxivorgabs251216793v1>#</a></h3><p><strong>Authors:</strong> Xiaopeng Lin, Shijie Lian, Bin Yu, Ruoqi Yang, Changti Wu, Yuzhuo Miao, Yurun Jin, Yukun Shi, Cong Huang, Bojun Cheng, Kai Chen
<strong>Venue:</strong> arXiv (2025)</p><p>Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9%), demonstrating effective transfer from human egocentric supervision to downstream robot control.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16793v1">üìÑ Download PDF</a></p><hr><h3 id=few-shot-specific-emitter-identification-via-integrated-complex-variational-mode-decomposition-and-spatial-attention-transferhttpsarxivorgabs251216786v1><a href=https://arxiv.org/abs/2512.16786v1>Few-Shot Specific Emitter Identification via Integrated Complex Variational Mode Decomposition and Spatial Attention Transfer</a><a hidden class=anchor aria-hidden=true href=#few-shot-specific-emitter-identification-via-integrated-complex-variational-mode-decomposition-and-spatial-attention-transferhttpsarxivorgabs251216786v1>#</a></h3><p><strong>Authors:</strong> Chenyu Zhu, Zeyang Li, Ziyi Xie, Jie Zhang
<strong>Venue:</strong> arXiv (2025)</p><p>Specific emitter identification (SEI) utilizes passive hardware characteristics to authenticate transmitters, providing a robust physical-layer security solution. However, most deep-learning-based methods rely on extensive data or require prior information, which poses challenges in real-world scenarios with limited labeled data. We propose an integrated complex variational mode decomposition algorithm that decomposes and reconstructs complex-valued signals to approximate the original transmitted signals, thereby enabling more accurate feature extraction. We further utilize a temporal convolutional network to effectively model the sequential signal characteristics, and introduce a spatial attention mechanism to adaptively weight informative signal segments, significantly enhancing identification performance. Additionally, the branch network allows leveraging pre-trained weights from other data while reducing the need for auxiliary datasets. Ablation experiments on the simulated data demonstrate the effectiveness of each component of the model. An accuracy comparison on a public dataset reveals that our method achieves 96% accuracy using only 10 symbols without requiring any prior knowledge.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16786v1">üìÑ Download PDF</a></p><hr><h3 id=dvgt-driving-visual-geometry-transformerhttpsarxivorgabs251216919v1><a href=https://arxiv.org/abs/2512.16919v1>DVGT: Driving Visual Geometry Transformer</a><a hidden class=anchor aria-hidden=true href=#dvgt-driving-visual-geometry-transformerhttpsarxivorgabs251216919v1>#</a></h3><p><strong>Authors:</strong> Sicheng Zuo, Zixun Xie, Wenzhao Zheng, Shaoqing Xu, Fang Li, Shengyin Jiang, Long Chen, Zhi-Xin Yang, Jiwen Lu
<strong>Venue:</strong> arXiv (2025)</p><p>Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at <a href=https://github.com/wzzheng/DVGT>https://github.com/wzzheng/DVGT</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16919v1">üìÑ Download PDF</a></p><hr><h3 id=sftok-bridging-the-performance-gap-in-discrete-tokenizershttpsarxivorgabs251216910v1><a href=https://arxiv.org/abs/2512.16910v1>SFTok: Bridging the Performance Gap in Discrete Tokenizers</a><a hidden class=anchor aria-hidden=true href=#sftok-bridging-the-performance-gap-in-discrete-tokenizershttpsarxivorgabs251216910v1>#</a></h3><p><strong>Authors:</strong> Qihang Rao, Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu
<strong>Venue:</strong> arXiv (2025)</p><p>Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16910v1">üìÑ Download PDF</a></p><hr><h3 id=scenediff-a-benchmark-and-method-for-multiview-object-change-detectionhttpsarxivorgabs251216908v1><a href=https://arxiv.org/abs/2512.16908v1>SceneDiff: A Benchmark and Method for Multiview Object Change Detection</a><a hidden class=anchor aria-hidden=true href=#scenediff-a-benchmark-and-method-for-multiview-object-change-detectionhttpsarxivorgabs251216908v1>#</a></h3><p><strong>Authors:</strong> Yuqun Wu, Chih-hao Lin, Henry Che, Aditi Tiwari, Chuhang Zou, Shenlong Wang, Derek Hoiem
<strong>Venue:</strong> arXiv (2025)</p><p>We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16908v1">üìÑ Download PDF</a></p><hr><h3 id=flowing-from-reasoning-to-motion-learning-3d-hand-trajectory-prediction-from-egocentric-human-interaction-videoshttpsarxivorgabs251216907v1><a href=https://arxiv.org/abs/2512.16907v1>Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos</a><a hidden class=anchor aria-hidden=true href=#flowing-from-reasoning-to-motion-learning-3d-hand-trajectory-prediction-from-egocentric-human-interaction-videoshttpsarxivorgabs251216907v1>#</a></h3><p><strong>Authors:</strong> Mingfei Chen, Yifan Wang, Zhengqin Li, Homanga Bharadhwaj, Yujin Chen, Chuan Qin, Ziyi Kou, Yuan Tian, Eric Whitmire, Rajinder Sodhi, Hrvoje Benko, Eli Shlizerman, Yue Liu
<strong>Venue:</strong> arXiv (2025)</p><p>Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16907v1">üìÑ Download PDF</a></p><hr><h3 id=flashportrait-6x-faster-infinite-portrait-animation-with-adaptive-latent-predictionhttpsarxivorgabs251216900v1><a href=https://arxiv.org/abs/2512.16900v1>FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction</a><a hidden class=anchor aria-hidden=true href=#flashportrait-6x-faster-infinite-portrait-animation-with-adaptive-latent-predictionhttpsarxivorgabs251216900v1>#</a></h3><p><strong>Authors:</strong> Shuyuan Tu, Yueming Pan, Yinming Huang, Xintong Han, Zhen Xing, Qi Dai, Kai Qiu, Chong Luo, Zuxuan Wu
<strong>Venue:</strong> arXiv (2025)</p><p>Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16900v1">üìÑ Download PDF</a></p><hr><h3 id=linkedout-linking-world-knowledge-representation-out-of-video-llm-for-next-generation-video-recommendationhttpsarxivorgabs251216891v1><a href=https://arxiv.org/abs/2512.16891v1>LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation</a><a hidden class=anchor aria-hidden=true href=#linkedout-linking-world-knowledge-representation-out-of-video-llm-for-next-generation-video-recommendationhttpsarxivorgabs251216891v1>#</a></h3><p><strong>Authors:</strong> Haichao Zhang, Yao Lu, Lichen Wang, Yunzhe Li, Daiwei Chen, Yunpeng Xu, Yun Fu
<strong>Venue:</strong> arXiv (2025)</p><p>Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16891v1">üìÑ Download PDF</a></p><hr><h3 id=opentouch-bringing-full-hand-touch-to-real-world-interactionhttpsarxivorgabs251216842v1><a href=https://arxiv.org/abs/2512.16842v1>OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction</a><a hidden class=anchor aria-hidden=true href=#opentouch-bringing-full-hand-touch-to-real-world-interactionhttpsarxivorgabs251216842v1>#</a></h3><p><strong>Authors:</strong> Yuxin Ray Song, Jinzhou Li, Rao Fu, Devin Murphy, Kaichen Zhou, Rishi Shiv, Yaqi Li, Haoyu Xiong, Crystal Elaine Owens, Yilun Du, Yiyue Luo, Xianyi Cheng, Antonio Torralba, Wojciech Matusik, Paul Pu Liang
<strong>Venue:</strong> arXiv (2025)</p><p>The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16842v1">üìÑ Download PDF</a></p><hr><h3 id=mepic-memory-efficient-position-independent-caching-for-llm-servinghttpsarxivorgabs251216822v1><a href=https://arxiv.org/abs/2512.16822v1>MEPIC: Memory Efficient Position Independent Caching for LLM Serving</a><a hidden class=anchor aria-hidden=true href=#mepic-memory-efficient-position-independent-caching-for-llm-servinghttpsarxivorgabs251216822v1>#</a></h3><p><strong>Authors:</strong> Qian Wang, Zahra Yousefijamarani, Morgan Lindsay Heisler, Rongzhi Gu, Bai Xiaolong, Shan Yizhou, Wei Zhang, Wang Lan, Ying Xiong, Yong Zhang, Zhenan Fan
<strong>Venue:</strong> arXiv (2025)</p><p>Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.
We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16822v1">üìÑ Download PDF</a></p><hr><h3 id=exploration-of-augmentation-strategies-in-multi-modal-retrieval-augmented-generation-for-the-biomedical-domain-a-case-study-evaluating-question-answering-in-glycobiologyhttpsarxivorgabs251216802v1><a href=https://arxiv.org/abs/2512.16802v1>Exploration of Augmentation Strategies in Multi-modal Retrieval-Augmented Generation for the Biomedical Domain: A Case Study Evaluating Question Answering in Glycobiology</a><a hidden class=anchor aria-hidden=true href=#exploration-of-augmentation-strategies-in-multi-modal-retrieval-augmented-generation-for-the-biomedical-domain-a-case-study-evaluating-question-answering-in-glycobiologyhttpsarxivorgabs251216802v1>#</a></h3><p><strong>Authors:</strong> Primo≈æ Kocbek, Azra Frkatoviƒá-Hod≈æiƒá, Dora Laliƒá, Vivian Hui, Gordan Lauc, Gregor ≈†tiglic
<strong>Venue:</strong> arXiv (2025)</p><p>Multi-modal retrieval-augmented generation (MM-RAG) promises grounded biomedical QA, but it is unclear when to (i) convert figures/tables into text versus (ii) use optical character recognition (OCR)-free visual retrieval that returns page images and leaves interpretation to the generator. We study this trade-off in glycobiology, a visually dense domain. We built a benchmark of 120 multiple-choice questions (MCQs) from 25 papers, stratified by retrieval difficulty (easy text, medium figures/tables, hard cross-evidence). We implemented four augmentations-None, Text RAG, Multi-modal conversion, and late-interaction visual retrieval (ColPali)-using Docling parsing and Qdrant indexing. We evaluated mid-size open-source and frontier proprietary models (e.g., Gemma-3-27B-IT, GPT-4o family). Additional testing used the GPT-5 family and multiple visual retrievers (ColPali/ColQwen/ColFlor). Accuracy with Agresti-Coull 95% confidence intervals (CIs) was computed over 5 runs per configuration. With Gemma-3-27B-IT, Text and Multi-modal augmentation outperformed OCR-free retrieval (0.722-0.740 vs. 0.510 average accuracy). With GPT-4o, Multi-modal achieved 0.808, with Text 0.782 and ColPali 0.745 close behind; within-model differences were small. In follow-on experiments with the GPT-5 family, the best results with ColPali and ColFlor improved by ~2% to 0.828 in both cases. In general, across the GPT-5 family, ColPali, ColQwen, and ColFlor were statistically indistinguishable. GPT-5-nano trailed larger GPT-5 variants by roughly 8-10%. Pipeline choice is capacity-dependent: converting visuals to text lowers the reader burden and is more reliable for mid-size models, whereas OCR-free visual retrieval becomes competitive under frontier models. Among retrievers, ColFlor offers parity with heavier options at a smaller footprint, making it an efficient default when strong generators are available.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16802v1">üìÑ Download PDF</a></p><hr><h3 id=from-facts-to-conclusions--integrating-deductive-reasoning-in-retrieval-augmented-llmshttpsarxivorgabs251216795v1><a href=https://arxiv.org/abs/2512.16795v1>From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs</a><a hidden class=anchor aria-hidden=true href=#from-facts-to-conclusions--integrating-deductive-reasoning-in-retrieval-augmented-llmshttpsarxivorgabs251216795v1>#</a></h3><p><strong>Authors:</strong> Shubham Mishra, Samyek Jain, Gorang Mehrishi, Shiv Tiwari, Harsh Sharma, Pratik Narang, Dhruv Kumar
<strong>Venue:</strong> arXiv (2025)</p><p>Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16795v1">üìÑ Download PDF</a></p><hr><h3 id=cityseeker-how-do-vlms-explore-embodied-urban-navigation-with-implicit-human-needshttpsarxivorgabs251216755v1><a href=https://arxiv.org/abs/2512.16755v1>CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?</a><a hidden class=anchor aria-hidden=true href=#cityseeker-how-do-vlms-explore-embodied-urban-navigation-with-implicit-human-needshttpsarxivorgabs251216755v1>#</a></h3><p><strong>Authors:</strong> Siqi Wang, Chao Liang, Yunfan Gao, Erxin Yu, Sen Li, Yushi Li, Jing Li, Haofen Wang
<strong>Venue:</strong> arXiv (2025)</p><p>Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., &ldquo;I am thirsty&rdquo;) in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs&rsquo; spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping&rsquo;s emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling &ldquo;last-mile&rdquo; navigation challenges.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16755v1">üìÑ Download PDF</a></p><hr><h2 id=-linguistics>üîç linguistics<a hidden class=anchor aria-hidden=true href=#-linguistics>#</a></h2><h3 id=generative-refocusing-flexible-defocus-control-from-a-single-imagehttpsarxivorgabs251216923v1><a href=https://arxiv.org/abs/2512.16923v1>Generative Refocusing: Flexible Defocus Control from a Single Image</a><a hidden class=anchor aria-hidden=true href=#generative-refocusing-flexible-defocus-control-from-a-single-imagehttpsarxivorgabs251216923v1>#</a></h3><p><strong>Authors:</strong> Chun-Wei Tuan Mu, Jia-Bin Huang, Yu-Lun Liu
<strong>Venue:</strong> arXiv (2025)</p><p>Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16923v1">üìÑ Download PDF</a></p><hr><h3 id=easyv2v-a-high-quality-instruction-based-video-editing-frameworkhttpsarxivorgabs251216920v1><a href=https://arxiv.org/abs/2512.16920v1>EasyV2V: A High-quality Instruction-based Video Editing Framework</a><a hidden class=anchor aria-hidden=true href=#easyv2v-a-high-quality-instruction-based-video-editing-frameworkhttpsarxivorgabs251216920v1>#</a></h3><p><strong>Authors:</strong> Jinjie Mai, Chaoyang Wang, Guocheng Gordon Qian, Willi Menapace, Sergey Tulyakov, Bernard Ghanem, Peter Wonka, Ashkan Mirzaei
<strong>Venue:</strong> arXiv (2025)</p><p>While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: <a href=https://snap-research.github.io/easyv2v/>https://snap-research.github.io/easyv2v/</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16920v1">üìÑ Download PDF</a></p><hr><h3 id=differences-that-matter-auditing-models-for-capability-gap-discovery-and-rectificationhttpsarxivorgabs251216921v1><a href=https://arxiv.org/abs/2512.16921v1>Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</a><a hidden class=anchor aria-hidden=true href=#differences-that-matter-auditing-models-for-capability-gap-discovery-and-rectificationhttpsarxivorgabs251216921v1>#</a></h3><p><strong>Authors:</strong> Qihao Liu, Chengzhi Mao, Yaojie Liu, Alan Yuille, Wen-Sheng Chu
<strong>Venue:</strong> arXiv (2025)</p><p>Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16921v1">üìÑ Download PDF</a></p><hr><h3 id=discovering-gravitational-waveform-distortions-from-lensing-a-deep-dive-into-gw231123httpsarxivorgabs251216916v1><a href=https://arxiv.org/abs/2512.16916v1>Discovering gravitational waveform distortions from lensing: a deep dive into GW231123</a><a hidden class=anchor aria-hidden=true href=#discovering-gravitational-waveform-distortions-from-lensing-a-deep-dive-into-gw231123httpsarxivorgabs251216916v1>#</a></h3><p><strong>Authors:</strong> Juno C. L. Chan, Jose Mar√≠a Ezquiaga, Rico K. L. Lo, Joey Bowman, Lorena Maga√±a Zertuche, Luka Vujeva
<strong>Venue:</strong> arXiv (2025)</p><p>Gravitational waves (GWs) are unique messengers as they travel through the Universe without alteration except for gravitational lensing. Their long wavelengths make them susceptible to diffraction by cosmic structures, providing an unprecedented opportunity to map dark matter substructures. Identifying lensed events requires the analysis of thousands to millions of simulated events to reach high statistical significances. This is computationally prohibitive with standard GW parameter estimation methods. We build on top of state-of-the-art neural posterior algorithms to accelerate the lensed inference from CPU days to minutes with DINGO-lensing. We showcase its capabilities by reanalyzing GW231123, the most promising lensed candidate so far, and find that its statistical significance cannot exceed 4$œÉ$. We observe that 8% of GW231123-like nonlensed simulations favor lensing, which could be explained by the self-similarity of short-duration signals. Still, 58% of GW231123-like lensed simulations have larger support for lensing, showing that higher detection statistics are possible. Although GW231123 exposes the challenges of claiming the first GW lensing detection, our deep-learning methods have demonstrated to be powerful enough to enable the upcoming discovery of lensed GWs.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16916v1">üìÑ Download PDF</a></p><hr><h3 id=in-context-algebrahttpsarxivorgabs251216902v1><a href=https://arxiv.org/abs/2512.16902v1>In-Context Algebra</a><a hidden class=anchor aria-hidden=true href=#in-context-algebrahttpsarxivorgabs251216902v1>#</a></h3><p><strong>Authors:</strong> Eric Todd, Jannik Brinkmann, Rohit Gandikota, David Bau
<strong>Venue:</strong> arXiv (2025)</p><p>We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16902v1">üìÑ Download PDF</a></p><hr><h3 id=instant-expressive-gaussian-head-avatar-via-3d-aware-expression-distillationhttpsarxivorgabs251216893v1><a href=https://arxiv.org/abs/2512.16893v1>Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation</a><a hidden class=anchor aria-hidden=true href=#instant-expressive-gaussian-head-avatar-via-3d-aware-expression-distillationhttpsarxivorgabs251216893v1>#</a></h3><p><strong>Authors:</strong> Kaiwen Jiang, Xueting Li, Seonwook Park, Ravi Ramamoorthi, Shalini De Mello, Koki Nagano
<strong>Venue:</strong> arXiv (2025)</p><p>Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods &ndash; built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting &ndash; ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face&rsquo;s 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is <a href=https://research.nvidia.com/labs/amri/projects/instant4d>https://research.nvidia.com/labs/amri/projects/instant4d</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16893v1">üìÑ Download PDF</a></p><hr><h3 id=machine-learning-assisted-high-throughput-prediction-of-moir√©-materialshttpsarxivorgabs251216892v1><a href=https://arxiv.org/abs/2512.16892v1>Machine learning assisted high throughput prediction of moir√© materials</a><a hidden class=anchor aria-hidden=true href=#machine-learning-assisted-high-throughput-prediction-of-moir√©-materialshttpsarxivorgabs251216892v1>#</a></h3><p><strong>Authors:</strong> Daniel Kaplan, Alexander C. Tyner, Eva Y. Andrei, J. H. Pixley
<strong>Venue:</strong> arXiv (2025)</p><p>The world of 2D materials is rapidly expanding with new discoveries of stackable and twistable layered systems composed of lattices of different symmetries, orbital character, and structural motifs. Often, however, it is not clear a priori whether a pair of monolayers twisted at a small angle will exhibit correlated or interaction-driven phenomena. The computational cost to make accurate predictions of the single particle states is significant, as small twists require very large unit cells, easily encompassing 10,000 atoms, and therefore implementing a high throughput prediction has been out of reach. Here we show a path to overcome this challenge by introducing a machine learning (ML) based methodology that efficiently estimates the twisted interlayer tunneling at arbitrarily low twist angles through the local-configuration based approach that enables interpolating the local stacking for a range of twist angles using a random forest regression algorithm. We leverage the kernel polynomial method to compute the density of states (DOS) on large real space graphs by reconstructing a lattice model of the twisted bilayer with the ML fitted hoppings. For twisted bilayer graphene (TBG), we show the ability of the method to resolve the magic angle DOS at a substantial improvement in computational time. We use this new technique to scan through the database of stable 2D monolayers (MC2D) and reveal new twistable candidates across the five possible points groups in two-dimensions with a large DOS near the Fermi energy, with potentially exciting interacting physics to be probed in future experiments.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16892v1">üìÑ Download PDF</a></p><hr><h3 id=revival-dynamics-from-equilibrium-states-scars-from-chords-in-sykhttpsarxivorgabs251216836v1><a href=https://arxiv.org/abs/2512.16836v1>Revival Dynamics from Equilibrium States: Scars from Chords in SYK</a><a hidden class=anchor aria-hidden=true href=#revival-dynamics-from-equilibrium-states-scars-from-chords-in-sykhttpsarxivorgabs251216836v1>#</a></h3><p><strong>Authors:</strong> Debarghya Chakraborty, Dario Rosa
<strong>Venue:</strong> arXiv (2025)</p><p>We develop a novel framework to build quantum many-body scar states in bipartite systems characterized by perfect correlation between the Hamiltonians governing the two sides. By means of a Krylov construction, we build an interaction term which supports a tower of equally-spaced energy eigenstates. This gives rise to finite-time revivals whenever the system is initialized in a purification of a generic equilibrium state. The dynamics is universally characterized, and is largely independent of the specific details of the Hamiltonians defining the individual partitions. By considering the two-sided chord states of the double-scaled SYK model, we find an approximate realization of this framework. We analytically study the revival dynamics, finding rigid motion for wavepackets localized on the spectrum of a single SYK copy. These findings are tested numerically for systems of finite size, showing excellent agreement with the analytical predictions.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16836v1">üìÑ Download PDF</a></p><hr><h3 id=experimental-measurement-of-enhanced-group-delay-silicon-photonic-waveguides-indicative-of-the-frozen-mode-regime-around-the-stationary-inflection-pointhttpsarxivorgabs251216831v1><a href=https://arxiv.org/abs/2512.16831v1>Experimental Measurement of Enhanced Group Delay Silicon Photonic Waveguides Indicative of the Frozen Mode Regime Around the Stationary Inflection Point</a><a hidden class=anchor aria-hidden=true href=#experimental-measurement-of-enhanced-group-delay-silicon-photonic-waveguides-indicative-of-the-frozen-mode-regime-around-the-stationary-inflection-pointhttpsarxivorgabs251216831v1>#</a></h3><p><strong>Authors:</strong> Nathaniel Furman, Albert Herrero-Parareda, Anthony Rapp, Ilya Vitebskiy, Ricky Gibson, Bradley J. Thompson, Dean P. Brown, Robert Bedford, Filippo Capolino
<strong>Venue:</strong> arXiv (2025)</p><p>The dispersion engineering of periodic silicon photonic waveguides presents opportunities for significant group delay enhancement compared to uniform waveguides of comparable length. We describe the spectral response characteristics for measured devices and compare their properties to modeled data. These waveguides support the frozen mode regime (FMR) around near infrared wavelengths and are expected to show enhanced group delays around the FMR resonances. Measurements of fabricated devices provide evidence for enhanced delays and spectral properties associated with the FMR. We study how perturbations to the waveguide model impact agreement with measurements and its meaning for these devices operating in the FMR.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16831v1">üìÑ Download PDF</a></p><hr><h3 id=rayleigh-b√©nard-thermal-convection-in-emulsions-a-short-reviewhttpsarxivorgabs251216830v1><a href=https://arxiv.org/abs/2512.16830v1>Rayleigh-B√©nard thermal convection in emulsions: a short review</a><a hidden class=anchor aria-hidden=true href=#rayleigh-b√©nard-thermal-convection-in-emulsions-a-short-reviewhttpsarxivorgabs251216830v1>#</a></h3><p><strong>Authors:</strong> Francesca Pelusi, Andrea Scagliarini, Mauro Sbragaglia, Massimo Bernaschi, Roberto Benzi
<strong>Venue:</strong> arXiv (2025)</p><p>Thermally driven emulsions arise in a broad range of natural and industrial contexts, yet their fundamental physical understanding remains only partially established. Emulsions exhibit a complex, concentration-dependent rheology, ranging from Newtonian (dilute emulsions) to yield-stress (concentrated emulsions). In buoyancy-driven flows, the complex structure and rheology of the emulsion are strongly coupled to convective flows, giving rise to fascinating and non-trivial phenomena involving stability, transient dynamics, and morphological evolution of the system. We review recent progress on thermally driven emulsions in the celebrated Rayleigh-B√©nard configuration, offering new perspectives on the behaviour of soft materials in thermal convection.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16830v1">üìÑ Download PDF</a></p><hr><h3 id=consistent-excesses-in-the-lhc-electroweak-susy-searches-gut-based-singlinohiggsino-interpretation-in-the-nmssmhttpsarxivorgabs251216783v1><a href=https://arxiv.org/abs/2512.16783v1>Consistent Excesses in the LHC Electroweak SUSY Searches: GUT-based Singlino/Higgsino Interpretation in the NMSSM</a><a hidden class=anchor aria-hidden=true href=#consistent-excesses-in-the-lhc-electroweak-susy-searches-gut-based-singlinohiggsino-interpretation-in-the-nmssmhttpsarxivorgabs251216783v1>#</a></h3><p><strong>Authors:</strong> Emanuele Bagnaschi, Manimala Chakraborti, Sven Heinemeyer, Ipsita Saha
<strong>Venue:</strong> arXiv (2025)</p><p>The search for supersymmetric models remains one of the main items on the BSM search program at the LHC, with EW SUSY partners still allowed with masses as low as a few hundred GeV. Over the last years, searches for the &ldquo;golden channel&rdquo;, $pp \to \tildeœá^0_2 \tildeœá^{\pm}<em>1 \to \tildeœá^0_1 Z^{(<em>)} \tildeœá^0_1 W^{\pm (</em>)}$ show consistent excesses between ATLAS and CMS in the 2<del>soft-lepton and 3</del>soft-lepton plus missing-$E_T$ searches, assuming $m</em>{\tildeœá^0_2} \approx m_{\tildeœá^{\pm}<em>1} \gtrsim 200$ GeV and $Œîm</em>{21} := m_{\tildeœá^0_2} - m_{\tildeœá^0_1} \approx 20$ GeV. We interpret these excesses in the framework of the Next-to-Minimal Supersymmetric Standard Model. We assume a singlino dominated lightest neutralino as a Dark Matter (DM) candidate. The second and third lightest neutralinos are higgsino like, with the higgsino mixing parameter $Œº$ being smaller than the soft SUSY-breaking bino and wino masses, $M_1$ and $M_2$. We furthermore assume the approximate GUT relations $M_1 \sim M_2/2 \sim M_3/6$, with the implication for our scenario of a gluino mass $m_{\tilde{g}} \sim M_3 \gtrsim 3$ TeV. Scalar masses are assumed to heavy and do not play a role in our analysis. We find that this scenario is in agreement with all relevant experimental constraints, comprising the LHC searches for SUSY particles and additional Higgs bosons, the LHC Higgs-boson rate measurements, the DM direct detection limits and the upper limit on the DM relic density. We demonstrate that this scenario gives an excellent description of the observed excesses in the search for 2<del>and 3</del>soft-leptons plus \ETmiss, with $m_{\tildeœá^0_2} \sim m_{\tildeœá^0_3} \sim m_{\tildeœá^{\pm}<em>1}$ and $Œîm</em>{21} \sim 20$ GeV. This constitutes the first explanation of the soft-lepton excesses in a model with GUT relations among the soft SUSY-breaking parameters.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16783v1">üìÑ Download PDF</a></p><hr><h3 id=field-quantisations-in-schwarzschild-spacetime-theory-versus-low-energy-experimentshttpsarxivorgabs251216667v1><a href=https://arxiv.org/abs/2512.16667v1>Field Quantisations in Schwarzschild Spacetime: Theory versus Low-Energy Experiments</a><a hidden class=anchor aria-hidden=true href=#field-quantisations-in-schwarzschild-spacetime-theory-versus-low-energy-experimentshttpsarxivorgabs251216667v1>#</a></h3><p><strong>Authors:</strong> Viacheslav A. Emelyanov
<strong>Venue:</strong> arXiv (2025)</p><p>Non-relativistic quantum particles in the Earth&rsquo;s gravitational field are successfully described by the Schr√∂dinger equation with Newton&rsquo;s gravitational potential. Particularly, quantum mechanics is in agreement with such experiments as free fall and quantum interference induced by gravity. However, quantum mechanics is a low-energy approximation to quantum field theory. The latter is successful by the description of high-energy experiments. Gravity is embedded in quantum field theory through the general-covariance principle. This framework is known in the literature as quantum field theory in curved spacetime, where the concept of a quantum particle is, though, ambiguous. In this article, we study in this framework how a Hawking particle moves in the far-horizon region of Schwarzschild spacetime by computing its propagator. We find this propagator differs from that which follows from the path-integral formalism &ndash; the formalism which adequately describes both free fall and quantum interference induced by gravity.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16667v1">üìÑ Download PDF</a></p><hr><h3 id=self-affine-scaling-of-earths-islandshttpsarxivorgabs251216659v1><a href=https://arxiv.org/abs/2512.16659v1>Self-Affine Scaling of Earth&rsquo;s Islands</a><a hidden class=anchor aria-hidden=true href=#self-affine-scaling-of-earths-islandshttpsarxivorgabs251216659v1>#</a></h3><p><strong>Authors:</strong> Matthew Oline, Jeremy Hoskins, David Seekell, Mary Silber, B. B. Cael
<strong>Venue:</strong> arXiv (2025)</p><p>Earth&rsquo;s relief is approximately self-affine, meaning a zoom-in on a small region looks statistically similar to a large region upon a suitable rescaling. Fractional Brownian surfaces give an idealized self-affine model of Earth&rsquo;s relief with one parameter, the Hurst exponent $H$, characterizing the roughness of the surface. To quantitatively assess agreement with Earth elevation data, we compile a large dataset of topographic profiles of islands (N=131,063 with the range of areas covering 8+ orders of magnitude) and obtain four estimates for the Hurst exponent of Earth&rsquo;s surface by fitting four statistical laws from the theory of self-affine surfaces concerning islands: (i) distribution of areas, (ii) volume-area relationship, (iii) perimeter-area relationship, and (iv) maximum height-area relationship. The estimated Hurst exponents differ greatly, indicating different fractal scaling behavior for different geometric features, but are sorted in order of increasing expected influence of erosion at the shorelines.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16659v1">üìÑ Download PDF</a></p><hr><h3 id=stereopilot-learning-unified-and-efficient-stereo-conversion-via-generative-priorshttpsarxivorgabs251216915v1><a href=https://arxiv.org/abs/2512.16915v1>StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors</a><a hidden class=anchor aria-hidden=true href=#stereopilot-learning-unified-and-efficient-stereo-conversion-via-generative-priorshttpsarxivorgabs251216915v1>#</a></h3><p><strong>Authors:</strong> Guibao Shen, Yihua Du, Wenhang Ge, Jing He, Chirui Chang, Donghao Zhou, Zhen Yang, Luozhou Wang, Xin Tao, Ying-Cong Chen
<strong>Venue:</strong> arXiv (2025)</p><p>The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint&rsquo;&rsquo; (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: <a href=https://hit-perfect.github.io/StereoPilot/>https://hit-perfect.github.io/StereoPilot/</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16915v1">üìÑ Download PDF</a></p><hr><h3 id=depth-any-panoramas-a-foundation-model-for-panoramic-depth-estimationhttpsarxivorgabs251216913v1><a href=https://arxiv.org/abs/2512.16913v1>Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation</a><a hidden class=anchor aria-hidden=true href=#depth-any-panoramas-a-foundation-model-for-panoramic-depth-estimationhttpsarxivorgabs251216913v1>#</a></h3><p><strong>Authors:</strong> Xin Lin, Meixi Song, Dizhe Zhang, Wenxuan Lu, Haodong Li, Bo Du, Ming-Hsuan Yang, Truong Nguyen, Lu Qi
<strong>Venue:</strong> arXiv (2025)</p><p>In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP_website/}</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16913v1">üìÑ Download PDF</a></p><hr><h3 id=opening-the-house-datasets-for-mixed-doubles-curlinghttpsarxivorgabs251216574v1><a href=https://arxiv.org/abs/2512.16574v1>Opening the House: Datasets for Mixed Doubles Curling</a><a hidden class=anchor aria-hidden=true href=#opening-the-house-datasets-for-mixed-doubles-curlinghttpsarxivorgabs251216574v1>#</a></h3><p><strong>Authors:</strong> Robyn Ritchie, Alexandre Leblanc, Thomas Loughin
<strong>Venue:</strong> arXiv (2025)</p><p>We introduce the most comprehensive publicly available datasets for mixed doubles curling, constructed from eleven top-level tournaments from the CurlIT (<a href=https://curlit.com/results>https://curlit.com/results</a>) Results Booklets spanning 53 countries, 1,112 games, and nearly 70,000 recorded shots. While curling analytics has grown in recent years, mixed doubles remains under-served due to limited access to data. Using a combined text-scraping and image-processing pipeline, we extract and standardize detailed game- and shot-level information, including player statistics, hammer possession, Power Play usage, stone coordinates, and post-shot scoring states. We describe the data engineering workflow, highlight challenges in parsing historical records, and derive additional contextual features that enable rigorous strategic analysis. Using these datasets, we present initial insights into shot selection and success rates, scoring distributions, and team efficiencies, illustrating key differences between mixed doubles and traditional 4-player curling. We highlight various ways to analyze this type of data including from a shot-, end-, game- or team-level to display its versatilely. The resulting resources provide a foundation for advanced performance modeling, strategic evaluation, and future research in mixed doubles curling analytics, supporting broader analytical engagement with this rapidly growing discipline.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16574v1">üìÑ Download PDF</a></p><hr><h3 id=brepllm-native-boundary-representation-understanding-with-large-language-modelshttpsarxivorgabs251216413v1><a href=https://arxiv.org/abs/2512.16413v1>BrepLLM: Native Boundary Representation Understanding with Large Language Models</a><a hidden class=anchor aria-hidden=true href=#brepllm-native-boundary-representation-understanding-with-large-language-modelshttpsarxivorgabs251216413v1>#</a></h3><p><strong>Authors:</strong> Liyuan Deng, Hao Guo, Yunpeng Bai, Yongkang Dai, Huaxi Huang, Yilei Shi
<strong>Venue:</strong> arXiv (2025)</p><p>Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16413v1">üìÑ Download PDF</a></p><hr><h3 id=occupational-tasks-automation-and-economic-growth-a-modeling-and-simulation-approachhttpsarxivorgabs251216261v1><a href=https://arxiv.org/abs/2512.16261v1>Occupational Tasks, Automation, and Economic Growth: A Modeling and Simulation Approach</a><a hidden class=anchor aria-hidden=true href=#occupational-tasks-automation-and-economic-growth-a-modeling-and-simulation-approachhttpsarxivorgabs251216261v1>#</a></h3><p><strong>Authors:</strong> Georgios A. Tritsaris
<strong>Venue:</strong> arXiv (2025)</p><p>The Fourth Industrial Revolution commonly refers to the accelerating technological transformation that has been taking place in the 21st century. Economic growth theories which treat the accumulation of knowledge and its effect on production endogenously remain relevant, yet they have been evolving to explain how the current wave of advancements in automation and artificial intelligence (AI) technology will affect productivity and different occupations. The work contributes to current economic discourse by developing an analytical task-based framework that endogenously integrates knowledge accumulation with frictions that describe technological lock-in and the burden of knowledge generation and validation. The interaction between production (or automation) and growth (or knowledge accumulation) is also described explicitly. To study how automation and AI shape economic outcomes, I rely on high-throughput calculations of the developed model. The effect of the model&rsquo;s structural parameters on key variables such as the production output, wages, and labor shares of output is quantified, and possible intervention strategies are briefly discussed. An important result is that wages and labor shares are not directly linked, instead they can be influenced independently through distinct policy levers.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16261v1">üìÑ Download PDF</a></p><hr><h3 id=privacy-discourse-and-emotional-dynamics-in-mental-health-information-interaction-on-reddithttpsarxivorgabs251215945v1><a href=https://arxiv.org/abs/2512.15945v1>Privacy Discourse and Emotional Dynamics in Mental Health Information Interaction on Reddit</a><a hidden class=anchor aria-hidden=true href=#privacy-discourse-and-emotional-dynamics-in-mental-health-information-interaction-on-reddithttpsarxivorgabs251215945v1>#</a></h3><p><strong>Authors:</strong> Jai Kruthunz Naveen Kumar, Aishwarya Umeshkumar Surani, Harkirat Singh, Sanchari Das
<strong>Venue:</strong> arXiv (2025)</p><p>Reddit is a major venue for mental-health information interaction and peer support, where privacy concerns increasingly surface in user discourse. Thus, we analyze privacy-related discussions across 14 mental-health and regulatory subreddits, comprising 10,119 posts and 65,385 comments collected with a custom web scraper. Using lexicon-based sentiment analysis, we quantify emotional alignment between communities via cosine similarity of sentiment distributions, observing high similarity for Bipolar and ADHD (0.877), Anxiety and Depression (0.849), and MentalHealthSupport and MentalIllness (0.989) subreddits. We also construct keyword dictionaries to tag privacy-related themes (e.g., HIPAA, GDPR) and perform temporal analysis from 2020 to 2025, finding a 50% increase in privacy discourse with intermittent regulatory spikes. A chi-square test of independence across subreddit domains indicates significant distributional differences. The results characterize how privacy-oriented discussion co-varies with user sentiment in online mental-health communities.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.15945v1">üìÑ Download PDF</a></p><hr><h3 id=the-moralization-corpus-frame-based-annotation-and-analysis-of-moralizing-speech-acts-across-diverse-text-genreshttpsarxivorgabs251215248v1><a href=https://arxiv.org/abs/2512.15248v1>The Moralization Corpus: Frame-Based Annotation and Analysis of Moralizing Speech Acts across Diverse Text Genres</a><a hidden class=anchor aria-hidden=true href=#the-moralization-corpus-frame-based-annotation-and-analysis-of-moralizing-speech-acts-across-diverse-text-genreshttpsarxivorgabs251215248v1>#</a></h3><p><strong>Authors:</strong> Maria Becker, Mirko Sommer, Lars Tapken, Yi Wan Teh, Bruno Brocai
<strong>Venue:</strong> arXiv (2025)</p><p>Moralizations - arguments that invoke moral values to justify demands or positions - are a yet underexplored form of persuasive communication. We present the Moralization Corpus, a novel multi-genre dataset designed to analyze how moral values are strategically used in argumentative discourse. Moralizations are pragmatically complex and often implicit, posing significant challenges for both human annotators and NLP systems. We develop a frame-based annotation scheme that captures the constitutive elements of moralizations - moral values, demands, and discourse protagonists - and apply it to a diverse set of German texts, including political debates, news articles, and online discussions. The corpus enables fine-grained analysis of moralizing language across communicative formats and domains. We further evaluate several large language models (LLMs) under varied prompting conditions for the task of moralization detection and moralization component extraction and compare it to human annotations in order to investigate the challenges of automatic and manual analysis of moralizations. Results show that detailed prompt instructions has a greater effect than few-shot or explanation-based prompting, and that moralization remains a highly subjective and context-sensitive task. We release all data, annotation guidelines, and code to foster future interdisciplinary research on moral discourse and moral reasoning in NLP.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.15248v1">üìÑ Download PDF</a></p><hr><h3 id=probing-scalar-and-pseudoscalar-new-physics-using-rare-kaon-decayshttpsarxivorgabs251216903v1><a href=https://arxiv.org/abs/2512.16903v1>Probing scalar and pseudoscalar new physics using rare kaon decays</a><a hidden class=anchor aria-hidden=true href=#probing-scalar-and-pseudoscalar-new-physics-using-rare-kaon-decayshttpsarxivorgabs251216903v1>#</a></h3><p><strong>Authors:</strong> G. D&rsquo;Ambrosio, A. M. Iyer, F. Mahmoudi, S. Neshatpour
<strong>Venue:</strong> arXiv (2025)</p><p>Rare kaon decays provide sensitive tests of new physics. In this work, we focus on scalar and pseudoscalar operators, analysing the $K\to œÄ\ell^+\ell^-$ and $K\to \ell^+\ell^-$ decays. We highlight the complementary role of different modes: $K^+\toœÄ^+\ell^+\ell^-$, in particular the forward-backward asymmetry in the muon channel as a clean probe of scalar effects, the stringent constraints from $K_L\to Œº^+Œº^-$, and the discovery potential of future measurements of $K_S\to Œº^+Œº^-$ and $K_L\to œÄ^0 \ell^+\ell^-$. The interplay between charged and neutral modes underscores the complementarity of NA62, the LHCb upgrade, and KOTO-II.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16903v1">üìÑ Download PDF</a></p><hr><h3 id=many-body-contextuality-and-self-testing-quantum-matter-via-nonlocal-gameshttpsarxivorgabs251216886v1><a href=https://arxiv.org/abs/2512.16886v1>Many-body contextuality and self-testing quantum matter via nonlocal games</a><a hidden class=anchor aria-hidden=true href=#many-body-contextuality-and-self-testing-quantum-matter-via-nonlocal-gameshttpsarxivorgabs251216886v1>#</a></h3><p><strong>Authors:</strong> Oliver Hart, David T. Stephen, Evan Wickenden, Rahul Nandkishore
<strong>Venue:</strong> arXiv (2025)</p><p>Contextuality is arguably the fundamental property that makes quantum mechanics different from classical physics. It is responsible for quantum computational speedups in both magic-state-injection-based and measurement-based models of computation, and can be directly probed in a many-body setting by multiplayer nonlocal quantum games. Here, we discuss a family of games that can be won with certainty when performing single-site Pauli measurements on a state that is a codeword of a Calderbank-Shor-Steane (CSS) error-correcting quantum code. We show that these games require deterministic computation of a code-dependent Boolean function, and that the classical probability of success is upper bounded by a generalized notion of nonlinearity/nonquadraticity. This success probability quantifies the state&rsquo;s contextuality, and is computed via the function&rsquo;s (generalized) Walsh-Hadamard spectrum. To calculate this, we introduce an efficient, many-body-physics-inspired method that involves identifying the symmetries of an auxiliary hypergraph state. We compute the classical probability of success for several paradigmatic CSS codes and relate it to both classical statistical mechanics models and to strange correlators of symmetry-protected topological states. We also consider CSS submeasurement games, which can only be won with certainty by sharing the appropriate codeword up to local isometries. These games therefore enable self-testing, which we illustrate explicitly for the 2D toric code. We also discuss how submeasurement games enable an extensive notion of contextuality in many-body states.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16886v1">üìÑ Download PDF</a></p><hr><h3 id=wiedemann-franz-violation-and-thermal-hall-effect-in-kagome-metal-tbcr6ge6httpsarxivorgabs251216868v1><a href=https://arxiv.org/abs/2512.16868v1>Wiedemann-Franz violation and thermal Hall effect in kagome metal TbCr6Ge6</a><a hidden class=anchor aria-hidden=true href=#wiedemann-franz-violation-and-thermal-hall-effect-in-kagome-metal-tbcr6ge6httpsarxivorgabs251216868v1>#</a></h3><p><strong>Authors:</strong> Jhinkyu Choi, Mohan B. Neupane, L. H. Vilela-Le√£o, Bishnu P. Belbase, Arjun Unnikrishnan, Syeda Neha Zaidi, Jukka I. V√§yrynen, Arnab Banerjee
<strong>Venue:</strong> arXiv (2025)</p><p>The thermal Hall effect has emerged as a powerful probe of exotic excitations in correlated quantum materials, providing access to charge-neutral heat carriers that remain invisible to electrical transport. To directly examine how heat and charge respond in relation within a kagome metal, we investigate the ferrimagnetic rare-earth 1-6-6 compound TbCr6Ge6 using the Wiedemann-Franz (WF) framework. We observe a dramatic breakdown of the WF law across the ferrimagnetic transition, where both longitudinal and transverse Lorenz ratios, L_{xx,xy} = Œ∫_{xx,xy} / (T œÉ_{xx,xy}), deviate strongly from the Sommerfeld value L_0. After a partial recovery toward L_0 near 5-7 K, the Lorenz ratios are sharply suppressed well below L_0 despite a metallic charge response. We further find a pronounced low-temperature suppression of both L_{xx} and L_{xy} and a sign-changing transverse Lorenz ratio, indicating a clear decoupling between heat and charge transport and signaling substantial contributions from charge-neutral excitations whose Berry-curvature-driven transverse response evolves with temperature and magnetic field. TbCr6Ge6 thus provides a tunable metallic platform in which exchange-driven ferrimagnetism governs both longitudinal and transverse thermal responses, enabling controlled departures from Wiedemann-Franz behavior over an experimentally accessible temperature and field range.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16868v1">üìÑ Download PDF</a></p><hr><h3 id=nonstabilizerness-in-stark-many-body-localizationhttpsarxivorgabs251216859v1><a href=https://arxiv.org/abs/2512.16859v1>Nonstabilizerness in Stark many-body localization</a><a hidden class=anchor aria-hidden=true href=#nonstabilizerness-in-stark-many-body-localizationhttpsarxivorgabs251216859v1>#</a></h3><p><strong>Authors:</strong> Han-Ze Li, Yi-Rui Zhang, Yu-Jun Zhao, Xuyang Huang, Jian-Xin Zhong
<strong>Venue:</strong> arXiv (2025)</p><p>Quantum many-body disorder-free localization can suppress transport while still allowing the buildup of computationally costly non-Clifford resources. In a transverse-field Ising chain realizing disorder-free Stark many-body localization, we show that the stabilizer R√©nyi entropy remains nonzero and grows slowly to a finite plateau deep in the strong Stark-field regime, with strong initial-state selectivity. As the Stark field strength increases, long-time magic and entanglement consistently signal a crossover from ergodic to constrained localized dynamics. These results establish nonstabilizerness (``magic&rsquo;&rsquo;) as a practical complexity probe for disorder-free ergodicity breaking and constrained localization, with direct relevance to benchmarking and designing near-term quantum simulators, and fill a gap in the understanding of nonstabilizerness in disorder-free many-body localization.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16859v1">üìÑ Download PDF</a></p><hr><h3 id=toggle-temporal-logic-guided-large-language-model-compression-for-edgehttpsarxivorgabs251216855v1><a href=https://arxiv.org/abs/2512.16855v1>TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge</a><a hidden class=anchor aria-hidden=true href=#toggle-temporal-logic-guided-large-language-model-compression-for-edgehttpsarxivorgabs251216855v1>#</a></h3><p><strong>Authors:</strong> Khurram Khalil, Khaza Anuarul Hoque
<strong>Venue:</strong> arXiv (2025)</p><p>Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16855v1">üìÑ Download PDF</a></p><hr><h3 id=fighting-non-locality-with-non-locality-microcausality-and-boundary-conditions-in-qedhttpsarxivorgabs251216898v1><a href=https://arxiv.org/abs/2512.16898v1>Fighting non-locality with non-locality: microcausality and boundary conditions in QED</a><a hidden class=anchor aria-hidden=true href=#fighting-non-locality-with-non-locality-microcausality-and-boundary-conditions-in-qedhttpsarxivorgabs251216898v1>#</a></h3><p><strong>Authors:</strong> Philipp A. Hoehn, Josh Kirklin
<strong>Venue:</strong> arXiv (2025)</p><p>In gauge theories, globally charged observables necessarily depend non-locally on the kinematical fields, with this dependence extending to the asymptotic boundary of spacetime. Despite this, we show that a subset of such observables can be consistently regarded as local to the bulk, in a manner that respects microcausality and leaves locality properties of uncharged observables untouched. A sufficient condition for this is to impose kinematically non-local boundary conditions on the large gauge sector of the theory, and to invoke a relational notion of localisation for observables. This reveals a relatively underappreciated link between boundary conditions, and different notions of microcausality and locality. We develop this point through a detailed case study in scalar QED, describing non-local boundary conditions that allow a large family of observables on a codimension-1 bulk surface to be viewed as local to that surface, despite being dressed by asymptotic Wilson lines. We show that this property continues to hold within a perturbative quantisation of the theory, and we argue that this leads to a consistent local net of algebras that includes these charged observables in bulk algebras. We explain how this setup may be understood in terms of a preferred dynamical reference frame for small gauge transformations appearing in the boundary conditions. Many features of the theory (such as microcausality, the vacuum state, and the net of algebras of observables) depend on the choice of this frame, and we briefly discuss some repercussions of this for algebraic formulations of QFT. While our analysis is performed in QED, we expect our results to carry over qualitatively to more complicated theories including gravity.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16898v1">üìÑ Download PDF</a></p><hr><h3 id=growing-self-similar-markov-treeshttpsarxivorgabs251216894v1><a href=https://arxiv.org/abs/2512.16894v1>Growing Self-Similar Markov Trees</a><a hidden class=anchor aria-hidden=true href=#growing-self-similar-markov-treeshttpsarxivorgabs251216894v1>#</a></h3><p><strong>Authors:</strong> Nicolas Curien, William Fleurat, Adrianus Twigt
<strong>Venue:</strong> arXiv (2025)</p><p>Can we obtain a Brownian CRT of mass $1/2$ from a CRT of mass $1$ by cutting certain branches?
In this paper, we will answer that question in the much more general setting of self-similar Markov trees. Self-similar Markov trees (ssMt) are random decorated trees that encode the genealogy of a system of particles carrying positive labels, and where particles undergo splitting and growth depending on their labels in a self-similar fashion. Introduced and developed in the recent monograph (Bertoin-Curien-Riera, 2024), they provide a broad generalization of Brownian and stable continuum random trees and arise naturally in various models of random geometry such as the Brownian sphere/disk. The law of a ssMt is characterized by its quadruplet $(\mathrm{a}, œÉ^2, \boldsymbolŒõ; Œ±)$, which specifies the features of the underlying growth-fragmentation mechanism, together with the initial decoration $x>0$. In this work, we focus on special cases of ssMt in which the trees started from different initial values $x>0$ can be coupled into a continuous, increasing family of nested subtrees. In the case of the Brownian and stable continuum random trees, this yields surprisingly simple novel dynamics corresponding to the scaling limit of the leaf-growth algorithms of Luczak-Winkler and Caraceni-Stauffer.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16894v1">üìÑ Download PDF</a></p><hr><h3 id=an-exciton-interacting-with-the-phonons-of-an-electronic-wigner-crystalhttpsarxivorgabs251216888v1><a href=https://arxiv.org/abs/2512.16888v1>An exciton interacting with the phonons of an electronic Wigner crystal</a><a hidden class=anchor aria-hidden=true href=#an-exciton-interacting-with-the-phonons-of-an-electronic-wigner-crystalhttpsarxivorgabs251216888v1>#</a></h3><p><strong>Authors:</strong> Jens Havgaard Nyhegn, Esben Rohan Christensen, Georg M. Bruun
<strong>Venue:</strong> arXiv (2025)</p><p>With the advent of atomically thin and tunable van der Waals materials, a two-dimensional electronic Wigner crystal has recently been observed. The smoking gun signal was the appearance of an umklapp branch in optical exciton spectroscopy coming from the periodic potential generated by the Wigner crystal assumed to be static. Vibrations of the Wigner crystal however leads to a gapless phonon spectrum, which may affect the exciton spectrum. To explore this, we develop a field theoretical description of an exciton interacting with electrons forming a Wigner crystal including the coupling to the phonons. We show that importance of the exciton-phonon coupling scales with the exciton-electron interaction strength relative to the typical phonon energy squared. The motion of the exciton leads to two kinds of scattering processes, where the exciton emits a phonon either staying within the same Bloch band (intraband scattering) or changing its band (interband scattering). Using a non-perturbative self-consistent Born approximation, we demonstrate that these scattering processes lead to the formation of quasiparticles (polarons) consisting of the exciton in Bloch states dressed by Wigner crystal phonons. The energy shift and damping of these polarons depend on the electron density in a non-trivial way since it affects both the exciton-phonon interaction strength, as well as the phonon and exciton spectra. In particular, the damping is strongly affected by whether the polaron energy is inside the gapless phonon scattering continuum or not. Using these results, we finally analyse their effects on the observed spectral properties of the exciton.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16888v1">üìÑ Download PDF</a></p><hr><h3 id=electric-field-diagnostics-in-a-continuous-rf-plasma-using-rydberg-eithttpsarxivorgabs251216867v1><a href=https://arxiv.org/abs/2512.16867v1>Electric field diagnostics in a continuous rf plasma using Rydberg-EIT</a><a hidden class=anchor aria-hidden=true href=#electric-field-diagnostics-in-a-continuous-rf-plasma-using-rydberg-eithttpsarxivorgabs251216867v1>#</a></h3><p><strong>Authors:</strong> Bineet Dash, Xinyan Xiang, Dingkun Feng, Eric Paradis, Georg Raithel
<strong>Venue:</strong> arXiv (2025)</p><p>We present a non-invasive spectroscopic technique to measure electric fields in plasma, leveraging large polarizabilities and Stark shifts of Rydberg atoms. Rydberg Stark shifts are measured with high precision using narrow-linewidth lasers via Electromagnetically Induced Transparency (EIT) of rubidium vapor seeded into a continuous, inductively coupled radio-frequency (rf) plasma in a few mTorr of argon gas. Without plasma, the Rydberg-EIT spectra exhibit rf modulation sidebands caused by electric- and magnetic-dipole transitions in the rf drive coil. With the plasma present, the rf modulation sidebands vanish due to screening of the rf drive field from the plasma interior. The lineshapes of the EIT spectra in the plasma reflect the plasma&rsquo;s Holtsmark microfield distribution, allowing us to determine plasma density and collisional line broadening over a range of pressures and rf drive powers. The work is expected to have applications in non-invasive spatio-temporal electric-field diagnostics of low-pressure plasma, plasma sheaths, process plasma and dusty plasma.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16867v1">üìÑ Download PDF</a></p><hr><h3 id=grammar-forced-translation-of-natural-language-to-temporal-logic-using-llmshttpsarxivorgabs251216814v1><a href=https://arxiv.org/abs/2512.16814v1>Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs</a><a hidden class=anchor aria-hidden=true href=#grammar-forced-translation-of-natural-language-to-temporal-logic-using-llmshttpsarxivorgabs251216814v1>#</a></h3><p><strong>Authors:</strong> William English, Dominic Simon, Sumit Kumar Jha, Rickard Ewetz
<strong>Venue:</strong> arXiv (2025)</p><p>Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16814v1">üìÑ Download PDF</a></p><hr><h3 id=scope-simple-coil-optimization-for-plasma-and-engineeringhttpsarxivorgabs251216546v1><a href=https://arxiv.org/abs/2512.16546v1>SCOPE: Simple Coil Optimization for Plasma and Engineering</a><a hidden class=anchor aria-hidden=true href=#scope-simple-coil-optimization-for-plasma-and-engineeringhttpsarxivorgabs251216546v1>#</a></h3><p><strong>Authors:</strong> Nathan Welch, Chris Marsden
<strong>Venue:</strong> arXiv (2025)</p><p>Designing superconducting coils for a tokamak fusion device is a highly coupled, non-linear design problem. The coils have many disparate engineering requirements from structural to power electronics, as well strict limits placed on the system by the high temperature superconducting (HTS) cables. Simultaneously, the coils must be able to contain multiple plasma scenarios from inception, through ramp up, to flat top, and ramp down, all whilst applying a large, controlled, inductive voltage to drive current. In addition, we wish to optimize divertor separatrices to increase the likelihood of designing a suitable divertor strikepoint. Lastly, the physical limits of the entire tokamak must be taken into account and space reserved for support structures, access for maintenance schemes, and installation limits. The method outlined here uses a combined simulated annealing method to find optimal coil sizes and positions with a constrained quadratic or quartic optimization for the coil currents. The method is designed to optimize coils for multiple scenarios simultaneously, including ramp-ups, to avoid over optimization of a single design point. A key enabler is the efficient implementation that allows millions of evaluations to be performed in a few hours with modest computational power. This optimization method is part of a larger, iterative workflow which enables further, detailed design work to feedback on the optimization.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16546v1">üìÑ Download PDF</a></p><hr><h3 id=smile-on-the-face-sadness-in-the-eyes-bridging-the-emotion-gap-with-a-multimodal-dataset-of-eye-and-facial-behaviorshttpsarxivorgabs251216485v1><a href=https://arxiv.org/abs/2512.16485v1>Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors</a><a hidden class=anchor aria-hidden=true href=#smile-on-the-face-sadness-in-the-eyes-bridging-the-emotion-gap-with-a-multimodal-dataset-of-eye-and-facial-behaviorshttpsarxivorgabs251216485v1>#</a></h3><p><strong>Authors:</strong> Kejun Liu, Yuanyuan Liu, Lin Wei, Chang Tang, Yibing Zhan, Zijing Chen, Zhe Chen
<strong>Venue:</strong> arXiv (2025)</p><p>Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at <a href=https://github.com/kejun1/EMER>https://github.com/kejun1/EMER</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16485v1">üìÑ Download PDF</a></p><hr><h3 id=black-start-power-capacity-sizing-and-control-strategy-for-an-islanded-dfig-wind-to-hydrogen-systemhttpsarxivorgabs251216263v1><a href=https://arxiv.org/abs/2512.16263v1>Black-Start Power Capacity Sizing and Control Strategy for an Islanded DFIG Wind-to-Hydrogen System</a><a hidden class=anchor aria-hidden=true href=#black-start-power-capacity-sizing-and-control-strategy-for-an-islanded-dfig-wind-to-hydrogen-systemhttpsarxivorgabs251216263v1>#</a></h3><p><strong>Authors:</strong> Bosen Yang, Kang Ma, Jin Lin, Yonghua Song
<strong>Venue:</strong> arXiv (2025)</p><p>This paper proposes a black-start method for an off-grid wind-to-hydrogen (W2H) system comprising a wind farm based on Doubly-Fed Induction Generators (DFIGs), proton exchange membrane fuel cells (PEMFCs) serving as the black-start power source, and a hydrogen production industry. The PEMFC is installed within the hydrogen industry to facilitate direct access to hydrogen fuel. Based on the microgrid topology and black-start scheme, this study innovatively sizes the rated capacity of the PEMFC through power flow analysis. The capacity must be sufficient to charge passive components such as transmission lines and transformers, provide rotor excitation, and supply wind turbine (WT) and electrolyzer (ELZ) auxiliaries during startup. The proposed system integrates wind-hydrogen coordinated control (WHCC) and hydrogen-storage coordinated control (HSCC). Under maximum power point tracking (MPPT) of the WTs, the ELZ follows power fluctuations to absorb wind output, ensuring stable voltage and frequency. Fixed-frequency control applied to either the DFIG or PEMFC converters enables DFIGs to retain conventional grid-following (GFL) operation, reducing converter development costs. For both control modes, this paper establishes the black-start sequence and formulates a comprehensive coordinated control strategy for the entire system. The entire control system is validated through simulations in MATLAB/Simulink. Results confirm that the calculated PEMFC capacity supports reliable black-start, while the black-start control strategy ensures smooth system self-startup. Furthermore, the coordinated control strategy maintains stable frequency and voltage under fluctuating wind power, demonstrating the practicality and robustness of the proposed approach.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16263v1">üìÑ Download PDF</a></p><hr><h3 id=grhayl-a-modern-infrastructure-agnostic-extensible-library-for-grmhd-simulationshttpsarxivorgabs251215846v1><a href=https://arxiv.org/abs/2512.15846v1>GRHayL: a modern, infrastructure-agnostic, extensible library for GRMHD simulations</a><a hidden class=anchor aria-hidden=true href=#grhayl-a-modern-infrastructure-agnostic-extensible-library-for-grmhd-simulationshttpsarxivorgabs251215846v1>#</a></h3><p><strong>Authors:</strong> Samuel Cupp, Leonardo R. Werneck, Terrence Pierre Jacques, Samuel Tootle, Zachariah B. Etienne
<strong>Venue:</strong> arXiv (2025)</p><p>Interpreting multi-messenger signals from neutron stars and black holes requires reliable general-relativistic magnetohydrodynamics (GRMHD) simulations across rapidly evolving high-performance-computing platforms, yet key algorithms are routinely rewritten within infrastructure-specific numerical-relativity codes, hindering verification and reuse. We present the General Relativistic Hydrodynamics Library (GRHayL), a modular, infrastructure-agnostic GR(M)HD library providing conservative-to-primitive recovery, reconstruction, flux/source and induction operators, equations of state, and neutrino leakage through an intuitive interface. GRHayL refactors and extends the mature IllinoisGRMHD code into reusable pointwise and stencil-wise kernels, enabling rapid development and cross-code validation in diverse frameworks, while easing adoption of new microphysics and future accelerators. We implement the same kernels in the Einstein Toolkit (Carpet and CarpetX) and BlackHoles@Home, demonstrating portability with minimal duplication. Validation combines continuous-integration unit tests with cross-infrastructure comparisons of analytic GRMHD Riemann problems, dynamical Tolman-Oppenheimer-Volkoff evolutions, and binary neutron-star mergers, showing comparable or improved behavior over legacy IllinoisGRMHD and established Einstein Toolkit codes.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.15846v1">üìÑ Download PDF</a></p><hr><h2 id=-psycholinguistics>üîç psycholinguistics<a hidden class=anchor aria-hidden=true href=#-psycholinguistics>#</a></h2><h3 id=solving-the-dirac-equation-on-a-gpu-for-strong-field-processes-in-multidimensional-background-fieldshttpsarxivorgabs251216889v1><a href=https://arxiv.org/abs/2512.16889v1>Solving the Dirac equation on a GPU for strong-field processes in multidimensional background fields</a><a hidden class=anchor aria-hidden=true href=#solving-the-dirac-equation-on-a-gpu-for-strong-field-processes-in-multidimensional-background-fieldshttpsarxivorgabs251216889v1>#</a></h3><p><strong>Authors:</strong> Greger Torgrimsson
<strong>Venue:</strong> arXiv (2025)</p><p>In this paper, we show how to solve the Dirac equation, $(iŒ≥^Œº[\partial_Œº+ieA_Œº(t,{\bf x})]-m)œà=0$, on a GPU. This is orders of magnitude faster than solving it on CPU and allows us to consider background fields, $A_Œº(t,{\bf x})$, that depend on $2+1$ or even $3+1$ coordinates. Our approach is conveniently implemented using the computational library JAX. We show how to obtain the probabilities of Schwinger and nonlinear Breit-Wheeler pair production from these solutions using a scattered-wave-function approach and compare the results with the worldline-instanton approximations.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16889v1">üìÑ Download PDF</a></p><hr><h3 id=askap-discovery-of-a-30-kpc-bipolar-outflow-from-the-edge-on-disk-of-the-nearby-spiral-galaxy-eso-130-g012httpsarxivorgabs251215991v1><a href=https://arxiv.org/abs/2512.15991v1>ASKAP discovery of a 30 kpc bipolar outflow from the edge-on disk of the nearby spiral galaxy ESO 130-G012</a><a hidden class=anchor aria-hidden=true href=#askap-discovery-of-a-30-kpc-bipolar-outflow-from-the-edge-on-disk-of-the-nearby-spiral-galaxy-eso-130-g012httpsarxivorgabs251215991v1>#</a></h3><p><strong>Authors:</strong> Baerbel S. Koribalski, Roland M. Crocker, Ildar Khabibullin, Anna Ivleva, Klaus Dolag, Umberto Maio, Ralf-Juergen Dettmar, Jacco Th. van Loon, Stanislav Shabala
<strong>Venue:</strong> arXiv (2025)</p><p>We present the discovery of a large-scale, limb-brightened outflow, extending at least 30 kpc above and below the star-forming disk of the edge-on galaxy ESO 130-G012 (D = 16.9 Mpc). Partially obscured by Galactic foreground stars and dust, this optically unremarkable, low-mass galaxy reveals one of the largest known hourglass-shaped outflows from the full extent of its bright stellar disk. The outflow was discovered in 944 MHz radio continuum images from the Australian Square Kilometre Array Pathfinder (ASKAP) obtained as part of the &ldquo;Evolutionary Map of the Universe&rdquo; (EMU) project. Its height is at least 3x that of the stellar disk diameter (~10 kpc), while its shape and size most resemble the large biconical, edge-brightened FUV and X-ray outflows in the nearby starburst galaxy NGC 3079. The large-scale, hourglass-shaped outflow of ESO 130-G012 appears to be hollow and originates from the star-forming disk, expanding into the halo with speeds close to the escape velocity before likely returning to the disk. Given ESO 130-G012&rsquo;s modest star formation rate, the height of the outflow is surprising and unusual, likely made possible by the galaxy&rsquo;s relatively low gravitational potential. Follow-up observations are expected to detect hot gas inside the bipolar outflow cones and magnetic fields along the X-shaped outflow wings. Neutral gas may also be lifted above the inner disk by the outflow.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.15991v1">üìÑ Download PDF</a></p><hr><h3 id=on-right-units-of-special-inverse-monoidshttpsarxivorgabs251215591v1><a href=https://arxiv.org/abs/2512.15591v1>On right units of special inverse monoids</a><a hidden class=anchor aria-hidden=true href=#on-right-units-of-special-inverse-monoidshttpsarxivorgabs251215591v1>#</a></h3><p><strong>Authors:</strong> Igor Dolinka, Robert D. Gray
<strong>Venue:</strong> arXiv (2025)</p><p>We study the class of monoids that arise as the submonoid of right units of finitely presented special inverse monoids (SIMs). Gray and Ru≈°kuc (2024) gave the first example of a finitely presented SIM whose submonoid of right units does not admit a decomposition into a free product of the group of units and a finite rank free monoid. In the first part of this paper we prove a general result which shows that the only instances where the right units of a finitely presented SIM can admit such a free product decomposition is when their group of units is finitely presented. In showing this, we establish some general results about finite generation and presentability of subgroups of SIMs. In particular, we give an exact characterisation of when an arbitrary subgroup is finitely generated in terms of connectedness properties of unions of its cosets in its $\mathscr{R}$-class, and also a characterisation of when an arbitrary subgroup is finitely presented. We also give a sufficient condition for finite generation and presentability of an arbitrary subgroup given in terms of a geometric finiteness property called boundary width. As a consequence, we show that the classes of monoids of right units of finitely presented SIMs and prefix monoids of finitely presented groups are independent. In the second part of the paper, we show that every finitely generated submonoid of a finitely RC-presented monoid is isomorphic to a submonoid $N$ of a finitely presented SIM $M$ such that $N$ is a submonoid of the right units of $M$, and $N$ contains the group of units of $M$. This result generalises and extends the classification of groups of units of finitely presented SIMs recently obtained by Gray and Kambites (2025). From this, we derive a number of surprising properties of RC-presentations for right cancellative monoids contrasting the classical theory of monoid presentations.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.15591v1">üìÑ Download PDF</a></p><hr><h3 id=wave-packet-dynamics-in-pseudo-hermitian-lattices-coexistence-of-hermitian-and-non-hermitian-wavefrontshttpsarxivorgabs251215333v1><a href=https://arxiv.org/abs/2512.15333v1>Wave-packet dynamics in pseudo-Hermitian lattices: Coexistence of Hermitian and non-Hermitian wavefronts</a><a hidden class=anchor aria-hidden=true href=#wave-packet-dynamics-in-pseudo-hermitian-lattices-coexistence-of-hermitian-and-non-hermitian-wavefrontshttpsarxivorgabs251215333v1>#</a></h3><p><strong>Authors:</strong> Alon Beck, Moshe Goldstein
<strong>Venue:</strong> arXiv (2025)</p><p>This paper investigates wave-packet dynamics in non-Hermitian lattice systems and reveals a surprising phenomenon: The simultaneous propagation of two distinct wavefronts, one traveling at the non-Hermitian velocity and the other at the Hermitian velocity. We show that this dual-front behavior arises naturally in systems governed by a pseudo-Hermitian Hamiltonian. Using the paradigmatic Hatano-Nelson model as our primary example, we demonstrate that this coexistence is essential for understanding a wide array of unconventional dynamical effects, including abrupt ``non-Hermitian reflections&rsquo;&rsquo;, sudden shifts of Gaussian wave-packets, and disorder-induced emergent packets seeded by the small initial tails. We present analytic predictions that closely match numerical simulations. These results may offer new insight into the topology of non-Hermitian systems and point toward measurable experimental consequences.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.15333v1">üìÑ Download PDF</a></p><hr><h3 id=offline-multi-task-multi-objective-data-driven-evolutionary-algorithm-with-language-surrogate-model-and-implicit-q-learninghttpsarxivorgabs251215149v1><a href=https://arxiv.org/abs/2512.15149v1>Offline Multi-Task Multi-Objective Data-Driven Evolutionary Algorithm with Language Surrogate Model and Implicit Q-Learning</a><a hidden class=anchor aria-hidden=true href=#offline-multi-task-multi-objective-data-driven-evolutionary-algorithm-with-language-surrogate-model-and-implicit-q-learninghttpsarxivorgabs251215149v1>#</a></h3><p><strong>Authors:</strong> Xian-Rong Zhang, Yue-Jiao Gong, Zeyuan Ma, Jun Zhang
<strong>Venue:</strong> arXiv (2025)</p><p>Data-driven evolutionary algorithms has shown surprising results in addressing expensive optimization problems through robust surrogate modeling. Though promising, existing surrogate modeling schemes may encounter limitations in complex optimization problems with many sub-objectives, which rely on repeated and tedious approximation. To address such technical gap, we propose Q-MetaSur as a plug-and-play surrogate modeling scheme capable of providing unified and generalized surrogate learning. Specifically, we consider multi-task-multi-objective optimization~(MTMOO) in offline setting. Several key designs are proposed: 1) we transform objective approximation into sequence-to-sequence modeling where MTMOO problem can be represented by tenxual tokenization. To operate under such auto-regressive modeling, we introduce a Large Language Model-based surrogate model that first encodes a MTMOO instance and then decodes objective values of unseen decision variables. To ensure stability in training the proposed model, we propose a two-stage offline training strategy that operates as a synergy of supervised tuning and RL fine-tuning, which first exploits offline dataset to fit existing knowledge and then leverages RL to enhance model&rsquo;s generalization performance. Extensive empirical results on the CEC2019 benchmark demonstrate that Q-MetaSur not only outperforms representative surrogate baselines in objective approximation accuracy, but also helps underlying evolutionary algorithms achieve both desired optimization convergence and improved pareto optimality.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.15149v1">üìÑ Download PDF</a></p><hr><h3 id=autonomous-learning-of-attractors-for-neuromorphic-computing-with-wien-bridge-oscillator-networkshttpsarxivorgabs251214869v1><a href=https://arxiv.org/abs/2512.14869v1>Autonomous Learning of Attractors for Neuromorphic Computing with Wien Bridge Oscillator Networks</a><a hidden class=anchor aria-hidden=true href=#autonomous-learning-of-attractors-for-neuromorphic-computing-with-wien-bridge-oscillator-networkshttpsarxivorgabs251214869v1>#</a></h3><p><strong>Authors:</strong> Riley Acker, Aman Desai, Garrett Kenyon, Frank Barrows
<strong>Venue:</strong> arXiv (2025)</p><p>We present an oscillatory neuromorphic primitive implemented with networks of coupled Wien bridge oscillators and tunable resistive couplings. Phase relationships between oscillators encode patterns, and a local Hebbian learning rule continuously adapts the couplings, allowing learning and recall to emerge from the same ongoing analog dynamics rather than from separate training and inference phases. Using a Kuramoto-style phase model with an effective energy function, we show that learned phase patterns form attractor states and validate this behavior in simulation and hardware. We further realize a 2-4-2 architecture with a hidden layer of oscillators, whose bipartite visible-hidden coupling allows multiple internal configurations to produce the same visible phase states. When inputs are switched, transient spikes in energy followed by relaxation indicate how the network can reduce surprise by reshaping its energy landscape. These results support coupled oscillator circuits as a hardware platform for energy-based neuromorphic computing with autonomous, continuous learning.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.14869v1">üìÑ Download PDF</a></p><hr><h3 id=maximum-mean-discrepancy-with-unequal-sample-sizes-via-generalized-u-statisticshttpsarxivorgabs251213997v1><a href=https://arxiv.org/abs/2512.13997v1>Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics</a><a hidden class=anchor aria-hidden=true href=#maximum-mean-discrepancy-with-unequal-sample-sizes-via-generalized-u-statisticshttpsarxivorgabs251213997v1>#</a></h3><p><strong>Authors:</strong> Aaron Wei, Milad Jalali, Danica J. Sutherland
<strong>Venue:</strong> arXiv (2025)</p><p>Existing two-sample testing techniques, particularly those based on choosing a kernel for the Maximum Mean Discrepancy (MMD), often assume equal sample sizes from the two distributions. Applying these methods in practice can require discarding valuable data, unnecessarily reducing test power. We address this long-standing limitation by extending the theory of generalized U-statistics and applying it to the usual MMD estimator, resulting in new characterization of the asymptotic distributions of the MMD estimator with unequal sample sizes (particularly outside the proportional regimes required by previous partial results). This generalization also provides a new criterion for optimizing the power of an MMD test with unequal sample sizes. Our approach preserves all available data, enhancing test accuracy and applicability in realistic settings. Along the way, we give much cleaner characterizations of the variance of MMD estimators, revealing something that might be surprising to those in the area: while zero MMD implies a degenerate estimator, it is sometimes possible to have a degenerate estimator with nonzero MMD as well; we give a construction and a proof that it does not happen in common situations.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.13997v1">üìÑ Download PDF</a></p><hr><h3 id=toward-6g-downlink-noma-crc-aided-grand-for-noise-resilient-noma-decoding-in-beyond-5g-networkshttpsarxivorgabs251216860v1><a href=https://arxiv.org/abs/2512.16860v1>Toward 6G Downlink NOMA: CRC-Aided GRAND for Noise-Resilient NOMA Decoding in Beyond-5G Networks</a><a hidden class=anchor aria-hidden=true href=#toward-6g-downlink-noma-crc-aided-grand-for-noise-resilient-noma-decoding-in-beyond-5g-networkshttpsarxivorgabs251216860v1>#</a></h3><p><strong>Authors:</strong> Emirhan Zor, Bora Bozkurt, Ferkan Yilmaz
<strong>Venue:</strong> arXiv (2025)</p><p>Non-Orthogonal Multiple Access (NOMA) technology has emerged as a promising technology to enable massive connectivity and enhanced spectral efficiency in next-generation wireless networks. In this study, we propose a novel two-user downlink power-domain NOMA framework that integrates a Cyclic Redundancy Check (CRC)-aided Guessing Random Additive Noise Decoding (GRAND) with successive interference cancellation (SIC). Unlike conventional SIC methods, which are susceptible to error propagation when there is low power disparity between users, the proposed scheme leverages GRAND&rsquo;s noise-centric strategy to systematically rank and test candidate error patterns until the correct codeword is identified. In this architecture, CRC is utilized not only to detect errors but also to aid the decoding process, effectively eliminating the need for separate Forward Error Correction (FEC) codes and reducing overall system overhead. Furthermore, the strong user enhances its decoding performance by applying SIC that is reinforced by GRAND-based decoding of the weaker user&rsquo;s signals, thereby minimizing error propagation and increasing throughput. Comprehensive simulation results over both Additive White Gaussian Noise (AWGN) and Rayleigh fading channels, under varying power allocations and user distances, show that the CRC-aided GRAND-NOMA approach significantly improves the Bit Error Rate (BER) performance compared to state-of-the-art NOMA decoding techniques. These findings underscore the potential of integrating universal decoding methods like GRAND into interference-limited multiuser environments for robust future wireless networks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16860v1">üìÑ Download PDF</a></p><hr><h3 id=connecting-current-and-future-dual-agn-searches-to-lisa-and-pta-gravitational-wave-detectionshttpsarxivorgabs251216844v1><a href=https://arxiv.org/abs/2512.16844v1>Connecting current and future dual AGN searches to LISA and PTA gravitational wave detections</a><a hidden class=anchor aria-hidden=true href=#connecting-current-and-future-dual-agn-searches-to-lisa-and-pta-gravitational-wave-detectionshttpsarxivorgabs251216844v1>#</a></h3><p><strong>Authors:</strong> Nianyi Chen, Yihao Zhou, Ekaterine Dadiani, Tiziana Di Matteo, Cici Wang, Antonella Palmese, Yue Shen, Junyao Li, Adi Foord, Simeon Bird, Yueying Ni, Yanhui Yang, Rupert Croft
<strong>Venue:</strong> arXiv (2025)</p><p>Dual active galactic nuclei (DAGN) mark an observable stage of massive black hole (MBH) pairing in galaxy mergers and are precursors to the MBH binaries that generate low-frequency gravitational waves. Using the large-volume ASTRID cosmological simulation, we construct DAGN catalogs matched to current (COSMOS-Web, DESI) and forthcoming (AXIS, Roman) searches. With realistic selection functions applied, ASTRID reproduces observed dual fractions, separations, and host-galaxy properties across redshifts. We predict a substantial population of small-separation (&lt;5 kpc) duals that current surveys fail to capture, indicating that the apparent paucity of sub-kpc systems in COSMOS-Web is driven primarily by selection effects rather than a physical deficit. By following each simulated dual forward in time, we show that dual AGN are robust tracers of MBH mergers: ~30-70% coalesce within $\lesssim 1$ Gyr, and 20-60% of these mergers produce gravitational-wave signals detectable by LISA. Duals accessible to AXIS and Roman are the progenitors of ~10% of low-redshift LISA events and ~30% of the PTA-band stochastic background. Massive green-valley galaxies with moderate-luminosity AGN, together with massive star-forming hosts containing bright quasars at $z>1$, emerge as the most likely environments for imminent MBH binaries. These results provide a unified cosmological framework linking dual AGN demographics, MBH binary formation, and gravitational-wave emission, and they identify concrete, high-priority targets for coordinated electromagnetic and GW searches in upcoming multi-messenger surveys.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16844v1">üìÑ Download PDF</a></p><hr><h3 id=a-maps-detector-for-high-resolution-low-dose-ebsdhttpsarxivorgabs251216839v1><a href=https://arxiv.org/abs/2512.16839v1>A MAPS Detector for High Resolution Low Dose EBSD</a><a hidden class=anchor aria-hidden=true href=#a-maps-detector-for-high-resolution-low-dose-ebsdhttpsarxivorgabs251216839v1>#</a></h3><p><strong>Authors:</strong> Barnaby D. A. Levin, Kalani Moore, Nicol√≤ M. Della Ventura, McLean P. Echlin, Tresa M. Pollock, Daniel S. Gianola
<strong>Venue:</strong> arXiv (2025)</p><p>The use of highly sensitive pixelated direct detectors has dramatically improved the performance of high energy instrumentation such as transmission electron microscopy. Here, we describe a recently developed monolithic active pixel sensor designed for low energy scanning electron microscopy applications. This detector enables electron backscatter diffraction (EBSD) at lower energies and dose than are accessible with existing scintillator-coupled detectors, expanding grain orientation mapping capabilities to materials such as ceramics that are poor electron conductors. The high detector sensitivity allows collection of rich diffraction information - providing dislocation defect contrast that is otherwise not accessible via EBSD. Indeed, even the energy of single electron interaction events can be measured with this detector, which we demonstrate to energy filter diffraction patterns revealing details of how diffraction occurs at low energy.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16839v1">üìÑ Download PDF</a></p><hr><h3 id=enhancing-kinematics-understanding-through-a-real-time-graph-based-motion-video-gamehttpsarxivorgabs251216838v1><a href=https://arxiv.org/abs/2512.16838v1>Enhancing Kinematics Understanding Through a Real-Time Graph-Based Motion Video Game</a><a hidden class=anchor aria-hidden=true href=#enhancing-kinematics-understanding-through-a-real-time-graph-based-motion-video-gamehttpsarxivorgabs251216838v1>#</a></h3><p><strong>Authors:</strong> Mateo Dutra, Marcos Abreu, Mart√≠n Monteiro, Silvia Sguilla, Cecilia Stari, Alvaro Suarez, Arturo C. Marti
<strong>Venue:</strong> arXiv (2025)</p><p>Kinematics is a core topic in early physics courses, yet students often struggle to interpret motion and its graphical representations. To tackle these difficulties, we developed MissionMotion, a physical-computational videogame where students reproduce target motion graphs using real-time data from their own movements or from sensors connected through micro:bit or Arduino. The system displays both the target and the user-generated graph, providing immediate visual feedback and a score based on similarity. We piloted the environment with ninth-grade students in different school contexts and evaluated their experience using the MEEGA+ instrument. The results show strong engagement, positive perceptions of usability, and evidence that the game promotes reflection on motion graphs in ways that rarely emerge in traditional lessons. MissionMotion runs on any web-enabled device and all materials are openly available, offering teachers an accessible tool to integrate experimentation, computational thinking, and playful learning into physics classrooms.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16838v1">üìÑ Download PDF</a></p><hr><h3 id=delay-aware-multi-stage-edge-server-upgrade-with-budget-constrainthttpsarxivorgabs251216792v1><a href=https://arxiv.org/abs/2512.16792v1>Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint</a><a hidden class=anchor aria-hidden=true href=#delay-aware-multi-stage-edge-server-upgrade-with-budget-constrainthttpsarxivorgabs251216792v1>#</a></h3><p><strong>Authors:</strong> Endar Suprih Wihidayat, Sieteng Soh, Kwan-Wu Chin, Duc-son Pham
<strong>Venue:</strong> arXiv (2025)</p><p>In this paper, the Multi-stage Edge Server Upgrade (M-ESU) is proposed as a new network planning problem, involving the upgrading of an existing multi-access edge computing (MEC) system through multiple stages (e.g., over several years). More precisely, the problem considers two key decisions: (i) whether to deploy additional edge servers or upgrade those already installed, and (ii) how tasks should be offloaded so that the average number of tasks that meet their delay requirement is maximized. The framework specifically involves: (i) deployment of new servers combined with capacity upgrades for existing servers, and (ii) the optimal task offloading to maximize the average number of tasks with a delay requirement. It also considers the following constraints: (i) budget per stage, (ii) server deployment and upgrade cost (in $) and cost depreciation rate, (iii) computation resource of servers, (iv) number of tasks and their growth rate (in %), and (v) the increase in task sizes and stricter delay requirements over time. We present two solutions: a Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). MILP yields the optimal solution for small networks, whereas M-ESU/H is used in large-scale networks. For small networks, the simulation results show that the solution computed by M-ESU/H is within 1.25% of the optimal solution while running several orders of magnitude faster. For large networks, M-ESU/H is compared against three alternative heuristic solutions that consider only server deployment, or giving priority to server deployment or upgrade. Our experiments show that M-ESU/H yields up to 21.57% improvement in task satisfaction under identical budget and demand growth conditions, confirming its scalability and practical value for long-term MEC systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16792v1">üìÑ Download PDF</a></p><hr><h3 id=the-social-responsibility-stack-a-control-theoretic-architecture-for-governing-socio-technical-aihttpsarxivorgabs251216873v1><a href=https://arxiv.org/abs/2512.16873v1>The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI</a><a hidden class=anchor aria-hidden=true href=#the-social-responsibility-stack-a-control-theoretic-architecture-for-governing-socio-technical-aihttpsarxivorgabs251216873v1>#</a></h3><p><strong>Authors:</strong> Otman A. Basir
<strong>Venue:</strong> arXiv (2025)</p><p>Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16873v1">üìÑ Download PDF</a></p><hr><h3 id=plausibility-as-failure-how-llms-and-humans-co-construct-epistemic-errorhttpsarxivorgabs251216750v1><a href=https://arxiv.org/abs/2512.16750v1>Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error</a><a hidden class=anchor aria-hidden=true href=#plausibility-as-failure-how-llms-and-humans-co-construct-epistemic-errorhttpsarxivorgabs251216750v1>#</a></h3><p><strong>Authors:</strong> Claudia Vale Oliveira, Nelson Zagalo, Filipe Silva, Anabela Brandao, Syeda Faryal Hussain Khurrum, Joaquim Santos
<strong>Venue:</strong> arXiv (2025)</p><p>Large language models (LLMs) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi LLM evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that LLM errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for LLM assessment, digital literacy, and the design of trustworthy human AI communication.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16750v1">üìÑ Download PDF</a></p><hr><h3 id=cyber-humanism-in-education-reclaiming-agency-through-ai-and-learning-scienceshttpsarxivorgabs251216701v1><a href=https://arxiv.org/abs/2512.16701v1>Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences</a><a hidden class=anchor aria-hidden=true href=#cyber-humanism-in-education-reclaiming-agency-through-ai-and-learning-scienceshttpsarxivorgabs251216701v1>#</a></h3><p><strong>Authors:</strong> Giovanni Adorni
<strong>Venue:</strong> arXiv (2025)</p><p>Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona-lisation of teachers. This paper proposes \emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures.
We articulate three pillars for cyber-humanist design, \emph{reflexive competence}, \emph{algorithmic citizenship}, and \emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \emph{prompt-based learning} and a new \emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16701v1">üìÑ Download PDF</a></p><hr><h3 id=the-hands-of-time-moving-my-body-to-keep-time-order-in-the-brainhttpsarxivorgabs251216616v1><a href=https://arxiv.org/abs/2512.16616v1>The hands of time: Moving my body to keep time order in the brain</a><a hidden class=anchor aria-hidden=true href=#the-hands-of-time-moving-my-body-to-keep-time-order-in-the-brainhttpsarxivorgabs251216616v1>#</a></h3><p><strong>Authors:</strong> Julien Lagarde
<strong>Venue:</strong> arXiv (2025)</p><p>The brain is very often viewed as a network of cells, mostly neurons. Here we introduce a conjecture, in the spirit of a philosophical though experiment, which proposes that the present cannot be obtained from within such networks, and that this limitation imposes burdens on network efficiency in information processing. We aim to argue this conjecture imposes recurrent contacts from within the brain to outside in the physical world via behavior, which create a flow of time stamps. This though experiment may contribute to make the divide between the foci toward inside versus outside, for example opposing ecological psychology and many frameworks adopted in neurosciences, superfluous. This piece proposes an ambulation triggered by a thought experiment: What if I was a neuron listening to another one and talking to a third? It is a modest echo to fruitful thought experiments, like Molyneux problem, the imitation game and the anti-sequel Chinese room, key gedankenexperiments in an elevator, or the cogito.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16616v1">üìÑ Download PDF</a></p><hr><h3 id=needle-in-the-web-a-benchmark-for-retrieving-targeted-web-pages-in-the-wildhttpsarxivorgabs251216553v1><a href=https://arxiv.org/abs/2512.16553v1>Needle in the Web: A Benchmark for Retrieving Targeted Web Pages in the Wild</a><a hidden class=anchor aria-hidden=true href=#needle-in-the-web-a-benchmark-for-retrieving-targeted-web-pages-in-the-wildhttpsarxivorgabs251216553v1>#</a></h3><p><strong>Authors:</strong> Yumeng Wang, Tianyu Fan, Lingrui Xu, Chao Huang
<strong>Venue:</strong> arXiv (2025)</p><p>Large Language Models (LLMs) have evolved from simple chatbots into sophisticated agents capable of automating complex real-world tasks, where browsing and reasoning over live web content is key to assessing retrieval and cognitive skills. Existing benchmarks like BrowseComp and xBench-DeepSearch emphasize complex reasoning searches requiring multi-hop synthesis but neglect Fuzzy Exploratory Search, namely queries that are vague and multifaceted, where users seek the most relevant webpage rather than a single factual answer. To address this gap, we introduce Needle in the Web, a novel benchmark specifically designed to evaluate modern search agents and LLM-based systems on their ability to retrieve and reason over real-world web content in response to ambiguous, exploratory queries under varying levels of difficulty. Needle in the Web comprises 663 questions spanning seven distinct domains. To ensure high query quality and answer uniqueness, we employ a flexible methodology that reliably generates queries of controllable difficulty based on factual claims of web contents. We benchmark three leading LLMs and three agent-based search systems on Needle in the Web, finding that most models struggle: many achieve below 35% accuracy, and none consistently excel across domains or difficulty levels. These findings reveal that Needle in the Web presents a significant challenge for current search systems and highlights the open problem of effective fuzzy retrieval under semantic ambiguity.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16553v1">üìÑ Download PDF</a></p><hr><h3 id=unraveling-persistent-urban-rural-gaps-a-long-term-provincial-analysis-of-residential-heating-and-cooling-loadshttpsarxivorgabs251216779v1><a href=https://arxiv.org/abs/2512.16779v1>Unraveling persistent urban-rural gaps: A long-term provincial analysis of residential heating and cooling loads</a><a hidden class=anchor aria-hidden=true href=#unraveling-persistent-urban-rural-gaps-a-long-term-provincial-analysis-of-residential-heating-and-cooling-loadshttpsarxivorgabs251216779v1>#</a></h3><p><strong>Authors:</strong> Qinwen Tang, Ran Yan, Nan Zhou, Minda Ma
<strong>Venue:</strong> arXiv (2025)</p><p>With global climate change and rising demand for thermal comfort, space heating and cooling have become increasingly critical to achieving carbon neutrality in the building sector. This study presents a first attempt to develop a bottom-up regional building energy model based on prototype buildings simulated in EnergyPlus, to assess space heating and cooling loads of urban and rural residential buildings across 30 Chinese provinces from 1980 to 2024. The results indicate that: (1) Guangdong recorded the highest cooling loads in 2020, reaching 76.5 TWh/a in urban areas and 63.0 TWh/a in rural areas; Henan exhibited the highest rural heating load at 174.6 TWh/a, while urban heating loads were highest in provinces such as Liaoning and Shandong. (2) From 1980 to 2024, average cooling loads increased from 12.4 to 15.1 kWh/m2/a in urban areas but declined from 22.63 to 19.87 kWh/m2/a in rural areas. Over the same period, average heating loads decreased from 44.08 to 39.92 kWh/m2/a in urban areas and from 100.15 to 72.42 kWh/m2/a in rural areas. (3) Urban residential building stock has surpassed rural stock in 22 provinces in recent years, compared with only 4 provinces in 2000, and the presence of 12 urban energy-efficiency standards versus only one rural standard further highlights substantial envelope performance gaps. Collectively, these dynamics have led to pronounced and persistent urban-rural disparities in residential heating and cooling loads. These findings underscore the need for differentiated standards and region-specific clean heating strategies, while providing a transferable modeling framework to inform targeted energy-saving policies and support the building sector&rsquo;s transition toward carbon neutrality.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16779v1">üìÑ Download PDF</a></p><hr><h3 id=hypervelocity-impact-debris-cloud-trajectory-planning-based-on-additive-manufactured-lattice-structureshttpsarxivorgabs251216611v1><a href=https://arxiv.org/abs/2512.16611v1>Hypervelocity Impact Debris Cloud Trajectory-Planning based on Additive Manufactured Lattice Structures</a><a hidden class=anchor aria-hidden=true href=#hypervelocity-impact-debris-cloud-trajectory-planning-based-on-additive-manufactured-lattice-structureshttpsarxivorgabs251216611v1>#</a></h3><p><strong>Authors:</strong> Bilin Zheng, Xiao Kang, Xiaoyu Zhang, Hao Zhou, Mengchuan Xu, Chang Liu
<strong>Venue:</strong> arXiv (2025)</p><p>Space debris and micrometeoroid (MMOD) impacts pose a serious threat to the safe operation of spacecraft. However, traditional protective structures typically suffer from limitations such as excessive thickness and inadequate load-bearing capacity. Guided by the design concepts of debris-cloud deflection and hierarchical energy dissipation, this study proposes a trajectory-planning lattice protective structure. First, the lattice parameters and geometry were designed according to the functional relationship between the incident angle and the transmitted/ricochet trajectory angles. Subsequently, multi-angle hypervelocity impact experiments were carried out to evaluate the proposed lattice protection structure. In combination with post-impact CT three-dimensional reconstruction and smoothed particle hydrodynamics (SPH) numerical simulations, the protective mechanisms of the lattice structure were systematically characterized and clarified. The results demonstrate that, for three oblique incidence conditions, the lattice structure remained intact and significantly deflected the debris-cloud momentum direction while effectively dissipating its kinetic energy. The angled plates with gradient designs enabled continuous changes in the momentum direction and stepwise kinetic energy dissipation through multiple cycles of debrisation, dispersion, and trajectory deflection. This research presents a novel, engineering-ready approach for spacecraft MMOD protection and validates the potential of trajectory-planning lattice structures for hypervelocity impact defense.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16611v1">üìÑ Download PDF</a></p><hr><h2 id=-llm>üîç llm<a hidden class=anchor aria-hidden=true href=#-llm>#</a></h2><h3 id=posterior-behavioral-cloning-pretraining-bc-policies-for-efficient-rl-finetuninghttpsarxivorgabs251216911v1><a href=https://arxiv.org/abs/2512.16911v1>Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning</a><a hidden class=anchor aria-hidden=true href=#posterior-behavioral-cloning-pretraining-bc-policies-for-efficient-rl-finetuninghttpsarxivorgabs251216911v1>#</a></h3><p><strong>Authors:</strong> Andrew Wagenmaker, Perry Dong, Raymond Tsao, Chelsea Finn, Sergey Levine
<strong>Venue:</strong> arXiv (2025)</p><p>Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) &ndash; which trains a policy to directly match the actions played by the demonstrator &ndash; can fail to ensure coverage over the demonstrator&rsquo;s actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator&rsquo;s behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator&rsquo;s actions, enabling more effective finetuning. Furthermore, this policy &ndash; which we refer to as the posterior behavioral cloning (PostBC) policy &ndash; achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains &ndash; relying only on standard supervised learning &ndash; and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16911v1">üìÑ Download PDF</a></p><hr><h3 id=viva-vlm-guided-instruction-based-video-editing-with-reward-optimizationhttpsarxivorgabs251216906v1><a href=https://arxiv.org/abs/2512.16906v1>VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization</a><a hidden class=anchor aria-hidden=true href=#viva-vlm-guided-instruction-based-video-editing-with-reward-optimizationhttpsarxivorgabs251216906v1>#</a></h3><p><strong>Authors:</strong> Xiaoyan Cong, Haotian Yang, Angtian Wang, Yizhi Wang, Yiding Yang, Canyu Zhang, Chongyang Ma
<strong>Venue:</strong> arXiv (2025)</p><p>Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: <a href=https://viva-paper.github.io>https://viva-paper.github.io</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16906v1">üìÑ Download PDF</a></p><hr><h3 id=impacts-of-racial-bias-in-historical-training-data-for-news-aihttpsarxivorgabs251216901v1><a href=https://arxiv.org/abs/2512.16901v1>Impacts of Racial Bias in Historical Training Data for News AI</a><a hidden class=anchor aria-hidden=true href=#impacts-of-racial-bias-in-historical-training-data-for-news-aihttpsarxivorgabs251216901v1>#</a></h3><p><strong>Authors:</strong> Rahul Bhargava, Malene Hornstrup Jespersen, Emily Boardman Ndulue, Vivica Dsouza
<strong>Venue:</strong> arXiv (2025)</p><p>AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning &ldquo;blacks&rdquo; thematic topic label. Through quantitative and qualitative means we investigate this label&rsquo;s use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the &ldquo;blacks&rdquo; label operates partially as a general &ldquo;racism detector&rdquo; across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16901v1">üìÑ Download PDF</a></p><hr><h3 id=checking-the-hal-interface-specification-continuously-right-from-the-starthttpsarxivorgabs251216897v1><a href=https://arxiv.org/abs/2512.16897v1>Checking the HAL Interface Specification Continuously, Right from the Start</a><a hidden class=anchor aria-hidden=true href=#checking-the-hal-interface-specification-continuously-right-from-the-starthttpsarxivorgabs251216897v1>#</a></h3><p><strong>Authors:</strong> Manuel Bentele, Onur Altinordu, Jan K√∂rner, Andreas Podelski, Axel Sikora
<strong>Venue:</strong> arXiv (2025)</p><p>The correct use of a Hardware Abstraction Layer (HAL) interface in embedded applications is crucial to prevent malfunctions, crashes, or even hardware damage. Software model checking has been successfully applied to check interface specifications in application programs, but its employment in industrial practice is hindered by its unpredictability (whether it succeeds for a given application program or not). In this paper, we present a novel approach to address this problem by checking the HAL interface specification continuously and right from the start of the development. I.e., we develop an embedded application in several iterations without a formal connection between the steps. The steps start from a program skeleton which does nothing but calling HAL functions. Actual functionality is added consecutively. The HAL interface specification is checked in each step of the sequence. The idea of the approach is to exploit a specific feature of software model checking: Its attempt to compute exactly the abstraction that is needed for the check to succeed may carry over from one step to the next, even if there is no formal connection between the steps. The experience from a preliminary experimental evaluation of our approach in the development of embedded applications is very promising. Following our approach, the check succeeds in each step and in particular in the final application program.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16897v1">üìÑ Download PDF</a></p><hr><h3 id=multimer-embedding-for-molecular-crystals-utilizing-up-to-tetramer-interactionshttpsarxivorgabs251216877v1><a href=https://arxiv.org/abs/2512.16877v1>Multimer Embedding for Molecular Crystals Utilizing up to Tetramer Interactions</a><a hidden class=anchor aria-hidden=true href=#multimer-embedding-for-molecular-crystals-utilizing-up-to-tetramer-interactionshttpsarxivorgabs251216877v1>#</a></h3><p><strong>Authors:</strong> Alexander List, A. Daniel Boese, Johannes Hoja
<strong>Venue:</strong> arXiv (2025)</p><p>Molecular crystals possess a highly complex crystallographic landscape which in many cases results in the experimental observation of multiple crystal structures for the same compound. Accurate results can often be obtained for such systems by employing periodic density functional theory using hybrid functionals; however, this is not always computationally feasible. One possibility to circumvent these expensive periodic calculations is the utilization of multimer embedding methods. Therein, the fully periodic crystal is described at a lower level of theory, and subsequently monomer energies, dimer interaction energies, etc. are corrected via high-level calculations. In this paper, we further extend such a multimer embedding approach by one multimer order for all investigated properties, allowing us to compute lattice energies up to the tetramer embedding level, and atomic forces, the stress tensor, and harmonic phonons up to the trimer level. We test the significance of including these higher-order multimers by embedding PBE0+MBD multimers into periodic PBE+MBD calculations utilizing the X23 benchmark set of molecular crystals and comparing the results to explicit periodic PBE0+MBD calculations. We show that tetramer interactions systematically improve the lattice energy approximation and explore multiple possibilities for multimer selection. Furthermore, we confirm that trimer interactions are crucial for the description of the stress tensor, yielding cell volumes within 1 % of those of PBE0+MBD. Subsequently, this also results in an improvement of the description of vibrational properties, giving on average gamma point frequencies within 1.3 wave numbers and vibrational free energies within 0.3 kJ/mol of the PBE0+MBD results.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16877v1">üìÑ Download PDF</a></p><hr><h3 id=alchemist-unlocking-efficiency-in-text-to-image-model-training-via-meta-gradient-data-selectionhttpsarxivorgabs251216905v1><a href=https://arxiv.org/abs/2512.16905v1>Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection</a><a hidden class=anchor aria-hidden=true href=#alchemist-unlocking-efficiency-in-text-to-image-model-training-via-meta-gradient-data-selectionhttpsarxivorgabs251216905v1>#</a></h3><p><strong>Authors:</strong> Kaixin Ding, Yang Zhou, Xi Chen, Miao Yang, Jiarong Ou, Rui Chen, Xin Tao, Hengshuang Zhao
<strong>Venue:</strong> arXiv (2025)</p><p>Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose <strong>Alchemist</strong>, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample&rsquo;s influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16905v1">üìÑ Download PDF</a></p><hr><h2 id=-neuroscience>üîç neuroscience<a hidden class=anchor aria-hidden=true href=#-neuroscience>#</a></h2><h3 id=an-evacuation-simulator-for-pedestrian-dynamics-based-on-the-social-force-modelhttpsarxivorgabs251216887v1><a href=https://arxiv.org/abs/2512.16887v1>An evacuation simulator for pedestrian dynamics based on the Social Force Model</a><a hidden class=anchor aria-hidden=true href=#an-evacuation-simulator-for-pedestrian-dynamics-based-on-the-social-force-modelhttpsarxivorgabs251216887v1>#</a></h3><p><strong>Authors:</strong> Juli√°n L√≥pez, Virginia Mazzone, M. Leticia Rubio Puzzo, Juan Cruz Moreno
<strong>Venue:</strong> arXiv (2025)</p><p>The evacuation of pedestrians from enclosed spaces represents a key problem in safety engineering and infrastructure design. Analyzing the collective dynamics that emerge during evacuation processes requires simulation tools capable of capturing individual interactions and spatial constraints realistically.
In this work, we present \textit{SiCoBioNa}, an open-source evacuation simulator based on the Social Force Model (SFM). The software provides an intuitive graphical interface that allows users to configure pedestrian properties, spatial geometries, and initial conditions without requiring prior expertise in numerical modeling techniques. The SFM framework enables the representation of goal-oriented motion, interpersonal interactions, and interactions with fixed obstacles.
The simulator generates both quantitative data and visual outputs, facilitating the analysis of evacuation dynamics and the evaluation of different spatial configurations. Due to its modular and extensible design, \textit{SiCoBioNa} serves as a reproducible research tool for studies on pedestrian dynamics providing practical support for evacuation planning.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16887v1">üìÑ Download PDF</a></p><hr><h3 id=privatexr-defending-privacy-attacks-in-extended-reality-through-explainable-ai-guided-differential-privacyhttpsarxivorgabs251216851v1><a href=https://arxiv.org/abs/2512.16851v1>PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy</a><a hidden class=anchor aria-hidden=true href=#privatexr-defending-privacy-attacks-in-extended-reality-through-explainable-ai-guided-differential-privacyhttpsarxivorgabs251216851v1>#</a></h3><p><strong>Authors:</strong> Ripan Kumar Kundu, Istiak Ahmed, Khaza Anuarul Hoque
<strong>Venue:</strong> arXiv (2025)</p><p>The convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43% and 39%, respectively, for cybersickness tasks while preserving model utility with up to 97% accuracy using Transformer models. Furthermore, it improves inference time by up to ~2x compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16851v1">üìÑ Download PDF</a></p><hr><h2 id=-data_resources>üîç data_resources<a hidden class=anchor aria-hidden=true href=#-data_resources>#</a></h2><h3 id=evaluating-openai-gpt-models-for-translation-of-endangered-uralic-languages-a-comparison-of-reasoning-and-non-reasoning-architectureshttpsarxivorgabs251216287v1><a href=https://arxiv.org/abs/2512.16287v1>Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures</a><a hidden class=anchor aria-hidden=true href=#evaluating-openai-gpt-models-for-translation-of-endangered-uralic-languages-a-comparison-of-reasoning-and-non-reasoning-architectureshttpsarxivorgabs251216287v1>#</a></h3><p><strong>Authors:</strong> Yehor Tereshchenko, Mika H√§m√§l√§inen, Svitlana Myroniuk
<strong>Venue:</strong> arXiv (2025)</p><p>The evaluation of Large Language Models (LLMs) for translation tasks has primarily focused on high-resource languages, leaving a significant gap in understanding their performance on low-resource and endangered languages. This study presents a comprehensive comparison of OpenAI&rsquo;s GPT models, specifically examining the differences between reasoning and non-reasoning architectures for translating between Finnish and four low-resource Uralic languages: Komi-Zyrian, Moksha, Erzya, and Udmurt. Using a parallel corpus of literary texts, we evaluate model willingness to attempt translation through refusal rate analysis across different model architectures. Our findings reveal significant performance variations between reasoning and non-reasoning models, with reasoning models showing 16 percentage points lower refusal rates. The results provide valuable insights for researchers and practitioners working with Uralic languages and contribute to the broader understanding of reasoning model capabilities for endangered language preservation.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16287v1">üìÑ Download PDF</a></p><hr><h3 id=sigma-moe-tiny-technical-reporthttpsarxivorgabs251216248v1><a href=https://arxiv.org/abs/2512.16248v1>Sigma-Moe-Tiny Technical Report</a><a hidden class=anchor aria-hidden=true href=#sigma-moe-tiny-technical-reporthttpsarxivorgabs251216248v1>#</a></h3><p><strong>Authors:</strong> Qingguo Hu, Zhenghao Lin, Ziyue Yang, Yucheng Ding, Xiao Liu, Yuting Jiang, Ruizhe Wang, Tianyu Chen, Zhongxin Guo, Yifan Xiong, Rui Gao, Lei Qu, Jinsong Su, Peng Cheng, Yeyun Gong
<strong>Venue:</strong> arXiv (2025)</p><p>Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.
Project page: <a href=https://qghuxmu.github.io/Sigma-MoE-Tiny>https://qghuxmu.github.io/Sigma-MoE-Tiny</a>
Code: <a href=https://github.com/microsoft/ltp-megatron-lm>https://github.com/microsoft/ltp-megatron-lm</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16248v1">üìÑ Download PDF</a></p><hr><h3 id=wemusic-agent-efficient-conversational-music-recommendation-via-knowledge-internalization-and-agentic-boundary-learninghttpsarxivorgabs251216108v1><a href=https://arxiv.org/abs/2512.16108v1>WeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning</a><a hidden class=anchor aria-hidden=true href=#wemusic-agent-efficient-conversational-music-recommendation-via-knowledge-internalization-and-agentic-boundary-learninghttpsarxivorgabs251216108v1>#</a></h3><p><strong>Authors:</strong> Wendong Bi, Yirong Mao, Xianglong Liu, Kai Tian, Jian Zhang, Hanjie Wang, Wenhui Que
<strong>Venue:</strong> arXiv (2025)</p><p>Personalized music recommendation in conversational scenarios usually requires a deep understanding of user preferences and nuanced musical context, yet existing methods often struggle with balancing specialized domain knowledge and flexible tool integration. This paper proposes WeMusic-Agent, a training framework for efficient LLM-based conversational music recommendation. By integrating the knowledge internalization and agentic boundary learning, the framework aims to teach the model to intelligently decide when to leverage internalized knowledge and when to call specialized tools (e.g., music retrieval APIs, music recommendation systems). Under this framework, we present WeMusic-Agent-M1, an agentic model that internalizes extensive musical knowledge via continued pretraining on 50B music-related corpus while acquiring the ability to invoke external tools when necessary. Additionally, considering the lack of open-source benchmarks for conversational music recommendation, we also construct a benchmark for personalized music recommendations derived from real-world data in WeChat Listen. This benchmark enables comprehensive evaluation across multiple dimensions, including relevance, personalization, and diversity of the recommendations. Experiments on real-world data demonstrate that WeMusic-Agent achieves significant improvements over existing models.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16108v1">üìÑ Download PDF</a></p><hr><h3 id=multimodal-rewardbench-2-evaluating-omni-reward-models-for-interleaved-text-and-imagehttpsarxivorgabs251216899v1><a href=https://arxiv.org/abs/2512.16899v1>Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image</a><a hidden class=anchor aria-hidden=true href=#multimodal-rewardbench-2-evaluating-omni-reward-models-for-interleaved-text-and-imagehttpsarxivorgabs251216899v1>#</a></h3><p><strong>Authors:</strong> Yushi Hu, Reyhane Askari-Hemmat, Melissa Hall, Emily Dinan, Luke Zettlemoyer, Marjan Ghazvininejad
<strong>Venue:</strong> arXiv (2025)</p><p>Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (&ldquo;thinking-with-images&rdquo;), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16899v1">üìÑ Download PDF</a></p><hr><h2 id=-emotion_language>üîç emotion_language<a hidden class=anchor aria-hidden=true href=#-emotion_language>#</a></h2><h3 id=on-the-universal-representation-property-of-spiking-neural-networkshttpsarxivorgabs251216872v1><a href=https://arxiv.org/abs/2512.16872v1>On the Universal Representation Property of Spiking Neural Networks</a><a hidden class=anchor aria-hidden=true href=#on-the-universal-representation-property-of-spiking-neural-networkshttpsarxivorgabs251216872v1>#</a></h3><p><strong>Authors:</strong> Shayan Hundrieser, Philipp Tuchel, Insung Kong, Johannes Schmidt-Hieber
<strong>Venue:</strong> arXiv (2025)</p><p>Inspired by biology, spiking neural networks (SNNs) process information via discrete spikes over time, offering an energy-efficient alternative to the classical computing paradigm and classical artificial neural networks (ANNs). In this work, we analyze the representational power of SNNs by viewing them as sequence-to-sequence processors of spikes, i.e., systems that transform a stream of input spikes into a stream of output spikes. We establish the universal representation property for a natural class of spike train functions. Our results are fully quantitative, constructive, and near-optimal in the number of required weights and neurons. The analysis reveals that SNNs are particularly well-suited to represent functions with few inputs, low temporal complexity, or compositions of such functions. The latter is of particular interest, as it indicates that deep SNNs can efficiently capture composite functions via a modular design. As an application of our results, we discuss spike train classification. Overall, these results contribute to a rigorous foundation for understanding the capabilities and limitations of spike-based neuromorphic systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16872v1">üìÑ Download PDF</a></p><hr><h3 id=what-do-prosody-and-text-convey-characterizing-how-meaningful-information-is-distributed-across-multiple-channelshttpsarxivorgabs251216832v1><a href=https://arxiv.org/abs/2512.16832v1>What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels</a><a hidden class=anchor aria-hidden=true href=#what-do-prosody-and-text-convey-characterizing-how-meaningful-information-is-distributed-across-multiple-channelshttpsarxivorgabs251216832v1>#</a></h3><p><strong>Authors:</strong> Aditya Yadavalli, Tiago Pimentel, Tamar I Regev, Ethan Wilcox, Alex Warstadt
<strong>Venue:</strong> arXiv (2025)</p><p>Prosody &ndash; the melody of speech &ndash; conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance&rsquo;s meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel &ndash; and by implication the prosodic channel &ndash; transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16832v1">üìÑ Download PDF</a></p><hr><h3 id=ginsign-grounding-natural-language-into-system-signatures-for-temporal-logic-translationhttpsarxivorgabs251216770v1><a href=https://arxiv.org/abs/2512.16770v1>GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation</a><a hidden class=anchor aria-hidden=true href=#ginsign-grounding-natural-language-into-system-signatures-for-temporal-logic-translationhttpsarxivorgabs251216770v1>#</a></h3><p><strong>Authors:</strong> William English, Chase Walker, Dominic Simon, Rickard Ewetz
<strong>Venue:</strong> arXiv (2025)</p><p>Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\mathcal{P}$. We decompose the grounding task hierarchically &ndash; first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5%$, a $1.4\times$ improvement over SOTA.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16770v1">üìÑ Download PDF</a></p><hr><h3 id=pattern-recognition-in-complex-systems-via-vector-field-representations-of-spatio-temporal-datahttpsarxivorgabs251216763v1><a href=https://arxiv.org/abs/2512.16763v1>Pattern recognition in complex systems via vector-field representations of spatio-temporal data</a><a hidden class=anchor aria-hidden=true href=#pattern-recognition-in-complex-systems-via-vector-field-representations-of-spatio-temporal-datahttpsarxivorgabs251216763v1>#</a></h3><p><strong>Authors:</strong> Ingrid Amaranta Membrillo Solis, Maria van Rossem, Tristan Madeleine, Tetiana Orlova, Nina Podoliak, Giampaolo D&rsquo;Alessandro, Jacek Brodzki, Malgosia Kaczmarek
<strong>Venue:</strong> arXiv (2025)</p><p>A complex system comprises multiple interacting entities whose interdependencies form a unified whole, exhibiting emergent behaviours not present in individual components. Examples include the human brain, living cells, soft matter, Earth&rsquo;s climate, ecosystems, and the economy. These systems exhibit high-dimensional, non-linear dynamics, making their modelling, classification, and prediction particularly challenging. Advances in information technology have enabled data-driven approaches to studying such systems. However, the sheer volume and complexity of spatio-temporal data often hinder traditional methods like dimensionality reduction, phase-space reconstruction, and attractor characterisation. This paper introduces a geometric framework for analysing spatio-temporal data from complex systems, grounded in the theory of vector fields over discrete measure spaces. We propose a two-parameter family of metrics suitable for data analysis and machine learning applications. The framework supports time-dependent images, image gradients, and real- or vector-valued functions defined on graphs and simplicial complexes. We validate our approach using data from numerical simulations of biological and physical systems on flat and curved domains. Our results show that the proposed metrics, combined with multidimensional scaling, effectively address key analytical challenges. They enable dimensionality reduction, mode decomposition, phase-space reconstruction, and attractor characterisation. Our findings offer a robust pathway for understanding complex dynamical systems, especially in contexts where traditional modelling is impractical but abundant experimental data are available.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16763v1">üìÑ Download PDF</a></p><hr><h3 id=how-good-is-post-hoc-watermarking-with-language-model-rephrasinghttpsarxivorgabs251216904v1><a href=https://arxiv.org/abs/2512.16904v1>How Good is Post-Hoc Watermarking With Language Model Rephrasing?</a><a hidden class=anchor aria-hidden=true href=#how-good-is-post-hoc-watermarking-with-language-model-rephrasinghttpsarxivorgabs251216904v1>#</a></h3><p><strong>Authors:</strong> Pierre Fernandez, Tom Sander, Hady Elsahar, Hongyan Chang, Tom√°≈° Souƒçek, Valeriu Lacatusu, Tuan Tran, Sylvestre-Alvise Rebuffi, Alexandre Mourachko
<strong>Venue:</strong> arXiv (2025)</p><p>Generation-time text watermarking embeds statistical signals into text for traceability of AI-generated content. We explore <em>post-hoc watermarking</em> where an LLM rewrites existing text while applying generation-time watermarking, to protect copyrighted documents, or detect their use in training or RAG via watermark radioactivity. Unlike generation-time approaches, which is constrained by how LLMs are served, this setting offers additional degrees of freedom for both generation and detection. We investigate how allocating compute (through larger rephrasing models, beam search, multi-candidate generation, or entropy filtering at detection) affects the quality-detectability trade-off. Our strategies achieve strong detectability and semantic fidelity on open-ended text such as books. Among our findings, the simple Gumbel-max scheme surprisingly outperforms more recent alternatives under nucleus sampling, and most methods benefit significantly from beam search. However, most approaches struggle when watermarking verifiable text such as code, where we counterintuitively find that smaller models outperform larger ones. This study reveals both the potential and limitations of post-hoc watermarking, laying groundwork for practical applications and future research.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16904v1">üìÑ Download PDF</a></p><hr><h3 id=learning-confidence-ellipsoids-and-applications-to-robust-subspace-recoveryhttpsarxivorgabs251216875v1><a href=https://arxiv.org/abs/2512.16875v1>Learning Confidence Ellipsoids and Applications to Robust Subspace Recovery</a><a hidden class=anchor aria-hidden=true href=#learning-confidence-ellipsoids-and-applications-to-robust-subspace-recoveryhttpsarxivorgabs251216875v1>#</a></h3><p><strong>Authors:</strong> Chao Gao, Liren Shan, Vaidehi Srinivas, Aravindan Vijayaraghavan
<strong>Venue:</strong> arXiv (2025)</p><p>We study the problem of finding confidence ellipsoids for an arbitrary distribution in high dimensions. Given samples from a distribution $D$ and a confidence parameter $Œ±$, the goal is to find the smallest volume ellipsoid $E$ which has probability mass $\Pr_{D}[E] \ge 1-Œ±$. Ellipsoids are a highly expressive class of confidence sets as they can capture correlations in the distribution, and can approximate any convex set. This problem has been studied in many different communities. In statistics, this is the classic minimum volume estimator introduced by Rousseeuw as a robust non-parametric estimator of location and scatter. However in high dimensions, it becomes NP-hard to obtain any non-trivial approximation factor in volume when the condition number $Œ≤$ of the ellipsoid (ratio of the largest to the smallest axis length) goes to $\infty$. This motivates the focus of our paper: can we efficiently find confidence ellipsoids with volume approximation guarantees when compared to ellipsoids of bounded condition number $Œ≤$?
Our main result is a polynomial time algorithm that finds an ellipsoid $E$ whose volume is within a $O(Œ≤^{Œ≥d})$ multiplicative factor of the volume of best $Œ≤$-conditioned ellipsoid while covering at least $1-O(Œ±/Œ≥)$ probability mass for any $Œ≥&lt; Œ±$. We complement this with a computational hardness result that shows that such a dependence seems necessary up to constants in the exponent. The algorithm and analysis uses the rich primal-dual structure of the minimum volume enclosing ellipsoid and the geometric Brascamp-Lieb inequality. As a consequence, we obtain the first polynomial time algorithm with approximation guarantees on worst-case instances of the robust subspace recovery problem.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16875v1">üìÑ Download PDF</a></p><hr><h3 id=m-phygs-multi-material-object-dynamics-from-videohttpsarxivorgabs251216885v1><a href=https://arxiv.org/abs/2512.16885v1>M-PhyGs: Multi-Material Object Dynamics from Video</a><a hidden class=anchor aria-hidden=true href=#m-phygs-multi-material-object-dynamics-from-videohttpsarxivorgabs251216885v1>#</a></h3><p><strong>Authors:</strong> Norika Wada, Kohei Yamashita, Ryo Kawahara, Ko Nishino
<strong>Venue:</strong> arXiv (2025)</p><p>Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16885v1">üìÑ Download PDF</a></p><hr><h3 id=a-radio-search-for-star-planet-interaction-in-toi-540-and-speculoos-3httpsarxivorgabs251216852v1><a href=https://arxiv.org/abs/2512.16852v1>A Radio Search for Star-Planet Interaction in TOI-540 and SPECULOOS-3</a><a hidden class=anchor aria-hidden=true href=#a-radio-search-for-star-planet-interaction-in-toi-540-and-speculoos-3httpsarxivorgabs251216852v1>#</a></h3><p><strong>Authors:</strong> Kevin N. Ortiz Ceballos, Yvette Cendes, Edo Berger
<strong>Venue:</strong> arXiv (2025)</p><p>We present the first targeted centimeter-band radio observations of two recently-discovered exoplanet systems that are prime candidates for magnetic star-planet interaction (SPI): TOI-540 and SPECULOOS-3. The targets were selected due to the small orbital separation of their known planets, as well as for indications of stellar magnetic activity, given that for SPI radio emission may be strongest when a sufficiently magnetized star hosts a close-in planet. The deep, multi-epoch Very Large Array (SPECULOOS-3) and MeerKAT (TOI-540) observations yield non-detections, with $3œÉ$ limits of $\lesssim 7.5$ $Œº$Jy ($4-8$ GHz) and $\lesssim 30-80$ $Œº$Jy ($0.8-1.7$ GHz), respectively. For SPECULOOS-3 b we rule out observable SPI for most of its orbit, while for TOI-540 b we sample a narrower range, around planetary transit. We model possible planetary magnetic field strength constraints for both systems, and conclude that our observations are sensitive enough to sample SPI emission in these systems if present and directed at us, even for a planetary field of only $\sim 1$ G.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16852v1">üìÑ Download PDF</a></p><hr><h3 id=on-the-edge-of-core-non-emptiness-an-automated-reasoning-approach-to-approval-based-multi-winner-votinghttpsarxivorgabs251216895v1><a href=https://arxiv.org/abs/2512.16895v1>On the Edge of Core (Non-)Emptiness: An Automated Reasoning Approach to Approval-Based Multi-Winner Voting</a><a hidden class=anchor aria-hidden=true href=#on-the-edge-of-core-non-emptiness-an-automated-reasoning-approach-to-approval-based-multi-winner-votinghttpsarxivorgabs251216895v1>#</a></h3><p><strong>Authors:</strong> Ratip Emin Berker, Emanuel Tewolde, Vincent Conitzer, Mingyu Guo, Marijn Heule, Lirong Xia
<strong>Venue:</strong> arXiv (2025)</p><p>Core stability is a natural and well-studied notion for group fairness in multi-winner voting, where the task is to select a committee from a pool of candidates. We study the setting where voters either approve or disapprove of each candidate; here, it remains a major open problem whether a core-stable committee always exists. In this work, we develop an approach based on mixed-integer linear programming for deciding whether and when core-stable committees are guaranteed to exist. In contrast to SAT-based approaches popular in computational social choice, our method can produce proofs for a specific number of candidates independent of the number of voters. In addition to these computational gains, our program lends itself to a novel duality-based reformulation of the core stability problem, from which we obtain new existence results in special cases. Further, we use our framework to reveal previously unknown relationships between core stability and other desirable properties, such as notions of priceability.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16895v1">üìÑ Download PDF</a></p><hr><h3 id=veblen-effects-and-broken-windows-in-an-environmental-olg-modelhttpsarxivorgabs251216806v1><a href=https://arxiv.org/abs/2512.16806v1>Veblen effects and broken windows in an environmental OLG model</a><a hidden class=anchor aria-hidden=true href=#veblen-effects-and-broken-windows-in-an-environmental-olg-modelhttpsarxivorgabs251216806v1>#</a></h3><p><strong>Authors:</strong> Nicol√°s Blampied, Alessia Cafferata, Marwil J. Davila-Fernandez
<strong>Venue:</strong> arXiv (2025)</p><p>Can constantly comparing ourselves to others lead to overconsumption, ultimately increasing the ecological footprint? How do social comparisons shape green preferences over time? To answer these questions, we develop an environmental Overlapping Generations (OLG) model that explicitly accounts for Veblen effects and allows green preferences to be updated asynchronously, influenced by past environmental conditions and relative status considerations. We show that, along the optimal path, positional spending leads to overconsumption, which is detrimental to the environment. Taxing consumption is counterproductive as it does not directly address the social comparisons issue, leaving the problem unchanged. When the Veblenian mechanism is weak, the introduction of a materialistic ``secular trend&rsquo;&rsquo; &ndash; that lowers the importance placed on the public good &ndash; gives rise to two stable equilibria separated by a saddle: one in which agents care about environmental quality as much as consuming, and the other in which they derive utility solely from the latter. Studying the basins of attraction reveals that green investments are highly fragile. Our numerical experiments further indicated that, when Veblen effects are strong, the model depicts endogenous, persistent, aperiodic oscillations. In this case, green preferences fluctuate close to zero, and environmental quality is very low. Taken together, these findings suggest environmental vulnerability grows in parallel with status-driven consumption.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16806v1">üìÑ Download PDF</a></p><hr><h3 id=a-survey-on-spatio-temporal-knowledge-graph-modelshttpsarxivorgabs251216487v1><a href=https://arxiv.org/abs/2512.16487v1>A Survey on Spatio-Temporal Knowledge Graph Models</a><a hidden class=anchor aria-hidden=true href=#a-survey-on-spatio-temporal-knowledge-graph-modelshttpsarxivorgabs251216487v1>#</a></h3><p><strong>Authors:</strong> Philipp Plamper, Hanna K√∂pcke, Anika Gro√ü
<strong>Venue:</strong> arXiv (2025)</p><p>Many complex real-world systems exhibit inherently intertwined temporal and spatial characteristics. Spatio-temporal knowledge graphs (STKGs) have therefore emerged as a powerful representation paradigm, as they integrate entities, relationships, time and space within a unified graph structure. They are increasingly applied across diverse domains, including environmental systems and urban, transportation, social and human mobility networks. However, modeling STKGs remains challenging: their foundations span classical graph theory as well as temporal and spatial graph models, which have evolved independently across different research communities and follow heterogeneous modeling assumptions and terminologies. As a result, existing approaches often lack conceptual alignment, generalizability and reusability. This survey provides a systematic review of spatio-temporal knowledge graph models, tracing their origins in static, temporal and spatial graph modeling. We analyze existing approaches along key modeling dimensions, including edge semantics, temporal and spatial annotation strategies, temporal and spatial semantics and relate these choices to their respective application domains. Our analysis reveals that unified modeling frameworks are largely absent and that most current models are tailored to specific use cases rather than designed for reuse or long-term knowledge preservation. Based on these findings, we derive modeling guidelines and identify open challenges to guide future research.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16487v1">üìÑ Download PDF</a></p><hr><h3 id=the-motile-josephson-array-bridging-active-turbulence-and-superconductivityhttpsarxivorgabs251216884v1><a href=https://arxiv.org/abs/2512.16884v1>The Motile Josephson Array: Bridging Active Turbulence and Superconductivity</a><a hidden class=anchor aria-hidden=true href=#the-motile-josephson-array-bridging-active-turbulence-and-superconductivityhttpsarxivorgabs251216884v1>#</a></h3><p><strong>Authors:</strong> Magnus F Ivarsen
<strong>Venue:</strong> arXiv (2025)</p><p>Recent minimalist modeling has demonstrated that overdamped polar chiral active matter can support an emergent, inviscid Euler turbulence, despite the system&rsquo;s strictly dissipative microscopic nature. In this letter, we establish the statistical mechanical foundation for the emergent inertial regime by deriving a formal isomorphism between the model&rsquo;s agent dynamics and the overdamped Langevin equation for disordered Josephson junctions. We identify the trapped agent state as a macroscopic superconducting phase governed by the Adler equation. The validity of this mapping is confirmed analytically by a disorder-broadened Adler-Ohmic crossover in the system&rsquo;s slip velocity, corresponding to the saddle-node bifurcation of phase-locking systems. These results define the new minimal chiral flocking model as a motile, disordered Josephson array, bridging active turbulence and quantum superconductivity.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.16884v1">üìÑ Download PDF</a></p><hr></div><footer class=post-footer><ul class=post-tags></ul></footer><div><div class=pagination__title><span class=pagination__title-h style=font-size:20px>Comments</span><hr></div><div id=tcomment></div><script src=https://cdn.staticfile.org/twikoo/1.6.39/twikoo.all.min.js></script><script>twikoo.init({envId:"https://mangodb-theta.vercel.app/",el:"#tcomment",lang:"en-US",path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://garyforreal.me/zh/>Gary's House</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>Views: <span id=busuanzi_value_site_pv></span>
</span><span id=busuanzi_container_site_uv>Visitors: <span id=busuanzi_value_site_uv></span></span></div></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="Â§çÂà∂";function s(){t.innerHTML="Â∑≤Â§çÂà∂ÔºÅ",setTimeout(()=>{t.innerHTML="Â§çÂà∂"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>