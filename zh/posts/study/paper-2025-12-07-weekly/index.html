<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Weekly Paper Notes - 2025-12-07 | Gary's House</title>
<meta name=keywords content><meta name=description content="Weekly Paper Notes
üîç multilingual
LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics
Authors: Weiye Shi, Zhaowei Zhang, Shaoheng Yan, Yaodong Yang
Venue: arXiv (2025)
Large language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training."><meta name=author content="Gary"><link rel=canonical href=https://garyforreal.me/zh/posts/study/paper-2025-12-07-weekly/><meta name=google-site-verification content="XYZabc"><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><link crossorigin=anonymous href=/assets/css/stylesheet.7e5251d8716d933fafcf3df70d7ecd02729661d6d232fc5dd0b8a85ef75e3409.css integrity="sha256-flJR2HFtkz+vzz33DX7NAnKWYdbSMvxd0LioXvdeNAk=" rel="preload stylesheet" as=style><link rel=icon href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=16x16 href=https://garyforreal.me/img/Q.jpg><link rel=icon type=image/png sizes=32x32 href=https://garyforreal.me/img/Q.jpg><link rel=apple-touch-icon href=https://garyforreal.me/Q.jpg><link rel=mask-icon href=https://garyforreal.me/Q.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://garyforreal.me/zh/posts/study/paper-2025-12-07-weekly/><link rel=alternate hreflang=en href=https://garyforreal.me/en/posts/study/paper-2025-12-07-weekly/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@200..900&display=swap" rel=stylesheet><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous referrerpolicy=no-referrer></script><meta property="og:title" content="Weekly Paper Notes - 2025-12-07"><meta property="og:description" content="Weekly Paper Notes
üîç multilingual
LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics
Authors: Weiye Shi, Zhaowei Zhang, Shaoheng Yan, Yaodong Yang
Venue: arXiv (2025)
Large language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training."><meta property="og:type" content="article"><meta property="og:url" content="https://garyforreal.me/zh/posts/study/paper-2025-12-07-weekly/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-07T15:21:36+00:00"><meta property="article:modified_time" content="2025-12-07T15:21:36+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Weekly Paper Notes - 2025-12-07"><meta name=twitter:description content="Weekly Paper Notes
üîç multilingual
LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics
Authors: Weiye Shi, Zhaowei Zhang, Shaoheng Yan, Yaodong Yang
Venue: arXiv (2025)
Large language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Â∏ñÂ≠ê","item":"https://garyforreal.me/zh/posts/"},{"@type":"ListItem","position":2,"name":"Â≠¶‰π†","item":"https://garyforreal.me/zh/posts/study/"},{"@type":"ListItem","position":3,"name":"Weekly Paper Notes - 2025-12-07","item":"https://garyforreal.me/zh/posts/study/paper-2025-12-07-weekly/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Weekly Paper Notes - 2025-12-07","name":"Weekly Paper Notes - 2025-12-07","description":"Weekly Paper Notes üîç multilingual LLMs Know More Than Words: A Genre Study with Syntax, Metaphor \u0026amp; Phonetics Authors: Weiye Shi, Zhaowei Zhang, Shaoheng Yan, Yaodong Yang Venue: arXiv (2025)\nLarge language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training.\n","keywords":[],"articleBody":"Weekly Paper Notes üîç multilingual LLMs Know More Than Words: A Genre Study with Syntax, Metaphor \u0026 Phonetics Authors: Weiye Shi, Zhaowei Zhang, Shaoheng Yan, Yaodong Yang Venue: arXiv (2025)\nLarge language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training.\nüì• Save to Zotero üìÑ Download PDF\nToward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases Authors: Raquel Norel, Michele Merler, Pavitra Modi Venue: arXiv (2025)\nPatients with rare neurological diseases report cognitive symptoms -‚Äúbrain fog‚Äù- invisible to traditional tests. We propose continuous neurocognitive monitoring via smartphone speech analysis integrated with Relational Graph Transformer (RELGT) architectures. Proof-of-concept in phenylketonuria (PKU) shows speech-derived ‚ÄúProficiency in Verbal Discourse‚Äù correlates with blood phenylalanine (p = -0.50, p \u003c 0.005) but not standard cognitive tests (all |r| \u003c 0.35). RELGT could overcome information bottlenecks in heterogeneous medical data (speech, labs, assessments), enabling predictive alerts weeks before decompensation. Key challenges: multi-disease validation, clinical workflow integration, equitable multilingual deployment. Success would transform episodic neurology into continuous personalized monitoring for millions globally.\nüì• Save to Zotero üìÑ Download PDF\nRRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS Authors: Cong Wang, Changfeng Gao, Yang Xiang, Zhihao Du, Keyu An, Han Zhao, Qian Chen, Xiangang Li, Yingming Gao, Ya Li Venue: arXiv (2025)\nDifferentiable reinforcement learning (RL) frameworks like DiffRO offer a powerful approach for controllable text-to-speech (TTS), but are vulnerable to reward hacking, particularly for nuanced tasks like emotion control. The policy model can exploit a vanilla Reward Model (RM) by generating acoustic artifacts to achieve spurious rewards, but at the cost of degrading perceptual quality. To address this, we propose Robust Reward Policy Optimization (RRPO), a novel framework that employs a hybrid regularization scheme. This scheme develops a robust RM whose reward signal is more reliably aligned with human perception, compelling the policy to abandon detrimental shortcuts and instead learn the complex features of genuine emotions. Our ablation study confirms the enhanced robustness of our RM, as evidenced by its strong cross-lingual generalization. The subjective evaluation demonstrates that this robust RM effectively mitigates reward hacking, leading to significant improvements in both emotional expressiveness and naturalness over all baselines. Demo page: https://lrwinr.github.io/RRPO-CosyVoice.\nüì• Save to Zotero üìÑ Download PDF\nAdapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study Authors: Lifeng Chen, Ryan Lai, Tianming Liu Venue: arXiv (2025)\nAdapting large language models (LLMs) to low-resource languages remains a major challenge due to data scarcity and cross-lingual drift. This work presents a two-stage adaptation of Qwen2.5-3B to Tibetan, a morphologically rich and underrepresented language. We employ Continual Pretraining (CPT) to establish Tibetan linguistic grounding, followed by Supervised Fine-Tuning (SFT) for task and translation specialization. Empirical evaluations demonstrate a consistent decrease in perplexity (from 2.98 $\\rightarrow$ 1.54) and substantial improvements in Chinese$\\rightarrow$Tibetan translation quality (BLEU: 0.046 $\\rightarrow$ 0.261; chrF: 2.2 $\\rightarrow$ 6.6). Layer-wise analysis across 435 layers in Qwen3-4B reveals that adaptation primarily concentrates on embedding and output heads, with mid‚Äìlate MLP projections encoding domain-specific transformations. Our findings suggest that CPT constructs a Tibetan semantic manifold while SFT sharpens task alignment with minimal representational disruption. This study provides the first quantitative exploration of Tibetan adaptation dynamics for LLMs, and offers an open, reproducible framework for extending multilingual foundation models to low-resource settings.\nüì• Save to Zotero üìÑ Download PDF\nAre LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case Authors: Vignesh Kumar Kembu, Pierandrea Morandini, Marta Bianca Maria Ranzini, Antonino Nocera Venue: arXiv (2025)\nLarge Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.\nüì• Save to Zotero üìÑ Download PDF\nHas ACL Lost Its Crown? A Decade-Long Quantitative Analysis of Scale and Impact Across Leading AI Conferences Authors: Jianglin Ma, Ben Yao, Xiang Li, Yazhou Zhang Venue: arXiv (2025)\nThe recent surge of language models has rapidly expanded NLP research, driving an exponential rise in submissions and acceptances at major conferences. Yet this growth has been shadowed by escalating concerns over conference quality, e.g., plagiarism, reviewer inexperience and collusive bidding. However, existing studies rely largely on qualitative accounts (e.g., expert interviews, social media discussions, etc.), lacking longitudinal empirical evidence. To fill this gap, we conduct a ten year empirical study spanning seven leading conferences. We build a four dimensional bibliometric framework covering conference scale, core citation statistics,impact dispersion, cross venue and journal influence, etc. Notably, we further propose a metric Quality Quantity Elasticity, which measures the elasticity of citation growth relative to acceptance growth. Our findings show that ML venues sustain dominant and stable impact, NLP venues undergo widening stratification with mixed expansion efficiency, and AI venues exhibit structural decline. This study provides the first decade-long, cross-venue empirical evidence on the evolution of major conferences.\nüì• Save to Zotero üìÑ Download PDF\nThe Decision Path to Control AI Risks Completely: Fundamental Control Mechanisms for AI Governance Authors: Yong Tao Venue: arXiv (2025)\nArtificial intelligence (AI) advances rapidly but achieving complete human control over AI risks remains an unsolved problem, akin to driving the fast AI ‚Äútrain‚Äù without a ‚Äúbrake system.‚Äù By exploring fundamental control mechanisms at key elements of AI decisions, this paper develops a systematic solution to thoroughly control AI risks, providing an architecture for AI governance and legislation with five pillars supported by six control mechanisms, illustrated through a minimum set of AI Mandates (AIMs). Three of the AIMs must be built inside AI systems and three in society to address major areas of AI risks: 1) align AI values with human users; 2) constrain AI decision-actions by societal ethics, laws, and regulations; 3) build in human intervention options for emergencies and shut-off switches for existential threats; 4) limit AI access to resources to reinforce controls inside AI; 5) mitigate spillover risks like job loss from AI. We also highlight the differences in AI governance on physical AI systems versus generative AI. We discuss how to strengthen analog physical safeguards to prevent smarter AI/AGI/ASI from circumventing core safety controls by exploiting AI‚Äôs intrinsic disconnect from the analog physical world: AI‚Äôs nature as pure software code run on chips controlled by humans, and the prerequisite that all AI-driven physical actions must be digitized. These findings establish a theoretical foundation for AI governance and legislation as the basic structure of a ‚Äúbrake system‚Äù for AI decisions. If enacted, these controls can rein in AI dangers as completely as humanly possible, removing large chunks of currently wide-open AI risks, substantially reducing overall AI risks to residual human errors.\nüì• Save to Zotero üìÑ Download PDF\nMinimizing the Number of Code Switching Operations in Fault-Tolerant Quantum Circuits Authors: Erik Weilandt, Tom Peham, Robert Wille Venue: arXiv (2025)\nFault-tolerant quantum computers rely on Quantum Error-Correcting Codes (QECCs) to protect information from noise. However, no single error-correcting code supports a fully transversal and therefore fault-tolerant implementation of all gates required for universal quantum computation. Code switching addresses this limitation by moving quantum information between different codes that, together, support a universal gate set. Unfortunately, each switch is costly-adding time and space overhead and increasing the logical error rate. Minimizing the number of switching operations is, therefore, essential for quantum computations using code switching. In this work, we study the problem of minimizing the number of code switches required to run a given quantum circuit. We show that this problem can be solved efficiently in polynomial time by reducing it to a minimum-cut instance on a graph derived from the circuit. Our formulation is flexible and can incorporate additional considerations, such as reducing depth overhead by preferring switches during idle periods or biasing the compilation to favor one code over another. To the best of our knowledge, this is the first automated approach for compiling and optimizing code-switching-based quantum computations at the logical level.\nüì• Save to Zotero üìÑ Download PDF\nMitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates Authors: Atsuki Yamaguchi, Terufumi Morishita, Aline Villavicencio, Nikolaos Aletras Venue: arXiv (2025)\nExpanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce Source-Shielded Updates (SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.\nüì• Save to Zotero üìÑ Download PDF\nDraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation Authors: Dongzhi Jiang, Renrui Zhang, Haodong Li, Zhuofan Zong, Ziyu Guo, Jun He, Claire Guo, Junyan Ye, Rongyao Fang, Weijia Li, Rui Liu, Hongsheng Li Venue: arXiv (2025)\nRecent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model‚Äôs inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.\nüì• Save to Zotero üìÑ Download PDF\nARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning Authors: Shengyuan Ding, Xinyu Fang, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiangyu Zhao, Haodong Duan, Xiaoyi Dong, Jianze Liang, Bin Wang, Conghui He, Dahua Lin, Jiaqi Wang Venue: arXiv (2025)\nReward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.\nüì• Save to Zotero üìÑ Download PDF\nThe Universal Weight Subspace Hypothesis Authors: Prakhar Kaushik, Shravan Chaudhari, Ankit Vaidya, Rama Chellappa, Alan Yuille Venue: arXiv (2025)\nWe show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.\nüì• Save to Zotero üìÑ Download PDF\nBreaking the bandwidth-efficiency trade-off in soliton microcombs via mode coupling Authors: Yang Liu, Andreas Jacobsen, Thibault Wildi, Yanjing Zhao, Chaochao Ye, Yi Zheng, Camiel Op de Beeck, Jos√© Carreira, Michael Geiselmann, Kresten Yvind, Tobias Herr, Minhao Pu Venue: arXiv (2025)\nDissipative Kerr solitons in optical microresonators have emerged as a powerful tool for compact and coherent frequency comb generation. Advances in nanofabrication have allowed precise dispersion engineering, unlocking octave-spanning soliton combs that are essential for applications such as optical atomic clocks, frequency synthesis, precision spectroscopy, and astronomical spectrometer calibration. However, a key challenge hindering their practical deployment is the intrinsic bandwidth-efficiency trade-off: achieving broadband soliton generation requires large pump detuning, which suppresses power coupling and limits pump-to-comb conversion efficiencies to only a few percent. Recent efforts using pulsed pumping or coupled-resonator architectures have improved efficiency to several tens of percent, yet their bandwidths remain below one-tenth of an octave, inadequate for applications demanding wide spectral coverage. Here, we overcome this limitation by harnessing mode interactions between spatial modes within a single microresonator. The mode hybridization creates an additional power-transfer channel that supports large pump detuning while maintaining strong pump-to-resonator coupling, enabling broadband soliton formation at substantially reduced pump power. Using this approach, we demonstrate an octave-spanning soliton microcomb with a record pump-to-comb conversion efficiency exceeding 50%. These results resolve the fundamental bandwidth-efficiency dilemma in soliton microcombs and paves the way toward fully-integrated, high-efficiency, ultrabroad comb sources for next-generation photonic systems.\nüì• Save to Zotero üìÑ Download PDF\nAnisotropic Response in Metamaterials with Elliptically Perforated Plates: Applications to Near-Field Radiative Heat Transfer Authors: J. E. P‚Äôerez-Rodr‚Äôiguez, R. Esquivel-Sirvent, A. Camacho de la Rosa Venue: arXiv (2025)\nMetamaterials with tunable optical properties provide a versatile platform for controlling electromagnetic interactions at the nanoscale. This study explores the anisotropic thermal behavior of metamaterials composed of planar plates perforated with periodic arrays of cylinders possessing elliptical cross sections. In contrast to conventional circular perforations, elliptical geometries inherently break rotational symmetry, introducing anisotropy in the effective electromagnetic and thermal response of the structure. Using a fluctuation electrodynamics framework combined with full-wave numerical simulations, we quantify the near-field radiative heat transfer between such elliptically perforated plates as a function of ellipse orientation, aspect ratio, and separation distance. The results reveal that elliptical perforations enable enhanced spectral and directional control of evanescent mode coupling and surface polariton excitation, leading to significant modulation of the near-field heat flux. These findings highlight the potential of geometrically engineered anisotropy for advanced thermal management and energy conversion applications, and offer new design strategies for the development of thermally functional metamaterials operating in the near-field regime.\nüì• Save to Zotero üìÑ Download PDF\nValue Gradient Guidance for Flow Matching Alignment Authors: Zhen Liu, Tim Z. Xiao, Carles Domingo-Enrich, Weiyang Liu, Dinghuai Zhang Venue: arXiv (2025)\nWhile methods exist for aligning flow matching models‚Äìa popular and effective class of generative models‚Äìwith human preferences, existing approaches fail to achieve both adaptation efficiency and probabilistically sound prior preservation. In this work, we leverage the theory of optimal control and propose VGG-Flow, a gradient-matching-based method for finetuning pretrained flow matching models. The key idea behind this algorithm is that the optimal difference between the finetuned velocity field and the pretrained one should be matched with the gradient field of a value function. This method not only incorporates first-order information from the reward model but also benefits from heuristic initialization of the value function to enable fast adaptation. Empirically, we show on a popular text-to-image flow matching model, Stable Diffusion 3, that our method can finetune flow matching models under limited computational budgets while achieving effective and prior-preserving alignment.\nüì• Save to Zotero üìÑ Download PDF\nPump Free Microwave-Optical Quantum Transduction Authors: Fangxin Li, Jaesung Heo, Zhaoyou Wang, Andrew P. Higginbotham, Alexander A. High, Liang Jiang Venue: arXiv (2025)\nDistributed quantum computing involves superconducting computation nodes operating at microwave frequencies, which are connected by long-distance transmission lines that transmit photons at optical frequencies. Quantum transduction, which coherently converts between microwave and optical (M-O) photons, is a critical component of such an architecture. Current approaches are hindered by the unavoidable problem of device heating due to the optical pump. In this work, we propose a pump-free scheme based on color centers that generates time-bin encoded M-O Bell pairs. Our scheme first creates spin-photon entanglement and then converts the spin state into a time-bin-encoded microwave photon using a strongly coupled Purcell-enhanced resonator. In our protocol, the microwave retrieval is heralded by detecting the microwave signal with a three-level transmon. We have analyzed the resulting Bell state fidelity and generation probability of this protocol. Our simulation shows that by combining a state-of-the-art spin-optical interface with our proposed strongly-coupled spin-microwave design, the pump-free scheme can generate M-O Bell pairs at a heralding rate exceeding one kilohertz with near-unity fidelity, which establishes the scheme as a promising source for M-O Bell pairs.\nüì• Save to Zotero üìÑ Download PDF\nüîç linguistics On the treatment of thermal effects in the equation of state on neutron star merger remnants Authors: Davide Guerra, Milton Ruiz, Michele Pasquali, Pablo Cerd√°-Dur√°n, Arnau Rios, Jos√© A. Font Venue: arXiv (2025)\nWe present results from long-term, numerical-relativity simulations of binary neutron star mergers modeled using both, fully tabulated, finite-temperature, equations of state and their corresponding hybrid representations. The simulations extend up to 150 ms which allows us to assess the role of the treatment of finite-temperature effects on the dynamics of the hypermassive neutron star remnant. Our study focuses on the analysis of the spectra of the post-merger gravitational-wave signals and on how these are affected by the treatment of thermal effects in the two EOS representations. Our simulations highlight distinct differences in the GW frequency evolution related to the thermal modeling of the EOS, demonstrating that deviations from established quasi-universal relations become significant at late post-merger phases. Furthermore, we investigate the stability of the HMNS against convection. Employing both the Ledoux criterion, necessary condition for the development of convective instabilities, and the Solberg-H√∏iland criterion, a generalized criterion for axisymmetric perturbations based on a combined analysis of the Brunt-V√§is√§l√§ frequency and of the epicyclic frequency, we show that differential rotation and thermal stratification in the HMNS give rise to local (yet sustained) convective patterns that persist beyond 100 ms after merger. Those convective patterns, while substantially different between tabulated and hybrid EOS treatments, trigger the the excitation of inertial modes with frequencies smaller than those attained by the fundamental quadrupolar mode, and are potentially within reach of third-generation GW detectors. The late-time excitation of inertial modes, previously reported in studies based on hybrid EOS, is fully supported by the tabulated, finite-temperature EOS simulations presented here, which account for thermal effects in a more consistent way.\nüì• Save to Zotero üìÑ Download PDF\nLight-X: Generative 4D Video Rendering with Camera and Illumination Control Authors: Tianqi Liu, Zhaoxi Chen, Zihao Huang, Shaocong Xu, Saining Zhang, Chongjie Ye, Bohan Li, Zhiguo Cao, Wei Li, Hao Zhao, Ziwei Liu Venue: arXiv (2025)\nRecent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.\nüì• Save to Zotero üìÑ Download PDF\nDeep infant brain segmentation from multi-contrast MRI Authors: Malte Hoffmann, Lilla Z√∂llei, Adrian V. Dalca Venue: arXiv (2025)\nSegmentation of magnetic resonance images (MRI) facilitates analysis of human brain development by delineating anatomical structures. However, in infants and young children, accurate segmentation is challenging due to development and imaging constraints. Pediatric brain MRI is notoriously difficult to acquire, with inconsistent availability of imaging modalities, substantial non-head anatomy in the field of view, and frequent motion artifacts. This has led to specialized segmentation models that are often limited to specific image types or narrow age groups, or that are fragile for more variable images such as those acquired clinically. We address this method fragmentation with BabySeg, a deep learning brain segmentation framework for infants and young children that supports diverse MRI protocols, including repeat scans and image types unavailable during training. Our approach builds on recent domain randomization techniques, which synthesize training images far beyond realistic bounds to promote dataset shift invariance. We also describe a mechanism that enables models to flexibly pool and interact features from any number of input scans. We demonstrate state-of-the-art performance that matches or exceeds the accuracy of several existing methods for various age cohorts and input configurations using a single model, in a fraction of the runtime required by many existing tools.\nüì• Save to Zotero üìÑ Download PDF\nFrom Generated Human Videos to Physically Plausible Robot Trajectories Authors: James Ni, Zekai Wang, Wei Lin, Amir Bar, Yann LeCun, Trevor Darrell, Jitendra Malik, Roei Herzig Venue: arXiv (2025)\nVideo generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.\nüì• Save to Zotero üìÑ Download PDF\nResolving the molecular gas emission of the z~2.5-2.8 starburst galaxies SPT0125-47 and SPT 2134-50 Authors: K. Kade, M. Bredberg, K. Knudsen, S. K√∂nig, G. Drouart, A. B. Romeo, T. J. L. C. Bakx Venue: arXiv (2025)\nThe comoving cosmic star formation rate density peaks at z2-3, with dusty star-forming galaxies being significant contributors to this peak. These galaxies are characterized by their high star formation rates and substantial infrared luminosities. The formation mechanisms remain an open question for these galaxies, particularly with respect to how such intense levels of star formation are triggered and maintained. We aim to resolve CO(3-2) emission toward two strongly lensed galaxies, SPT0125-47 and SPT2134-50, at z2.5-2.8 to determine their morphology and physical properties. We used high-resolution ALMA band 3 observations of CO(3-2) emission toward both sources to investigate their properties. We performed parametric and nonparametric lens modeling using the publicly available lens modeling software PyAutoLens. We divided the CO(3-2) emission line into two bins corresponding to the red and blue portions of the emission line and nonparametrically modeled the source plane emission for both bins. We performed a basic analysis of the morphology and kinematics in the source plane using nonparametric lens modeling of the red and blue bins. We found tentative evidence of a velocity gradient across both sources and no evidence of any clumpy structure, companions, or ongoing mergers. The previously calculated high star formation rates and low depletion times of both SPT0125-47 and SPT2134-50 suggest that these galaxies are undergoing a dramatic phase in their evolution. Given the lack of evidence of ongoing interactions or mergers in our source plane models, we suggest that the intense star formation was triggered by a recent interaction and/or merger. We also consider the possibility that these galaxies might be in the process of settling into disks.\nüì• Save to Zotero üìÑ Download PDF\nSEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs Authors: Hao Wang, Jialun Zhong, Changcheng Wang, Zhujun Nie, Zheng Li, Shunyu Yao, Yanzeng Li, Xinchi Li Venue: arXiv (2025)\nKnowledge-based conversational question answering (KBCQA) confronts persistent challenges in resolving coreference, modeling contextual dependencies, and executing complex logical reasoning. Existing approaches, whether end-to-end semantic parsing or stepwise agent-based reasoning, often suffer from structural inaccuracies and prohibitive computational costs, particularly when processing intricate queries over large knowledge graphs. To address these limitations, we introduce SEAL, a novel two-stage semantic parsing framework grounded in self-evolving agentic learning. In the first stage, a large language model (LLM) extracts a minimal S-expression core that captures the essential semantics of the input query. This core is then refined by an agentic calibration module, which corrects syntactic inconsistencies and aligns entities and relations precisely with the underlying knowledge graph. The second stage employs template-based completion, guided by question-type prediction and placeholder instantiation, to construct a fully executable S-expression. This decomposition not only simplifies logical form generation but also significantly enhances structural fidelity and linking efficiency. Crucially, SEAL incorporates a self-evolving mechanism that integrates local and global memory with a reflection module, enabling continuous adaptation from dialog history and execution feedback without explicit retraining. Extensive experiments on the SPICE benchmark demonstrate that SEAL achieves state-of-the-art performance, especially in multi-hop reasoning, comparison, and aggregation tasks. The results validate notable gains in both structural accuracy and computational efficiency, underscoring the framework‚Äôs capacity for robust and scalable conversational reasoning.\nüì• Save to Zotero üìÑ Download PDF\nSTARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models Authors: Feng Xu, Guangyao Zhai, Xin Kong, Tingzhong Fu, Daniel F. N. Gordon, Xueli An, Benjamin Busam Venue: arXiv (2025)\nRecent advances in Vision-Language-Action (VLA) models, powered by large language models and reinforcement learning-based fine-tuning, have shown remarkable progress in robotic manipulation. Existing methods often treat long-horizon actions as linguistic sequences and apply trajectory-level optimization methods such as Trajectory-wise Preference Optimization (TPO) or Proximal Policy Optimization (PPO), leading to coarse credit assignment and unstable training. However, unlike language, where a unified semantic meaning is preserved despite flexible sentence order, action trajectories progress through causally chained stages with different learning difficulties. This motivates progressive stage optimization. Thereby, we present Stage-Aware Reinforcement (STARE), a module that decomposes a long-horizon action trajectory into semantically meaningful stages and provides dense, interpretable, and stage-aligned reinforcement signals. Integrating STARE into TPO and PPO, we yield Stage-Aware TPO (STA-TPO) and Stage-Aware PPO (STA-PPO) for offline stage-wise preference and online intra-stage interaction, respectively. Further building on supervised fine-tuning as initialization, we propose the Imitation -\u003e Preference -\u003e Interaction (IPI), a serial fine-tuning pipeline for improving action accuracy in VLA models. Experiments on SimplerEnv and ManiSkill3 demonstrate substantial gains, achieving state-of-the-art success rates of 98.0 percent on SimplerEnv and 96.4 percent on ManiSkill3 tasks.\nüì• Save to Zotero üìÑ Download PDF\nMultimode RF Reflectometry for Spin Qubit Readout and Device Characterization Authors: Joffrey Rivard, Alexis Morel, Olivier Romain, El Bachir Ndiaye, Idris Aboubakari, Christian Lupien, Cl√©ment Godfrin, Julien Jussot, Stefan Kubicek, Kristiaan De Greve, Danny Wan, Claude Rohrbacher, Eva Dupont-Ferrier Venue: arXiv (2025)\nWe introduce a multimode superconducting inductor architecture that enables radio-frequency reflectometry at multiple discrete frequencies up to 2 GHz, addressing limitations of conventional single-mode designs. The spiral inductor‚Äôs distributed inter-turn capacitance yields distinct resonant modes with varied impedance-matching conditions. By probing a quantum dot across several modes, we extract tunneling rates over a broad frequency range and identify signatures of nearby charge defects. Using one of the higher-order modes, we demonstrate single-shot spin readout via a radio-frequency single-electron transistor (RF-SET), achieving singlet-triplet readout with an integration time of 8 us and a readout fidelity of 98%. These results establish multimode inductance as a scalable and flexible component for fast spin-qubit readout and device-quality characterization.\nüì• Save to Zotero üìÑ Download PDF\nEvolutionary Architecture Search through Grammar-Based Sequence Alignment Authors: Adri G√≥mez Mart√≠n, Felix M√∂ller, Steven McDonagh, Monica Abella, Manuel Desco, Elliot J. Crowley, Aaron Klein, Linus Ericsson Venue: arXiv (2025)\nNeural architecture search (NAS) in expressive search spaces is a computationally hard problem, but it also holds the potential to automatically discover completely novel and performant architectures. To achieve this we need effective search algorithms that can identify powerful components and reuse them in new candidate architectures. In this paper, we introduce two adapted variants of the Smith-Waterman algorithm for local sequence alignment and use them to compute the edit distance in a grammar-based evolutionary architecture search. These algorithms enable us to efficiently calculate a distance metric for neural architectures and to generate a set of hybrid offspring from two parent models. This facilitates the deployment of crossover-based search heuristics, allows us to perform a thorough analysis on the architectural loss landscape, and track population diversity during search. We highlight how our method vastly improves computational complexity over previous work and enables us to efficiently compute shortest paths between architectures. When instantiating the crossover in evolutionary searches, we achieve competitive results, outperforming competing methods. Future work can build upon this new tool, discovering novel components that can be used more broadly across neural architecture design, and broadening its applications beyond NAS.\nüì• Save to Zotero üìÑ Download PDF\nüîç psycholinguistics Anomaly cancellation and one-loop finiteness of 6D half-maximal supergravities Authors: Renata Kallosh Venue: arXiv (2025)\nWe explain why the surprising one-loop finiteness of 6D half-maximal supergravities recently discovered by Huang et al [1] is the result of the cancellation of the six-dimensional gravitational and gauge anomalies in (2,0) supergravity with 21 tensor multiplets and (1,1) supergravity with 20 vector multiplets.\nüì• Save to Zotero üìÑ Download PDF\nAn elementary approach to Wehrl-type entropy bounds in quantitative form Authors: Fabio Nicola, Federico Riccardi, Paolo Tilli Venue: arXiv (2025)\nWe consider the problem of the stability (with sharp exponent) of the Lieb‚ÄìSolovej inequality for symmetric $SU(N)$ coherent states, which was obtained only recently by the authors. Here, we propose an elementary proof of this result, based on reformulating the Wehrl-type entropy as a function defined on the unit sphere in $\\mathbb{C}^d$, for some suitable $d$, and on some explicit (and somewhat surprising) computations.\nüì• Save to Zotero üìÑ Download PDF\nPerceptually-Minimal Color Optimization for Web Accessibility: A Multi-Phase Constrained Approach Authors: Lalitha A R Venue: arXiv (2025)\nWeb accessibility guidelines require sufficient color contrast between text and backgrounds; yet, manually adjusting colors often necessitates significant visual deviation, compromising vital brand aesthetics. We present a novel, multi-phase optimization approach for automatically generating WCAG-compliant colors while minimizing perceptual change to original design choices. Our method treats this as a constrained, non-linear optimization problem, utilizing the modern perceptually uniform OKLCH color space. Crucially, the optimization is constrained to preserve the original hue ($\\text{H}$) of the color, ensuring that modifications are strictly limited to necessary adjustments in lightness ($\\text{L}$) and chroma ($\\text{C}$). This is achieved through a three-phase sequence: binary search, gradient descent, and progressive constraint relaxation. Evaluation on a dataset of 10,000 procedurally generated color pairs demonstrates that the algorithm successfully resolves accessibility violations in $77.22%$ of cases, with $88.51%$ of successful corrections exhibiting imperceptible color difference ($ŒîE_{2000} \u003c 2.0$) as defined by standard perceptibility thresholds. The median perceptual change for successful adjustments is only $0.76\\ ŒîE_{2000}$, and the algorithm achieves this with a median processing time of $0.876\\text{ms}$ per color pair. The approach demonstrates that accessibility compliance and visual design integrity can be achieved simultaneously through a computationally efficient, perceptually-aware optimization that respects brand identity. The algorithm is publicly implemented in the open-source cm-colors Python library.\nüì• Save to Zotero üìÑ Download PDF\nIntroduction to quantum control: From basic concepts to applications in quantum technologies Authors: Christiane P. Koch Venue: arXiv (2025)\nQuantum control refers to our ability to manipulate quantum systems. This tutorial-style chapter focuses on the use of classical electromagnetic fields to steer the system dynamics. In this approach, the quantum nature of the control stems solely from the underlying dynamics, through the exploitation of destructive and constructive interference to reach the control target. We first discuss two basic control principles ‚Äì coherent control which uses manipulation in frequency or time to design these interferences, and adiabatic following where access to the control target is enabled by tracking the time-dependent ground state. For complex control targets and system dynamics that exceed the scope of these basic principles, optimal control theory provides a powerful suite of tools to design the necessary protocols. A key consideration for the successful application of optimal control theory is a proper choice of the optimization functional. All concepts are illustrated using recent work from my research group, with a focus on controlling atoms and superconducting qubits. The chapter concludes with an outlook on integrating coherent control with engineered dissipation and a discussion of open questions in the field.\nüì• Save to Zotero üìÑ Download PDF\nSemantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning Authors: Purbesh Mitra, Sennur Ulukus Venue: arXiv (2025)\nLong context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programming. However, RLVR is limited by several bottlenecks, such as, lack of dense reward, and inadequate sample efficiency. As a result, it requires significant compute resources in post-training phase. To overcome these limitations, in this work, we propose \\textbf{Semantic Soft Bootstrapping (SSB)}, a self-distillation technique, in which the same base language model plays the role of both teacher and student, but receives different semantic contexts about the correctness of its outcome at training time. The model is first prompted with a math problem and several rollouts are generated. From them, the correct and most common incorrect response are filtered, and then provided to the model in context to produce a more robust, step-by-step explanation with a verified final answer. This pipeline automatically curates a paired teacher-student training set from raw problem-answer data, without any human intervention. This generation process also produces a sequence of logits, which is what the student model tries to match in the training phase just from the bare question alone. In our experiment, Qwen2.5-3B-Instruct on GSM8K dataset via parameter-efficient fine-tuning. We then tested its accuracy on MATH500, and AIME2024 benchmarks. Our experiments show a jump of 10.6%, and 10% improvements in accuracy, respectively, over group relative policy optimization (GRPO), which is a commonly used RLVR algorithm. Our code is available at https://github.com/purbeshmitra/semantic-soft-bootstrapping, and the model, curated dataset is available at https://huggingface.co/purbeshmitra/semantic-soft-bootstrapping.\nüì• Save to Zotero üìÑ Download PDF\nRevealing stimulus-dependent dynamics through statistical complexity Authors: Edson V. de Paula, Rafael M. Jungmann, Antonio J. Fontenele, Leandro A. A. Aguiar, Pedro V. Carelli, Fernanda S. Matias, Mauro Copelli, Nivaldo A. P. de Vasconcelos Venue: arXiv (2025)\nAdvances in large-scale neural recordings have expanded our ability to describe the activity of distributed brain circuits. However, understanding how neural population dynamics differ across regions and behavioral contexts remains challenging. Here, we surveyed neuronal population dynamics across multiple mouse brain areas (visual cortex, hippocampus, thalamus, and midbrain) using spike data from local ensembles. Two complementary measures were used to characterize these dynamics: the coefficient of variation (CV), a classical indicator of spike-time variability, and statistical complexity, an information-theoretic quantifier of organizational structure. To probe stimulus-dependent activity, we segmented and concatenated recordings from behavioral experiments into distinct time series corresponding to natural image presentations, blank screens during visual task, and spontaneous activity. While the CV failed to discriminate between these conditions, statistical complexity revealed clear, stimulus-specific motifs in population activity. These results indicate that information-theoretic measures can uncover structured, stimulus-dependent patterns in neural population dynamics that remain unobserved in traditional variability metrics.\nüì• Save to Zotero üìÑ Download PDF\nContact-Implicit Modeling and Simulation of a Snake Robot on Compliant and Granular Terrain Authors: Haroon Hublikar Venue: arXiv (2025)\nThis thesis presents a unified modeling and simulation framework for analyzing sidewinding and tumbling locomotion of the COBRA snake robot across rigid, compliant, and granular terrains. A contact-implicit formulation is used to model distributed frictional interactions during sidewinding, and validated through MATLAB Simscape simulations and physical experiments on rigid ground and loose sand. To capture terrain deformation effects, Project Chrono‚Äôs Soil Contact Model (SCM) is integrated with the articulated multibody dynamics, enabling prediction of slip, sinkage, and load redistribution that reduce stride efficiency on deformable substrates. For high-energy rolling locomotion on steep slopes, the Chrono DEM Engine is used to simulate particle-resolved granular interactions, revealing soil failure, intermittent lift-off, and energy dissipation mechanisms not captured by rigid models. Together, these methods span real-time control-oriented simulation and high-fidelity granular physics. Results demonstrate that rigid-ground models provide accurate short-horizon motion prediction, while continuum and particle-based terrain modeling becomes necessary for reliable mobility analysis in soft and highly dynamic environments. This work establishes a hierarchical simulation pipeline that advances robust, terrain-aware locomotion for robots operating in challenging unstructured settings.\nüì• Save to Zotero üìÑ Download PDF\nüîç llm Emergence of ER=EPR from non-local gravitational energy Authors: Kimet Jusufi, Francisco S. N. Lobo, Emmanuel N. Saridakis, Douglas Singleton Venue: arXiv (2025)\nWe construct a class of wormhole geometries supported by the non-local gravitational self-energy that regularizes the particle and black-hole sectors of spacetime. Using this framework, inspired by T-duality, we show that two entangled particles (or particle-black-hole pairs) naturally source an Einstein-Rosen-type geometry in which the required violation of the strong energy condition arises from intrinsic quantum-gravity effects rather than from ad hoc exotic matter, which is matter that violates the null energy condition. We classify the resulting wormholes, analyze their horizons, throat structure and embedding properties, and we identify the exotic energy needed at the minimal surface. Imposing the ER=EPR requirement of non-traversability and the absence of a macroscopic throat, we find that only the zero-throat geometry is compatible with an entanglement-induced Einstein-Rosen bridge, providing a concrete realization of ER=EPR within a fully regular spacetime. Finally, we briefly discuss possible implications for microscopic ER networks from vacuum fluctuations, replica-wormhole interpretations of Hawking radiation, and possible links to entanglement-driven dark-energy scenarios.\nüì• Save to Zotero üìÑ Download PDF\nShadowDraw: From Any Object to Shadow-Drawing Compositional Art Authors: Rundong Luo, Noah Snavely, Wei-Chiu Ma Venue: arXiv (2025)\nWe introduce ShadowDraw, a framework that transforms ordinary 3D objects into shadow-drawing compositional art. Given a 3D object, our system predicts scene parameters, including object pose and lighting, together with a partial line drawing, such that the cast shadow completes the drawing into a recognizable image. To this end, we optimize scene configurations to reveal meaningful shadows, employ shadow strokes to guide line drawing generation, and adopt automatic evaluation to enforce shadow-drawing coherence and visual quality. Experiments show that ShadowDraw produces compelling results across diverse inputs, from real-world scans and curated datasets to generative assets, and naturally extends to multi-object scenes, animations, and physical deployments. Our work provides a practical pipeline for creating shadow-drawing art and broadens the design space of computational visual art, bridging the gap between algorithmic design and artistic storytelling. Check out our project page https://red-fairy.github.io/ShadowDraw/ for more results and an end-to-end real-world demonstration of our pipeline!\nüì• Save to Zotero üìÑ Download PDF\nüîç neuroscience Tuning the Electronic States of Bi2Se3 Films with Large Spin-Orbit Interaction Using Molecular Heterojunctions Authors: Matthew Rogers, Craig Knox, Bryan Hickey, Lida Ansari, Farzan Gity, Timothy Moorsom, Mairi McCauley, Gilberto Teobaldi, Manuel dos Santos Dias, Hari B. Vasili, Manuel Valvidares, Mannan Ali, Gavin Burnell, Ahmet Yagmur, Satoshi Sasaki, Oscar Cespedes Venue: arXiv (2025)\nAn electric bias can shift the Fermi level along the Dirac cone of a topological insulator and modify its charge transport, but tuning the electronic states and spin-orbit interaction (SOI) without destroying the surface topology is challenging. Here, we show that thin film Bi2Se3/n-p (p-n) molecular diodes form ordered interfaces where charge transfer and orbital re-hybridisation result in a decrease (increase) of the carrier density and improved mobility. In Bi2Se3 the spin-orbit lifetime, t_so, is 0.13 ps, which is comparable to the strongest spin-orbit materials. This lifetime drops further to 0.06 ps (0.09 ps) with the addition of p-n (n-p) molecular diodes, at the limit of measurable values. This strengthened spin-orbit interaction occurs even though molecules are made of light elements and increase the mean free path of the charge carriers by almost 50%, indicating changes to the Berry curvature and/or Rashba splitting around the hybridisation points. Raman spectroscopy gives evidence that the coupling effect may be controlled by optical irradiation, opening a pathway towards the design of heavy-light element hybrids with optically tunable quantum transport.\nüì• Save to Zotero üìÑ Download PDF\nüîç data_resources TV2TV: A Unified Framework for Interleaved Language and Video Generation Authors: Xiaochuang Han, Youssef Emad, Melissa Hall, John Nguyen, Karthik Padthe, Liam Robbins, Amir Bar, Delong Chen, Michal Drozdzal, Maha Elbayad, Yushi Hu, Shang-Wen Li, Sreya Dutta Roy, Jakob Verbeek, XuDong Wang, Marjan Ghazvininejad, Luke Zettlemoyer, Emily Dinan Venue: arXiv (2025)\nVideo generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to ‚Äúthink in words‚Äù about subsequent content before ``acting in pixels‚Äô‚Äô to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model‚Äôs ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.\nüì• Save to Zotero üìÑ Download PDF\nSA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards Authors: Yuan Gao, Jin Song Venue: arXiv (2025)\nIn recent years, Image Quality Assessment (IQA) for AI-generated images (AIGI) has advanced rapidly; however, existing methods primarily target portraits and artistic images, lacking a systematic evaluation of interior scenes. We introduce Spatial Aesthetics, a paradigm that assesses the aesthetic quality of interior images along four dimensions: layout, harmony, lighting, and distortion. We construct SA-BENCH, the first benchmark for spatial aesthetics, comprising 18,000 images and 50,000 precise annotations. Employing SA-BENCH, we systematically evaluate current IQA methodologies and develop SA-IQA, through MLLM fine-tuning and a multidimensional fusion approach, as a comprehensive reward framework for assessing spatial aesthetics. We apply SA-IQA to two downstream tasks: (1) serving as a reward signal integrated with GRPO reinforcement learning to optimize the AIGC generation pipeline, and (2) Best-of-N selection to filter high-quality images and improve generation quality. Experiments indicate that SA-IQA significantly outperforms existing methods on SA-BENCH, setting a new standard for spatial aesthetics evaluation. Code and dataset will be open-sourced to advance research and applications in this domain.\nüì• Save to Zotero üìÑ Download PDF\nüîç emotion_language Geometric Data Science Authors: Olga D Anosova, Vitaliy A Kurlin Venue: arXiv (2025)\nThis book introduces the new research area of Geometric Data Science, where data can represent any real objects through geometric measurements. The first part of the book focuses on finite point sets. The most important result is a complete and continuous classification of all finite clouds of unordered points under rigid motion in any Euclidean space. The key challenge was to avoid the exponential complexity arising from permutations of the given unordered points. For a fixed dimension of the ambient Euclidean space, the times of all algorithms for the resulting invariants and distance metrics depend polynomially on the number of points. The second part of the book advances a similar classification in the much more difficult case of periodic point sets, which model all periodic crystals at the atomic scale. The most significant result is the hierarchy of invariants from the ultra-fast to complete ones. The key challenge was to resolve the discontinuity of crystal representations that break down under almost any noise. Experimental validation on all major materials databases confirmed the Crystal Isometry Principle: any real periodic crystal has a unique location in a common moduli space of all periodic structures under rigid motion. The resulting moduli space contains all known and not yet discovered periodic crystals and hence continuously extends Mendeleev‚Äôs table to the full crystal universe.\nüì• Save to Zotero üìÑ Download PDF\nHTR-ConvText: Leveraging Convolution and Textual Information for Handwritten Text Recognition Authors: Pham Thach Thanh Truc, Dang Hoai Nam, Huynh Tong Dang Khoa, Vo Nguyen Le Duy Venue: arXiv (2025)\nHandwritten Text Recognition remains challenging due to the limited data, high writing style variance, and scripts with complex diacritics. Existing approaches, though partially address these issues, often struggle to generalize without massive synthetic data. To address these challenges, we propose HTR-ConvText, a model designed to capture fine-grained, stroke-level local features while preserving global contextual dependencies. In the feature extraction stage, we integrate a residual Convolutional Neural Network backbone with a MobileViT with Positional Encoding block. This enables the model to both capture structural patterns and learn subtle writing details. We then introduce the ConvText encoder, a hybrid architecture combining global context and local features within a hierarchical structure that reduces sequence length for improved efficiency. Additionally, an auxiliary module injects textual context to mitigate the weakness of Connectionist Temporal Classification. Evaluations on IAM, READ2016, LAM and HANDS-VNOnDB demonstrate that our approach achieves improved performance and better generalization compared to existing methods, especially in scenarios with limited training samples and high handwriting diversity.\nüì• Save to Zotero üìÑ Download PDF\nSplannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting Authors: Hao-Jen Chien, Yi-Chuan Huang, Chung-Ho Wu, Wei-Lun Chao, Yu-Lun Liu Venue: arXiv (2025)\nSynthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model‚Äôs time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/\nüì• Save to Zotero üìÑ Download PDF\nHighly-ionized gas in lensed z = 6.027 Little Red Dot seen through [OIII] 88$Œº$m with ALMA Authors: Kirsten K. Knudsen, Johan Richard, Mathilde Jauzac, Tom J. L. C. Bakx, Thiago S. Goncalves, Eiichi Egami, Kiana Kade, Rahul Rana, Laura Sommovigo, Flora Stanley, Daniel P. Stark Venue: arXiv (2025)\nDetermining the physical properties of galaxies during the first billion years after the big bang is key to understanding both early galaxy evolution and how galaxies contributed to the epoch of reionization. We present deep ALMA observations of the redshifted [OIII] 88um line for the gravitationally lensed ($Œº= 11.4\\pm1.9$) galaxy A383-5.1 (z=6.027) that has previously been detected in [CII] 158um. Recent James Webb Space Telescope (JWST) imaging identified this sub-L* galaxy as a ‚Äò‚ÄòLittle Red Dot‚Äô‚Äô (LRD). With a line luminosity of $L_{\\rm [OIII]} = (1.29\\pm0.24)\\times10^8$ L$_\\odot$ (corrected for lensing magnification) A383-5.1 is one of the faintest galaxies with combined [CII] and [OIII] detections. The ALMA data reveal no dust continuum emission, consistent with previous observations. The high line luminosity ratio of [OIII]/[CII] $\\sim 14\\pm5$ is consistent with A383-5.1 being low-metallicity and dust-poor. The non-detection of dust continuum in bands 6 and 8 is consistent with the high [OIII]/[CII] ratio and suggests a presence of a strong ultraviolet radiation field, which would be less affect by dust attenuation, implying that galaxies of this type could contribute significantly to the ionization of the intergalactic medium. The presence of strong ionizing field could provide an important piece of information for understanding the nature of LRDs and their role in cosmic reionization.\nüì• Save to Zotero üìÑ Download PDF\nFactuality and Transparency Are All RAG Needs! Self-Explaining Contrastive Evidence Re-ranking Authors: Francielle Vargas, Daniel Pedronette Venue: arXiv (2025)\nThis extended abstract introduces Self-Explaining Contrastive Evidence Re-Ranking (CER), a novel method that restructures retrieval around factual evidence by fine-tuning embeddings with contrastive learning and generating token-level attribution rationales for each retrieved passage. Hard negatives are automatically selected using a subjectivity-based criterion, forcing the model to pull factual rationales closer while pushing subjective or misleading explanations apart. As a result, the method creates an embedding space explicitly aligned with evidential reasoning. We evaluated our method on clinical trial reports, and initial experimental results show that CER improves retrieval accuracy, mitigates the potential for hallucinations in RAG systems, and provides transparent, evidence-based retrieval that enhances reliability, especially in safety-critical domains.\nüì• Save to Zotero üìÑ Download PDF\nAligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models Authors: NaHyeon Park, Namin An, Kunhee Kim, Soyeon Yoon, Jiahao Huo, Hyunjung Shim Venue: arXiv (2025)\nLarge vision-language model (LVLM) based text-to-image (T2I) systems have become the dominant paradigm in image generation, yet whether they amplify social biases remains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 prompt benchmark spanning four levels of linguistic complexity and evaluate demographic bias across multiple attributes in a systematic manner. Our analysis identifies system prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis. To this end, we propose FairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-aware system prompts at test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show that FairPro substantially reduces demographic bias while preserving text-image alignment. We believe our findings provide deeper insight into the central role of system prompts in bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems.\nüì• Save to Zotero üìÑ Download PDF\n","wordCount":"9149","inLanguage":"zh","datePublished":"2025-12-07T15:21:36.292348Z","dateModified":"2025-12-07T15:21:36.292348Z","author":{"@type":"Person","name":"Gary"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://garyforreal.me/zh/posts/study/paper-2025-12-07-weekly/"},"publisher":{"@type":"Organization","name":"Gary's House","logo":{"@type":"ImageObject","url":"https://garyforreal.me/img/Q.jpg"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><script src=https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://garyforreal.me/zh/ accesskey=h title="Gary's Blog (Alt + H)"><img src=https://garyforreal.me/img/me.jpg alt aria-label=logo height=35>Gary's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://garyforreal.me/en/ title=English aria-label=English>English</a></li></ul></div></div><ul id=menu><li><a href=https://garyforreal.me/zh/search title="üîçÊêúÁ¥¢ (Alt + /)" accesskey=/><span>üîçÊêúÁ¥¢</span></a></li><li><a href=https://garyforreal.me/zh/ title=üè†‰∏ªÈ°µ><span>üè†‰∏ªÈ°µ</span></a></li><li><a href=https://garyforreal.me/zh/posts/ title=üìöÊñáÁ´†><span>üìöÊñáÁ´†</span></a></li><li><a href=https://garyforreal.me/zh/archives/ title=‚è±Â≠òÊ°£><span>‚è±Â≠òÊ°£</span></a></li><li><a href=https://garyforreal.me/zh/music/ title=üéµÈü≥‰πê><span>üéµÈü≥‰πê</span></a></li><li><a href=https://garyforreal.me/zh/about title=üôãüèª‚Äç‚ôÇÔ∏èÂÖ≥‰∫é><span>üôãüèª‚Äç‚ôÇÔ∏èÂÖ≥‰∫é</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://garyforreal.me/zh/>‰∏ªÈ°µ</a>&nbsp;¬ª&nbsp;<a href=https://garyforreal.me/zh/posts/>Â∏ñÂ≠ê</a>&nbsp;¬ª&nbsp;<a href=https://garyforreal.me/zh/posts/study/>Â≠¶‰π†</a></div><h1 class="post-title entry-hint-parent">Weekly Paper Notes - 2025-12-07</h1><div class=post-meta><span title='2025-12-07 15:21:36.292348 +0000 UTC'>2025-12-07</span>&nbsp;¬∑&nbsp;43 ÂàÜÈíü&nbsp;¬∑&nbsp;Gary&nbsp;|&nbsp;ËØ≠Ë®Ä:<ul class=i18n_list><li><a href=https://garyforreal.me/en/posts/study/paper-2025-12-07-weekly/>English</a></li></ul><div class=meta-item>&nbsp¬∑&nbsp
        <span id=busuanzi_container_page_pv>Êú¨ÊñáÈòÖËØªÈáè<span id=busuanzi_value_page_pv></span>Ê¨°</span></div></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>ÁõÆÂΩï</span></summary><div class=inner><ul><li><a href=#weekly-paper-notes aria-label="Weekly Paper Notes">Weekly Paper Notes</a><ul><li><a href=#-multilingual aria-label="üîç multilingual">üîç multilingual</a><ul><li><a href=#llms-know-more-than-words-a-genre-study-with-syntax-metaphor--phoneticshttpsarxivorgabs251204957v1 aria-label="LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics"><a href=https://arxiv.org/abs/2512.04957v1>LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics</a></a></li><li><a href=#toward-continuous-neurocognitive-monitoring-integrating-speech-ai-with-relational-graph-transformers-for-rare-neurological-diseaseshttpsarxivorgabs251204938v1 aria-label="Toward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases"><a href=https://arxiv.org/abs/2512.04938v1>Toward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases</a></a></li><li><a href=#rrpo-robust-reward-policy-optimization-for-llm-based-emotional-ttshttpsarxivorgabs251204552v1 aria-label="RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS"><a href=https://arxiv.org/abs/2512.04552v1>RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS</a></a></li><li><a href=#adapting-large-language-models-to-low-resource-tibetan-a-two-stage-continual-and-supervised-fine-tuning-studyhttpsarxivorgabs251203976v1 aria-label="Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study"><a href=https://arxiv.org/abs/2512.03976v1>Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study</a></a></li><li><a href=#are-llms-truly-multilingual-exploring-zero-shot-multilingual-capability-of-llms-for-information-retrieval-an-italian-healthcare-use-casehttpsarxivorgabs251204834v1 aria-label="Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case"><a href=https://arxiv.org/abs/2512.04834v1>Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case</a></a></li><li><a href=#has-acl-lost-its-crown-a-decade-long-quantitative-analysis-of-scale-and-impact-across-leading-ai-conferenceshttpsarxivorgabs251204448v1 aria-label="Has ACL Lost Its Crown? A Decade-Long Quantitative Analysis of Scale and Impact Across Leading AI Conferences"><a href=https://arxiv.org/abs/2512.04448v1>Has ACL Lost Its Crown? A Decade-Long Quantitative Analysis of Scale and Impact Across Leading AI Conferences</a></a></li><li><a href=#the-decision-path-to-control-ai-risks-completely-fundamental-control-mechanisms-for-ai-governancehttpsarxivorgabs251204489v1 aria-label="The Decision Path to Control AI Risks Completely: Fundamental Control Mechanisms for AI Governance"><a href=https://arxiv.org/abs/2512.04489v1>The Decision Path to Control AI Risks Completely: Fundamental Control Mechanisms for AI Governance</a></a></li><li><a href=#minimizing-the-number-of-code-switching-operations-in-fault-tolerant-quantum-circuitshttpsarxivorgabs251204170v1 aria-label="Minimizing the Number of Code Switching Operations in Fault-Tolerant Quantum Circuits"><a href=https://arxiv.org/abs/2512.04170v1>Minimizing the Number of Code Switching Operations in Fault-Tolerant Quantum Circuits</a></a></li><li><a href=#mitigating-catastrophic-forgetting-in-target-language-adaptation-of-llms-via-source-shielded-updateshttpsarxivorgabs251204844v1 aria-label="Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates"><a href=https://arxiv.org/abs/2512.04844v1>Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates</a></a></li><li><a href=#draco-draft-as-cot-for-text-to-image-preview-and-rare-concept-generationhttpsarxivorgabs251205112v1 aria-label="DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation"><a href=https://arxiv.org/abs/2512.05112v1>DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation</a></a></li><li><a href=#arm-thinker-reinforcing-multimodal-generative-reward-models-with-agentic-tool-use-and-visual-reasoninghttpsarxivorgabs251205111v1 aria-label="ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning"><a href=https://arxiv.org/abs/2512.05111v1>ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning</a></a></li><li><a href=#the-universal-weight-subspace-hypothesishttpsarxivorgabs251205117v1 aria-label="The Universal Weight Subspace Hypothesis"><a href=https://arxiv.org/abs/2512.05117v1>The Universal Weight Subspace Hypothesis</a></a></li><li><a href=#breaking-the-bandwidth-efficiency-trade-off-in-soliton-microcombs-via-mode-couplinghttpsarxivorgabs251205090v1 aria-label="Breaking the bandwidth-efficiency trade-off in soliton microcombs via mode coupling"><a href=https://arxiv.org/abs/2512.05090v1>Breaking the bandwidth-efficiency trade-off in soliton microcombs via mode coupling</a></a></li><li><a href=#anisotropic-response-in-metamaterials-with-elliptically-perforated-plates-applications-to-near-field-radiative-heat-transferhttpsarxivorgabs251205075v1 aria-label="Anisotropic Response in Metamaterials with Elliptically Perforated Plates: Applications to Near-Field Radiative Heat Transfer"><a href=https://arxiv.org/abs/2512.05075v1>Anisotropic Response in Metamaterials with Elliptically Perforated Plates: Applications to Near-Field Radiative Heat Transfer</a></a></li><li><a href=#value-gradient-guidance-for-flow-matching-alignmenthttpsarxivorgabs251205116v1 aria-label="Value Gradient Guidance for Flow Matching Alignment"><a href=https://arxiv.org/abs/2512.05116v1>Value Gradient Guidance for Flow Matching Alignment</a></a></li><li><a href=#pump-free-microwave-optical-quantum-transductionhttpsarxivorgabs251205096v1 aria-label="Pump Free Microwave-Optical Quantum Transduction"><a href=https://arxiv.org/abs/2512.05096v1>Pump Free Microwave-Optical Quantum Transduction</a></a></li></ul></li><li><a href=#-linguistics aria-label="üîç linguistics">üîç linguistics</a><ul><li><a href=#on-the-treatment-of-thermal-effects-in-the-equation-of-state-on-neutron-star-merger-remnantshttpsarxivorgabs251205118v1 aria-label="On the treatment of thermal effects in the equation of state on neutron star merger remnants"><a href=https://arxiv.org/abs/2512.05118v1>On the treatment of thermal effects in the equation of state on neutron star merger remnants</a></a></li><li><a href=#light-x-generative-4d-video-rendering-with-camera-and-illumination-controlhttpsarxivorgabs251205115v1 aria-label="Light-X: Generative 4D Video Rendering with Camera and Illumination Control"><a href=https://arxiv.org/abs/2512.05115v1>Light-X: Generative 4D Video Rendering with Camera and Illumination Control</a></a></li><li><a href=#deep-infant-brain-segmentation-from-multi-contrast-mrihttpsarxivorgabs251205114v1 aria-label="Deep infant brain segmentation from multi-contrast MRI"><a href=https://arxiv.org/abs/2512.05114v1>Deep infant brain segmentation from multi-contrast MRI</a></a></li><li><a href=#from-generated-human-videos-to-physically-plausible-robot-trajectorieshttpsarxivorgabs251205094v1 aria-label="From Generated Human Videos to Physically Plausible Robot Trajectories"><a href=https://arxiv.org/abs/2512.05094v1>From Generated Human Videos to Physically Plausible Robot Trajectories</a></a></li><li><a href=#resolving-the-molecular-gas-emission-of-the-z25-28-starburst-galaxies-spt0125-47-and-spt-2134-50httpsarxivorgabs251205093v1 aria-label="Resolving the molecular gas emission of the z~2.5-2.8 starburst galaxies SPT0125-47 and SPT 2134-50"><a href=https://arxiv.org/abs/2512.05093v1>Resolving the molecular gas emission of the z~2.5-2.8 starburst galaxies SPT0125-47 and SPT 2134-50</a></a></li><li><a href=#seal-self-evolving-agentic-learning-for-conversational-question-answering-over-knowledge-graphshttpsarxivorgabs251204868v1 aria-label="SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs"><a href=https://arxiv.org/abs/2512.04868v1>SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs</a></a></li><li><a href=#stare-vla-progressive-stage-aware-reinforcement-for-fine-tuning-vision-language-action-modelshttpsarxivorgabs251205107v1 aria-label="STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models"><a href=https://arxiv.org/abs/2512.05107v1>STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models</a></a></li><li><a href=#multimode-rf-reflectometry-for-spin-qubit-readout-and-device-characterizationhttpsarxivorgabs251205087v1 aria-label="Multimode RF Reflectometry for Spin Qubit Readout and Device Characterization"><a href=https://arxiv.org/abs/2512.05087v1>Multimode RF Reflectometry for Spin Qubit Readout and Device Characterization</a></a></li><li><a href=#evolutionary-architecture-search-through-grammar-based-sequence-alignmenthttpsarxivorgabs251204992v1 aria-label="Evolutionary Architecture Search through Grammar-Based Sequence Alignment"><a href=https://arxiv.org/abs/2512.04992v1>Evolutionary Architecture Search through Grammar-Based Sequence Alignment</a></a></li></ul></li><li><a href=#-psycholinguistics aria-label="üîç psycholinguistics">üîç psycholinguistics</a><ul><li><a href=#anomaly-cancellation-and-one-loop-finiteness-of-6d-half-maximal-supergravitieshttpsarxivorgabs251205082v1 aria-label="Anomaly cancellation and one-loop finiteness of 6D half-maximal supergravities"><a href=https://arxiv.org/abs/2512.05082v1>Anomaly cancellation and one-loop finiteness of 6D half-maximal supergravities</a></a></li><li><a href=#an-elementary-approach-to-wehrl-type-entropy-bounds-in-quantitative-formhttpsarxivorgabs251204245v1 aria-label="An elementary approach to Wehrl-type entropy bounds in quantitative form"><a href=https://arxiv.org/abs/2512.04245v1>An elementary approach to Wehrl-type entropy bounds in quantitative form</a></a></li><li><a href=#perceptually-minimal-color-optimization-for-web-accessibility-a-multi-phase-constrained-approachhttpsarxivorgabs251205067v1 aria-label="Perceptually-Minimal Color Optimization for Web Accessibility: A Multi-Phase Constrained Approach"><a href=https://arxiv.org/abs/2512.05067v1>Perceptually-Minimal Color Optimization for Web Accessibility: A Multi-Phase Constrained Approach</a></a></li><li><a href=#introduction-to-quantum-control-from-basic-concepts-to-applications-in-quantum-technologieshttpsarxivorgabs251204990v1 aria-label="Introduction to quantum control: From basic concepts to applications in quantum technologies"><a href=https://arxiv.org/abs/2512.04990v1>Introduction to quantum control: From basic concepts to applications in quantum technologies</a></a></li><li><a href=#semantic-soft-bootstrapping-long-context-reasoning-in-llms-without-reinforcement-learninghttpsarxivorgabs251205105v1 aria-label="Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning"><a href=https://arxiv.org/abs/2512.05105v1>Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning</a></a></li><li><a href=#revealing-stimulus-dependent-dynamics-through-statistical-complexityhttpsarxivorgabs251205007v1 aria-label="Revealing stimulus-dependent dynamics through statistical complexity"><a href=https://arxiv.org/abs/2512.05007v1>Revealing stimulus-dependent dynamics through statistical complexity</a></a></li><li><a href=#contact-implicit-modeling-and-simulation-of-a-snake-robot-on-compliant-and-granular-terrainhttpsarxivorgabs251205008v1 aria-label="Contact-Implicit Modeling and Simulation of a Snake Robot on Compliant and Granular Terrain"><a href=https://arxiv.org/abs/2512.05008v1>Contact-Implicit Modeling and Simulation of a Snake Robot on Compliant and Granular Terrain</a></a></li></ul></li><li><a href=#-llm aria-label="üîç llm">üîç llm</a><ul><li><a href=#emergence-of-erepr-from-non-local-gravitational-energyhttpsarxivorgabs251205022v1 aria-label="Emergence of ER=EPR from non-local gravitational energy"><a href=https://arxiv.org/abs/2512.05022v1>Emergence of ER=EPR from non-local gravitational energy</a></a></li><li><a href=#shadowdraw-from-any-object-to-shadow-drawing-compositional-arthttpsarxivorgabs251205110v1 aria-label="ShadowDraw: From Any Object to Shadow-Drawing Compositional Art"><a href=https://arxiv.org/abs/2512.05110v1>ShadowDraw: From Any Object to Shadow-Drawing Compositional Art</a></a></li></ul></li><li><a href=#-neuroscience aria-label="üîç neuroscience">üîç neuroscience</a><ul><li><a href=#tuning-the-electronic-states-of-bi2se3-films-with-large-spin-orbit-interaction-using-molecular-heterojunctionshttpsarxivorgabs251204922v1 aria-label="Tuning the Electronic States of Bi2Se3 Films with Large Spin-Orbit Interaction Using Molecular Heterojunctions"><a href=https://arxiv.org/abs/2512.04922v1>Tuning the Electronic States of Bi2Se3 Films with Large Spin-Orbit Interaction Using Molecular Heterojunctions</a></a></li></ul></li><li><a href=#-data_resources aria-label="üîç data_resources">üîç data_resources</a><ul><li><a href=#tv2tv-a-unified-framework-for-interleaved-language-and-video-generationhttpsarxivorgabs251205103v1 aria-label="TV2TV: A Unified Framework for Interleaved Language and Video Generation"><a href=https://arxiv.org/abs/2512.05103v1>TV2TV: A Unified Framework for Interleaved Language and Video Generation</a></a></li><li><a href=#sa-iqa-redefining-image-quality-assessment-for-spatial-aesthetics-with-multi-dimensional-rewardshttpsarxivorgabs251205098v1 aria-label="SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards"><a href=https://arxiv.org/abs/2512.05098v1>SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards</a></a></li></ul></li><li><a href=#-emotion_language aria-label="üîç emotion_language">üîç emotion_language</a><ul><li><a href=#geometric-data-sciencehttpsarxivorgabs251205040v1 aria-label="Geometric Data Science"><a href=https://arxiv.org/abs/2512.05040v1>Geometric Data Science</a></a></li><li><a href=#htr-convtext-leveraging-convolution-and-textual-information-for-handwritten-text-recognitionhttpsarxivorgabs251205021v1 aria-label="HTR-ConvText: Leveraging Convolution and Textual Information for Handwritten Text Recognition"><a href=https://arxiv.org/abs/2512.05021v1>HTR-ConvText: Leveraging Convolution and Textual Information for Handwritten Text Recognition</a></a></li><li><a href=#splannequin-freezing-monocular-mannequin-challenge-footage-with-dual-detection-splattinghttpsarxivorgabs251205113v1 aria-label="Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting"><a href=https://arxiv.org/abs/2512.05113v1>Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting</a></a></li><li><a href=#highly-ionized-gas-in-lensed-z--6027-little-red-dot-seen-through-oiii-88%ce%bcm-with-almahttpsarxivorgabs251205097v1 aria-label="Highly-ionized gas in lensed z = 6.027 Little Red Dot seen through [OIII] 88$Œº$m with ALMA"><a href=https://arxiv.org/abs/2512.05097v1>Highly-ionized gas in lensed z = 6.027 Little Red Dot seen through [OIII] 88$Œº$m with ALMA</a></a></li><li><a href=#factuality-and-transparency-are-all-rag-needs-self-explaining-contrastive-evidence-re-rankinghttpsarxivorgabs251205012v1 aria-label="Factuality and Transparency Are All RAG Needs! Self-Explaining Contrastive Evidence Re-ranking"><a href=https://arxiv.org/abs/2512.05012v1>Factuality and Transparency Are All RAG Needs! Self-Explaining Contrastive Evidence Re-ranking</a></a></li><li><a href=#aligned-but-stereotypical-the-hidden-influence-of-system-prompts-on-social-bias-in-lvlm-based-text-to-image-modelshttpsarxivorgabs251204981v1 aria-label="Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models"><a href=https://arxiv.org/abs/2512.04981v1>Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models</a></a></li></ul></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h1 id=weekly-paper-notes>Weekly Paper Notes<a hidden class=anchor aria-hidden=true href=#weekly-paper-notes>#</a></h1><h2 id=-multilingual>üîç multilingual<a hidden class=anchor aria-hidden=true href=#-multilingual>#</a></h2><h3 id=llms-know-more-than-words-a-genre-study-with-syntax-metaphor--phoneticshttpsarxivorgabs251204957v1><a href=https://arxiv.org/abs/2512.04957v1>LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics</a><a hidden class=anchor aria-hidden=true href=#llms-know-more-than-words-a-genre-study-with-syntax-metaphor--phoneticshttpsarxivorgabs251204957v1>#</a></h3><p><strong>Authors:</strong> Weiye Shi, Zhaowei Zhang, Shaoheng Yan, Yaodong Yang
<strong>Venue:</strong> arXiv (2025)</p><p>Large language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.04957v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.04957v1">üìÑ Download PDF</a></p><hr><h3 id=toward-continuous-neurocognitive-monitoring-integrating-speech-ai-with-relational-graph-transformers-for-rare-neurological-diseaseshttpsarxivorgabs251204938v1><a href=https://arxiv.org/abs/2512.04938v1>Toward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases</a><a hidden class=anchor aria-hidden=true href=#toward-continuous-neurocognitive-monitoring-integrating-speech-ai-with-relational-graph-transformers-for-rare-neurological-diseaseshttpsarxivorgabs251204938v1>#</a></h3><p><strong>Authors:</strong> Raquel Norel, Michele Merler, Pavitra Modi
<strong>Venue:</strong> arXiv (2025)</p><p>Patients with rare neurological diseases report cognitive symptoms -&ldquo;brain fog&rdquo;- invisible to traditional tests. We propose continuous neurocognitive monitoring via smartphone speech analysis integrated with Relational Graph Transformer (RELGT) architectures. Proof-of-concept in phenylketonuria (PKU) shows speech-derived &ldquo;Proficiency in Verbal Discourse&rdquo; correlates with blood phenylalanine (p = -0.50, p &lt; 0.005) but not standard cognitive tests (all |r| &lt; 0.35). RELGT could overcome information bottlenecks in heterogeneous medical data (speech, labs, assessments), enabling predictive alerts weeks before decompensation. Key challenges: multi-disease validation, clinical workflow integration, equitable multilingual deployment. Success would transform episodic neurology into continuous personalized monitoring for millions globally.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.04938v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.04938v1">üìÑ Download PDF</a></p><hr><h3 id=rrpo-robust-reward-policy-optimization-for-llm-based-emotional-ttshttpsarxivorgabs251204552v1><a href=https://arxiv.org/abs/2512.04552v1>RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS</a><a hidden class=anchor aria-hidden=true href=#rrpo-robust-reward-policy-optimization-for-llm-based-emotional-ttshttpsarxivorgabs251204552v1>#</a></h3><p><strong>Authors:</strong> Cong Wang, Changfeng Gao, Yang Xiang, Zhihao Du, Keyu An, Han Zhao, Qian Chen, Xiangang Li, Yingming Gao, Ya Li
<strong>Venue:</strong> arXiv (2025)</p><p>Differentiable reinforcement learning (RL) frameworks like DiffRO offer a powerful approach for controllable text-to-speech (TTS), but are vulnerable to reward hacking, particularly for nuanced tasks like emotion control. The policy model can exploit a vanilla Reward Model (RM) by generating acoustic artifacts to achieve spurious rewards, but at the cost of degrading perceptual quality. To address this, we propose Robust Reward Policy Optimization (RRPO), a novel framework that employs a hybrid regularization scheme. This scheme develops a robust RM whose reward signal is more reliably aligned with human perception, compelling the policy to abandon detrimental shortcuts and instead learn the complex features of genuine emotions. Our ablation study confirms the enhanced robustness of our RM, as evidenced by its strong cross-lingual generalization. The subjective evaluation demonstrates that this robust RM effectively mitigates reward hacking, leading to significant improvements in both emotional expressiveness and naturalness over all baselines. Demo page: <a href=https://lrwinr.github.io/RRPO-CosyVoice>https://lrwinr.github.io/RRPO-CosyVoice</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.04552v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.04552v1">üìÑ Download PDF</a></p><hr><h3 id=adapting-large-language-models-to-low-resource-tibetan-a-two-stage-continual-and-supervised-fine-tuning-studyhttpsarxivorgabs251203976v1><a href=https://arxiv.org/abs/2512.03976v1>Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study</a><a hidden class=anchor aria-hidden=true href=#adapting-large-language-models-to-low-resource-tibetan-a-two-stage-continual-and-supervised-fine-tuning-studyhttpsarxivorgabs251203976v1>#</a></h3><p><strong>Authors:</strong> Lifeng Chen, Ryan Lai, Tianming Liu
<strong>Venue:</strong> arXiv (2025)</p><p>Adapting large language models (LLMs) to low-resource languages remains a major challenge due to data scarcity and cross-lingual drift. This work presents a two-stage adaptation of Qwen2.5-3B to Tibetan, a morphologically rich and underrepresented language. We employ Continual Pretraining (CPT) to establish Tibetan linguistic grounding, followed by Supervised Fine-Tuning (SFT) for task and translation specialization. Empirical evaluations demonstrate a consistent decrease in perplexity (from 2.98 $\rightarrow$ 1.54) and substantial improvements in Chinese$\rightarrow$Tibetan translation quality (BLEU: 0.046 $\rightarrow$ 0.261; chrF: 2.2 $\rightarrow$ 6.6). Layer-wise analysis across 435 layers in Qwen3-4B reveals that adaptation primarily concentrates on embedding and output heads, with mid&ndash;late MLP projections encoding domain-specific transformations. Our findings suggest that CPT constructs a Tibetan semantic manifold while SFT sharpens task alignment with minimal representational disruption. This study provides the first quantitative exploration of Tibetan adaptation dynamics for LLMs, and offers an open, reproducible framework for extending multilingual foundation models to low-resource settings.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.03976v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.03976v1">üìÑ Download PDF</a></p><hr><h3 id=are-llms-truly-multilingual-exploring-zero-shot-multilingual-capability-of-llms-for-information-retrieval-an-italian-healthcare-use-casehttpsarxivorgabs251204834v1><a href=https://arxiv.org/abs/2512.04834v1>Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case</a><a hidden class=anchor aria-hidden=true href=#are-llms-truly-multilingual-exploring-zero-shot-multilingual-capability-of-llms-for-information-retrieval-an-italian-healthcare-use-casehttpsarxivorgabs251204834v1>#</a></h3><p><strong>Authors:</strong> Vignesh Kumar Kembu, Pierandrea Morandini, Marta Bianca Maria Ranzini, Antonino Nocera
<strong>Venue:</strong> arXiv (2025)</p><p>Large Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.04834v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.04834v1">üìÑ Download PDF</a></p><hr><h3 id=has-acl-lost-its-crown-a-decade-long-quantitative-analysis-of-scale-and-impact-across-leading-ai-conferenceshttpsarxivorgabs251204448v1><a href=https://arxiv.org/abs/2512.04448v1>Has ACL Lost Its Crown? A Decade-Long Quantitative Analysis of Scale and Impact Across Leading AI Conferences</a><a hidden class=anchor aria-hidden=true href=#has-acl-lost-its-crown-a-decade-long-quantitative-analysis-of-scale-and-impact-across-leading-ai-conferenceshttpsarxivorgabs251204448v1>#</a></h3><p><strong>Authors:</strong> Jianglin Ma, Ben Yao, Xiang Li, Yazhou Zhang
<strong>Venue:</strong> arXiv (2025)</p><p>The recent surge of language models has rapidly expanded NLP research, driving an exponential rise in submissions and acceptances at major conferences. Yet this growth has been shadowed by escalating concerns over conference quality, e.g., plagiarism, reviewer inexperience and collusive bidding. However, existing studies rely largely on qualitative accounts (e.g., expert interviews, social media discussions, etc.), lacking longitudinal empirical evidence. To fill this gap, we conduct a ten year empirical study spanning seven leading conferences. We build a four dimensional bibliometric framework covering conference scale, core citation statistics,impact dispersion, cross venue and journal influence, etc. Notably, we further propose a metric Quality Quantity Elasticity, which measures the elasticity of citation growth relative to acceptance growth. Our findings show that ML venues sustain dominant and stable impact, NLP venues undergo widening stratification with mixed expansion efficiency, and AI venues exhibit structural decline. This study provides the first decade-long, cross-venue empirical evidence on the evolution of major conferences.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.04448v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.04448v1">üìÑ Download PDF</a></p><hr><h3 id=the-decision-path-to-control-ai-risks-completely-fundamental-control-mechanisms-for-ai-governancehttpsarxivorgabs251204489v1><a href=https://arxiv.org/abs/2512.04489v1>The Decision Path to Control AI Risks Completely: Fundamental Control Mechanisms for AI Governance</a><a hidden class=anchor aria-hidden=true href=#the-decision-path-to-control-ai-risks-completely-fundamental-control-mechanisms-for-ai-governancehttpsarxivorgabs251204489v1>#</a></h3><p><strong>Authors:</strong> Yong Tao
<strong>Venue:</strong> arXiv (2025)</p><p>Artificial intelligence (AI) advances rapidly but achieving complete human control over AI risks remains an unsolved problem, akin to driving the fast AI &ldquo;train&rdquo; without a &ldquo;brake system.&rdquo; By exploring fundamental control mechanisms at key elements of AI decisions, this paper develops a systematic solution to thoroughly control AI risks, providing an architecture for AI governance and legislation with five pillars supported by six control mechanisms, illustrated through a minimum set of AI Mandates (AIMs). Three of the AIMs must be built inside AI systems and three in society to address major areas of AI risks: 1) align AI values with human users; 2) constrain AI decision-actions by societal ethics, laws, and regulations; 3) build in human intervention options for emergencies and shut-off switches for existential threats; 4) limit AI access to resources to reinforce controls inside AI; 5) mitigate spillover risks like job loss from AI. We also highlight the differences in AI governance on physical AI systems versus generative AI. We discuss how to strengthen analog physical safeguards to prevent smarter AI/AGI/ASI from circumventing core safety controls by exploiting AI&rsquo;s intrinsic disconnect from the analog physical world: AI&rsquo;s nature as pure software code run on chips controlled by humans, and the prerequisite that all AI-driven physical actions must be digitized. These findings establish a theoretical foundation for AI governance and legislation as the basic structure of a &ldquo;brake system&rdquo; for AI decisions. If enacted, these controls can rein in AI dangers as completely as humanly possible, removing large chunks of currently wide-open AI risks, substantially reducing overall AI risks to residual human errors.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.04489v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.04489v1">üìÑ Download PDF</a></p><hr><h3 id=minimizing-the-number-of-code-switching-operations-in-fault-tolerant-quantum-circuitshttpsarxivorgabs251204170v1><a href=https://arxiv.org/abs/2512.04170v1>Minimizing the Number of Code Switching Operations in Fault-Tolerant Quantum Circuits</a><a hidden class=anchor aria-hidden=true href=#minimizing-the-number-of-code-switching-operations-in-fault-tolerant-quantum-circuitshttpsarxivorgabs251204170v1>#</a></h3><p><strong>Authors:</strong> Erik Weilandt, Tom Peham, Robert Wille
<strong>Venue:</strong> arXiv (2025)</p><p>Fault-tolerant quantum computers rely on Quantum Error-Correcting Codes (QECCs) to protect information from noise. However, no single error-correcting code supports a fully transversal and therefore fault-tolerant implementation of all gates required for universal quantum computation. Code switching addresses this limitation by moving quantum information between different codes that, together, support a universal gate set. Unfortunately, each switch is costly-adding time and space overhead and increasing the logical error rate. Minimizing the number of switching operations is, therefore, essential for quantum computations using code switching. In this work, we study the problem of minimizing the number of code switches required to run a given quantum circuit. We show that this problem can be solved efficiently in polynomial time by reducing it to a minimum-cut instance on a graph derived from the circuit. Our formulation is flexible and can incorporate additional considerations, such as reducing depth overhead by preferring switches during idle periods or biasing the compilation to favor one code over another. To the best of our knowledge, this is the first automated approach for compiling and optimizing code-switching-based quantum computations at the logical level.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.04170v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.04170v1">üìÑ Download PDF</a></p><hr><h3 id=mitigating-catastrophic-forgetting-in-target-language-adaptation-of-llms-via-source-shielded-updateshttpsarxivorgabs251204844v1><a href=https://arxiv.org/abs/2512.04844v1>Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates</a><a hidden class=anchor aria-hidden=true href=#mitigating-catastrophic-forgetting-in-target-language-adaptation-of-llms-via-source-shielded-updateshttpsarxivorgabs251204844v1>#</a></h3><p><strong>Authors:</strong> Atsuki Yamaguchi, Terufumi Morishita, Aline Villavicencio, Nikolaos Aletras
<strong>Venue:</strong> arXiv (2025)</p><p>Expanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce Source-Shielded Updates (SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.04844v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.04844v1">üìÑ Download PDF</a></p><hr><h3 id=draco-draft-as-cot-for-text-to-image-preview-and-rare-concept-generationhttpsarxivorgabs251205112v1><a href=https://arxiv.org/abs/2512.05112v1>DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation</a><a hidden class=anchor aria-hidden=true href=#draco-draft-as-cot-for-text-to-image-preview-and-rare-concept-generationhttpsarxivorgabs251205112v1>#</a></h3><p><strong>Authors:</strong> Dongzhi Jiang, Renrui Zhang, Haodong Li, Zhuofan Zong, Ziyu Guo, Jun He, Claire Guo, Junyan Ye, Rongyao Fang, Weijia Li, Rui Liu, Hongsheng Li
<strong>Venue:</strong> arXiv (2025)</p><p>Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model&rsquo;s inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05112v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05112v1">üìÑ Download PDF</a></p><hr><h3 id=arm-thinker-reinforcing-multimodal-generative-reward-models-with-agentic-tool-use-and-visual-reasoninghttpsarxivorgabs251205111v1><a href=https://arxiv.org/abs/2512.05111v1>ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning</a><a hidden class=anchor aria-hidden=true href=#arm-thinker-reinforcing-multimodal-generative-reward-models-with-agentic-tool-use-and-visual-reasoninghttpsarxivorgabs251205111v1>#</a></h3><p><strong>Authors:</strong> Shengyuan Ding, Xinyu Fang, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiangyu Zhao, Haodong Duan, Xiaoyi Dong, Jianze Liang, Bin Wang, Conghui He, Dahua Lin, Jiaqi Wang
<strong>Venue:</strong> arXiv (2025)</p><p>Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05111v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05111v1">üìÑ Download PDF</a></p><hr><h3 id=the-universal-weight-subspace-hypothesishttpsarxivorgabs251205117v1><a href=https://arxiv.org/abs/2512.05117v1>The Universal Weight Subspace Hypothesis</a><a hidden class=anchor aria-hidden=true href=#the-universal-weight-subspace-hypothesishttpsarxivorgabs251205117v1>#</a></h3><p><strong>Authors:</strong> Prakhar Kaushik, Shravan Chaudhari, Ankit Vaidya, Rama Chellappa, Alan Yuille
<strong>Venue:</strong> arXiv (2025)</p><p>We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05117v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05117v1">üìÑ Download PDF</a></p><hr><h3 id=breaking-the-bandwidth-efficiency-trade-off-in-soliton-microcombs-via-mode-couplinghttpsarxivorgabs251205090v1><a href=https://arxiv.org/abs/2512.05090v1>Breaking the bandwidth-efficiency trade-off in soliton microcombs via mode coupling</a><a hidden class=anchor aria-hidden=true href=#breaking-the-bandwidth-efficiency-trade-off-in-soliton-microcombs-via-mode-couplinghttpsarxivorgabs251205090v1>#</a></h3><p><strong>Authors:</strong> Yang Liu, Andreas Jacobsen, Thibault Wildi, Yanjing Zhao, Chaochao Ye, Yi Zheng, Camiel Op de Beeck, Jos√© Carreira, Michael Geiselmann, Kresten Yvind, Tobias Herr, Minhao Pu
<strong>Venue:</strong> arXiv (2025)</p><p>Dissipative Kerr solitons in optical microresonators have emerged as a powerful tool for compact and coherent frequency comb generation. Advances in nanofabrication have allowed precise dispersion engineering, unlocking octave-spanning soliton combs that are essential for applications such as optical atomic clocks, frequency synthesis, precision spectroscopy, and astronomical spectrometer calibration. However, a key challenge hindering their practical deployment is the intrinsic bandwidth-efficiency trade-off: achieving broadband soliton generation requires large pump detuning, which suppresses power coupling and limits pump-to-comb conversion efficiencies to only a few percent. Recent efforts using pulsed pumping or coupled-resonator architectures have improved efficiency to several tens of percent, yet their bandwidths remain below one-tenth of an octave, inadequate for applications demanding wide spectral coverage. Here, we overcome this limitation by harnessing mode interactions between spatial modes within a single microresonator. The mode hybridization creates an additional power-transfer channel that supports large pump detuning while maintaining strong pump-to-resonator coupling, enabling broadband soliton formation at substantially reduced pump power. Using this approach, we demonstrate an octave-spanning soliton microcomb with a record pump-to-comb conversion efficiency exceeding 50%. These results resolve the fundamental bandwidth-efficiency dilemma in soliton microcombs and paves the way toward fully-integrated, high-efficiency, ultrabroad comb sources for next-generation photonic systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05090v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05090v1">üìÑ Download PDF</a></p><hr><h3 id=anisotropic-response-in-metamaterials-with-elliptically-perforated-plates-applications-to-near-field-radiative-heat-transferhttpsarxivorgabs251205075v1><a href=https://arxiv.org/abs/2512.05075v1>Anisotropic Response in Metamaterials with Elliptically Perforated Plates: Applications to Near-Field Radiative Heat Transfer</a><a hidden class=anchor aria-hidden=true href=#anisotropic-response-in-metamaterials-with-elliptically-perforated-plates-applications-to-near-field-radiative-heat-transferhttpsarxivorgabs251205075v1>#</a></h3><p><strong>Authors:</strong> J. E. P&rsquo;erez-Rodr&rsquo;iguez, R. Esquivel-Sirvent, A. Camacho de la Rosa
<strong>Venue:</strong> arXiv (2025)</p><p>Metamaterials with tunable optical properties provide a versatile platform for controlling electromagnetic interactions at the nanoscale. This study explores the anisotropic thermal behavior of metamaterials composed of planar plates perforated with periodic arrays of cylinders possessing elliptical cross sections. In contrast to conventional circular perforations, elliptical geometries inherently break rotational symmetry, introducing anisotropy in the effective electromagnetic and thermal response of the structure. Using a fluctuation electrodynamics framework combined with full-wave numerical simulations, we quantify the near-field radiative heat transfer between such elliptically perforated plates as a function of ellipse orientation, aspect ratio, and separation distance. The results reveal that elliptical perforations enable enhanced spectral and directional control of evanescent mode coupling and surface polariton excitation, leading to significant modulation of the near-field heat flux. These findings highlight the potential of geometrically engineered anisotropy for advanced thermal management and energy conversion applications, and offer new design strategies for the development of thermally functional metamaterials operating in the near-field regime.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05075v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05075v1">üìÑ Download PDF</a></p><hr><h3 id=value-gradient-guidance-for-flow-matching-alignmenthttpsarxivorgabs251205116v1><a href=https://arxiv.org/abs/2512.05116v1>Value Gradient Guidance for Flow Matching Alignment</a><a hidden class=anchor aria-hidden=true href=#value-gradient-guidance-for-flow-matching-alignmenthttpsarxivorgabs251205116v1>#</a></h3><p><strong>Authors:</strong> Zhen Liu, Tim Z. Xiao, Carles Domingo-Enrich, Weiyang Liu, Dinghuai Zhang
<strong>Venue:</strong> arXiv (2025)</p><p>While methods exist for aligning flow matching models&ndash;a popular and effective class of generative models&ndash;with human preferences, existing approaches fail to achieve both adaptation efficiency and probabilistically sound prior preservation. In this work, we leverage the theory of optimal control and propose VGG-Flow, a gradient-matching-based method for finetuning pretrained flow matching models. The key idea behind this algorithm is that the optimal difference between the finetuned velocity field and the pretrained one should be matched with the gradient field of a value function. This method not only incorporates first-order information from the reward model but also benefits from heuristic initialization of the value function to enable fast adaptation. Empirically, we show on a popular text-to-image flow matching model, Stable Diffusion 3, that our method can finetune flow matching models under limited computational budgets while achieving effective and prior-preserving alignment.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05116v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05116v1">üìÑ Download PDF</a></p><hr><h3 id=pump-free-microwave-optical-quantum-transductionhttpsarxivorgabs251205096v1><a href=https://arxiv.org/abs/2512.05096v1>Pump Free Microwave-Optical Quantum Transduction</a><a hidden class=anchor aria-hidden=true href=#pump-free-microwave-optical-quantum-transductionhttpsarxivorgabs251205096v1>#</a></h3><p><strong>Authors:</strong> Fangxin Li, Jaesung Heo, Zhaoyou Wang, Andrew P. Higginbotham, Alexander A. High, Liang Jiang
<strong>Venue:</strong> arXiv (2025)</p><p>Distributed quantum computing involves superconducting computation nodes operating at microwave frequencies, which are connected by long-distance transmission lines that transmit photons at optical frequencies. Quantum transduction, which coherently converts between microwave and optical (M-O) photons, is a critical component of such an architecture. Current approaches are hindered by the unavoidable problem of device heating due to the optical pump. In this work, we propose a pump-free scheme based on color centers that generates time-bin encoded M-O Bell pairs. Our scheme first creates spin-photon entanglement and then converts the spin state into a time-bin-encoded microwave photon using a strongly coupled Purcell-enhanced resonator. In our protocol, the microwave retrieval is heralded by detecting the microwave signal with a three-level transmon. We have analyzed the resulting Bell state fidelity and generation probability of this protocol. Our simulation shows that by combining a state-of-the-art spin-optical interface with our proposed strongly-coupled spin-microwave design, the pump-free scheme can generate M-O Bell pairs at a heralding rate exceeding one kilohertz with near-unity fidelity, which establishes the scheme as a promising source for M-O Bell pairs.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05096v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05096v1">üìÑ Download PDF</a></p><hr><h2 id=-linguistics>üîç linguistics<a hidden class=anchor aria-hidden=true href=#-linguistics>#</a></h2><h3 id=on-the-treatment-of-thermal-effects-in-the-equation-of-state-on-neutron-star-merger-remnantshttpsarxivorgabs251205118v1><a href=https://arxiv.org/abs/2512.05118v1>On the treatment of thermal effects in the equation of state on neutron star merger remnants</a><a hidden class=anchor aria-hidden=true href=#on-the-treatment-of-thermal-effects-in-the-equation-of-state-on-neutron-star-merger-remnantshttpsarxivorgabs251205118v1>#</a></h3><p><strong>Authors:</strong> Davide Guerra, Milton Ruiz, Michele Pasquali, Pablo Cerd√°-Dur√°n, Arnau Rios, Jos√© A. Font
<strong>Venue:</strong> arXiv (2025)</p><p>We present results from long-term, numerical-relativity simulations of binary neutron star mergers modeled using both, fully tabulated, finite-temperature, equations of state and their corresponding hybrid representations. The simulations extend up to 150 ms which allows us to assess the role of the treatment of finite-temperature effects on the dynamics of the hypermassive neutron star remnant. Our study focuses on the analysis of the spectra of the post-merger gravitational-wave signals and on how these are affected by the treatment of thermal effects in the two EOS representations. Our simulations highlight distinct differences in the GW frequency evolution related to the thermal modeling of the EOS, demonstrating that deviations from established quasi-universal relations become significant at late post-merger phases. Furthermore, we investigate the stability of the HMNS against convection. Employing both the Ledoux criterion, necessary condition for the development of convective instabilities, and the Solberg-H√∏iland criterion, a generalized criterion for axisymmetric perturbations based on a combined analysis of the Brunt-V√§is√§l√§ frequency and of the epicyclic frequency, we show that differential rotation and thermal stratification in the HMNS give rise to local (yet sustained) convective patterns that persist beyond 100 ms after merger. Those convective patterns, while substantially different between tabulated and hybrid EOS treatments, trigger the the excitation of inertial modes with frequencies smaller than those attained by the fundamental quadrupolar mode, and are potentially within reach of third-generation GW detectors. The late-time excitation of inertial modes, previously reported in studies based on hybrid EOS, is fully supported by the tabulated, finite-temperature EOS simulations presented here, which account for thermal effects in a more consistent way.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05118v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05118v1">üìÑ Download PDF</a></p><hr><h3 id=light-x-generative-4d-video-rendering-with-camera-and-illumination-controlhttpsarxivorgabs251205115v1><a href=https://arxiv.org/abs/2512.05115v1>Light-X: Generative 4D Video Rendering with Camera and Illumination Control</a><a hidden class=anchor aria-hidden=true href=#light-x-generative-4d-video-rendering-with-camera-and-illumination-controlhttpsarxivorgabs251205115v1>#</a></h3><p><strong>Authors:</strong> Tianqi Liu, Zhaoxi Chen, Zihao Huang, Shaocong Xu, Saining Zhang, Chongjie Ye, Bohan Li, Zhiguo Cao, Wei Li, Hao Zhao, Ziwei Liu
<strong>Venue:</strong> arXiv (2025)</p><p>Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05115v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05115v1">üìÑ Download PDF</a></p><hr><h3 id=deep-infant-brain-segmentation-from-multi-contrast-mrihttpsarxivorgabs251205114v1><a href=https://arxiv.org/abs/2512.05114v1>Deep infant brain segmentation from multi-contrast MRI</a><a hidden class=anchor aria-hidden=true href=#deep-infant-brain-segmentation-from-multi-contrast-mrihttpsarxivorgabs251205114v1>#</a></h3><p><strong>Authors:</strong> Malte Hoffmann, Lilla Z√∂llei, Adrian V. Dalca
<strong>Venue:</strong> arXiv (2025)</p><p>Segmentation of magnetic resonance images (MRI) facilitates analysis of human brain development by delineating anatomical structures. However, in infants and young children, accurate segmentation is challenging due to development and imaging constraints. Pediatric brain MRI is notoriously difficult to acquire, with inconsistent availability of imaging modalities, substantial non-head anatomy in the field of view, and frequent motion artifacts. This has led to specialized segmentation models that are often limited to specific image types or narrow age groups, or that are fragile for more variable images such as those acquired clinically. We address this method fragmentation with BabySeg, a deep learning brain segmentation framework for infants and young children that supports diverse MRI protocols, including repeat scans and image types unavailable during training. Our approach builds on recent domain randomization techniques, which synthesize training images far beyond realistic bounds to promote dataset shift invariance. We also describe a mechanism that enables models to flexibly pool and interact features from any number of input scans. We demonstrate state-of-the-art performance that matches or exceeds the accuracy of several existing methods for various age cohorts and input configurations using a single model, in a fraction of the runtime required by many existing tools.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05114v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05114v1">üìÑ Download PDF</a></p><hr><h3 id=from-generated-human-videos-to-physically-plausible-robot-trajectorieshttpsarxivorgabs251205094v1><a href=https://arxiv.org/abs/2512.05094v1>From Generated Human Videos to Physically Plausible Robot Trajectories</a><a hidden class=anchor aria-hidden=true href=#from-generated-human-videos-to-physically-plausible-robot-trajectorieshttpsarxivorgabs251205094v1>#</a></h3><p><strong>Authors:</strong> James Ni, Zekai Wang, Wei Lin, Amir Bar, Yann LeCun, Trevor Darrell, Jitendra Malik, Roei Herzig
<strong>Venue:</strong> arXiv (2025)</p><p>Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05094v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05094v1">üìÑ Download PDF</a></p><hr><h3 id=resolving-the-molecular-gas-emission-of-the-z25-28-starburst-galaxies-spt0125-47-and-spt-2134-50httpsarxivorgabs251205093v1><a href=https://arxiv.org/abs/2512.05093v1>Resolving the molecular gas emission of the z~2.5-2.8 starburst galaxies SPT0125-47 and SPT 2134-50</a><a hidden class=anchor aria-hidden=true href=#resolving-the-molecular-gas-emission-of-the-z25-28-starburst-galaxies-spt0125-47-and-spt-2134-50httpsarxivorgabs251205093v1>#</a></h3><p><strong>Authors:</strong> K. Kade, M. Bredberg, K. Knudsen, S. K√∂nig, G. Drouart, A. B. Romeo, T. J. L. C. Bakx
<strong>Venue:</strong> arXiv (2025)</p><p>The comoving cosmic star formation rate density peaks at z<del>2-3, with dusty star-forming galaxies being significant contributors to this peak. These galaxies are characterized by their high star formation rates and substantial infrared luminosities. The formation mechanisms remain an open question for these galaxies, particularly with respect to how such intense levels of star formation are triggered and maintained. We aim to resolve CO(3-2) emission toward two strongly lensed galaxies, SPT0125-47 and SPT2134-50, at z</del>2.5-2.8 to determine their morphology and physical properties. We used high-resolution ALMA band 3 observations of CO(3-2) emission toward both sources to investigate their properties. We performed parametric and nonparametric lens modeling using the publicly available lens modeling software PyAutoLens. We divided the CO(3-2) emission line into two bins corresponding to the red and blue portions of the emission line and nonparametrically modeled the source plane emission for both bins. We performed a basic analysis of the morphology and kinematics in the source plane using nonparametric lens modeling of the red and blue bins. We found tentative evidence of a velocity gradient across both sources and no evidence of any clumpy structure, companions, or ongoing mergers. The previously calculated high star formation rates and low depletion times of both SPT0125-47 and SPT2134-50 suggest that these galaxies are undergoing a dramatic phase in their evolution. Given the lack of evidence of ongoing interactions or mergers in our source plane models, we suggest that the intense star formation was triggered by a recent interaction and/or merger. We also consider the possibility that these galaxies might be in the process of settling into disks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05093v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05093v1">üìÑ Download PDF</a></p><hr><h3 id=seal-self-evolving-agentic-learning-for-conversational-question-answering-over-knowledge-graphshttpsarxivorgabs251204868v1><a href=https://arxiv.org/abs/2512.04868v1>SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs</a><a hidden class=anchor aria-hidden=true href=#seal-self-evolving-agentic-learning-for-conversational-question-answering-over-knowledge-graphshttpsarxivorgabs251204868v1>#</a></h3><p><strong>Authors:</strong> Hao Wang, Jialun Zhong, Changcheng Wang, Zhujun Nie, Zheng Li, Shunyu Yao, Yanzeng Li, Xinchi Li
<strong>Venue:</strong> arXiv (2025)</p><p>Knowledge-based conversational question answering (KBCQA) confronts persistent challenges in resolving coreference, modeling contextual dependencies, and executing complex logical reasoning. Existing approaches, whether end-to-end semantic parsing or stepwise agent-based reasoning, often suffer from structural inaccuracies and prohibitive computational costs, particularly when processing intricate queries over large knowledge graphs. To address these limitations, we introduce SEAL, a novel two-stage semantic parsing framework grounded in self-evolving agentic learning. In the first stage, a large language model (LLM) extracts a minimal S-expression core that captures the essential semantics of the input query. This core is then refined by an agentic calibration module, which corrects syntactic inconsistencies and aligns entities and relations precisely with the underlying knowledge graph. The second stage employs template-based completion, guided by question-type prediction and placeholder instantiation, to construct a fully executable S-expression. This decomposition not only simplifies logical form generation but also significantly enhances structural fidelity and linking efficiency. Crucially, SEAL incorporates a self-evolving mechanism that integrates local and global memory with a reflection module, enabling continuous adaptation from dialog history and execution feedback without explicit retraining. Extensive experiments on the SPICE benchmark demonstrate that SEAL achieves state-of-the-art performance, especially in multi-hop reasoning, comparison, and aggregation tasks. The results validate notable gains in both structural accuracy and computational efficiency, underscoring the framework&rsquo;s capacity for robust and scalable conversational reasoning.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.04868v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.04868v1">üìÑ Download PDF</a></p><hr><h3 id=stare-vla-progressive-stage-aware-reinforcement-for-fine-tuning-vision-language-action-modelshttpsarxivorgabs251205107v1><a href=https://arxiv.org/abs/2512.05107v1>STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models</a><a hidden class=anchor aria-hidden=true href=#stare-vla-progressive-stage-aware-reinforcement-for-fine-tuning-vision-language-action-modelshttpsarxivorgabs251205107v1>#</a></h3><p><strong>Authors:</strong> Feng Xu, Guangyao Zhai, Xin Kong, Tingzhong Fu, Daniel F. N. Gordon, Xueli An, Benjamin Busam
<strong>Venue:</strong> arXiv (2025)</p><p>Recent advances in Vision-Language-Action (VLA) models, powered by large language models and reinforcement learning-based fine-tuning, have shown remarkable progress in robotic manipulation. Existing methods often treat long-horizon actions as linguistic sequences and apply trajectory-level optimization methods such as Trajectory-wise Preference Optimization (TPO) or Proximal Policy Optimization (PPO), leading to coarse credit assignment and unstable training. However, unlike language, where a unified semantic meaning is preserved despite flexible sentence order, action trajectories progress through causally chained stages with different learning difficulties. This motivates progressive stage optimization. Thereby, we present Stage-Aware Reinforcement (STARE), a module that decomposes a long-horizon action trajectory into semantically meaningful stages and provides dense, interpretable, and stage-aligned reinforcement signals. Integrating STARE into TPO and PPO, we yield Stage-Aware TPO (STA-TPO) and Stage-Aware PPO (STA-PPO) for offline stage-wise preference and online intra-stage interaction, respectively. Further building on supervised fine-tuning as initialization, we propose the Imitation -> Preference -> Interaction (IPI), a serial fine-tuning pipeline for improving action accuracy in VLA models. Experiments on SimplerEnv and ManiSkill3 demonstrate substantial gains, achieving state-of-the-art success rates of 98.0 percent on SimplerEnv and 96.4 percent on ManiSkill3 tasks.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05107v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05107v1">üìÑ Download PDF</a></p><hr><h3 id=multimode-rf-reflectometry-for-spin-qubit-readout-and-device-characterizationhttpsarxivorgabs251205087v1><a href=https://arxiv.org/abs/2512.05087v1>Multimode RF Reflectometry for Spin Qubit Readout and Device Characterization</a><a hidden class=anchor aria-hidden=true href=#multimode-rf-reflectometry-for-spin-qubit-readout-and-device-characterizationhttpsarxivorgabs251205087v1>#</a></h3><p><strong>Authors:</strong> Joffrey Rivard, Alexis Morel, Olivier Romain, El Bachir Ndiaye, Idris Aboubakari, Christian Lupien, Cl√©ment Godfrin, Julien Jussot, Stefan Kubicek, Kristiaan De Greve, Danny Wan, Claude Rohrbacher, Eva Dupont-Ferrier
<strong>Venue:</strong> arXiv (2025)</p><p>We introduce a multimode superconducting inductor architecture that enables radio-frequency reflectometry at multiple discrete frequencies up to 2 GHz, addressing limitations of conventional single-mode designs. The spiral inductor&rsquo;s distributed inter-turn capacitance yields distinct resonant modes with varied impedance-matching conditions. By probing a quantum dot across several modes, we extract tunneling rates over a broad frequency range and identify signatures of nearby charge defects. Using one of the higher-order modes, we demonstrate single-shot spin readout via a radio-frequency single-electron transistor (RF-SET), achieving singlet-triplet readout with an integration time of 8 us and a readout fidelity of 98%. These results establish multimode inductance as a scalable and flexible component for fast spin-qubit readout and device-quality characterization.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05087v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05087v1">üìÑ Download PDF</a></p><hr><h3 id=evolutionary-architecture-search-through-grammar-based-sequence-alignmenthttpsarxivorgabs251204992v1><a href=https://arxiv.org/abs/2512.04992v1>Evolutionary Architecture Search through Grammar-Based Sequence Alignment</a><a hidden class=anchor aria-hidden=true href=#evolutionary-architecture-search-through-grammar-based-sequence-alignmenthttpsarxivorgabs251204992v1>#</a></h3><p><strong>Authors:</strong> Adri G√≥mez Mart√≠n, Felix M√∂ller, Steven McDonagh, Monica Abella, Manuel Desco, Elliot J. Crowley, Aaron Klein, Linus Ericsson
<strong>Venue:</strong> arXiv (2025)</p><p>Neural architecture search (NAS) in expressive search spaces is a computationally hard problem, but it also holds the potential to automatically discover completely novel and performant architectures. To achieve this we need effective search algorithms that can identify powerful components and reuse them in new candidate architectures. In this paper, we introduce two adapted variants of the Smith-Waterman algorithm for local sequence alignment and use them to compute the edit distance in a grammar-based evolutionary architecture search. These algorithms enable us to efficiently calculate a distance metric for neural architectures and to generate a set of hybrid offspring from two parent models. This facilitates the deployment of crossover-based search heuristics, allows us to perform a thorough analysis on the architectural loss landscape, and track population diversity during search. We highlight how our method vastly improves computational complexity over previous work and enables us to efficiently compute shortest paths between architectures. When instantiating the crossover in evolutionary searches, we achieve competitive results, outperforming competing methods. Future work can build upon this new tool, discovering novel components that can be used more broadly across neural architecture design, and broadening its applications beyond NAS.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.04992v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.04992v1">üìÑ Download PDF</a></p><hr><h2 id=-psycholinguistics>üîç psycholinguistics<a hidden class=anchor aria-hidden=true href=#-psycholinguistics>#</a></h2><h3 id=anomaly-cancellation-and-one-loop-finiteness-of-6d-half-maximal-supergravitieshttpsarxivorgabs251205082v1><a href=https://arxiv.org/abs/2512.05082v1>Anomaly cancellation and one-loop finiteness of 6D half-maximal supergravities</a><a hidden class=anchor aria-hidden=true href=#anomaly-cancellation-and-one-loop-finiteness-of-6d-half-maximal-supergravitieshttpsarxivorgabs251205082v1>#</a></h3><p><strong>Authors:</strong> Renata Kallosh
<strong>Venue:</strong> arXiv (2025)</p><p>We explain why the surprising one-loop finiteness of 6D half-maximal supergravities recently discovered by Huang et al [1] is the result of the cancellation of the six-dimensional gravitational and gauge anomalies in (2,0) supergravity with 21 tensor multiplets and (1,1) supergravity with 20 vector multiplets.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05082v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05082v1">üìÑ Download PDF</a></p><hr><h3 id=an-elementary-approach-to-wehrl-type-entropy-bounds-in-quantitative-formhttpsarxivorgabs251204245v1><a href=https://arxiv.org/abs/2512.04245v1>An elementary approach to Wehrl-type entropy bounds in quantitative form</a><a hidden class=anchor aria-hidden=true href=#an-elementary-approach-to-wehrl-type-entropy-bounds-in-quantitative-formhttpsarxivorgabs251204245v1>#</a></h3><p><strong>Authors:</strong> Fabio Nicola, Federico Riccardi, Paolo Tilli
<strong>Venue:</strong> arXiv (2025)</p><p>We consider the problem of the stability (with sharp exponent) of the Lieb&ndash;Solovej inequality for symmetric $SU(N)$ coherent states, which was obtained only recently by the authors. Here, we propose an elementary proof of this result, based on reformulating the Wehrl-type entropy as a function defined on the unit sphere in $\mathbb{C}^d$, for some suitable $d$, and on some explicit (and somewhat surprising) computations.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.04245v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.04245v1">üìÑ Download PDF</a></p><hr><h3 id=perceptually-minimal-color-optimization-for-web-accessibility-a-multi-phase-constrained-approachhttpsarxivorgabs251205067v1><a href=https://arxiv.org/abs/2512.05067v1>Perceptually-Minimal Color Optimization for Web Accessibility: A Multi-Phase Constrained Approach</a><a hidden class=anchor aria-hidden=true href=#perceptually-minimal-color-optimization-for-web-accessibility-a-multi-phase-constrained-approachhttpsarxivorgabs251205067v1>#</a></h3><p><strong>Authors:</strong> Lalitha A R
<strong>Venue:</strong> arXiv (2025)</p><p>Web accessibility guidelines require sufficient color contrast between text and backgrounds; yet, manually adjusting colors often necessitates significant visual deviation, compromising vital brand aesthetics. We present a novel, multi-phase optimization approach for automatically generating WCAG-compliant colors while minimizing perceptual change to original design choices.
Our method treats this as a constrained, non-linear optimization problem, utilizing the modern perceptually uniform OKLCH color space. Crucially, the optimization is constrained to preserve the original hue ($\text{H}$) of the color, ensuring that modifications are strictly limited to necessary adjustments in lightness ($\text{L}$) and chroma ($\text{C}$). This is achieved through a three-phase sequence: binary search, gradient descent, and progressive constraint relaxation.
Evaluation on a dataset of 10,000 procedurally generated color pairs demonstrates that the algorithm successfully resolves accessibility violations in $77.22%$ of cases, with $88.51%$ of successful corrections exhibiting imperceptible color difference ($ŒîE_{2000} &lt; 2.0$) as defined by standard perceptibility thresholds. The median perceptual change for successful adjustments is only $0.76\ ŒîE_{2000}$, and the algorithm achieves this with a median processing time of $0.876\text{ms}$ per color pair.
The approach demonstrates that accessibility compliance and visual design integrity can be achieved simultaneously through a computationally efficient, perceptually-aware optimization that respects brand identity. The algorithm is publicly implemented in the open-source cm-colors Python library.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05067v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05067v1">üìÑ Download PDF</a></p><hr><h3 id=introduction-to-quantum-control-from-basic-concepts-to-applications-in-quantum-technologieshttpsarxivorgabs251204990v1><a href=https://arxiv.org/abs/2512.04990v1>Introduction to quantum control: From basic concepts to applications in quantum technologies</a><a hidden class=anchor aria-hidden=true href=#introduction-to-quantum-control-from-basic-concepts-to-applications-in-quantum-technologieshttpsarxivorgabs251204990v1>#</a></h3><p><strong>Authors:</strong> Christiane P. Koch
<strong>Venue:</strong> arXiv (2025)</p><p>Quantum control refers to our ability to manipulate quantum systems. This tutorial-style chapter focuses on the use of classical electromagnetic fields to steer the system dynamics. In this approach, the quantum nature of the control stems solely from the underlying dynamics, through the exploitation of destructive and constructive interference to reach the control target. We first discuss two basic control principles &ndash; coherent control which uses manipulation in frequency or time to design these interferences, and adiabatic following where access to the control target is enabled by tracking the time-dependent ground state. For complex control targets and system dynamics that exceed the scope of these basic principles, optimal control theory provides a powerful suite of tools to design the necessary protocols. A key consideration for the successful application of optimal control theory is a proper choice of the optimization functional. All concepts are illustrated using recent work from my research group, with a focus on controlling atoms and superconducting qubits. The chapter concludes with an outlook on integrating coherent control with engineered dissipation and a discussion of open questions in the field.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.04990v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.04990v1">üìÑ Download PDF</a></p><hr><h3 id=semantic-soft-bootstrapping-long-context-reasoning-in-llms-without-reinforcement-learninghttpsarxivorgabs251205105v1><a href=https://arxiv.org/abs/2512.05105v1>Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning</a><a hidden class=anchor aria-hidden=true href=#semantic-soft-bootstrapping-long-context-reasoning-in-llms-without-reinforcement-learninghttpsarxivorgabs251205105v1>#</a></h3><p><strong>Authors:</strong> Purbesh Mitra, Sennur Ulukus
<strong>Venue:</strong> arXiv (2025)</p><p>Long context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programming. However, RLVR is limited by several bottlenecks, such as, lack of dense reward, and inadequate sample efficiency. As a result, it requires significant compute resources in post-training phase. To overcome these limitations, in this work, we propose \textbf{Semantic Soft Bootstrapping (SSB)}, a self-distillation technique, in which the same base language model plays the role of both teacher and student, but receives different semantic contexts about the correctness of its outcome at training time. The model is first prompted with a math problem and several rollouts are generated. From them, the correct and most common incorrect response are filtered, and then provided to the model in context to produce a more robust, step-by-step explanation with a verified final answer. This pipeline automatically curates a paired teacher-student training set from raw problem-answer data, without any human intervention. This generation process also produces a sequence of logits, which is what the student model tries to match in the training phase just from the bare question alone. In our experiment, Qwen2.5-3B-Instruct on GSM8K dataset via parameter-efficient fine-tuning. We then tested its accuracy on MATH500, and AIME2024 benchmarks. Our experiments show a jump of 10.6%, and 10% improvements in accuracy, respectively, over group relative policy optimization (GRPO), which is a commonly used RLVR algorithm. Our code is available at <a href=https://github.com/purbeshmitra/semantic-soft-bootstrapping>https://github.com/purbeshmitra/semantic-soft-bootstrapping</a>, and the model, curated dataset is available at <a href=https://huggingface.co/purbeshmitra/semantic-soft-bootstrapping>https://huggingface.co/purbeshmitra/semantic-soft-bootstrapping</a>.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05105v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05105v1">üìÑ Download PDF</a></p><hr><h3 id=revealing-stimulus-dependent-dynamics-through-statistical-complexityhttpsarxivorgabs251205007v1><a href=https://arxiv.org/abs/2512.05007v1>Revealing stimulus-dependent dynamics through statistical complexity</a><a hidden class=anchor aria-hidden=true href=#revealing-stimulus-dependent-dynamics-through-statistical-complexityhttpsarxivorgabs251205007v1>#</a></h3><p><strong>Authors:</strong> Edson V. de Paula, Rafael M. Jungmann, Antonio J. Fontenele, Leandro A. A. Aguiar, Pedro V. Carelli, Fernanda S. Matias, Mauro Copelli, Nivaldo A. P. de Vasconcelos
<strong>Venue:</strong> arXiv (2025)</p><p>Advances in large-scale neural recordings have expanded our ability to describe the activity of distributed brain circuits. However, understanding how neural population dynamics differ across regions and behavioral contexts remains challenging. Here, we surveyed neuronal population dynamics across multiple mouse brain areas (visual cortex, hippocampus, thalamus, and midbrain) using spike data from local ensembles. Two complementary measures were used to characterize these dynamics: the coefficient of variation (CV), a classical indicator of spike-time variability, and statistical complexity, an information-theoretic quantifier of organizational structure. To probe stimulus-dependent activity, we segmented and concatenated recordings from behavioral experiments into distinct time series corresponding to natural image presentations, blank screens during visual task, and spontaneous activity. While the CV failed to discriminate between these conditions, statistical complexity revealed clear, stimulus-specific motifs in population activity. These results indicate that information-theoretic measures can uncover structured, stimulus-dependent patterns in neural population dynamics that remain unobserved in traditional variability metrics.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05007v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05007v1">üìÑ Download PDF</a></p><hr><h3 id=contact-implicit-modeling-and-simulation-of-a-snake-robot-on-compliant-and-granular-terrainhttpsarxivorgabs251205008v1><a href=https://arxiv.org/abs/2512.05008v1>Contact-Implicit Modeling and Simulation of a Snake Robot on Compliant and Granular Terrain</a><a hidden class=anchor aria-hidden=true href=#contact-implicit-modeling-and-simulation-of-a-snake-robot-on-compliant-and-granular-terrainhttpsarxivorgabs251205008v1>#</a></h3><p><strong>Authors:</strong> Haroon Hublikar
<strong>Venue:</strong> arXiv (2025)</p><p>This thesis presents a unified modeling and simulation framework for analyzing sidewinding and tumbling locomotion of the COBRA snake robot across rigid, compliant, and granular terrains. A contact-implicit formulation is used to model distributed frictional interactions during sidewinding, and validated through MATLAB Simscape simulations and physical experiments on rigid ground and loose sand. To capture terrain deformation effects, Project Chrono&rsquo;s Soil Contact Model (SCM) is integrated with the articulated multibody dynamics, enabling prediction of slip, sinkage, and load redistribution that reduce stride efficiency on deformable substrates. For high-energy rolling locomotion on steep slopes, the Chrono DEM Engine is used to simulate particle-resolved granular interactions, revealing soil failure, intermittent lift-off, and energy dissipation mechanisms not captured by rigid models. Together, these methods span real-time control-oriented simulation and high-fidelity granular physics. Results demonstrate that rigid-ground models provide accurate short-horizon motion prediction, while continuum and particle-based terrain modeling becomes necessary for reliable mobility analysis in soft and highly dynamic environments. This work establishes a hierarchical simulation pipeline that advances robust, terrain-aware locomotion for robots operating in challenging unstructured settings.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05008v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05008v1">üìÑ Download PDF</a></p><hr><h2 id=-llm>üîç llm<a hidden class=anchor aria-hidden=true href=#-llm>#</a></h2><h3 id=emergence-of-erepr-from-non-local-gravitational-energyhttpsarxivorgabs251205022v1><a href=https://arxiv.org/abs/2512.05022v1>Emergence of ER=EPR from non-local gravitational energy</a><a hidden class=anchor aria-hidden=true href=#emergence-of-erepr-from-non-local-gravitational-energyhttpsarxivorgabs251205022v1>#</a></h3><p><strong>Authors:</strong> Kimet Jusufi, Francisco S. N. Lobo, Emmanuel N. Saridakis, Douglas Singleton
<strong>Venue:</strong> arXiv (2025)</p><p>We construct a class of wormhole geometries supported by the non-local gravitational self-energy that regularizes the particle and black-hole sectors of spacetime. Using this framework, inspired by T-duality, we show that two entangled particles (or particle-black-hole pairs) naturally source an Einstein-Rosen-type geometry in which the required violation of the strong energy condition arises from intrinsic quantum-gravity effects rather than from ad hoc exotic matter, which is matter that violates the null energy condition. We classify the resulting wormholes, analyze their horizons, throat structure and embedding properties, and we identify the exotic energy needed at the minimal surface. Imposing the ER=EPR requirement of non-traversability and the absence of a macroscopic throat, we find that only the zero-throat geometry is compatible with an entanglement-induced Einstein-Rosen bridge, providing a concrete realization of ER=EPR within a fully regular spacetime. Finally, we briefly discuss possible implications for microscopic ER networks from vacuum fluctuations, replica-wormhole interpretations of Hawking radiation, and possible links to entanglement-driven dark-energy scenarios.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05022v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05022v1">üìÑ Download PDF</a></p><hr><h3 id=shadowdraw-from-any-object-to-shadow-drawing-compositional-arthttpsarxivorgabs251205110v1><a href=https://arxiv.org/abs/2512.05110v1>ShadowDraw: From Any Object to Shadow-Drawing Compositional Art</a><a hidden class=anchor aria-hidden=true href=#shadowdraw-from-any-object-to-shadow-drawing-compositional-arthttpsarxivorgabs251205110v1>#</a></h3><p><strong>Authors:</strong> Rundong Luo, Noah Snavely, Wei-Chiu Ma
<strong>Venue:</strong> arXiv (2025)</p><p>We introduce ShadowDraw, a framework that transforms ordinary 3D objects into shadow-drawing compositional art. Given a 3D object, our system predicts scene parameters, including object pose and lighting, together with a partial line drawing, such that the cast shadow completes the drawing into a recognizable image. To this end, we optimize scene configurations to reveal meaningful shadows, employ shadow strokes to guide line drawing generation, and adopt automatic evaluation to enforce shadow-drawing coherence and visual quality. Experiments show that ShadowDraw produces compelling results across diverse inputs, from real-world scans and curated datasets to generative assets, and naturally extends to multi-object scenes, animations, and physical deployments. Our work provides a practical pipeline for creating shadow-drawing art and broadens the design space of computational visual art, bridging the gap between algorithmic design and artistic storytelling. Check out our project page <a href=https://red-fairy.github.io/ShadowDraw/>https://red-fairy.github.io/ShadowDraw/</a> for more results and an end-to-end real-world demonstration of our pipeline!</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05110v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05110v1">üìÑ Download PDF</a></p><hr><h2 id=-neuroscience>üîç neuroscience<a hidden class=anchor aria-hidden=true href=#-neuroscience>#</a></h2><h3 id=tuning-the-electronic-states-of-bi2se3-films-with-large-spin-orbit-interaction-using-molecular-heterojunctionshttpsarxivorgabs251204922v1><a href=https://arxiv.org/abs/2512.04922v1>Tuning the Electronic States of Bi2Se3 Films with Large Spin-Orbit Interaction Using Molecular Heterojunctions</a><a hidden class=anchor aria-hidden=true href=#tuning-the-electronic-states-of-bi2se3-films-with-large-spin-orbit-interaction-using-molecular-heterojunctionshttpsarxivorgabs251204922v1>#</a></h3><p><strong>Authors:</strong> Matthew Rogers, Craig Knox, Bryan Hickey, Lida Ansari, Farzan Gity, Timothy Moorsom, Mairi McCauley, Gilberto Teobaldi, Manuel dos Santos Dias, Hari B. Vasili, Manuel Valvidares, Mannan Ali, Gavin Burnell, Ahmet Yagmur, Satoshi Sasaki, Oscar Cespedes
<strong>Venue:</strong> arXiv (2025)</p><p>An electric bias can shift the Fermi level along the Dirac cone of a topological insulator and modify its charge transport, but tuning the electronic states and spin-orbit interaction (SOI) without destroying the surface topology is challenging. Here, we show that thin film Bi2Se3/n-p (p-n) molecular diodes form ordered interfaces where charge transfer and orbital re-hybridisation result in a decrease (increase) of the carrier density and improved mobility. In Bi2Se3 the spin-orbit lifetime, t_so, is 0.13 ps, which is comparable to the strongest spin-orbit materials. This lifetime drops further to 0.06 ps (0.09 ps) with the addition of p-n (n-p) molecular diodes, at the limit of measurable values. This strengthened spin-orbit interaction occurs even though molecules are made of light elements and increase the mean free path of the charge carriers by almost 50%, indicating changes to the Berry curvature and/or Rashba splitting around the hybridisation points. Raman spectroscopy gives evidence that the coupling effect may be controlled by optical irradiation, opening a pathway towards the design of heavy-light element hybrids with optically tunable quantum transport.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.04922v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.04922v1">üìÑ Download PDF</a></p><hr><h2 id=-data_resources>üîç data_resources<a hidden class=anchor aria-hidden=true href=#-data_resources>#</a></h2><h3 id=tv2tv-a-unified-framework-for-interleaved-language-and-video-generationhttpsarxivorgabs251205103v1><a href=https://arxiv.org/abs/2512.05103v1>TV2TV: A Unified Framework for Interleaved Language and Video Generation</a><a hidden class=anchor aria-hidden=true href=#tv2tv-a-unified-framework-for-interleaved-language-and-video-generationhttpsarxivorgabs251205103v1>#</a></h3><p><strong>Authors:</strong> Xiaochuang Han, Youssef Emad, Melissa Hall, John Nguyen, Karthik Padthe, Liam Robbins, Amir Bar, Delong Chen, Michal Drozdzal, Maha Elbayad, Yushi Hu, Shang-Wen Li, Sreya Dutta Roy, Jakob Verbeek, XuDong Wang, Marjan Ghazvininejad, Luke Zettlemoyer, Emily Dinan
<strong>Venue:</strong> arXiv (2025)</p><p>Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to &ldquo;think in words&rdquo; about subsequent content before ``acting in pixels&rsquo;&rsquo; to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model&rsquo;s ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05103v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05103v1">üìÑ Download PDF</a></p><hr><h3 id=sa-iqa-redefining-image-quality-assessment-for-spatial-aesthetics-with-multi-dimensional-rewardshttpsarxivorgabs251205098v1><a href=https://arxiv.org/abs/2512.05098v1>SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards</a><a hidden class=anchor aria-hidden=true href=#sa-iqa-redefining-image-quality-assessment-for-spatial-aesthetics-with-multi-dimensional-rewardshttpsarxivorgabs251205098v1>#</a></h3><p><strong>Authors:</strong> Yuan Gao, Jin Song
<strong>Venue:</strong> arXiv (2025)</p><p>In recent years, Image Quality Assessment (IQA) for AI-generated images (AIGI) has advanced rapidly; however, existing methods primarily target portraits and artistic images, lacking a systematic evaluation of interior scenes. We introduce Spatial Aesthetics, a paradigm that assesses the aesthetic quality of interior images along four dimensions: layout, harmony, lighting, and distortion. We construct SA-BENCH, the first benchmark for spatial aesthetics, comprising 18,000 images and 50,000 precise annotations. Employing SA-BENCH, we systematically evaluate current IQA methodologies and develop SA-IQA, through MLLM fine-tuning and a multidimensional fusion approach, as a comprehensive reward framework for assessing spatial aesthetics. We apply SA-IQA to two downstream tasks: (1) serving as a reward signal integrated with GRPO reinforcement learning to optimize the AIGC generation pipeline, and (2) Best-of-N selection to filter high-quality images and improve generation quality. Experiments indicate that SA-IQA significantly outperforms existing methods on SA-BENCH, setting a new standard for spatial aesthetics evaluation. Code and dataset will be open-sourced to advance research and applications in this domain.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05098v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05098v1">üìÑ Download PDF</a></p><hr><h2 id=-emotion_language>üîç emotion_language<a hidden class=anchor aria-hidden=true href=#-emotion_language>#</a></h2><h3 id=geometric-data-sciencehttpsarxivorgabs251205040v1><a href=https://arxiv.org/abs/2512.05040v1>Geometric Data Science</a><a hidden class=anchor aria-hidden=true href=#geometric-data-sciencehttpsarxivorgabs251205040v1>#</a></h3><p><strong>Authors:</strong> Olga D Anosova, Vitaliy A Kurlin
<strong>Venue:</strong> arXiv (2025)</p><p>This book introduces the new research area of Geometric Data Science, where data can represent any real objects through geometric measurements.
The first part of the book focuses on finite point sets. The most important result is a complete and continuous classification of all finite clouds of unordered points under rigid motion in any Euclidean space. The key challenge was to avoid the exponential complexity arising from permutations of the given unordered points. For a fixed dimension of the ambient Euclidean space, the times of all algorithms for the resulting invariants and distance metrics depend polynomially on the number of points.
The second part of the book advances a similar classification in the much more difficult case of periodic point sets, which model all periodic crystals at the atomic scale. The most significant result is the hierarchy of invariants from the ultra-fast to complete ones. The key challenge was to resolve the discontinuity of crystal representations that break down under almost any noise. Experimental validation on all major materials databases confirmed the Crystal Isometry Principle: any real periodic crystal has a unique location in a common moduli space of all periodic structures under rigid motion. The resulting moduli space contains all known and not yet discovered periodic crystals and hence continuously extends Mendeleev&rsquo;s table to the full crystal universe.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05040v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05040v1">üìÑ Download PDF</a></p><hr><h3 id=htr-convtext-leveraging-convolution-and-textual-information-for-handwritten-text-recognitionhttpsarxivorgabs251205021v1><a href=https://arxiv.org/abs/2512.05021v1>HTR-ConvText: Leveraging Convolution and Textual Information for Handwritten Text Recognition</a><a hidden class=anchor aria-hidden=true href=#htr-convtext-leveraging-convolution-and-textual-information-for-handwritten-text-recognitionhttpsarxivorgabs251205021v1>#</a></h3><p><strong>Authors:</strong> Pham Thach Thanh Truc, Dang Hoai Nam, Huynh Tong Dang Khoa, Vo Nguyen Le Duy
<strong>Venue:</strong> arXiv (2025)</p><p>Handwritten Text Recognition remains challenging due to the limited data, high writing style variance, and scripts with complex diacritics. Existing approaches, though partially address these issues, often struggle to generalize without massive synthetic data. To address these challenges, we propose HTR-ConvText, a model designed to capture fine-grained, stroke-level local features while preserving global contextual dependencies. In the feature extraction stage, we integrate a residual Convolutional Neural Network backbone with a MobileViT with Positional Encoding block. This enables the model to both capture structural patterns and learn subtle writing details. We then introduce the ConvText encoder, a hybrid architecture combining global context and local features within a hierarchical structure that reduces sequence length for improved efficiency. Additionally, an auxiliary module injects textual context to mitigate the weakness of Connectionist Temporal Classification. Evaluations on IAM, READ2016, LAM and HANDS-VNOnDB demonstrate that our approach achieves improved performance and better generalization compared to existing methods, especially in scenarios with limited training samples and high handwriting diversity.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05021v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05021v1">üìÑ Download PDF</a></p><hr><h3 id=splannequin-freezing-monocular-mannequin-challenge-footage-with-dual-detection-splattinghttpsarxivorgabs251205113v1><a href=https://arxiv.org/abs/2512.05113v1>Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting</a><a hidden class=anchor aria-hidden=true href=#splannequin-freezing-monocular-mannequin-challenge-footage-with-dual-detection-splattinghttpsarxivorgabs251205113v1>#</a></h3><p><strong>Authors:</strong> Hao-Jen Chien, Yi-Chuan Huang, Chung-Ho Wu, Wei-Lun Chao, Yu-Lun Liu
<strong>Venue:</strong> arXiv (2025)</p><p>Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model&rsquo;s time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: <a href=https://chien90190.github.io/splannequin/>https://chien90190.github.io/splannequin/</a></p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05113v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05113v1">üìÑ Download PDF</a></p><hr><h3 id=highly-ionized-gas-in-lensed-z--6027-little-red-dot-seen-through-oiii-88Œºm-with-almahttpsarxivorgabs251205097v1><a href=https://arxiv.org/abs/2512.05097v1>Highly-ionized gas in lensed z = 6.027 Little Red Dot seen through [OIII] 88$Œº$m with ALMA</a><a hidden class=anchor aria-hidden=true href=#highly-ionized-gas-in-lensed-z--6027-little-red-dot-seen-through-oiii-88Œºm-with-almahttpsarxivorgabs251205097v1>#</a></h3><p><strong>Authors:</strong> Kirsten K. Knudsen, Johan Richard, Mathilde Jauzac, Tom J. L. C. Bakx, Thiago S. Goncalves, Eiichi Egami, Kiana Kade, Rahul Rana, Laura Sommovigo, Flora Stanley, Daniel P. Stark
<strong>Venue:</strong> arXiv (2025)</p><p>Determining the physical properties of galaxies during the first billion years after the big bang is key to understanding both early galaxy evolution and how galaxies contributed to the epoch of reionization. We present deep ALMA observations of the redshifted [OIII] 88um line for the gravitationally lensed ($Œº= 11.4\pm1.9$) galaxy A383-5.1 (z=6.027) that has previously been detected in [CII] 158um. Recent James Webb Space Telescope (JWST) imaging identified this sub-L* galaxy as a &lsquo;&lsquo;Little Red Dot&rsquo;&rsquo; (LRD). With a line luminosity of $L_{\rm [OIII]} = (1.29\pm0.24)\times10^8$ L$_\odot$ (corrected for lensing magnification) A383-5.1 is one of the faintest galaxies with combined [CII] and [OIII] detections. The ALMA data reveal no dust continuum emission, consistent with previous observations. The high line luminosity ratio of [OIII]/[CII] $\sim 14\pm5$ is consistent with A383-5.1 being low-metallicity and dust-poor. The non-detection of dust continuum in bands 6 and 8 is consistent with the high [OIII]/[CII] ratio and suggests a presence of a strong ultraviolet radiation field, which would be less affect by dust attenuation, implying that galaxies of this type could contribute significantly to the ionization of the intergalactic medium. The presence of strong ionizing field could provide an important piece of information for understanding the nature of LRDs and their role in cosmic reionization.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05097v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05097v1">üìÑ Download PDF</a></p><hr><h3 id=factuality-and-transparency-are-all-rag-needs-self-explaining-contrastive-evidence-re-rankinghttpsarxivorgabs251205012v1><a href=https://arxiv.org/abs/2512.05012v1>Factuality and Transparency Are All RAG Needs! Self-Explaining Contrastive Evidence Re-ranking</a><a hidden class=anchor aria-hidden=true href=#factuality-and-transparency-are-all-rag-needs-self-explaining-contrastive-evidence-re-rankinghttpsarxivorgabs251205012v1>#</a></h3><p><strong>Authors:</strong> Francielle Vargas, Daniel Pedronette
<strong>Venue:</strong> arXiv (2025)</p><p>This extended abstract introduces Self-Explaining Contrastive Evidence Re-Ranking (CER), a novel method that restructures retrieval around factual evidence by fine-tuning embeddings with contrastive learning and generating token-level attribution rationales for each retrieved passage. Hard negatives are automatically selected using a subjectivity-based criterion, forcing the model to pull factual rationales closer while pushing subjective or misleading explanations apart. As a result, the method creates an embedding space explicitly aligned with evidential reasoning. We evaluated our method on clinical trial reports, and initial experimental results show that CER improves retrieval accuracy, mitigates the potential for hallucinations in RAG systems, and provides transparent, evidence-based retrieval that enhances reliability, especially in safety-critical domains.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.05012v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.05012v1">üìÑ Download PDF</a></p><hr><h3 id=aligned-but-stereotypical-the-hidden-influence-of-system-prompts-on-social-bias-in-lvlm-based-text-to-image-modelshttpsarxivorgabs251204981v1><a href=https://arxiv.org/abs/2512.04981v1>Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models</a><a hidden class=anchor aria-hidden=true href=#aligned-but-stereotypical-the-hidden-influence-of-system-prompts-on-social-bias-in-lvlm-based-text-to-image-modelshttpsarxivorgabs251204981v1>#</a></h3><p><strong>Authors:</strong> NaHyeon Park, Namin An, Kunhee Kim, Soyeon Yoon, Jiahao Huo, Hyunjung Shim
<strong>Venue:</strong> arXiv (2025)</p><p>Large vision-language model (LVLM) based text-to-image (T2I) systems have become the dominant paradigm in image generation, yet whether they amplify social biases remains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 prompt benchmark spanning four levels of linguistic complexity and evaluate demographic bias across multiple attributes in a systematic manner. Our analysis identifies system prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis. To this end, we propose FairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-aware system prompts at test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show that FairPro substantially reduces demographic bias while preserving text-image alignment. We believe our findings provide deeper insight into the central role of system prompts in bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems.</p><p><a href="https://papersdb.wangyilin-0429.workers.dev/api/save?pid=2512.04981v1">üì• Save to Zotero</a> ¬†¬† <a href="https://papersdb.wangyilin-0429.workers.dev/api/pdf?pid=2512.04981v1">üìÑ Download PDF</a></p><hr></div><footer class=post-footer><ul class=post-tags></ul></footer><div><div class=pagination__title><span class=pagination__title-h style=font-size:20px>Comments</span><hr></div><div id=tcomment></div><script src=https://cdn.staticfile.org/twikoo/1.6.39/twikoo.all.min.js></script><script>twikoo.init({envId:"https://mangodb-theta.vercel.app/",el:"#tcomment",lang:"en-US",path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://garyforreal.me/zh/>Gary's House</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>Views: <span id=busuanzi_value_site_pv></span>
</span><span id=busuanzi_container_site_uv>Visitors: <span id=busuanzi_value_site_uv></span></span></div></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="Â§çÂà∂";function s(){t.innerHTML="Â∑≤Â§çÂà∂ÔºÅ",setTimeout(()=>{t.innerHTML="Â§çÂà∂"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>